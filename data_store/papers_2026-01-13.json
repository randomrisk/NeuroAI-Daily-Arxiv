{
  "Brain": {
    "2601.05893v1": {
      "title": "An outlook on extracellular waveforms produced by the three neuronal compartments",
      "url": "http://arxiv.org/abs/2601.05893v1",
      "authors": "J\u00e9r\u00e9mie Sibille, Kai Lun Teh, Alexandra Tzilivaki, Dietmar Schmitz, Paula T. Kuokkanen",
      "update_time": "2026-01-09",
      "abstract": "The brain is composed of billions of neurons with virtually endless morphologies and ion channel compositions, resulting in unique extracellular waveforms. Nevertheless, almost all neuronal morphologies can be reduced to a simple architecture made of three principal compartments: 1) the soma and nearby axonal hillock, 2) axonal projections ending in arbors or single synaptic contacts, and 3) dendrites. This review offers a perspective on how these three ubiquitous neuronal compartments can be identified and how they shape the extracellularly recorded waveforms, when spatial considerations are taken into account. This outlook utilizes biophysical modelling to complement existing experimental observations. Modeling has predicted a rich landscape of putative extracellular waveforms based on morphology, channel density, and sequential temporal activation. Recent advances in extracellular in vivo recording, combining low noise with high spatial density of recording sites, have improved the precision of extracellular waveform measurements, particularly in capturing waveforms beyond the classical somatic spikes, and in some cases, combinations originating from different compartments. This review aims to reorganize extracellular waveform heterogeneity by separating signals stemming from three neuronal compartments using three dimensions: amplitude, duration, and spatial extent or footprint. In doing so, we argue for a change of perspective, looking beyond somatic spikes to include spatiality and waveform combinations.",
      "code_url": null
    },
    "2601.05825v1": {
      "title": "Decoding Workload and Agreement From EEG During Spoken Dialogue With Conversational AI",
      "url": "http://arxiv.org/abs/2601.05825v1",
      "authors": "Lucija Mihi\u0107 Zidar, Philipp Wicke, Praneel Bhatia, Rosa Lutz, Marius Klug, Thorsten O. Zander",
      "update_time": "2026-01-09",
      "abstract": "Passive brain-computer interfaces offer a potential source of implicit feedback for alignment of large language models, but most mental state decoding has been done in controlled tasks. This paper investigates whether established EEG classifiers for mental workload and implicit agreement can be transferred to spoken human-AI dialogue. We introduce two conversational paradigms - a Spelling Bee task and a sentence completion task- and an end-to-end pipeline for transcribing, annotating, and aligning word-level conversational events with continuous EEG classifier output. In a pilot study, workload decoding showed interpretable trends during spoken interaction, supporting cross-paradigm transfer. For implicit agreement, we demonstrate continuous application and precise temporal alignment to conversational events, while identifying limitations related to construct transfer and asynchronous application of event-based classifiers. Overall, the results establish feasibility and constraints for integrating passive BCI signals into conversational AI systems.",
      "code_url": null
    },
    "2601.05789v1": {
      "title": "SAFE: Secure and Accurate Federated Learning for Privacy-Preserving Brain-Computer Interfaces",
      "url": "http://arxiv.org/abs/2601.05789v1",
      "authors": "Tianwang Jia, Xiaoqing Chen, Dongrui Wu",
      "update_time": "2026-01-09",
      "abstract": "Electroencephalogram (EEG)-based brain-computer interfaces (BCIs) are widely adopted due to their efficiency and portability; however, their decoding algorithms still face multiple challenges, including inadequate generalization, adversarial vulnerability, and privacy leakage. This paper proposes Secure and Accurate FEderated learning (SAFE), a federated learning-based approach that protects user privacy by keeping data local during model training. SAFE employs local batch-specific normalization to mitigate cross-subject feature distribution shifts and hence improves model generalization. It further enhances adversarial robustness by introducing perturbations in both the input space and the parameter space through federated adversarial training and adversarial weight perturbation. Experiments on five EEG datasets from motor imagery (MI) and event-related potential (ERP) BCI paradigms demonstrated that SAFE consistently outperformed 14 state-of-the-art approaches in both decoding accuracy and adversarial robustness, while ensuring privacy protection. Notably, it even outperformed centralized training approaches that do not consider privacy protection at all. To our knowledge, SAFE is the first algorithm to simultaneously achieve high decoding accuracy, strong adversarial robustness, and reliable privacy protection without using any calibration data from the target subject, making it highly desirable for real-world BCIs.",
      "code_url": null
    },
    "2601.05418v1": {
      "title": "Group-patch joint compression for highly accelerated MRI: compressing dynamic B0 and static RF spatial modulations across k-space subregion groups",
      "url": "http://arxiv.org/abs/2601.05418v1",
      "authors": "Rui Tian, Klaus Scheffler",
      "update_time": "2026-01-08",
      "abstract": "Purpose: To accelerate MRI further, rapid B0 field modulations can be applied during oversampled readout to capture additional physical information, e.g., Wave-CAIPI, FRONSAC, local B0 coils modulations. These methods, however, introduce additional non-Fourier-encoded dimension that cannot be resolved by FFT, posing significant reconstruction challenges particularly in compressed-sensing or neural-network frameworks.   Theory and methods: Because B0 modulations vary slowly relative to the oversampled ADC dwell time, we exploit this encoding redundancy by compressing k-space patch-by-patch across subregions, each of which is jointly encoded by a distinct subset of B0 and RF (receive) spatial encoding functions. For each patch, a compression matrix is computed once and reused to compress all patches encoded by the same B0/RF spatial modulations. This can be implemented by feeding subsets of B0 and RF spatial encoding maps into an adapted conventional RF array compression algorithm, mimicking an expanded set of virtual receiver channels. This approach was evaluated on ex-vivo/in-vivo human brain scans at 9.4T/3T.   Results: The proposed group-patch joint compression achieves substantially higher compression factors than conventional RF-only compression, while minimally compromising encoding efficiency. Typically, joint compression factors of 11x-20x led to negligible encoding loss, dramatically reducing reconstruction time and peak memory usage. For example, compressed-sensing reconstruction took 1.4-5.1s/2D slice, 177s-10.1min/3D volume, on a high-memory CPU node.   Conclusion: Given joint encoding of dynamic B0 and static RF fields, compressing multidimensional k-space patches in separate groups outperforms compressing RF receiver channels alone. This substantially mitigates a fundamental computational bottleneck that arises when combining rapid B0 and RF-receiver modulations.",
      "code_url": null
    },
    "2601.05391v1": {
      "title": "DynaSTy: A Framework for SpatioTemporal Node Attribute Prediction in Dynamic Graphs",
      "url": "http://arxiv.org/abs/2601.05391v1",
      "authors": "Namrata Banerji, Tanya Berger-Wolf",
      "update_time": "2026-01-08",
      "abstract": "Accurate multistep forecasting of node-level attributes on dynamic graphs is critical for applications ranging from financial trust networks to biological networks. Existing spatiotemporal graph neural networks typically assume a static adjacency matrix. In this work, we propose an end-to-end dynamic edge-biased spatiotemporal model that ingests a multi-dimensional timeseries of node attributes and a timeseries of adjacency matrices, to predict multiple future steps of node attributes. At each time step, our transformer-based model injects the given adjacency as an adaptable attention bias, allowing the model to focus on relevant neighbors as the graph evolves. We further deploy a masked node-time pretraining objective that primes the encoder to reconstruct missing features, and train with scheduled sampling and a horizon-weighted loss to mitigate compounding error over long horizons. Unlike prior work, our model accommodates dynamic graphs that vary across input samples, enabling forecasting in multi-system settings such as brain networks across different subjects, financial systems in different contexts, or evolving social systems. Empirical results demonstrate that our method consistently outperforms strong baselines on Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).",
      "code_url": null
    },
    "2601.05212v1": {
      "title": "FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching",
      "url": "http://arxiv.org/abs/2601.05212v1",
      "authors": "Danilo Danese, Angela Lombardi, Matteo Attimonelli, Giuseppe Fasano, Tommaso Di Noia",
      "update_time": "2026-01-08",
      "abstract": "Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.",
      "code_url": null
    },
    "2601.05084v1": {
      "title": "Driver-Intention Prediction with Deep Learning: Real-Time Brain-to-Vehicle Communication",
      "url": "http://arxiv.org/abs/2601.05084v1",
      "authors": "Niloufar Alavi, Swati Shah, Rezvan Alamian, Stefan Goetz",
      "update_time": "2026-01-08",
      "abstract": "Brain-computer interfaces (BCIs) allow direct communication between the brain and electronics without the need for speech or physical movement. Such interfaces can be particularly beneficial in applications requiring rapid response times, such as driving, where a vehicle's advanced driving assistance systems could benefit from immediate understanding of a driver's intentions. This study presents a novel method for predicting a driver's intention to steer using electroencephalography (EEG) signals through deep learning. A driving simulator created a controlled environment in which participants imagined controlling a vehicle during various driving scenarios, including left and right turns, as well as straight driving. A convolutional neural network (CNN) classified the detected EEG data with minimal pre-processing. Our model achieved an accuracy of 83.7% in distinguishing between the three steering intentions and demonstrated the ability of CNNs to process raw EEG data effectively. The classification accuracy was highest for right-turn segments, which suggests a potential spatial bias in brain activity. This study lays the foundation for more intuitive brain-to-vehicle communication systems.",
      "code_url": null
    },
    "2601.05021v1": {
      "title": "Geometric developmental principles for the emergence of brain-like weighted and directed neuronal networks",
      "url": "http://arxiv.org/abs/2601.05021v1",
      "authors": "Aitor Morales-Gregorio, Anno C. Kurth, Karol\u00edna Korvasov\u00e1",
      "update_time": "2026-01-08",
      "abstract": "Brain networks exhibit remarkable structural properties, including high local clustering, short path lengths, and heavy-tailed weight and degree distributions. While these features are thought to enable efficient information processing with minimal wiring costs, the fundamental principles that generate such complex network architectures across species remain unclear. Here, we analyse single-neuron resolution connectomes across five species (C. Elegans, Platynereis, Drosophila M., zebrafish and mouse) to investigate the fundamental wiring principles underlying brain network formation. We show that distance-dependent connectivity alone produces small-world networks, but fails to generate heavy-tailed distributions. By incorporating weight-preferential attachment, which arises from spatial clustering of synapses along neurites, we reproduce heavy-tailed weight distributions while maintaining small-world topology. Adding degree-preferential attachment, linked to the extent of dendritic and axonal arborization, enables the generation of heavy-tailed degree distributions. Through systematic parameter exploration, we demonstrate that the combination of distance dependence, weight-preferential attachment, and degree-preferential attachment is sufficient to reproduce all characteristic properties of empirical brain networks. Our results reveal that activity-independent geometric constraints during neural development can account for the conserved architectural principles observed across evolutionarily distant species, suggesting universal mechanisms governing neural circuit assembly.",
      "code_url": null
    },
    "2601.04926v1": {
      "title": "Entrainment of the suprachiasmatic nucleus network by a light-dark cycle",
      "url": "http://arxiv.org/abs/2601.04926v1",
      "authors": "Jinshan Xu, Changgui Gu, Alain Pumir, Nicolas Garnier, Zonghua Liu",
      "update_time": "2026-01-08",
      "abstract": "The synchronization of biological activity with the alternation of day and night (circadian rhythm) is performed in the brain by a group of neurons, constituting the suprachiasmatic nucleus (SCN). The SCN is divided into two subgroups of oscillating cells: the ventro-lateral (VL) neurons, which are exposed to light (photic signal) and the dorso-medial (DM) neurons which are coupled to the VL cells. When the coupling between these neurons is strong enough, the system synchronizes with the photic period. Upon increasing the cell coupling, the entrainment of the DM cells has been recently shown to occur via a very sharp (jumping) transition when the period of the photic input is larger than the intrinsic period of the cells. Here, we characterize this transition with a simple realistic model. We show that two bifurcations possibly lead to the disappearance of the endogenous mode. Using a mean field model, we show that the jumping transition results from a supercritical Hopf-like bifurcation. This finding implies that both the period and strength of the stimulating photic signal, and the relative fraction of cells in the VL and DM compartments are crucial in determining the synchronization of the system.",
      "code_url": null
    },
    "2601.04519v1": {
      "title": "TokenSeg: Efficient 3D Medical Image Segmentation via Hierarchical Visual Token Compression",
      "url": "http://arxiv.org/abs/2601.04519v1",
      "authors": "Sen Zeng, Hong Zhou, Zheng Zhu, Yang Liu",
      "update_time": "2026-01-08",
      "abstract": "Three-dimensional medical image segmentation is a fundamental yet computationally demanding task due to the cubic growth of voxel processing and the redundant computation on homogeneous regions. To address these limitations, we propose \\textbf{TokenSeg}, a boundary-aware sparse token representation framework for efficient 3D medical volume segmentation. Specifically, (1) we design a \\emph{multi-scale hierarchical encoder} that extracts 400 candidate tokens across four resolution levels to capture both global anatomical context and fine boundary details; (2) we introduce a \\emph{boundary-aware tokenizer} that combines VQ-VAE quantization with importance scoring to select 100 salient tokens, over 60\\% of which lie near tumor boundaries; and (3) we develop a \\emph{sparse-to-dense decoder} that reconstructs full-resolution masks through token reprojection, progressive upsampling, and skip connections. Extensive experiments on a 3D breast DCE-MRI dataset comprising 960 cases demonstrate that TokenSeg achieves state-of-the-art performance with 94.49\\% Dice and 89.61\\% IoU, while reducing GPU memory and inference latency by 64\\% and 68\\%, respectively. To verify the generalization capability, our evaluations on MSD cardiac and brain MRI benchmark datasets demonstrate that TokenSeg consistently delivers optimal performance across heterogeneous anatomical structures. These results highlight the effectiveness of anatomically informed sparse representation for accurate and efficient 3D medical image segmentation.",
      "code_url": null
    }
  },
  "EEG": {
    "2601.06016v1": {
      "title": "LookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection",
      "url": "http://arxiv.org/abs/2601.06016v1",
      "authors": "\u00de\u00f3r Sverrisson, Steinn Gu\u00f0mundsson",
      "update_time": "2026-01-09",
      "abstract": "Automated seizure detection from electroencephalography (EEG) remains difficult due to the large variability of seizure dynamics across patients, recording conditions, and clinical settings. We introduce LookAroundNet, a transformer-based seizure detector that uses a wider temporal window of EEG data to model seizure activity. The seizure detector incorporates EEG signals before and after the segment of interest, reflecting how clinicians use surrounding context when interpreting EEG recordings. We evaluate the proposed method on multiple EEG datasets spanning diverse clinical environments, patient populations, and recording modalities, including routine clinical EEG and long-term ambulatory recordings, in order to study performance across varying data distributions. The evaluation includes publicly available datasets as well as a large proprietary collection of home EEG recordings, providing complementary views of controlled clinical data and unconstrained home-monitoring conditions. Our results show that LookAroundNet achieves strong performance across datasets, generalizes well to previously unseen recording conditions, and operates with computational costs compatible with real-world clinical deployment. The results indicate that extended temporal context, increased training data diversity, and model ensembling are key factors for improving performance. This work contributes to moving automatic seizure detection models toward clinically viable solutions.",
      "code_url": null
    },
    "2601.05923v1": {
      "title": "Cedalion Tutorial: A Python-based framework for comprehensive analysis of multimodal fNIRS & DOT from the lab to the everyday world",
      "url": "http://arxiv.org/abs/2601.05923v1",
      "authors": "E. Middell, L. Carlton, S. Moradi, T. Codina, T. Fischer, J. Cutler, S. Kelley, J. Behrendt, T. Dissanayake, N. Harmening, M. A. Y\u00fccel, D. A. Boas, A. von L\u00fchmann",
      "update_time": "2026-01-09",
      "abstract": "Functional near-infrared spectroscopy (fNIRS) and diffuse optical tomography (DOT) are rapidly evolving toward wearable, multimodal, and data-driven, AI-supported neuroimaging in the everyday world. However, current analytical tools are fragmented across platforms, limiting reproducibility, interoperability, and integration with modern machine learning (ML) workflows. Cedalion is a Python-based open-source framework designed to unify advanced model-based and data-driven analysis of multimodal fNIRS and DOT data within a reproducible, extensible, and community-driven environment. Cedalion integrates forward modelling, photogrammetric optode co-registration, signal processing, GLM Analysis, DOT image reconstruction, and ML-based data-driven methods within a single standardized architecture based on the Python ecosystem. It adheres to SNIRF and BIDS standards, supports cloud-executable Jupyter notebooks, and provides containerized workflows for scalable, fully reproducible analysis pipelines that can be provided alongside original research publications. Cedalion connects established optical-neuroimaging pipelines with ML frameworks such as scikit-learn and PyTorch, enabling seamless multimodal fusion with EEG, MEG, and physiological data. It implements validated algorithms for signal-quality assessment, motion correction, GLM modelling, and DOT reconstruction, complemented by modules for simulation, data augmentation, and multimodal physiology analysis. Automated documentation links each method to its source publication, and continuous-integration testing ensures robustness. This tutorial paper provides seven fully executable notebooks that demonstrate core features. Cedalion offers an open, transparent, and community extensible foundation that supports reproducible, scalable, cloud- and ML-ready fNIRS/DOT workflows for laboratory-based and real-world neuroimaging.",
      "code_url": null
    },
    "2601.05825v1": {
      "title": "Decoding Workload and Agreement From EEG During Spoken Dialogue With Conversational AI",
      "url": "http://arxiv.org/abs/2601.05825v1",
      "authors": "Lucija Mihi\u0107 Zidar, Philipp Wicke, Praneel Bhatia, Rosa Lutz, Marius Klug, Thorsten O. Zander",
      "update_time": "2026-01-09",
      "abstract": "Passive brain-computer interfaces offer a potential source of implicit feedback for alignment of large language models, but most mental state decoding has been done in controlled tasks. This paper investigates whether established EEG classifiers for mental workload and implicit agreement can be transferred to spoken human-AI dialogue. We introduce two conversational paradigms - a Spelling Bee task and a sentence completion task- and an end-to-end pipeline for transcribing, annotating, and aligning word-level conversational events with continuous EEG classifier output. In a pilot study, workload decoding showed interpretable trends during spoken interaction, supporting cross-paradigm transfer. For implicit agreement, we demonstrate continuous application and precise temporal alignment to conversational events, while identifying limitations related to construct transfer and asynchronous application of event-based classifiers. Overall, the results establish feasibility and constraints for integrating passive BCI signals into conversational AI systems.",
      "code_url": null
    },
    "2601.05789v1": {
      "title": "SAFE: Secure and Accurate Federated Learning for Privacy-Preserving Brain-Computer Interfaces",
      "url": "http://arxiv.org/abs/2601.05789v1",
      "authors": "Tianwang Jia, Xiaoqing Chen, Dongrui Wu",
      "update_time": "2026-01-09",
      "abstract": "Electroencephalogram (EEG)-based brain-computer interfaces (BCIs) are widely adopted due to their efficiency and portability; however, their decoding algorithms still face multiple challenges, including inadequate generalization, adversarial vulnerability, and privacy leakage. This paper proposes Secure and Accurate FEderated learning (SAFE), a federated learning-based approach that protects user privacy by keeping data local during model training. SAFE employs local batch-specific normalization to mitigate cross-subject feature distribution shifts and hence improves model generalization. It further enhances adversarial robustness by introducing perturbations in both the input space and the parameter space through federated adversarial training and adversarial weight perturbation. Experiments on five EEG datasets from motor imagery (MI) and event-related potential (ERP) BCI paradigms demonstrated that SAFE consistently outperformed 14 state-of-the-art approaches in both decoding accuracy and adversarial robustness, while ensuring privacy protection. Notably, it even outperformed centralized training approaches that do not consider privacy protection at all. To our knowledge, SAFE is the first algorithm to simultaneously achieve high decoding accuracy, strong adversarial robustness, and reliable privacy protection without using any calibration data from the target subject, making it highly desirable for real-world BCIs.",
      "code_url": null
    },
    "2601.05450v1": {
      "title": "Feedback Effects on Cognitive Dynamics: Network-Based Insights from EEG Patterns and Behavioral Performance",
      "url": "http://arxiv.org/abs/2601.05450v1",
      "authors": "Behdokht Kiafar, Mohammad Fahim Abrar, Roghayeh Leila Barmaki",
      "update_time": "2026-01-09",
      "abstract": "This study examines the impact of feedback on Electroencephalography (EEG) activity and performance during the Reading the Mind in the Eyes Test. In a within-subject design, eleven participants completed the test under Feedback and No-Feedback conditions. Using the principles of Epistemic Network Analysis (ENA) and Ordered Network Analysis (ONA), we extend these network-based models to explore the link between neural dynamics and task outcomes. ENA results showed that feedback is associated with stronger connections between higher frequency EEG bands (Beta and Gamma) and correct responses, while the absence of feedback activated lower frequency bands (Theta and Alpha). ONA further disclosed directional shifts toward higher frequency activity preceding correct answers in the Feedback condition, whereas the No-Feedback condition showed more self-connections in lower bands and a higher occurrence of wrong answers, suggesting less effective reasoning strategies without feedback. Both ENA and ONA revealed statistically significant differences between conditions (p = 0.01, Cohen's d > 2). This study highlights the methodological benefits of integrating EEG with ENA and ONA for network analysis, capturing both temporal and relational dynamics, as well as the practical insight that feedback can foster more effective reasoning processes and improve task performance.",
      "code_url": null
    },
    "2601.05095v2": {
      "title": "Advanced Multimodal Learning for Seizure Detection and Prediction: Concept, Challenges, and Future Directions",
      "url": "http://arxiv.org/abs/2601.05095v2",
      "authors": "Ijaz Ahmad, Faizan Ahmad, Sunday Timothy Aboyeji, Yongtao Zhang, Peng Yang, Javed Ali Khan, Rab Nawaz, Baiying Lei",
      "update_time": "2026-01-09",
      "abstract": "Epilepsy is a chronic neurological disorder characterized by recurrent unprovoked seizures, affects over 50 million people worldwide, and poses significant risks, including sudden unexpected death in epilepsy (SUDEP). Conventional unimodal approaches, primarily reliant on electroencephalography (EEG), face several key challenges, including low SNR, nonstationarity, inter- and intrapatient heterogeneity, portability, and real-time applicability in clinical settings. To address these issues, a comprehensive survey highlights the concept of advanced multimodal learning for epileptic seizure detection and prediction (AMLSDP). The survey presents the evolution of epileptic seizure detection (ESD) and prediction (ESP) technologies across different eras. The survey also explores the core challenges of multimodal and non-EEG-based ESD and ESP. To overcome the key challenges of the multimodal system, the survey introduces the advanced processing strategies for efficient AMLSDP. Furthermore, this survey highlights future directions for researchers and practitioners. We believe this work will advance neurotechnology toward wearable and imaging-based solutions for epilepsy monitoring, serving as a valuable resource for future innovations in this domain.",
      "code_url": null
    },
    "2601.05084v1": {
      "title": "Driver-Intention Prediction with Deep Learning: Real-Time Brain-to-Vehicle Communication",
      "url": "http://arxiv.org/abs/2601.05084v1",
      "authors": "Niloufar Alavi, Swati Shah, Rezvan Alamian, Stefan Goetz",
      "update_time": "2026-01-08",
      "abstract": "Brain-computer interfaces (BCIs) allow direct communication between the brain and electronics without the need for speech or physical movement. Such interfaces can be particularly beneficial in applications requiring rapid response times, such as driving, where a vehicle's advanced driving assistance systems could benefit from immediate understanding of a driver's intentions. This study presents a novel method for predicting a driver's intention to steer using electroencephalography (EEG) signals through deep learning. A driving simulator created a controlled environment in which participants imagined controlling a vehicle during various driving scenarios, including left and right turns, as well as straight driving. A convolutional neural network (CNN) classified the detected EEG data with minimal pre-processing. Our model achieved an accuracy of 83.7% in distinguishing between the three steering intentions and demonstrated the ability of CNNs to process raw EEG data effectively. The classification accuracy was highest for right-turn segments, which suggests a potential spatial bias in brain activity. This study lays the foundation for more intuitive brain-to-vehicle communication systems.",
      "code_url": null
    },
    "2601.04286v1": {
      "title": "Enhancing Robustness of Asynchronous EEG-Based Movement Prediction using Classifier Ensembles",
      "url": "http://arxiv.org/abs/2601.04286v1",
      "authors": "Niklas Kueper, Kartik Chari, Elsa Andrea Kirchner",
      "update_time": "2026-01-07",
      "abstract": "Objective: Stroke is one of the leading causes of disabilities. One promising approach is to extend the rehabilitation with self-initiated robot-assisted movement therapy. To enable this, it is required to detect the patient's intention to move to trigger the assistance of a robotic device. This intention to move can be detected from human surface electroencephalography (EEG) signals; however, it is particularly challenging to decode when classifications are performed online and asynchronously. In this work, the effectiveness of classifier ensembles and a sliding-window postprocessing technique was investigated to enhance the robustness of such asynchronous classification. Approach: To investigate the effectiveness of classifier ensembles and a sliding-window postprocessing, two EEG datasets with 14 healthy subjects who performed self-initiated arm movements were analyzed. Offline and pseudo-online evaluations were conducted to compare ensemble combinations of the support vector machine (SVM), multilayer perceptron (MLP), and EEGNet classification models. Results: The results of the pseudo-online evaluation show that the two model ensembles significantly outperformed the best single model for the optimal number of postprocessing windows. In particular, for single models, an increased number of postprocessing windows significantly improved classification performances. Interestingly, we found no significant improvements between performances of the best single model and classifier ensembles in the offline evaluation. Significance: We demonstrated that classifier ensembles and appropriate postprocessing methods effectively enhance the asynchronous detection of movement intentions from EEG signals. In particular, the classifier ensemble approach yields greater improvements in online classification than in offline classification, and reduces false detections, i.e., early false positives.",
      "code_url": null
    },
    "2601.03478v1": {
      "title": "Emergent togetherness in collaborative dance improvisation: neural and motor synchronization reveal a coupling-decoupling paradox",
      "url": "http://arxiv.org/abs/2601.03478v1",
      "authors": "Yago Emanoel Ramos, Raphael Silva do Ros\u00e1rio, Adriana de Faria Gehres, Maria Jo\u00e3o Alves, Ana Maria Leit\u00e3o, Cec\u00edlia Bastos da Costa Accioly, Fatima Wachowicz, Ivani L\u00facia Oliveira de Santana, Jos\u00e9 Garcia Vivas Miranda",
      "update_time": "2026-01-07",
      "abstract": "Collective improvisation in dance provides a rich natural laboratory for studying emergent coordination in coupled neuro-motor systems. Here, we investigate how training shapes spontaneous synchronization patterns in both movement and brain signals during collaborative performance. Using a dual-recording protocol integrating 3D motion capture and hyperscanning EEG, participants engaged in free, interaction-driven, and rule-based improvisation before and after a program of generative dance, grounded in cellular-automata. Motor behavior was modeled through a time-resolved \u03b1-exponent derived from Movement Element Decomposition scaling between mean velocity and displacement, revealing fluctuations in energetic strategies and degrees of freedom. Synchronization events were quantified using Motif Synchronization (biomechanical data) and multilayer Time-Varying Graphs (neural data), enabling the detection of nontrivial lead-lag dependencies beyond zero-lag entrainment. Results indicate that training produced an intriguing dissociation: inter-brain synchronization increased, particularly within the frontal lobe, while interpersonal motor synchrony decreased. This opposite trend suggests that enhanced participatory sense-making fosters neural alignment while simultaneously expanding individual motor explorations, thereby reducing coupling in movement. Our findings position collaborative improvisation as a complex dynamical regime in which togetherness emerges not from identical motor outputs but from shared neural intentionality distributed across multilayer interaction networks, exemplifying the coupling-decoupling paradox, whereby increasing inter-brain synchrony supports the exploration of broader and mutually divergent motor trajectories. These results highlight the nonlinear nature of social coordination, offering new avenues for modeling creative joint action in human systems.",
      "code_url": null
    },
    "2601.03322v1": {
      "title": "HEEGNet: Hyperbolic Embeddings for EEG",
      "url": "http://arxiv.org/abs/2601.03322v1",
      "authors": "Shanglin Li, Shiwen Chu, Okan Ko\u00e7, Yi Ding, Qibin Zhao, Motoaki Kawanabe, Ziheng Chen",
      "update_time": "2026-01-06",
      "abstract": "Electroencephalography (EEG)-based brain-computer interfaces facilitate direct communication with a computer, enabling promising applications in human-computer interactions. However, their utility is currently limited because EEG decoding often suffers from poor generalization due to distribution shifts across domains (e.g., subjects). Learning robust representations that capture underlying task-relevant information would mitigate these shifts and improve generalization. One promising approach is to exploit the underlying hierarchical structure in EEG, as recent studies suggest that hierarchical cognitive processes, such as visual processing, can be encoded in EEG. While many decoding methods still rely on Euclidean embeddings, recent work has begun exploring hyperbolic geometry for EEG. Hyperbolic spaces, regarded as the continuous analogue of tree structures, provide a natural geometry for representing hierarchical data. In this study, we first empirically demonstrate that EEG data exhibit hyperbolicity and show that hyperbolic embeddings improve generalization. Motivated by these findings, we propose HEEGNet, a hybrid hyperbolic network architecture to capture the hierarchical structure in EEG and learn domain-invariant hyperbolic embeddings. To this end, HEEGNet combines both Euclidean and hyperbolic encoders and employs a novel coarse-to-fine domain adaptation strategy. Extensive experiments on multiple public EEG datasets, covering visual evoked potentials, emotion recognition, and intracranial EEG, demonstrate that HEEGNet achieves state-of-the-art performance.",
      "code_url": null
    }
  },
  "BCI": {
    "2601.05855v1": {
      "title": "Bidirectional Channel-selective Semantic Interaction for Semi-Supervised Medical Segmentation",
      "url": "http://arxiv.org/abs/2601.05855v1",
      "authors": "Kaiwen Huang, Yizhe Zhang, Yi Zhou, Tianyang Xu, Tao Zhou",
      "update_time": "2026-01-09",
      "abstract": "Semi-supervised medical image segmentation is an effective method for addressing scenarios with limited labeled data. Existing methods mainly rely on frameworks such as mean teacher and dual-stream consistency learning. These approaches often face issues like error accumulation and model structural complexity, while also neglecting the interaction between labeled and unlabeled data streams. To overcome these challenges, we propose a Bidirectional Channel-selective Semantic Interaction~(BCSI) framework for semi-supervised medical image segmentation. First, we propose a Semantic-Spatial Perturbation~(SSP) mechanism, which disturbs the data using two strong augmentation operations and leverages unsupervised learning with pseudo-labels from weak augmentations. Additionally, we employ consistency on the predictions from the two strong augmentations to further improve model stability and robustness. Second, to reduce noise during the interaction between labeled and unlabeled data, we propose a Channel-selective Router~(CR) component, which dynamically selects the most relevant channels for information exchange. This mechanism ensures that only highly relevant features are activated, minimizing unnecessary interference. Finally, the Bidirectional Channel-wise Interaction~(BCI) strategy is employed to supplement additional semantic information and enhance the representation of important channels. Experimental results on multiple benchmarking 3D medical datasets demonstrate that the proposed method outperforms existing semi-supervised approaches.",
      "code_url": null
    },
    "2601.05825v1": {
      "title": "Decoding Workload and Agreement From EEG During Spoken Dialogue With Conversational AI",
      "url": "http://arxiv.org/abs/2601.05825v1",
      "authors": "Lucija Mihi\u0107 Zidar, Philipp Wicke, Praneel Bhatia, Rosa Lutz, Marius Klug, Thorsten O. Zander",
      "update_time": "2026-01-09",
      "abstract": "Passive brain-computer interfaces offer a potential source of implicit feedback for alignment of large language models, but most mental state decoding has been done in controlled tasks. This paper investigates whether established EEG classifiers for mental workload and implicit agreement can be transferred to spoken human-AI dialogue. We introduce two conversational paradigms - a Spelling Bee task and a sentence completion task- and an end-to-end pipeline for transcribing, annotating, and aligning word-level conversational events with continuous EEG classifier output. In a pilot study, workload decoding showed interpretable trends during spoken interaction, supporting cross-paradigm transfer. For implicit agreement, we demonstrate continuous application and precise temporal alignment to conversational events, while identifying limitations related to construct transfer and asynchronous application of event-based classifiers. Overall, the results establish feasibility and constraints for integrating passive BCI signals into conversational AI systems.",
      "code_url": null
    },
    "2601.05789v1": {
      "title": "SAFE: Secure and Accurate Federated Learning for Privacy-Preserving Brain-Computer Interfaces",
      "url": "http://arxiv.org/abs/2601.05789v1",
      "authors": "Tianwang Jia, Xiaoqing Chen, Dongrui Wu",
      "update_time": "2026-01-09",
      "abstract": "Electroencephalogram (EEG)-based brain-computer interfaces (BCIs) are widely adopted due to their efficiency and portability; however, their decoding algorithms still face multiple challenges, including inadequate generalization, adversarial vulnerability, and privacy leakage. This paper proposes Secure and Accurate FEderated learning (SAFE), a federated learning-based approach that protects user privacy by keeping data local during model training. SAFE employs local batch-specific normalization to mitigate cross-subject feature distribution shifts and hence improves model generalization. It further enhances adversarial robustness by introducing perturbations in both the input space and the parameter space through federated adversarial training and adversarial weight perturbation. Experiments on five EEG datasets from motor imagery (MI) and event-related potential (ERP) BCI paradigms demonstrated that SAFE consistently outperformed 14 state-of-the-art approaches in both decoding accuracy and adversarial robustness, while ensuring privacy protection. Notably, it even outperformed centralized training approaches that do not consider privacy protection at all. To our knowledge, SAFE is the first algorithm to simultaneously achieve high decoding accuracy, strong adversarial robustness, and reliable privacy protection without using any calibration data from the target subject, making it highly desirable for real-world BCIs.",
      "code_url": null
    },
    "2601.05084v1": {
      "title": "Driver-Intention Prediction with Deep Learning: Real-Time Brain-to-Vehicle Communication",
      "url": "http://arxiv.org/abs/2601.05084v1",
      "authors": "Niloufar Alavi, Swati Shah, Rezvan Alamian, Stefan Goetz",
      "update_time": "2026-01-08",
      "abstract": "Brain-computer interfaces (BCIs) allow direct communication between the brain and electronics without the need for speech or physical movement. Such interfaces can be particularly beneficial in applications requiring rapid response times, such as driving, where a vehicle's advanced driving assistance systems could benefit from immediate understanding of a driver's intentions. This study presents a novel method for predicting a driver's intention to steer using electroencephalography (EEG) signals through deep learning. A driving simulator created a controlled environment in which participants imagined controlling a vehicle during various driving scenarios, including left and right turns, as well as straight driving. A convolutional neural network (CNN) classified the detected EEG data with minimal pre-processing. Our model achieved an accuracy of 83.7% in distinguishing between the three steering intentions and demonstrated the ability of CNNs to process raw EEG data effectively. The classification accuracy was highest for right-turn segments, which suggests a potential spatial bias in brain activity. This study lays the foundation for more intuitive brain-to-vehicle communication systems.",
      "code_url": null
    },
    "2601.01772v1": {
      "title": "EdgeSSVEP: A Fully Embedded SSVEP BCI Platform for Low-Power Real-Time Applications",
      "url": "http://arxiv.org/abs/2601.01772v1",
      "authors": "Manh-Dat Nguyen, Thomas Do, Nguyen Thanh Trung Le, Xuan-The Tran, Fred Chang, Chin-Teng Lin",
      "update_time": "2026-01-05",
      "abstract": "Brain-Computer Interfaces (BCIs) enable users to interact with machines directly via neural activity, yet their real-world deployment is often hindered by bulky and powerhungry hardware. We present EdgeSSVEP, a fully embedded microcontroller-based Steady-State Visually Evoked Potential (SSVEP) BCI platform that performs real-time EEG acquisition, zero-phase filtering, and on-device classification within a lowpower 240 MHz MCU operating at only 222 mW. The system incorporates an 8-channel EEG front end, supports 5-second stimulus durations, and executes the entire SSVEP decoding pipeline locally, eliminating dependence on PC-based processing. EdgeSSVEP was evaluated using six stimulus frequencies (7, 8, 9, 11, 7.5, and 8.5 Hz) with 10 participants. The device achieved 99.17% classification accuracy and 27.33 bits/min Information Transfer Rate (ITR), while consuming substantially less power than conventional desktop-based systems. The system integrates motion sensing to support artifact detection and improve robustness and signal stability in practical environments. For development and debugging, the system also provides optional TCP data streaming to external clients. Overall, EdgeSSVEP offers a scalable, energy-efficient, and secure embedded BCI platform suitable for assistive communication and neurofeedback applications, with potential extensions to accelerometer-based artifact mitigation and broader real-world deployments.",
      "code_url": null
    },
    "2601.01539v1": {
      "title": "Neural Digital Twins: Toward Next-Generation Brain-Computer Interfaces",
      "url": "http://arxiv.org/abs/2601.01539v1",
      "authors": "Mohammad Mahdi Habibi Bina, Sepideh Baghernezhad, Mohammad Reza Daliri, Mohammad Hassan Moradi",
      "update_time": "2026-01-04",
      "abstract": "Current neural interfaces such as brain-computer interfaces (BCIs) face several fundamental challenges, including frequent recalibration due to neuroplasticity and session-to-session variability, real-time processing latency, limited personalization and generalization across subjects, hardware constraints, surgical risks in invasive systems, and cognitive burden in patients with neurological impairments. These limitations significantly affect the accuracy, stability, and long-term usability of BCIs. This article introduces the concept of the Neural Digital Twin (NDT) as an advanced solution to overcome these barriers. NDT represents a dynamic, personalized computational model of the brain-BCI system that is continuously updated with real-time neural data, enabling prediction of brain states, optimization of control commands, and adaptive tuning of decoding algorithms. The design of NDT draws inspiration from the application of Digital Twin technology in advanced industries such as aerospace and autonomous vehicles, and leverages recent advances in artificial intelligence and neuroscience data acquisition technologies. In this work, we discuss the structure and implementation of NDT and explore its potential applications in next-generation BCIs and neural decoding, highlighting its ability to enhance precision, robustness, and individualized control in neurotechnology.",
      "code_url": null
    },
    "2601.00843v1": {
      "title": "OmniNeuro: A Multimodal HCI Framework for Explainable BCI Feedback via Generative AI and Sonification",
      "url": "http://arxiv.org/abs/2601.00843v1",
      "authors": "Ayda Aghaei Nia",
      "update_time": "2025-12-28",
      "abstract": "While Deep Learning has improved Brain-Computer Interface (BCI) decoding accuracy, clinical adoption is hindered by the \"Black Box\" nature of these algorithms, leading to user frustration and poor neuroplasticity outcomes. We propose OmniNeuro, a novel HCI framework that transforms the BCI from a silent decoder into a transparent feedback partner. OmniNeuro integrates three interpretability engines: (1) Physics (Energy), (2) Chaos (Fractal Complexity), and (3) Quantum-Inspired uncertainty modeling. These metrics drive real-time Neuro-Sonification and Generative AI Clinical Reports. Evaluated on the PhysioNet dataset ($N=109$), the system achieved a mean accuracy of 58.52%, with qualitative pilot studies ($N=3$) confirming that explainable feedback helps users regulate mental effort and reduces the \"trial-and-error\" phase. OmniNeuro is decoder-agnostic, acting as an essential interpretability layer for any state-of-the-art architecture.",
      "code_url": null
    },
    "2512.22785v1": {
      "title": "Nonlinear Dynamical Modeling of Human Intracranial Brain Activity with Flexible Inference",
      "url": "http://arxiv.org/abs/2512.22785v1",
      "authors": "Kiarash Vaziri, Lucine L. Oganesian, HyeongChan Jo, Roberto M. C. Vera, Charles Y. Liu, Brian Lee, Maryam M. Shanechi",
      "update_time": "2025-12-28",
      "abstract": "Dynamical modeling of multisite human intracranial neural recordings is essential for developing neurotechnologies such as brain-computer interfaces (BCIs). Linear dynamical models are widely used for this purpose due to their interpretability and their suitability for BCIs. In particular, these models enable flexible real-time inference, even in the presence of missing neural samples, which often occur in wireless BCIs. However, neural activity can exhibit nonlinear structure that is not captured by linear models. Furthermore, while recurrent neural network models can capture nonlinearity, their inference does not directly address handling missing observations. To address this gap, recent work introduced DFINE, a deep learning framework that integrates neural networks with linear state-space models to capture nonlinearities while enabling flexible inference. However, DFINE was developed for intracortical recordings that measure localized neuronal populations. Here we extend DFINE to modeling of multisite human intracranial electroencephalography (iEEG) recordings. We find that DFINE significantly outperforms linear state-space models (LSSMs) in forecasting future neural activity. Furthermore, DFINE matches or exceeds the accuracy of a gated recurrent unit (GRU) model in neural forecasting, indicating that a linear dynamical backbone, when paired and jointly trained with nonlinear neural networks, can effectively describe the dynamics of iEEG signals while also enabling flexible inference. Additionally, DFINE handles missing observations more robustly than the baselines, demonstrating its flexible inference and utility for BCIs. Finally, DFINE's advantage over LSSM is more pronounced in high gamma spectral bands. Taken together, these findings highlight DFINE as a strong and flexible framework for modeling human iEEG dynamics, with potential applications in next-generation BCIs.",
      "code_url": null
    },
    "2601.00020v2": {
      "title": "Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing",
      "url": "http://arxiv.org/abs/2601.00020v2",
      "authors": "Nikhil Garg, Anxiong Song, Niklas Plessnig, Nathan Savoia, Laura B\u00e9gon-Lours",
      "update_time": "2026-01-05",
      "abstract": "Electroencephalography (EEG)-based brain-computer interfaces (BCIs) are strongly affected by non-stationary neural signals that vary across sessions and individuals, limiting the generalization of subject-agnostic models and motivating adaptive and personalized learning on resource-constrained platforms. Programmable memristive hardware offers a promising substrate for such post-deployment adaptation; however, practical realization is challenged by limited weight resolution, device variability, nonlinear programming dynamics, and finite device endurance. In this work, we show that spiking neural networks (SNNs) can be deployed on ferroelectric memristive synaptic devices for adaptive EEG-based motor imagery decoding under realistic device constraints. We fabricate, characterize, and model ferroelectric synapses. We evaluate a convolutional-recurrent SNN architecture under two complementary deployment strategies: (i) device-aware training using a ferroelectric synapse model, and (ii) transfer of software-trained weights followed by low-overhead on-device re-tuning. To enable efficient adaptation, we introduce a device-aware weight-update strategy in which gradient-based updates are accumulated digitally and converted into discrete programming events only when a threshold is exceeded, emulating nonlinear, state-dependent programming dynamics while reducing programming frequency. Both deployment strategies achieve classification performance comparable to state-of-the-art software-based SNNs. Furthermore, subject-specific transfer learning achieved by retraining only the final network layers improves classification accuracy. These results demonstrate that programmable ferroelectric hardware can support robust, low-overhead adaptation in spiking neural networks, opening a practical path toward personalized neuromorphic processing of neural signals.",
      "code_url": null
    },
    "2512.17775v1": {
      "title": "How Light Shapes Memory: Beta Synchrony in the Temporal-Parietal Cortex Predicts Cognitive Ergonomics for BCI Applications",
      "url": "http://arxiv.org/abs/2512.17775v1",
      "authors": "Jiajia Li, Tian Guo, Fan Li, Huichao Ding, Guozheng Xu, Jian Song",
      "update_time": "2025-12-19",
      "abstract": "Working memory is a promising paradigm for assessing cognitive ergonomics of brain states in brain-computer interfaces(BCIs). This study decodes these states with a focus on environmental illumination effects via two distinct working memory tasks(Recall and Sequence) for mixed-recognition analysis. Leveraging nonlinear patterns in brain connectivity, we propose an innovative framework: multi-regional dynamic interplay patterns based on beta phase synchrony dynamics, to identify low-dimensional EEG regions (prefrontal, temporal, parietal) for state recognition. Based on nonlinear phase map analysis of the above three brain regions using beta-phase connectivity, we found that: (1)Temporal-parietal phase clustering outperforms other regional combinations in distinguishing memory states; (2)Illumination-enhanced environments optimize temporoparietal balance;(3) Machine learning confirms temporal-parietal synchrony as the dominant cross-task classification feature. These results provide a precise prediction algorithm, facilitating a low-dimensional system using temporal and parietal EEG channels with practical value for real-time cognitive ergonomics assessment in BCIs and optimized human-machine interaction.",
      "code_url": null
    }
  },
  "fMRI": {
    "2601.03796v1": {
      "title": "Data-driven inference of brain dynamical states from the r-spectrum of correlation matrices",
      "url": "http://arxiv.org/abs/2601.03796v1",
      "authors": "Christopher Gabaldon, Adria Mulero, Rong Wang, Daniel A. Martin, Sabrina Camargo, Qian-Yuan Tang, Ignacio Cifre, Changsong Zhou, Dante R. Chialvo",
      "update_time": "2026-01-07",
      "abstract": "We present a data-driven framework to characterize large-scale brain dynamical states directly from correlation matrices at the single-subject level. By treating correlation thresholding as a percolation-like probe of connectivity, the approach tracks multiple cluster- and network-level observables and identifies a characteristic percolation threshold, rc, at which these signatures converge. We use $r_c$ as an operational and physically interpretable descriptor of large-scale brain dynamical state. Applied to resting-state fMRI data from a large cohort of healthy individuals (N = 996), the method yields stable, subject-specific estimates that covary systematically with established dynamical indicators such as temporal autocorrelations. Numerical simulations of a whole-brain model with a known critical regime further show that $r_c$ tracks changes in collective dynamics under controlled variations of excitability. By replacing arbitrary threshold selection with a criterion intrinsic to correlation structure, the r-spectra provides a physically grounded approach for comparing brain dynamical states across individuals.",
      "code_url": null
    },
    "2601.02143v1": {
      "title": "Responses of the Neurobiological Craving Signature to smoking versus alternative social rewards predict craving and monthly smoking in adolescents",
      "url": "http://arxiv.org/abs/2601.02143v1",
      "authors": "Maddalena Tamellini, Joyce Dieleman, Guillaume Sescousse, Maartje Luijten, Leonie Koban",
      "update_time": "2026-01-05",
      "abstract": "Smoking remains the leading cause of preventable mortality worldwide. Adolescents are particularly vulnerable to the development of tobacco addiction due to ongoing brain maturation and susceptibility to social influences, such as exposure to environmental tobacco smoke (ETS). Craving -the strong desire to use drugs -already emerges with non-daily tobacco use and predicts continued use and relapse. However, the roles of craving and ETS exposure during the early stages of tobacco use in adolescence remain poorly understood. In this pre-registered study, we harness a recently developed fMRI marker of craving -the Neurobiological Craving Signature (NCS) -to compare craving-related brain responses to smoking versus social cues in adolescent Experimental Smokers (N=100) and Non-smokers (N=48) with varying levels of ETS exposure levels. Results showed that NCS responses to smoking cues compared to alternative social rewards were higher in Experimental Smokers compared to Non-smokers and predicted individual differences in self-reported craving and monthly smoking. Both smoking behavior and NCS responses were correlated with the relative amount of ETS exposure from peers compared to exposure from family members. Together, these findings indicate a heightened sensitivity of craving-related brain circuits already during experimental smoking and highlight the important role of peer social norms on craving and smoking initiation in the critical period of adolescence.",
      "code_url": null
    },
    "2601.02008v1": {
      "title": "XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging",
      "url": "http://arxiv.org/abs/2601.02008v1",
      "authors": "Midhat Urooj, Ayan Banerjee, Sandeep Gupta",
      "update_time": "2026-01-05",
      "abstract": "Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.",
      "code_url": null
    },
    "2601.01339v1": {
      "title": "Achieving Fine-grained Cross-modal Understanding through Brain-inspired Hierarchical Representation Learning",
      "url": "http://arxiv.org/abs/2601.01339v1",
      "authors": "Weihang You, Hanqi Jiang, Yi Pan, Junhao Chen, Tianming Liu, Fei Dou",
      "update_time": "2026-01-04",
      "abstract": "Understanding neural responses to visual stimuli remains challenging due to the inherent complexity of brain representations and the modality gap between neural data and visual inputs. Existing methods, mainly based on reducing neural decoding to generation tasks or simple correlations, fail to reflect the hierarchical and temporal processes of visual processing in the brain. To address these limitations, we present NeuroAlign, a novel framework for fine-grained fMRI-video alignment inspired by the hierarchical organization of the human visual system. Our framework implements a two-stage mechanism that mirrors biological visual pathways: global semantic understanding through Neural-Temporal Contrastive Learning (NTCL) and fine-grained pattern matching through enhanced vector quantization. NTCL explicitly models temporal dynamics through bidirectional prediction between modalities, while our DynaSyncMM-EMA approach enables dynamic multi-modal fusion with adaptive weighting. Experiments demonstrate that NeuroAlign significantly outperforms existing methods in cross-modal retrieval tasks, establishing a new paradigm for understanding visual cognitive mechanisms.",
      "code_url": null
    },
    "2601.01229v1": {
      "title": "NeuroSSM: Multiscale Differential State-Space Modeling for Context-Aware fMRI Analysis",
      "url": "http://arxiv.org/abs/2601.01229v1",
      "authors": "Furkan Gen\u00e7, Boran \u0130smet Macun, Sait Sarper \u00d6zaslan, Emine U. Saritas, Tolga \u00c7ukur",
      "update_time": "2026-01-03",
      "abstract": "Accurate fMRI analysis requires sensitivity to temporal structure across multiple scales, as BOLD signals encode cognitive processes that emerge from fast transient dynamics to slower, large-scale fluctuations. Existing deep learning (DL) approaches to temporal modeling face challenges in jointly capturing these dynamics over long fMRI time series. Among current DL models, transformers address long-range dependencies by explicitly modeling pairwise interactions through attention, but the associated quadratic computational cost limits effective integration of temporal dependencies across long fMRI sequences. Selective state-space models (SSMs) instead model long-range temporal dependencies implicitly through latent state evolution in a dynamical system, enabling efficient propagation of dependencies over time. However, recent SSM-based approaches for fMRI commonly operate on derived functional connectivity representations and employ single-scale temporal processing. These design choices constrain the ability to jointly represent fast transient dynamics and slower global trends within a single model. We propose NeuroSSM, a selective state-space architecture designed for end-to-end analysis of raw BOLD signals in fMRI time series. NeuroSSM addresses the above limitations through two complementary design components: a multiscale state-space backbone that captures fast and slow dynamics concurrently, and a parallel differencing branch that increases sensitivity to transient state changes. Experiments on clinical and non-clinical datasets demonstrate that NeuroSSM achieves competitive performance and efficiency against state-of-the-art fMRI analysis methods.",
      "code_url": null
    },
    "2601.00973v1": {
      "title": "Learned Hemodynamic Coupling Inference in Resting-State Functional MRI",
      "url": "http://arxiv.org/abs/2601.00973v1",
      "authors": "William Consagra, Eardi Lila",
      "update_time": "2026-01-02",
      "abstract": "Functional magnetic resonance imaging (fMRI) provides an indirect measurement of neuronal activity via hemodynamic responses that vary across brain regions and individuals. Ignoring this hemodynamic variability can bias downstream connectivity estimates. Furthermore, the hemodynamic parameters themselves may serve as important imaging biomarkers. Estimating spatially varying hemodynamics from resting-state fMRI (rsfMRI) is therefore an important but challenging blind inverse problem, since both the latent neural activity and the hemodynamic coupling are unknown. In this work, we propose a methodology for inferring hemodynamic coupling on the cortical surface from rsfMRI. Our approach avoids the highly unstable joint recovery of neural activity and hemodynamics by marginalizing out the latent neural signal and basing inference on the resulting marginal likelihood. To enable scalable, high-resolution estimation, we employ a deep neural network combined with conditional normalizing flows to accurately approximate this intractable marginal likelihood, while enforcing spatial coherence through priors defined on the cortical surface that admit sparse representations. The proposed approach is extensively validated using synthetic data and real fMRI datasets, demonstrating clear improvements over current methods for hemodynamic estimation and downstream connectivity analysis.",
      "code_url": null
    },
    "2601.00284v1": {
      "title": "Deep learning estimation of the spectral density of functional time series on large domains",
      "url": "http://arxiv.org/abs/2601.00284v1",
      "authors": "Neda Mohammadi, Soham Sarkar, Piotr Kokoszka",
      "update_time": "2026-01-01",
      "abstract": "We derive an estimator of the spectral density of a functional time series that is the output of a multilayer perceptron neural network. The estimator is motivated by difficulties with the computation of existing spectral density estimators for time series of functions defined on very large grids that arise, for example, in climate compute models and medical scans. Existing estimators use autocovariance kernels represented as large $G \\times G$ matrices, where $G$ is the number of grid points on which the functions are evaluated. In many recent applications, functions are defined on 2D and 3D domains, and $G$ can be of the order $G \\sim 10^5$, making the evaluation of the autocovariance kernels computationally intensive or even impossible. We use the theory of spectral functional principal components to derive our deep learning estimator and prove that it is a universal approximator to the spectral density under general assumptions. Our estimator can be trained without computing the autocovariance kernels and it can be parallelized to provide the estimates much faster than existing approaches. We validate its performance by simulations and an application to fMRI images.",
      "code_url": null
    },
    "2601.00904v1": {
      "title": "Deep Deterministic Nonlinear ICA via Total Correlation Minimization with Matrix-Based Entropy Functional",
      "url": "http://arxiv.org/abs/2601.00904v1",
      "authors": "Qiang Li, Shujian Yu, Liang Ma, Chen Ma, Jingyu Liu, Tulay Adali, Vince D. Calhoun",
      "update_time": "2025-12-31",
      "abstract": "Blind source separation, particularly through independent component analysis (ICA), is widely utilized across various signal processing domains for disentangling underlying components from observed mixed signals, owing to its fully data-driven nature that minimizes reliance on prior assumptions. However, conventional ICA methods rely on an assumption of linear mixing, limiting their ability to capture complex nonlinear relationships and to maintain robustness in noisy environments. In this work, we present deep deterministic nonlinear independent component analysis (DDICA), a novel deep neural network-based framework designed to address these limitations. DDICA leverages a matrix-based entropy function to directly optimize the independence criterion via stochastic gradient descent, bypassing the need for variational approximations or adversarial schemes. This results in a streamlined training process and improved resilience to noise. We validated the effectiveness and generalizability of DDICA across a range of applications, including simulated signal mixtures, hyperspectral image unmixing, modeling of primary visual receptive fields, and resting-state functional magnetic resonance imaging (fMRI) data analysis. Experimental results demonstrate that DDICA effectively separates independent components with high accuracy across a range of applications. These findings suggest that DDICA offers a robust and versatile solution for blind source separation in diverse signal processing tasks.",
      "code_url": null
    },
    "2512.24901v1": {
      "title": "Spectral Graph Neural Networks for Cognitive Task Classification in fMRI Connectomes",
      "url": "http://arxiv.org/abs/2512.24901v1",
      "authors": "Debasis Maji, Arghya Banerjee, Debaditya Barman",
      "update_time": "2025-12-31",
      "abstract": "Cognitive task classification using machine learning plays a central role in decoding brain states from neuroimaging data. By integrating machine learning with brain network analysis, complex connectivity patterns can be extracted from functional magnetic resonance imaging connectomes. This process transforms raw blood-oxygen-level-dependent (BOLD) signals into interpretable representations of cognitive processes. Graph neural networks (GNNs) further advance this paradigm by modeling brain regions as nodes and functional connections as edges, capturing topological dependencies and multi-scale interactions that are often missed by conventional approaches. Our proposed SpectralBrainGNN model, a spectral convolution framework based on graph Fourier transforms (GFT) computed via normalized Laplacian eigendecomposition. Experiments on the Human Connectome Project-Task (HCPTask) dataset demonstrate the effectiveness of the proposed approach, achieving a classification accuracy of 96.25\\%. The implementation is publicly available at https://github.com/gnnplayground/SpectralBrainGNN to support reproducibility and future research.",
      "code_url": null
    },
    "2512.23137v1": {
      "title": "Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use",
      "url": "http://arxiv.org/abs/2512.23137v1",
      "authors": "Runzhi Zhou, Xi Luo",
      "update_time": "2025-12-29",
      "abstract": "Integrating non-Euclidean brain imaging data with Euclidean tabular data, such as clinical and demographic information, poses a substantial challenge for medical imaging analysis, particularly in forecasting future outcomes. While machine learning and deep learning techniques have been applied successfully to cross-sectional classification and prediction tasks, effectively forecasting outcomes in longitudinal imaging studies remains challenging. To address this challenge, we introduce a time-aware graph neural network model with transformer fusion (GNN-TF). This model flexibly integrates both tabular data and dynamic brain connectivity data, leveraging the temporal order of these variables within a coherent framework. By incorporating non-Euclidean and Euclidean sources of information from a longitudinal resting-state fMRI dataset from the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA), the GNN-TF enables a comprehensive analysis that captures critical aspects of longitudinal imaging data. Comparative analyses against a variety of established machine learning and deep learning models demonstrate that GNN-TF outperforms these state-of-the-art methods, delivering superior predictive accuracy for predicting future tobacco usage. The end-to-end, time-aware transformer fusion structure of the proposed GNN-TF model successfully integrates multiple data modalities and leverages temporal dynamics, making it a valuable analytic tool for functional brain imaging studies focused on clinical outcome prediction.",
      "code_url": null
    }
  },
  "MEG": {
    "2601.05923v1": {
      "title": "Cedalion Tutorial: A Python-based framework for comprehensive analysis of multimodal fNIRS & DOT from the lab to the everyday world",
      "url": "http://arxiv.org/abs/2601.05923v1",
      "authors": "E. Middell, L. Carlton, S. Moradi, T. Codina, T. Fischer, J. Cutler, S. Kelley, J. Behrendt, T. Dissanayake, N. Harmening, M. A. Y\u00fccel, D. A. Boas, A. von L\u00fchmann",
      "update_time": "2026-01-09",
      "abstract": "Functional near-infrared spectroscopy (fNIRS) and diffuse optical tomography (DOT) are rapidly evolving toward wearable, multimodal, and data-driven, AI-supported neuroimaging in the everyday world. However, current analytical tools are fragmented across platforms, limiting reproducibility, interoperability, and integration with modern machine learning (ML) workflows. Cedalion is a Python-based open-source framework designed to unify advanced model-based and data-driven analysis of multimodal fNIRS and DOT data within a reproducible, extensible, and community-driven environment. Cedalion integrates forward modelling, photogrammetric optode co-registration, signal processing, GLM Analysis, DOT image reconstruction, and ML-based data-driven methods within a single standardized architecture based on the Python ecosystem. It adheres to SNIRF and BIDS standards, supports cloud-executable Jupyter notebooks, and provides containerized workflows for scalable, fully reproducible analysis pipelines that can be provided alongside original research publications. Cedalion connects established optical-neuroimaging pipelines with ML frameworks such as scikit-learn and PyTorch, enabling seamless multimodal fusion with EEG, MEG, and physiological data. It implements validated algorithms for signal-quality assessment, motion correction, GLM modelling, and DOT reconstruction, complemented by modules for simulation, data augmentation, and multimodal physiology analysis. Automated documentation links each method to its source publication, and continuous-integration testing ensures robustness. This tutorial paper provides seven fully executable notebooks that demonstrate core features. Cedalion offers an open, transparent, and community extensible foundation that supports reproducible, scalable, cloud- and ML-ready fNIRS/DOT workflows for laboratory-based and real-world neuroimaging.",
      "code_url": null
    },
    "2601.00723v1": {
      "title": "Nematic-fluctuation-mediated superconductivity in CuxTiSe2",
      "url": "http://arxiv.org/abs/2601.00723v1",
      "authors": "Xingyu Lv, Yang Fu, Shangjie Tian, Ying Ma, Shouguo Wang, Cedomir Petrovic, Xiao Zhang, Hechang Lei",
      "update_time": "2026-01-02",
      "abstract": "The interplay among electronic nematicity, charge density wave, and superconductivity in correlated electronic systems has induced extensive research interest. Here, we discover the existence of nematic fluctuations in TiSe2 single crystal and investigate its evolution with Cu intercalation. It is observed that the elastoresistivity coefficient mEg exhibits a divergent temperature dependence following a Curie-Weiss law at high temperature. Upon Cu intercalation, the characteristic temperature T* of nematic fluctuation is progressively suppressed and becomes near zero when the superconductivity is optimized. Further intercalation of Cu leads to the sign change of T* and the suppression of superconductivity. These results strongly indicate that nematic phase transition may play a vital role in enhancing superconductivity in CuxTiSe2. Therefore, CuxTiSe2 provides a unique material platform to explore the nematic-fluctuation-mediated superconductivity.",
      "code_url": null
    },
    "2512.19482v1": {
      "title": "Transformer-Based Approach to Enhance Positron Tracking Performance in MEG II",
      "url": "http://arxiv.org/abs/2512.19482v1",
      "authors": "Lapo Dispoto, Fedor Ignatov, Atsushi Oya, Yusuke Uchiyama, Antoine Venturini",
      "update_time": "2025-12-22",
      "abstract": "We developed a Transformer-based pattern recognition method for positron track reconstruction in the MEG II experiment. The model acts as a classifier to remove pileup hits in the MEG II drift chamber, which operates under a high pileup occupancy of 35 - 50 %. The trained model significantly improved hit purity, leading to enhancements in tracking efficiency and resolution by 15 % and 5 %, respectively, at a muon stopping rate of $5\\times 10^7 \u03bc$/sec. This improvement translates into an approximately 10 % increase in the sensitivity of the $\u03bc\\to e\u03b3$ branching ratio measurement.",
      "code_url": null
    },
    "2512.19399v1": {
      "title": "Brain-Grounded Axes for Reading and Steering LLM States",
      "url": "http://arxiv.org/abs/2512.19399v1",
      "authors": "Sandro Andric",
      "update_time": "2025-12-22",
      "abstract": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
      "code_url": null
    },
    "2512.17978v1": {
      "title": "MEGState: Phoneme Decoding from Magnetoencephalography Signals",
      "url": "http://arxiv.org/abs/2512.17978v1",
      "authors": "Shuntaro Suzuki, Chia-Chun Dan Hsu, Yu Tsao, Komei Sugiura",
      "update_time": "2025-12-19",
      "abstract": "Decoding linguistically meaningful representations from non-invasive neural recordings remains a central challenge in neural speech decoding. Among available neuroimaging modalities, magnetoencephalography (MEG) provides a safe and repeatable means of mapping speech-related cortical dynamics, yet its low signal-to-noise ratio and high temporal dimensionality continue to hinder robust decoding. In this work, we introduce MEGState, a novel architecture for phoneme decoding from MEG signals that captures fine-grained cortical responses evoked by auditory stimuli. Extensive experiments on the LibriBrain dataset demonstrate that MEGState consistently surpasses baseline model across multiple evaluation metrics. These findings highlight the potential of MEG-based phoneme decoding as a scalable pathway toward non-invasive brain-computer interfaces for speech.",
      "code_url": null
    },
    "2512.14395v3": {
      "title": "Massive Editing for Large Language Models Based on Dynamic Weight Generation",
      "url": "http://arxiv.org/abs/2512.14395v3",
      "authors": "Wentao Wan, Qiqing Lao, Zhiwei Xie, Hefeng Wu, Runnan Lin, Liang Lin, Keze Wang",
      "update_time": "2026-01-06",
      "abstract": "Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.",
      "code_url": null
    },
    "2512.10791v1": {
      "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
      "url": "http://arxiv.org/abs/2512.10791v1",
      "authors": "Aileen Cheng, Alon Jacovi, Amir Globerson, Ben Golan, Charles Kwong, Chris Alberti, Connie Tao, Eyal Ben-David, Gaurav Singh Tomar, Lukas Haas, Yonatan Bitton, Adam Bloniarz, Aijun Bai, Andrew Wang, Anfal Siddiqui, Arturo Bajuelos Castillo, Aviel Atias, Chang Liu, Corey Fry, Daniel Balle, Deepanway Ghosal, Doron Kukliansky, Dror Marcus, Elena Gribovskaya, Eran Ofek, Honglei Zhuang, Itay Laish, Jan Ackermann, Lily Wang, Meg Risdal, Megan Barnes, Michael Fink, Mohamed Amin, Moran Ambar, Natan Potikha, Nikita Gupta, Nitzan Katz, Noam Velan, Ofir Roval, Ori Ram, Polina Zablotskaia, Prathamesh Bang, Priyanka Agrawal, Rakesh Ghiya, Sanjay Ganapathy, Simon Baumgartner, Sofia Erell, Sushant Prakash, Thibault Sellam, Vikram Rao, Xuanhui Wang, Yaroslav Akulov, Yulong Yang, Zhen Yang, Zhixin Lai, Zhongru Wu, Anca Dragan, Avinatan Hassidim, Fernando Pereira, Slav Petrov, Srinivasan Venkatachary, Tulsee Doshi, Yossi Matias, Sasha Goldshtein, Dipanjan Das",
      "update_time": "2025-12-11",
      "abstract": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .",
      "code_url": null
    },
    "2512.09063v1": {
      "title": "A novel two loop inverse seesaw model",
      "url": "http://arxiv.org/abs/2512.09063v1",
      "authors": "Gonzalo Ben\u00edtez-Irarr\u00e1zabal, Roc\u00edo Branada Balbont\u00edn, Cesar Bonilla, A. E. C\u00e1rcamo Hern\u00e1ndez, Sergey Kovalenko, Juan Marchant Gonz\u00e1lez",
      "update_time": "2025-12-09",
      "abstract": "We propose a Standard Model (SM) extension where neutrinos get masses through a two-loop inverse seesaw mechanism. This naturally explains the smallness of the neutrino masses and allows seesaw mediators to be at the TeV scale with testable phenomenology. The model adds two real singlet scalars and four electrically neutral leptons to the SM. The extension considers the existence of two global Abelian symmetries, a continuous $U(1)$ and a discrete $Z_3$. The latter, remains unbroken after spontaneous symmetry breaking and forbids tree-level and one-loop neutrino masses, and stabilizes the dark matter (DM) candidates. This setup accommodates neutrino-oscillation data, yields two pseudo-Dirac heavy pairs with small active-sterile mixing, and predicts an effective Majorana mass $m_{ee}$ in the $2.1$-$4.4$ meV range for normal ordering. Charged-lepton flavor violation is naturally suppressed yet testable: for a representative benchmark we obtain BR$(\u03bc\\to e \u03b3)\\simeq 1.6 \\times 10^{-14}$, with correlated signals in $\u03bc\\to eee$ and $\u03bc$-$e$ conversion within next-generation experimental reach. Altogether, the radiative origin of neutrino masses links low-energy flavor observables to collider signatures, delineating discovery targets for MEG II, Mu2e/COMET, and the HL-LHC and distinguishing this framework from conventional inverse- and radiative-seesaw models. Moreover, the $Z_3$ guarantees a stable DM candidate, either scalar ($\u03c1$) or fermionic ($\u03a9$). Then, here we analyze and identify the viable parameter space that is consistent with the observed DM relic abundance for both situations.",
      "code_url": null
    },
    "2512.10982v1": {
      "title": "Rosetta Stone of Neural Mass Models",
      "url": "http://arxiv.org/abs/2512.10982v1",
      "authors": "Francesca Castaldo, Raul de Palma Aristides, Pau Clusella, Jordi Garcia-Ojalvo, Giulio Ruffini",
      "update_time": "2025-12-04",
      "abstract": "Brain dynamics dominate every level of neural organization -- from single-neuron spiking to the macroscopic waves captured by fMRI, MEG, and EEG -- yet the mathematical tools used to interrogate those dynamics remain scattered across a patchwork of traditions. Neural mass models (NMMs) (aggregate neural models) provide one of the most popular gateways into this landscape, but their sheer variety -- spanning lumped parameter models, firing-rate equations, and multi-layer generators -- demands a unifying framework that situates diverse architectures along a continuum of abstraction and biological detail. Here, we start from the idea that oscillations originate from a simple push-pull interaction between two or more neural populations. We build from the undamped harmonic oscillator and, guided by a simple push-pull motif between excitatory and inhibitory populations, climb a systematic ladder of detail. Each rung is presented first in isolation, next under forcing, and then within a coupled network, reflecting the progression from single-node to whole-brain modeling. By transforming a repertoire of disparate formalisms into a navigable ladder, we hope to turn NMM choice from a subjective act into a principled design decision, helping both theorists and experimentalists translate between scales, modalities, and interventions. In doing so, we offer a \\emph{Rosetta Stone} for brain oscillation models -- one that lets the field speak a common dynamical language while preserving the dialectical richness that fuels discovery.",
      "code_url": null
    },
    "2512.03458v1": {
      "title": "A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses",
      "url": "http://arxiv.org/abs/2512.03458v1",
      "authors": "Maryam Maghsoudi, Mohsen Rezaeizadeh, Shihab Shamma",
      "update_time": "2025-12-03",
      "abstract": "Decoding imagined speech engages complex neural processes that are difficult to interpret due to uncertainty in timing and the limited availability of imagined-response datasets. In this study, we present a Magnetoencephalography (MEG) dataset collected from trained musicians as they imagined and listened to musical and poetic stimuli. We show that both imagined and perceived brain responses contain consistent, condition-specific information. Using a sliding-window ridge regression model, we first mapped imagined responses to listened responses at the single-subject level, but found limited generalization across subjects. At the group level, we developed an encoder-decoder convolutional neural network with a subject-specific calibration layer that produced stable and generalizable mappings. The CNN consistently outperformed the null model, yielding significantly higher correlations between predicted and true listened responses for nearly all held-out subjects. Our findings demonstrate that imagined neural activity can be transformed into perception-like responses, providing a foundation for future brain-computer interface applications involving imagined speech and music.",
      "code_url": null
    }
  },
  "neuroAI": {
    "2511.19548v1": {
      "title": "When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics",
      "url": "http://arxiv.org/abs/2511.19548v1",
      "authors": "Yiven, Zhu",
      "update_time": "2025-11-24",
      "abstract": "Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, \"brain-based\" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies \"true\" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.",
      "code_url": null
    },
    "2510.22178v1": {
      "title": "Dopamine-driven synaptic credit assignment in neural networks",
      "url": "http://arxiv.org/abs/2510.22178v1",
      "authors": "Saranraj Nambusubramaniyan, Shervin Safavi, Raja Guru, Andreas Knoblauch",
      "update_time": "2025-10-25",
      "abstract": "Solving the synaptic Credit Assignment Problem(CAP) is central to learning in both biological and artificial neural systems. Finding an optimal solution for synaptic CAP means setting the synaptic weights that assign credit to each neuron for influencing the final output and behavior of neural networks or animals. Gradient-based methods solve this problem in artificial neural networks using back-propagation, however, not in the most efficient way. For instance, back-propagation requires a chain of top-down gradient computations. This leads to an expensive optimization process in terms of computing power and memory linked with well-known weight transport and update locking problems. To address these shortcomings, we take a NeuroAI approach and draw inspiration from neural Reinforcement Learning to develop a derivative-free optimizer for training neural networks, Dopamine. Dopamine is developed for Weight Perturbation (WP) learning that exploits stochastic updating of weights towards optima. It achieves this by minimizing the regret, a form of Reward Prediction Error (RPE) between the expected outcome from the perturbed model and the actual outcome from the unperturbed model. We use this RPE to adjust the learning rate in the network (i.e., creating an adaptive learning rate strategy, similar to the role of dopamine in the brain). We tested the Dopamine optimizer for training multi-layered perceptrons for XOR tasks, and recurrent neural networks for chaotic time series forecasting. Dopamine-trained models demonstrate accelerated convergence and outperform standard WP, and give comparable performance to gradient-based algorithms, while consuming significantly less computation and memory. Overall, the Dopamine optimizer not only finds robust solutions and comparable performance to the state-of-the-art Machine Learning optimizers but is also neurobiologically more plausible.",
      "code_url": null
    },
    "2509.23896v2": {
      "title": "A Computational Perspective on NeuroAI and Synthetic Biological Intelligence",
      "url": "http://arxiv.org/abs/2509.23896v2",
      "authors": "Dhruvik Patel, Md Sayed Tanveer, Jesus Gonzalez-Ferrer, Alon Loeffler, Brett J. Kagan, Mohammed A. Mostajo-Radji, Ge Wang",
      "update_time": "2025-10-09",
      "abstract": "NeuroAI is an emerging field at the intersection of neuroscience and artificial intelligence, where insights from brain function guide the design of intelligent systems. A central area within this field is synthetic biological intelligence (SBI), which combines the adaptive learning properties of biological neural networks with engineered hardware and software. SBI systems provide a platform for modeling neural computation, developing biohybrid architectures, and enabling new forms of embodied intelligence. In this review, we organize the NeuroAI landscape into three interacting domains: hardware, software, and wetware. We outline computational frameworks that integrate biological and non-biological systems and highlight recent advances in organoid intelligence, neuromorphic computing, and neuro-symbolic learning. These developments collectively point toward a new class of systems that compute through interactions between living neural tissue and digital algorithms.",
      "code_url": null
    },
    "2507.06645v2": {
      "title": "Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers",
      "url": "http://arxiv.org/abs/2507.06645v2",
      "authors": "Thomas Klein, Sascha Meyen, Wieland Brendel, Felix A. Wichmann, Kristof Meding",
      "update_time": "2025-11-07",
      "abstract": "Benchmarking models is a key factor for the rapid progress in machine learning (ML) research. Thus, further progress depends on improving benchmarking metrics. A standard metric to measure the behavioral alignment between ML models and human observers is error consistency (EC). EC allows for more fine-grained comparisons of behavior than other metrics such as accuracy, and has been used in the influential Brain-Score benchmark to rank different DNNs by their behavioral consistency with humans. Previously, EC values have been reported without confidence intervals. However, empirically measured EC values are typically noisy -- thus, without confidence intervals, valid benchmarking conclusions are problematic. Here we improve on standard EC in two ways: First, we show how to obtain confidence intervals for EC using a bootstrapping technique, allowing us to derive significance tests for EC. Second, we propose a new computational model relating the EC between two classifiers to the implicit probability that one of them copies responses from the other. This view of EC allows us to give practical guidance to scientists regarding the number of trials required for sufficiently powerful, conclusive experiments. Finally, we use our methodology to revisit popular NeuroAI-results. We find that while the general trend of behavioral differences between humans and machines holds up to scrutiny, many reported differences between deep vision models are statistically insignificant. Our methodology enables researchers to design adequately powered experiments that can reliably detect behavioral differences between models, providing a foundation for more rigorous benchmarking of behavioral alignment.",
      "code_url": null
    },
    "2507.02103v1": {
      "title": "What Neuroscience Can Teach AI About Learning in Continuously Changing Environments",
      "url": "http://arxiv.org/abs/2507.02103v1",
      "authors": "Daniel Durstewitz, Bruno Averbeck, Georgia Koppe",
      "update_time": "2025-07-02",
      "abstract": "Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.",
      "code_url": null
    },
    "2506.04536v3": {
      "title": "NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models",
      "url": "http://arxiv.org/abs/2506.04536v3",
      "authors": "Luca Ghafourpour, Valentin Duruisseaux, Bahareh Tolooshams, Philip H. Wong, Costas A. Anastassiou, Anima Anandkumar",
      "update_time": "2025-10-27",
      "abstract": "Characterizing the cellular properties of neurons is fundamental to understanding their function in the brain. In this quest, the generation of bio-realistic models is central towards integrating multimodal cellular data sets and establishing causal relationships. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. The deterministic formalism of bio-realistic models currently precludes accounting for the natural variability observed experimentally. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on synthetic data generated from bio-realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE enables the efficient generation of synthetic neurons that closely resemble experimental data and exhibit trial-to-trial variability, offering a $4200\\times$ speedup over the numerical solver. NOBLE is the first scaled-up deep learning framework that validates its generalization with real experimental data. To this end, NOBLE captures fundamental neural properties in a unique and emergent manner that opens the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.",
      "code_url": null
    },
    "2505.16080v1": {
      "title": "SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation",
      "url": "http://arxiv.org/abs/2505.16080v1",
      "authors": "Jiayue Liu, Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Kuo Yang, Xu Wang, Yang Wang",
      "update_time": "2025-05-21",
      "abstract": "Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.",
      "code_url": null
    },
    "2502.16238v1": {
      "title": "Brain-Model Evaluations Need the NeuroAI Turing Test",
      "url": "http://arxiv.org/abs/2502.16238v1",
      "authors": "Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi",
      "update_time": "2025-02-22",
      "abstract": "What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \\emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.",
      "code_url": null
    },
    "2501.02402v1": {
      "title": "Asynchronous Hebbian/anti-Hebbian networks",
      "url": "http://arxiv.org/abs/2501.02402v1",
      "authors": "Henrique Reis Aguiar, Matthias H. Hennig",
      "update_time": "2025-01-04",
      "abstract": "Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.",
      "code_url": null
    },
    "2411.18526v2": {
      "title": "NeuroAI for AI Safety",
      "url": "http://arxiv.org/abs/2411.18526v2",
      "authors": "Patrick Mineault, Niccol\u00f2 Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador",
      "update_time": "2025-04-03",
      "abstract": "As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.",
      "code_url": null
    }
  },
  "medical": {
    "2601.05981v1": {
      "title": "Adaptive Conditional Contrast-Agnostic Deformable Image Registration with Uncertainty Estimation",
      "url": "http://arxiv.org/abs/2601.05981v1",
      "authors": "Yinsong Wang, Xinzhe Luo, Siyi Du, Chen Qin",
      "update_time": "2026-01-09",
      "abstract": "Deformable multi-contrast image registration is a challenging yet crucial task due to the complex, non-linear intensity relationships across different imaging contrasts. Conventional registration methods typically rely on iterative optimization of the deformation field, which is time-consuming. Although recent learning-based approaches enable fast and accurate registration during inference, their generalizability remains limited to the specific contrasts observed during training. In this work, we propose an adaptive conditional contrast-agnostic deformable image registration framework (AC-CAR) based on a random convolution-based contrast augmentation scheme. AC-CAR can generalize to arbitrary imaging contrasts without observing them during training. To encourage contrast-invariant feature learning, we propose an adaptive conditional feature modulator (ACFM) that adaptively modulates the features and the contrast-invariant latent regularization to enforce the consistency of the learned feature across different imaging contrasts. Additionally, we enable our framework to provide contrast-agnostic registration uncertainty by integrating a variance network that leverages the contrast-agnostic registration encoder to improve the trustworthiness and reliability of AC-CAR. Experimental results demonstrate that AC-CAR outperforms baseline methods in registration accuracy and exhibits superior generalization to unseen imaging contrasts. Code is available at https://github.com/Yinsong0510/AC-CAR.",
      "code_url": null
    },
    "2601.05974v1": {
      "title": "A Framework for Optimizing Human-Machine Interaction in Classification Systems",
      "url": "http://arxiv.org/abs/2601.05974v1",
      "authors": "Goran Muric, Steven Minton",
      "update_time": "2026-01-09",
      "abstract": "Automated decision systems increasingly rely on human oversight to ensure accuracy in uncertain cases. This paper presents a practical framework for optimizing such human-in-the-loop classification systems using a double-threshold policy. Instead of relying on a single decision cutoff, the system defines two thresholds (a lower and an upper) to automatically accept or reject confident cases while routing ambiguous ones for human review. We formalize this problem as an optimization task that balances system accuracy against human review workload and demonstrate its behavior through extensive Monte Carlo simulations. Our results quantify how different probability score distributions affect the efficiency of human intervention and identify the regions of diminishing returns where additional review yields minimal benefit. The framework provides a general, reproducible method for improving reliability in any decision pipeline requiring selective human validation, including applications in entity resolution, fraud detection, medical triage, and content moderation.",
      "code_url": null
    },
    "2601.05855v1": {
      "title": "Bidirectional Channel-selective Semantic Interaction for Semi-Supervised Medical Segmentation",
      "url": "http://arxiv.org/abs/2601.05855v1",
      "authors": "Kaiwen Huang, Yizhe Zhang, Yi Zhou, Tianyang Xu, Tao Zhou",
      "update_time": "2026-01-09",
      "abstract": "Semi-supervised medical image segmentation is an effective method for addressing scenarios with limited labeled data. Existing methods mainly rely on frameworks such as mean teacher and dual-stream consistency learning. These approaches often face issues like error accumulation and model structural complexity, while also neglecting the interaction between labeled and unlabeled data streams. To overcome these challenges, we propose a Bidirectional Channel-selective Semantic Interaction~(BCSI) framework for semi-supervised medical image segmentation. First, we propose a Semantic-Spatial Perturbation~(SSP) mechanism, which disturbs the data using two strong augmentation operations and leverages unsupervised learning with pseudo-labels from weak augmentations. Additionally, we employ consistency on the predictions from the two strong augmentations to further improve model stability and robustness. Second, to reduce noise during the interaction between labeled and unlabeled data, we propose a Channel-selective Router~(CR) component, which dynamically selects the most relevant channels for information exchange. This mechanism ensures that only highly relevant features are activated, minimizing unnecessary interference. Finally, the Bidirectional Channel-wise Interaction~(BCI) strategy is employed to supplement additional semantic information and enhance the representation of important channels. Experimental results on multiple benchmarking 3D medical datasets demonstrate that the proposed method outperforms existing semi-supervised approaches.",
      "code_url": null
    },
    "2601.05847v1": {
      "title": "Semantic NLP Pipelines for Interoperable Patient Digital Twins from Unstructured EHRs",
      "url": "http://arxiv.org/abs/2601.05847v1",
      "authors": "Rafael Brens, Yuqiao Meng, Luoxi Tang, Zhaohan Xi",
      "update_time": "2026-01-09",
      "abstract": "Digital twins -- virtual replicas of physical entities -- are gaining traction in healthcare for personalized monitoring, predictive modeling, and clinical decision support. However, generating interoperable patient digital twins from unstructured electronic health records (EHRs) remains challenging due to variability in clinical documentation and lack of standardized mappings. This paper presents a semantic NLP-driven pipeline that transforms free-text EHR notes into FHIR-compliant digital twin representations. The pipeline leverages named entity recognition (NER) to extract clinical concepts, concept normalization to map entities to SNOMED-CT or ICD-10, and relation extraction to capture structured associations between conditions, medications, and observations. Evaluation on MIMIC-IV Clinical Database Demo with validation against MIMIC-IV-on-FHIR reference mappings demonstrates high F1-scores for entity and relation extraction, with improved schema completeness and interoperability compared to baseline methods.",
      "code_url": null
    },
    "2601.05717v1": {
      "title": "Inclusion of Inter-crystal Scattering in PET: Analytical Models and Dedicated Reconstruction",
      "url": "http://arxiv.org/abs/2601.05717v1",
      "authors": "Jorge Roser, Hong Phuc Vo, Rebecca Kantorek, Steven Seeger, Magdalena Rafecas",
      "update_time": "2026-01-09",
      "abstract": "Inter-crystal scattering (ICS) in Positron Emission Tomography (PET) is commonly regarded as a degradation effect that might compromise the image spatial resolution. In parallel, the inclusion of ICS events has also been recognized as a potential approach to increase PET sensitivity, which could be especially beneficial in scenarios where the latter is a limiting factor, such as very small animal imaging. Several methods for the recovery of ICS events have been proposed, many of which aim to locate the first interaction, i.e., the Compton scattering site, usually limited by their success rate, computational burden or data and training dependency. Conversely, this work proposes a physics-based model for ICS events, leading to analytical expressions of the sensitivity image and the system matrix (required by statistical reconstruction algorithms), without the need to identify the original line of response. After validating the model, the work shows how ICS events can be integrated into a joint image reconstruction algorithm (based on list-mode MLEM) together with conventional PET events, for which dedicated analytical models are also developed. To assess the performance of the proposed approach, Monte-Carlo simulated and experimental data of an image quality phantom were obtained with the MERMAID small-fish PET scanner prototype. Both simulation and experimental results indicate that, while slightly decreasing the recovery coefficient values, the inclusion of ICS clearly reduces statistical noise and improves uniformity.",
      "code_url": null
    },
    "2601.05661v1": {
      "title": "Motion Compensation for Real Time Ultrasound Scanning in Robotically Assisted Prostate Biopsy Procedures",
      "url": "http://arxiv.org/abs/2601.05661v1",
      "authors": "Matija Markulin, Luka Matijevi\u0107, Luka Siktar, Janko Jurdana, Branimir Caran, Marko \u0160vaco, Filip \u0160uligoj, Bojan \u0160ekoranja",
      "update_time": "2026-01-09",
      "abstract": "Prostate cancer is one of the most common types of cancer in men. Its diagnosis by biopsy requires a high level of expertise and precision from the surgeon, so the results are highly operator-dependent. The aim of this work is to develop a robotic system for assisted ultrasound (US) examination of the prostate, a prebiopsy step that could reduce the dexterity requirements and enable faster, more accurate and more available prostate biopsy. We developed and validated a laboratory setup with a collaborative robotic arm that can autonomously scan a prostate phantom and attached the phantom to a medical robotic arm that mimics the patient's movements. The scanning robot keeps the relative position of the US probe and the prostate constant, ensuring a consistent and robust approach to reconstructing the prostate. To reconstruct the prostate, each slice is segmented to generate a series of prostate contours converted into a 3D point cloud used for biopsy planning. The average scan time of the prostate was 30 s, and the average 3D reconstruction of the prostate took 3 s. We performed four motion scenarios: the phantom was scanned in a stationary state (S), with horizontal motion (H), with vertical motion (V), and with a combination of the two (C). System validation is performed by registering the prostate point cloud reconstructions acquired during different motions (H, V, C) with those obtained in the stationary state. ICP registration with a threshold of 0.8 mm yields mean 83.2\\% fitness and 0.35 mm RMSE for S-H registration, 84.1\\% fitness and 0.37 mm RMSE for S-V registration and 79.4\\% fitness and 0.37 mm RMSE for S-C registration. Due to the elastic and soft material properties of the prostate phantom, the maximum robot tracking error was 3 mm, which can be sufficient for prostate biopsy according to medical literature. The maximum delay in motion compensation was 0.5 s.",
      "code_url": null
    },
    "2601.05552v1": {
      "title": "One Language-Free Foundation Model Is Enough for Universal Vision Anomaly Detection",
      "url": "http://arxiv.org/abs/2601.05552v1",
      "authors": "Bin-Bin Gao, Chengjie Wang",
      "update_time": "2026-01-09",
      "abstract": "Universal visual anomaly detection (AD) aims to identify anomaly images and segment anomaly regions towards open and dynamic scenarios, following zero- and few-shot paradigms without any dataset-specific fine-tuning. We have witnessed significant progress in widely use of visual-language foundational models in recent approaches. However, current methods often struggle with complex prompt engineering, elaborate adaptation modules, and challenging training strategies, ultimately limiting their flexibility and generality. To address these issues, this paper rethinks the fundamental mechanism behind visual-language models for AD and presents an embarrassingly simple, general, and effective framework for Universal vision Anomaly Detection (UniADet). Specifically, we first find language encoder is used to derive decision weights for anomaly classification and segmentation, and then demonstrate that it is unnecessary for universal AD. Second, we propose an embarrassingly simple method to completely decouple classification and segmentation, and decouple cross-level features, i.e., learning independent weights for different tasks and hierarchical features. UniADet is highly simple (learning only decoupled weights), parameter-efficient (only 0.002M learnable parameters), general (adapting a variety of foundation models), and effective (surpassing state-of-the-art zero-/few-shot by a large margin and even full-shot AD methods for the first time) on 14 real-world AD benchmarks covering both industrial and medical domains. We will make the code and model of UniADet available at https://github.com/gaobb/UniADet.",
      "code_url": null
    },
    "2601.05469v1": {
      "title": "Group-kernel auto-calibration and group-patch k-space reconstruction: Fast MRI with time-variant B0 kernels partitioned into time-invariant subsets",
      "url": "http://arxiv.org/abs/2601.05469v1",
      "authors": "Rui Tian, Martin Uecker, Oliver Holder, Pavel Povolni, Theodor Steffen, Klaus Scheffler",
      "update_time": "2026-01-09",
      "abstract": "Purpose: Pushing MRI speed further demands more spatially-encoded information captured per unit time, e.g., by superimposing additional field modulations during oversampled readout. However, this can introduce calibration errors and increase reconstruction time. Thus, we propose a continuous field calibration approach and an efficient k-space reconstruction technique.   Theory and Methods: Our auto-calibration generalizes GRAPPA kernels to explicitly extract continuous B0 modulation kernels, solving interpolation relationships between two ACS regions differing only in the extra field modulation. The k-space locations sharing the same instantaneous image-space modulation are grouped, so that subsets of time-invariant kernels can be separately estimated, as a generalized solution for Wave-CAIPI/FRONSAC-type scans. This view further inspires a k-space subregion-wise reconstruction technique, as an efficient alternative to conventional hybrid-space reconstruction. At 9.4T, FLASH accelerated by a local B0 coil array and Wave-CAIPI were tested with retrospective undersampling.   Results: Artifact-free images were reconstructed, under diverse rapid B0 modulation schemes, reaching maximum acceleration factors of 8-fold in 2D and 14.6-fold in 3D. Some nonlinear gradients modulation schemes reach similar sampling efficiency as linear gradients modulation. The proposed reconstruction shows potentials in reducing reconstruction time.   Conclusion: Rapid B0 modulations and widely-adopted parallel imaging can share a common mathematical framework, and consequently, achieve similarly-robust reconstructions. Specifically, for scans using dynamic B0 and static RF kernels, not only signal encoding, but also auto-calibration and reconstruction can be performed in k-space. This paves the way to robustly remove eddy currents, and explore more complex B0 modulation strategies towards ultimate MRI speed.",
      "code_url": null
    },
    "2601.05448v1": {
      "title": "All-optical photoacoustic tomography via beam deflection",
      "url": "http://arxiv.org/abs/2601.05448v1",
      "authors": "Xingchi Yan, Siyuan Song, Hanxun Jin",
      "update_time": "2026-01-09",
      "abstract": "Photoacoustic imaging (PAI) uniquely combines the advantages of optical contrast with deep tissue penetration capability of acoustic waves, enabling imaging at depths of several centimeters. Conventional photoacoustic imaging methods have relied on pulsed lasers to induce the photoacoustic effect, coupled with arrays of pressure transducers to detect the resulting ultrasound signals. In this work, we propose an alternative all-optical approach that leverages optical deflection to record photoacoustic waves by an array of detection beams. The measured signal is shown to be the Radon transform of the pressure gradients. An optimization-based inversion procedure is used to reconstruct the initial time pressure gradient field. Subsequently, a Galerkin method is used to reconstruct the pressure field from the pressure gradient field. The new modality offers the potential for enhanced sensitivity and reduced signal distortion, advancing the capabilities of photoacoustic imaging beyond traditional transducer-based systems.",
      "code_url": null
    },
    "2601.05418v1": {
      "title": "Group-patch joint compression for highly accelerated MRI: compressing dynamic B0 and static RF spatial modulations across k-space subregion groups",
      "url": "http://arxiv.org/abs/2601.05418v1",
      "authors": "Rui Tian, Klaus Scheffler",
      "update_time": "2026-01-08",
      "abstract": "Purpose: To accelerate MRI further, rapid B0 field modulations can be applied during oversampled readout to capture additional physical information, e.g., Wave-CAIPI, FRONSAC, local B0 coils modulations. These methods, however, introduce additional non-Fourier-encoded dimension that cannot be resolved by FFT, posing significant reconstruction challenges particularly in compressed-sensing or neural-network frameworks.   Theory and methods: Because B0 modulations vary slowly relative to the oversampled ADC dwell time, we exploit this encoding redundancy by compressing k-space patch-by-patch across subregions, each of which is jointly encoded by a distinct subset of B0 and RF (receive) spatial encoding functions. For each patch, a compression matrix is computed once and reused to compress all patches encoded by the same B0/RF spatial modulations. This can be implemented by feeding subsets of B0 and RF spatial encoding maps into an adapted conventional RF array compression algorithm, mimicking an expanded set of virtual receiver channels. This approach was evaluated on ex-vivo/in-vivo human brain scans at 9.4T/3T.   Results: The proposed group-patch joint compression achieves substantially higher compression factors than conventional RF-only compression, while minimally compromising encoding efficiency. Typically, joint compression factors of 11x-20x led to negligible encoding loss, dramatically reducing reconstruction time and peak memory usage. For example, compressed-sensing reconstruction took 1.4-5.1s/2D slice, 177s-10.1min/3D volume, on a high-memory CPU node.   Conclusion: Given joint encoding of dynamic B0 and static RF fields, compressing multidimensional k-space patches in separate groups outperforms compressing RF receiver channels alone. This substantially mitigates a fundamental computational bottleneck that arises when combining rapid B0 and RF-receiver modulations.",
      "code_url": null
    }
  }
}