{
  "Brain": {
    "2503.15487v1": {
      "title": "Fast Two-photon Microscopy by Neuroimaging with Oblong Random Acquisition (NORA)",
      "url": "http://arxiv.org/abs/2503.15487v1",
      "authors": "Esther Whang, Skyler Thomas, Ji Yi, Adam S. Charles",
      "update_time": "2025-03-19",
      "abstract": "Advances in neural imaging have enabled neuroscience to study how the joint activity of large neural populations conspire to produce perception, behavior and cognition. Despite many advances in optical methods, there exists a fundamental tradeoff between imaging speed, field of view, and resolution that limits the scope of neural imaging, especially for the raster-scanning multi-photon imaging needed for imaging deeper into the brain. One approach to overcoming this trade-off is in computational imaging: the co-development of optics and algorithms where the optics are designed to encode the target images into fewer measurements that are faster to acquire, and the algorithms compensate by inverting the optical image coding process to recover a larger or higher resolution image. We present here one such approach for raster-scanning two-photon imaging: Neuroimaging with Oblong Random Acquisition (NORA). NORA quickly acquires each frame in a microscopic video by subsampling only a fraction of the fast scanning lines, ignoring large portions of each frame. NORA mitigates the information loss by extending the point-spread function in the slow-scan direction to integrate the fluorescence of neighboring lines into a single set of measurements. By imaging different, randomly selected, lines at each frame, NORA diversifies the information collected across frames and enables video-level reconstruction. Rather than reconstruct the video frame-by-frame using image-level recovery, NORA recovers full video sequences through a nuclear-norm minimization (i.e., matrix completion) on the pixels-by-time matrix. We simulated NORA imaging using the Neural Anatomy and Optical Microscopy (NAOMi) biophysical simulation suite. Using these simulations we demonstrate that NORA imaging can accurately recover 400 um X 400 um fields of view at subsampling rates up to 20X, despite realistic noise and motion conditions."
    },
    "2503.15255v1": {
      "title": "Fractional Brownian motion with mean-density interaction",
      "url": "http://arxiv.org/abs/2503.15255v1",
      "authors": "Jonathan House, Skirmantas Janu\u0161onis, Ralf Metzler, Thomas Vojta",
      "update_time": "2025-03-19",
      "abstract": "Fractional Brownian motion is a Gaussian stochastic process with long-range correlations in time; it has been shown to be a useful model of anomalous diffusion. Here, we investigate the effects of mutual interactions in an ensemble of particles undergoing fractional Brownian motion. Specifically, we introduce a mean-density interaction in which each particle in the ensemble is coupled to the gradient of the total, time-integrated density produced by the entire ensemble. We report the results of extensive computer simulations for the mean-square displacements and the probability densities of particles undergoing one-dimensional fractional Brownian motion with such a mean-density interaction. We find two qualitatively different regimes, depending on the anomalous diffusion exponent $\\alpha$ characterizing the fractional Gaussian noise. The motion is governed by the interactions for $\\alpha < 4/3$ whereas it is dominated by the fractional Gaussian noise for $\\alpha > 4/3$. We develop a scaling theory explaining our findings. We also discuss generalizations to higher space dimensions and nonlinear interactions as well as applications to the growth of strongly stochastic axons (e.g., serotonergic fibers) in vertebrate brains."
    },
    "2503.15130v1": {
      "title": "A Foundational Theory for Decentralized Sensory Learning",
      "url": "http://arxiv.org/abs/2503.15130v1",
      "authors": "Linus M\u00e5rtensson, Jonas M. D. Enander, Udaya B. Rongala, Henrik J\u00f6rntell",
      "update_time": "2025-03-19",
      "abstract": "In both neuroscience and artificial intelligence, popular functional frameworks and neural network formulations operate by making use of extrinsic error measurements and global learning algorithms. Through a set of conjectures based on evolutionary insights on the origin of cellular adaptive mechanisms, we reinterpret the core meaning of sensory signals to allow the brain to be interpreted as a negative feedback control system, and show how this could lead to local learning algorithms without the need for global error correction metrics. Thereby, a sufficiently good minima in sensory activity can be the complete reward signal of the network, as well as being both necessary and sufficient for biological learning to arise. We show that this method of learning was likely already present in the earliest unicellular life forms on earth. We show evidence that the same principle holds and scales to multicellular organisms where it in addition can lead to division of labour between cells. Available evidence shows that the evolution of the nervous system likely was an adaptation to more effectively communicate intercellular signals to support such division of labour. We therefore propose that the same learning principle that evolved already in the earliest unicellular life forms, i.e. negative feedback control of externally and internally generated sensor signals, has simply been scaled up to become a fundament of the learning we see in biological brains today. We illustrate diverse biological settings, from the earliest unicellular organisms to humans, where this operational principle appears to be a plausible interpretation of the meaning of sensor signals in biology, and how this relates to current neuroscientific theories and findings."
    },
    "2503.15012v1": {
      "title": "Benchmarking Brain Connectivity Graph Inference: A Novel Validation Approach",
      "url": "http://arxiv.org/abs/2503.15012v1",
      "authors": "Alice Chevaux, Ali Fahkar, K\u00e9vin Polisano, Ir\u00e8ne Gannaz, Sophie Achard",
      "update_time": "2025-03-19",
      "abstract": "Inferring a binary connectivity graph from resting-state fMRI data for a single subject requires making several methodological choices and assumptions that can significantly affect the results. In this study, we investigate the robustness of existing edge detection methods when relaxing a common assumption: the sparsity of the graph. We propose a new pipeline to generate synthetic data and to benchmark the state of the art in graph inference. Simulated correlation matrices are designed to have a set of given zeros and a constraint on the signal-to-noise ratio. We compare approaches based on covariance or precision matrices, emphasizing their implications for connectivity inference. This framework allows us to assess the sensitivity of connectivity estimations and edge detection methods to different parameters."
    },
    "2503.14956v1": {
      "title": "Machine learning predictions from unpredictable chaos",
      "url": "http://arxiv.org/abs/2503.14956v1",
      "authors": "Jian Jiang, Long Chen, Lu ke, Bozheng Dou, Yueying Zhu, Yazhou Shi, Huahai Qiu, Bengong Zhang, Tianshou Zhou, Guo-Wei Wei",
      "update_time": "2025-03-19",
      "abstract": "Chaos is omnipresent in nature, and its understanding provides enormous social and economic benefits. However, the unpredictability of chaotic systems is a textbook concept due to their sensitivity to initial conditions, aperiodic behavior, fractal dimensions, nonlinearity, and strange attractors. In this work, we introduce, for the first time, chaotic learning, a novel multiscale topological paradigm that enables accurate predictions from chaotic systems. We show that seemingly random and unpredictable chaotic dynamics counterintuitively offer unprecedented quantitative predictions. Specifically, we devise multiscale topological Laplacians to embed real-world data into a family of interactive chaotic dynamical systems, modulate their dynamical behaviors, and enable the accurate prediction of the input data. As a proof of concept, we consider 28 datasets from four categories of realistic problems: 10 brain waves, four benchmark protein datasets, 13 single-cell RNA sequencing datasets, and an image dataset, as well as two distinct chaotic dynamical systems, namely the Lorenz and Rossler attractors. We demonstrate chaotic learning predictions of the physical properties from chaos. Our new chaotic learning paradigm profoundly changes the textbook perception of chaos and bridges topology, chaos, and learning for the first time."
    },
    "2503.14847v1": {
      "title": "Project Jenkins: Turning Monkey Neural Data into Robotic Arm Movement, and Back",
      "url": "http://arxiv.org/abs/2503.14847v1",
      "authors": "Andrii Zahorodnii, Dima Yanovsky",
      "update_time": "2025-03-19",
      "abstract": "Project Jenkins explores how neural activity in the brain can be decoded into robotic movement and, conversely, how movement patterns can be used to generate synthetic neural data. Using real neural data recorded from motor and premotor cortex areas of a macaque monkey named Jenkins, we develop models for decoding (converting brain signals into robotic arm movements) and encoding (simulating brain activity corresponding to a given movement). For the interface between the brain simulation and the physical world, we utilized Koch v1.1 leader and follower robotic arms. We developed an interactive web console that allows users to generate synthetic brain data from joystick movements in real time. Our results are a step towards brain-controlled robotics, prosthetics, and enhancing normal motor function. By accurately modeling brain activity, we take a step toward flexible brain-computer interfaces that generalize beyond predefined movements. To support the research community, we provide open source tools for both synthetic data generation and neural decoding, fostering reproducibility and accelerating progress. The project is available at https://www.808robots.com/projects/jenkins"
    },
    "2503.14700v1": {
      "title": "From Density to Void: Why Brain Networks Fail to Reveal Complex Higher-Order Structures",
      "url": "http://arxiv.org/abs/2503.14700v1",
      "authors": "Moo K. Chung, Anass B. El-Yaagoubi, Anqi Qiu, Hernando Ombao",
      "update_time": "2025-03-18",
      "abstract": "In brain network analysis using resting-state fMRI, there is growing interest in modeling higher-order interactions beyond simple pairwise connectivity via persistent homology. Despite the promise of these advanced topological tools, robust and consistently observed higher-order interactions over time remain elusive. In this study, we investigate why conventional analyses often fail to reveal complex higher-order structures - such as interactions involving four or more nodes - and explore whether such interactions truly exist in functional brain networks. We utilize a simplicial complex framework often used in persistent homology to address this question."
    },
    "2503.14655v1": {
      "title": "Core-Periphery Principle Guided State Space Model for Functional Connectome Classification",
      "url": "http://arxiv.org/abs/2503.14655v1",
      "authors": "Minheng Chen, Xiaowei Yu, Jing Zhang, Tong Chen, Chao Cao, Yan Zhuang, Yanjun Lyu, Lu Zhang, Tianming Liu, Dajiang Zhu",
      "update_time": "2025-03-18",
      "abstract": "Understanding the organization of human brain networks has become a central focus in neuroscience, particularly in the study of functional connectivity, which plays a crucial role in diagnosing neurological disorders. Advances in functional magnetic resonance imaging and machine learning techniques have significantly improved brain network analysis. However, traditional machine learning approaches struggle to capture the complex relationships between brain regions, while deep learning methods, particularly Transformer-based models, face computational challenges due to their quadratic complexity in long-sequence modeling. To address these limitations, we propose a Core-Periphery State-Space Model (CP-SSM), an innovative framework for functional connectome classification. Specifically, we introduce Mamba, a selective state-space model with linear complexity, to effectively capture long-range dependencies in functional brain networks. Furthermore, inspired by the core-periphery (CP) organization, a fundamental characteristic of brain networks that enhances efficient information transmission, we design CP-MoE, a CP-guided Mixture-of-Experts that improves the representation learning of brain connectivity patterns. We evaluate CP-SSM on two benchmark fMRI datasets: ABIDE and ADNI. Experimental results demonstrate that CP-SSM surpasses Transformer-based models in classification performance while significantly reducing computational complexity. These findings highlight the effectiveness and efficiency of CP-SSM in modeling brain functional connectivity, offering a promising direction for neuroimaging-based neurological disease diagnosis."
    },
    "2503.14333v1": {
      "title": "Revealing higher-order neural representations with generative artificial intelligence",
      "url": "http://arxiv.org/abs/2503.14333v1",
      "authors": "Hojjat Azimi Asrari, Megan A. K. Peters",
      "update_time": "2025-03-18",
      "abstract": "Studies often aim to reveal how neural representations encode aspects of an observer's environment, such as its contents or structure. These are ``first-order\" representations (FORs), because they're ``about\" the external world. A less-common target is ``higher-order\" representations (HORs), which are ``about\" FORs -- their contents, stability, or uncertainty. HORs of uncertainty appear critically involved in adaptive behaviors including learning under uncertainty, influencing learning rates and internal model updating based on environmental feedback. However, HORs about uncertainty are unlikely to be direct ``read-outs\" of FOR characteristics, instead reflecting estimation processes which may be lossy, bias-prone, or distortive and which may also incorporate estimates of distributions of uncertainty the observer is likely to experience. While some research has targeted neural representations of ``instantaneously\" estimated uncertainty, how the brain represents \\textit{distributions} of expected uncertainty remains largely unexplored. Here, we propose a novel reinforcement learning (RL) based generative artificial intelligence (genAI) approach to explore neural representations of uncertainty distributions. We use existing functional magnetic resonance imaging data, where humans learned to `de-noise' their brain states to achieve target neural patterns, to train denoising diffusion genAI models with RL algorithms to learn noise distributions similar to how humans might learn to do the same. We then explore these models' learned noise-distribution HORs compared to control models trained with traditional backpropagation. Results reveal model-dependent differences in noise distribution representations -- with the RL-based model offering much higher explanatory power for human behavior -- offering an exciting path towards using genAI to explore neural noise-distribution HORs."
    },
    "2503.14322v1": {
      "title": "Consumer-grade EEG-based Eye Tracking",
      "url": "http://arxiv.org/abs/2503.14322v1",
      "authors": "Tiago Vasconcelos Afonso, Florian Heinrichs",
      "update_time": "2025-03-18",
      "abstract": "Electroencephalography-based eye tracking (EEG-ET) leverages eye movement artifacts in EEG signals as an alternative to camera-based tracking. While EEG-ET offers advantages such as robustness in low-light conditions and better integration with brain-computer interfaces, its development lags behind traditional methods, particularly in consumer-grade settings. To support research in this area, we present a dataset comprising simultaneous EEG and eye-tracking recordings from 113 participants across 116 sessions, amounting to 11 hours and 45 minutes of recordings. Data was collected using a consumer-grade EEG headset and webcam-based eye tracking, capturing eye movements under four experimental paradigms with varying complexity. The dataset enables the evaluation of EEG-ET methods across different gaze conditions and serves as a benchmark for assessing feasibility with affordable hardware. Data preprocessing includes handling of missing values and filtering to enhance usability. In addition to the dataset, code for data preprocessing and analysis is available to support reproducibility and further research."
    }
  },
  "EEG": {
    "2503.15039v1": {
      "title": "A Note on Local Linear Regression for Time Series in Banach Spaces",
      "url": "http://arxiv.org/abs/2503.15039v1",
      "authors": "Florian Heinrichs",
      "update_time": "2025-03-19",
      "abstract": "This work extends local linear regression to Banach space-valued time series for estimating smoothly varying means and their derivatives in non-stationary data. The asymptotic properties of both the standard and bias-reduced Jackknife estimators are analyzed under mild moment conditions, establishing their convergence rates. Simulation studies assess the finite sample performance of these estimators and compare them with the Nadaraya-Watson estimator. Additionally, the proposed methods are applied to smooth EEG recordings for reconstructing eye movements and to video analysis for detecting pedestrians and abandoned objects."
    },
    "2503.14322v1": {
      "title": "Consumer-grade EEG-based Eye Tracking",
      "url": "http://arxiv.org/abs/2503.14322v1",
      "authors": "Tiago Vasconcelos Afonso, Florian Heinrichs",
      "update_time": "2025-03-18",
      "abstract": "Electroencephalography-based eye tracking (EEG-ET) leverages eye movement artifacts in EEG signals as an alternative to camera-based tracking. While EEG-ET offers advantages such as robustness in low-light conditions and better integration with brain-computer interfaces, its development lags behind traditional methods, particularly in consumer-grade settings. To support research in this area, we present a dataset comprising simultaneous EEG and eye-tracking recordings from 113 participants across 116 sessions, amounting to 11 hours and 45 minutes of recordings. Data was collected using a consumer-grade EEG headset and webcam-based eye tracking, capturing eye movements under four experimental paradigms with varying complexity. The dataset enables the evaluation of EEG-ET methods across different gaze conditions and serves as a benchmark for assessing feasibility with affordable hardware. Data preprocessing includes handling of missing values and filtering to enhance usability. In addition to the dataset, code for data preprocessing and analysis is available to support reproducibility and further research."
    },
    "2503.12392v1": {
      "title": "Central and Central-Parietal EEG Signatures of Parkinson's Disease",
      "url": "http://arxiv.org/abs/2503.12392v1",
      "authors": "Artem Lensky",
      "update_time": "2025-03-16",
      "abstract": "This study investigates EEG as a potential early biomarker by applying deep learning techniques to resting-state EEG recordings from 31 subjects (15 with PD and 16 healthy controls). EEG signals were rigorously preprocessed to remove tremor artifacts, then converted to wavelet-based images by grouping spatially adjacent electrodes into triplets for convolutional neural network (CNN) classification. Our analysis across different brain regions and frequency bands showed distinct spatial-spectral patterns of PD-related neural oscillations. We identified high classification accuracy (74%) in the gamma band (40-62.4 Hz) for central-parietal electrodes (CP1, Pz, CP2), and 76% accuracy using central electrodes (C3, Cz, C4) with full-spectrum 0.4-62.4 Hz. In particular, we observed pronounced right-hemisphere involvement, specifically in parieto-occipital regions. Unlike previous studies that achieved higher accuracies by potentially including tremor artifacts, our approach isolates genuine neurophysiological alterations in cortical activity. These findings suggest that specific EEG-based oscillatory patterns, especially central-parietal gamma activity, may provide diagnostic information for PD, potentially before the onset of motor symptoms."
    },
    "2503.12334v1": {
      "title": "When neural implant meets multimodal LLM: A dual-loop system for neuromodulation and naturalistic neuralbehavioral research",
      "url": "http://arxiv.org/abs/2503.12334v1",
      "authors": "Edward Hong Wang, Cynthia Xin Wen",
      "update_time": "2025-03-16",
      "abstract": "We propose a novel dual-loop system that synergistically combines responsive neurostimulation (RNS) implants with artificial intelligence-driven wearable devices for treating post-traumatic stress disorder (PTSD) and enabling naturalistic brain research. In PTSD Therapy Mode, an implanted closed-loop neural device monitors amygdala activity and provides on-demand stimulation upon detecting pathological theta oscillations, while an ensemble of wearables (smart glasses, smartwatches, smartphones) uses multimodal large language model (LLM) analysis of sensory data to detect environmental or physiological PTSD triggers and deliver timely audiovisual interventions. Logged events from both the neural and wearable loops are analyzed to personalize trigger detection and progressively transition patients to non-invasive interventions. In Neuroscience Research Mode, the same platform is adapted for real-world brain activity capture. Wearable-LLM systems recognize naturalistic events (social interactions, emotional situations, compulsive behaviors, decision making) and signal implanted RNS devices (via wireless triggers) to record synchronized intracranial data during these moments. This approach builds on recent advances in mobile intracranial EEG recording and closed-loop neuromodulation in humans (BRAIN Initiative, 2023) (Mobbs et al., 2021). We discuss how our interdisciplinary system could revolutionize PTSD therapy and cognitive neuroscience by enabling 24/7 monitoring, context-aware intervention, and rich data collection outside traditional labs. The vision is a future where AI-enhanced devices continuously collaborate with the human brain, offering therapeutic support and deep insights into neural function, with the resulting real-world context rich neural data, in turn, accelerating the development of more biologically-grounded and human-centric AI."
    },
    "2503.11741v2": {
      "title": "BioMamba: Leveraging Spectro-Temporal Embedding in Bidirectional Mamba for Enhanced Biosignal Classification",
      "url": "http://arxiv.org/abs/2503.11741v2",
      "authors": "Jian Qian, Teck Lun Goh, Bingyu Xie, Chengyao Zhu, Biao Wan, Yawen Guan, Patrick Yin Chiang",
      "update_time": "2025-03-18",
      "abstract": "Biological signals, such as electroencephalograms (EEGs) and electrocardiograms (ECGs), play a pivotal role in numerous clinical practices, such as diagnosing brain and cardiac arrhythmic diseases. Existing methods for biosignal classification rely on Attention-based frameworks with dense Feed Forward layers, which lead to inefficient learning, high computational overhead, and suboptimal performance. In this work, we introduce BioMamba, a Spectro-Temporal Embedding strategy applied to the Bidirectional Mamba framework with Sparse Feed Forward layers to enable effective learning of biosignal sequences. By integrating these three key components, BioMamba effectively addresses the limitations of existing methods. Extensive experiments demonstrate that BioMamba significantly outperforms state-of-the-art methods with marked improvement in classification performance. The advantages of the proposed BioMamba include (1) Reliability: BioMamba consistently delivers robust results, confirmed across six evaluation metrics. (2) Efficiency: We assess both model and training efficiency, the BioMamba demonstrates computational effectiveness by reducing model size and resource consumption compared to existing approaches. (3) Generality: With the capacity to effectively classify a diverse set of tasks, BioMamba demonstrates adaptability and effectiveness across various domains and applications."
    },
    "2503.11202v1": {
      "title": "Decoding Imagined Handwriting from EEG",
      "url": "http://arxiv.org/abs/2503.11202v1",
      "authors": "Srinivas Ravishankar, Nora Zajzon, Virginia de Sa",
      "update_time": "2025-03-14",
      "abstract": "Patients with extreme forms of paralysis face challenges in communication, adversely impacting their quality of life. Recent studies have reported higher-than-chance performance in decoding handwritten letters from EEG signals, potentially allowing these subjects to communicate. However, all prior works have attempted to decode handwriting from EEG during actual motion. Furthermore, they assume that precise movement-onset is known. In this work, we focus on settings closer to real-world use where either movement onset is not known or movement does not occur at all, fully utilizing motor imagery. We show that several existing studies are affected by confounds that make them inapplicable to the imagined handwriting setting. We also investigate how sample complexity affects handwriting decoding performance, guiding future data collection efforts. Our work shows that (a) Sample complexity analysis in single-trial EEG reveals a noise ceiling, which can be alleviated by averaging over trials. (b) Knowledge of movement-onset is crucial to reported performance in prior works. (c) Fully imagined handwriting can be decoded from EEG with higher-than-chance performance. Taken together, these results highlight both the unique challenges and avenues to pursue to build a practical EEG-based handwriting BCI."
    },
    "2503.10783v1": {
      "title": "EEG-Based Decoding of Sound Location: Comparing Free-Field to Headphone-Based Non-Individual HRTFs",
      "url": "http://arxiv.org/abs/2503.10783v1",
      "authors": "Nils Marggraf-Turley, Martha Shiell, Niels Pontoppidan, Drew Cappotto, Lorenzo Picinali",
      "update_time": "2025-03-13",
      "abstract": "Sound source localization relies on spatial cues such as interaural time differences (ITD), interaural level differences (ILD), and monaural spectral cues. Individually measured Head-Related Transfer Functions (HRTFs) facilitate precise spatial hearing but are impractical to measure, necessitating non-individual HRTFs, which may compromise localization accuracy and externalization. To further investigate this phenomenon, the neurophysiological differences between free-field and non-individual HRTF listening are explored by decoding sound locations from EEG-derived Event-Related Potentials (ERPs). Twenty-two participants localized stimuli under both conditions with EEG responses recorded and logistic regression classifiers trained to distinguish sound source locations.   Lower cortical response amplitudes were observed for KEMAR compared to free-field, especially in front-central and occipital-parietal regions. ANOVA identified significant main effects of auralization condition (F(1, 21) = 34.56, p < 0.0001) and location (F(3, 63) = 18.17, p < 0.0001) on decoding accuracy (DA), which was higher in free-field and interaural-cue-dominated locations. DA negatively correlated with front-back confusion rates (r = -0.57, p < 0.01), linking neural DA to perceptual confusion.   These findings demonstrate that headphone-based non-individual HRTFs elicit lower amplitude cortical responses to static, azimuthally-varying locations than free-field conditions. The correlation between EEG-based DA and front-back confusion underscores neurophysiological markers' potential for assessing spatial auditory discrimination."
    },
    "2503.10362v1": {
      "title": "BioSerenity-E1: a self-supervised EEG model for medical applications",
      "url": "http://arxiv.org/abs/2503.10362v1",
      "authors": "Ruggero G. Bettinardi, Mohamed Rahmouni, Ulysse Gimenez",
      "update_time": "2025-03-13",
      "abstract": "Electroencephalography (EEG) serves as an essential diagnostic tool in neurology; however, its accurate manual interpretation is a time-intensive process that demands highly specialized expertise, which remains relatively scarce and not consistently accessible. To address these limitations, the implementation of automated pre-screening and analysis systems for EEG data holds considerable promise. Advances in self-supervised learning made it possible to pre-train complex deep learning architectures on large volumes of unlabeled EEG data to learn generalizable representations, that can later be used to enhance performance on multiple tasks while needing less downstream data. In the present paper, we introduce BioSerenity-E1, the first of a family of self-supervised foundation models for clinical EEG applications that combines spectral tokenization with masked prediction to achieve state-of-the-art performance across relevant diagnostic tasks. The two-phase self-supervised pretraining framework initially acquires compressed EEG representations via a transformer-based VQ-VAE architecture designed to reconstruct log-multitaper spectral projections, then implements extensive (70% block) masked token prediction to force the model to learn complex spatiotemporal dependencies in EEG signals. BioSerenity-E1 achieves strong performance across three clinical tasks, either in line or above state-of-the-art methods: seizure detection (AUROC = 0.926, Sensitivity = 0.909), normal/abnormal classification (AUPRC = 0.970 on proprietary data; 0.910 on TUH-Abnormal), and multiclass pathology differentiation on unbalanced data (Weighted F1 = 0.730). The utility of BioSerenity-E1 is further confirmed in low-data regimes scenarios, showing clear improvements in AUPRC (from +2% to 17%) when trained on less than 10% of the available data."
    },
    "2503.09961v1": {
      "title": "Edge-Fog Computing-Enabled EEG Data Compression via Asymmetrical Variational Discrete Cosine Transform Network",
      "url": "http://arxiv.org/abs/2503.09961v1",
      "authors": "Xin Zhu, Hongyi Pan, Ahmet Enis Cetin",
      "update_time": "2025-03-13",
      "abstract": "The large volume of electroencephalograph (EEG) data produced by brain-computer interface (BCI) systems presents challenges for rapid transmission over bandwidth-limited channels in Internet of Things (IoT) networks. To address the issue, we propose a novel multi-channel asymmetrical variational discrete cosine transform (DCT) network for EEG data compression within an edge-fog computing framework. At the edge level, low-complexity DCT compression units are designed using parallel trainable hard-thresholding and scaling operators to remove redundant data and extract the effective latent space representation. At the fog level, an adaptive filter bank is applied to merge important features from adjacent channels into each individual channel by leveraging inter-channel correlations. Then, the inverse DCT reconstructed multi-head attention is developed to capture both local and global dependencies and reconstruct the original signals. Furthermore, by applying the principles of variational inference, a new evidence lower bound is formulated as the loss function, driving the model to balance compression efficiency and reconstruction accuracy. Experimental results on two public datasets demonstrate that the proposed method achieves superior compression performance without sacrificing any useful information for BCI detection compared with state-of-the-art techniques, indicating a feasible solution for EEG data compression."
    },
    "2503.13497v1": {
      "title": "Is Limited Participant Diversity Impeding EEG-based Machine Learning?",
      "url": "http://arxiv.org/abs/2503.13497v1",
      "authors": "Philipp Bomatter, Henry Gouk",
      "update_time": "2025-03-11",
      "abstract": "The application of machine learning (ML) to electroencephalography (EEG) has great potential to advance both neuroscientific research and clinical applications. However, the generalisability and robustness of EEG-based ML models often hinge on the amount and diversity of training data. It is common practice to split EEG recordings into small segments, thereby increasing the number of samples substantially compared to the number of individual recordings or participants. We conceptualise this as a multi-level data generation process and investigate the scaling behaviour of model performance with respect to the overall sample size and the participant diversity through large-scale empirical studies. We then use the same framework to investigate the effectiveness of different ML strategies designed to address limited data problems: data augmentations and self-supervised learning. Our findings show that model performance scaling can be severely constrained by participant distribution shifts and provide actionable guidance for data collection and ML research."
    }
  },
  "BCI": {
    "2503.12705v1": {
      "title": "A Brain-Computer Interface Data Persistence System for Multi-Scenario and Multi-Modal Data: NeuroStore",
      "url": "http://arxiv.org/abs/2503.12705v1",
      "authors": "Yang Chen, Hongxin Zhang, Guanyu Xiong, Chenxu Li, Chengcheng Hong, Chen Yang",
      "update_time": "2025-03-17",
      "abstract": "With the rapid advancement of brain-computer interface (BCI) technology, the volume of physiological data generated in related research and applications has grown significantly. Data is a critical resource in BCI research and a key factor in the development of BCI technology, making efficient storage and management of this data increasingly vital. In the realm of research, ample data can facilitate the development of novel algorithms, which can be more accurately validated. In terms of applications, well-organized data can foster the emergence of new business opportunities, thereby maximizing the commercial value of the data. Currently, there are two major challenges in the storage and management of BCI data: providing different classification storage modes for multi-modal data, and adapting to varying application scenarios while improving storage strategies. To address these challenges, this study has developed the NeuroStore BCI data persistence system, which provides a general and easily scalable data model and can effectively handle multiple types of data storage. The system has a flexible distributed framework and can be widely applied to various scenarios. It has been utilized as the core support platform for efficient data storage and management services in the \"BCI Controlled Robot Contest in World Robot Contest.\""
    },
    "2503.11202v1": {
      "title": "Decoding Imagined Handwriting from EEG",
      "url": "http://arxiv.org/abs/2503.11202v1",
      "authors": "Srinivas Ravishankar, Nora Zajzon, Virginia de Sa",
      "update_time": "2025-03-14",
      "abstract": "Patients with extreme forms of paralysis face challenges in communication, adversely impacting their quality of life. Recent studies have reported higher-than-chance performance in decoding handwritten letters from EEG signals, potentially allowing these subjects to communicate. However, all prior works have attempted to decode handwriting from EEG during actual motion. Furthermore, they assume that precise movement-onset is known. In this work, we focus on settings closer to real-world use where either movement onset is not known or movement does not occur at all, fully utilizing motor imagery. We show that several existing studies are affected by confounds that make them inapplicable to the imagined handwriting setting. We also investigate how sample complexity affects handwriting decoding performance, guiding future data collection efforts. Our work shows that (a) Sample complexity analysis in single-trial EEG reveals a noise ceiling, which can be alleviated by averaging over trials. (b) Knowledge of movement-onset is crucial to reported performance in prior works. (c) Fully imagined handwriting can be decoded from EEG with higher-than-chance performance. Taken together, these results highlight both the unique challenges and avenues to pursue to build a practical EEG-based handwriting BCI."
    },
    "2503.09961v1": {
      "title": "Edge-Fog Computing-Enabled EEG Data Compression via Asymmetrical Variational Discrete Cosine Transform Network",
      "url": "http://arxiv.org/abs/2503.09961v1",
      "authors": "Xin Zhu, Hongyi Pan, Ahmet Enis Cetin",
      "update_time": "2025-03-13",
      "abstract": "The large volume of electroencephalograph (EEG) data produced by brain-computer interface (BCI) systems presents challenges for rapid transmission over bandwidth-limited channels in Internet of Things (IoT) networks. To address the issue, we propose a novel multi-channel asymmetrical variational discrete cosine transform (DCT) network for EEG data compression within an edge-fog computing framework. At the edge level, low-complexity DCT compression units are designed using parallel trainable hard-thresholding and scaling operators to remove redundant data and extract the effective latent space representation. At the fog level, an adaptive filter bank is applied to merge important features from adjacent channels into each individual channel by leveraging inter-channel correlations. Then, the inverse DCT reconstructed multi-head attention is developed to capture both local and global dependencies and reconstruct the original signals. Furthermore, by applying the principles of variational inference, a new evidence lower bound is formulated as the loss function, driving the model to balance compression efficiency and reconstruction accuracy. Experimental results on two public datasets demonstrate that the proposed method achieves superior compression performance without sacrificing any useful information for BCI detection compared with state-of-the-art techniques, indicating a feasible solution for EEG data compression."
    },
    "2503.08284v1": {
      "title": "Neural cyberattacks applied to the vision under realistic visual stimuli",
      "url": "http://arxiv.org/abs/2503.08284v1",
      "authors": "Victoria Magdalena L\u00f3pez Madejska, Sergio L\u00f3pez Bernal, Gregorio Mart\u00ednez P\u00e9rez, Alberto Huertas Celdr\u00e1n",
      "update_time": "2025-03-11",
      "abstract": "Brain-Computer Interfaces (BCIs) are systems traditionally used in medicine and designed to interact with the brain to record or stimulate neurons. Despite their benefits, the literature has demonstrated that invasive BCIs focused on neurostimulation present vulnerabilities allowing attackers to gain control. In this context, neural cyberattacks emerged as threats able to disrupt spontaneous neural activity by performing neural overstimulation or inhibition. Previous work validated these attacks in small-scale simulations with a reduced number of neurons, lacking real-world complexity. Thus, this work tackles this limitation by analyzing the impact of two existing neural attacks, Neuronal Flooding (FLO) and Neuronal Jamming (JAM), on a complex neuronal topology of the primary visual cortex of mice consisting of approximately 230,000 neurons, tested on three realistic visual stimuli: flash effect, movie, and drifting gratings. Each attack was evaluated over three relevant events per stimulus, also testing the impact of attacking 25% and 50% of the neurons. The results, based on the number of spikes and shift percentages metrics, showed that the attacks caused the greatest impact on the movie, while dark and fixed events are the most robust. Although both attacks can significantly affect neural activity, JAM was generally more damaging, producing longer temporal delays, and had a larger prevalence. Finally, JAM did not require to alter many neurons to significantly affect neural activity, while the impact in FLO increased with the number of neurons attacked.",
      "code_url": "https://github.com/victorialm/neural-cyberattacks-complete-topology-v1"
    },
    "2503.05349v1": {
      "title": "Spatial Distillation based Distribution Alignment (SDDA) for Cross-Headset EEG Classification",
      "url": "http://arxiv.org/abs/2503.05349v1",
      "authors": "Dingkun Liu, Siyang Li, Ziwei Wang, Wei Li, Dongrui Wu",
      "update_time": "2025-03-07",
      "abstract": "A non-invasive brain-computer interface (BCI) enables direct interaction between the user and external devices, typically via electroencephalogram (EEG) signals. However, decoding EEG signals across different headsets remains a significant challenge due to differences in the number and locations of the electrodes. To address this challenge, we propose a spatial distillation based distribution alignment (SDDA) approach for heterogeneous cross-headset transfer in non-invasive BCIs. SDDA uses first spatial distillation to make use of the full set of electrodes, and then input/feature/output space distribution alignments to cope with the significant differences between the source and target domains. To our knowledge, this is the first work to use knowledge distillation in cross-headset transfers. Extensive experiments on six EEG datasets from two BCI paradigms demonstrated that SDDA achieved superior performance in both offline unsupervised domain adaptation and online supervised domain adaptation scenarios, consistently outperforming 10 classical and state-of-the-art transfer learning algorithms.",
      "code_url": "https://github.com/Dingkun0817/SDDA"
    },
    "2503.04163v1": {
      "title": "VLA Model-Expert Collaboration for Bi-directional Manipulation Learning",
      "url": "http://arxiv.org/abs/2503.04163v1",
      "authors": "Tian-Yu Xiang, Ao-Qun Jin, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Sheng-Bin Duang, Si-Cheng Wang, Zheng Lei, Zeng-Guang Hou",
      "update_time": "2025-03-06",
      "abstract": "The emergence of vision-language-action (VLA) models has given rise to foundation models for robot manipulation. Although these models have achieved significant improvements, their generalization in multi-task manipulation remains limited. This study proposes a VLA model-expert collaboration framework that leverages a limited number of expert actions to enhance VLA model performance. This approach reduces expert workload relative to manual operation while simultaneously improving the reliability and generalization of VLA models. Furthermore, manipulation data collected during collaboration can further refine the VLA model, while human participants concurrently enhance their skills. This bi-directional learning loop boosts the overall performance of the collaboration system. Experimental results across various VLA models demonstrate the effectiveness of the proposed system in collaborative manipulation and learning, as evidenced by improved success rates across tasks. Additionally, validation using a brain-computer interface (BCI) indicates that the collaboration system enhances the efficiency of low-speed action systems by involving VLA model during manipulation. These promising results pave the way for advancing human-robot interaction in the era of foundation models for robotics. (Project website: https://aoqunjin.github.io/Expert-VLA/)"
    },
    "2502.19281v1": {
      "title": "Integrating Biological and Machine Intelligence: Attention Mechanisms in Brain-Computer Interfaces",
      "url": "http://arxiv.org/abs/2502.19281v1",
      "authors": "Jiyuan Wang, Weishan Ye, Jialin He, Li Zhang, Gan Huang, Zhuliang Yu, Zhen Liang",
      "update_time": "2025-02-26",
      "abstract": "With the rapid advancement of deep learning, attention mechanisms have become indispensable in electroencephalography (EEG) signal analysis, significantly enhancing Brain-Computer Interface (BCI) applications. This paper presents a comprehensive review of traditional and Transformer-based attention mechanisms, their embedding strategies, and their applications in EEG-based BCI, with a particular emphasis on multimodal data fusion. By capturing EEG variations across time, frequency, and spatial channels, attention mechanisms improve feature extraction, representation learning, and model robustness. These methods can be broadly categorized into traditional attention mechanisms, which typically integrate with convolutional and recurrent networks, and Transformer-based multi-head self-attention, which excels in capturing long-range dependencies. Beyond single-modality analysis, attention mechanisms also enhance multimodal EEG applications, facilitating effective fusion between EEG and other physiological or sensory data. Finally, we discuss existing challenges and emerging trends in attention-based EEG modeling, highlighting future directions for advancing BCI technology. This review aims to provide valuable insights for researchers seeking to leverage attention mechanisms for improved EEG interpretation and application."
    },
    "2502.18719v1": {
      "title": "Enhancing Subject-Independent Accuracy in fNIRS-based Brain-Computer Interfaces with Optimized Channel Selection",
      "url": "http://arxiv.org/abs/2502.18719v1",
      "authors": "Yuxin Li, Hao Fang, Wen Liu, Chuantong Cheng, Hongda Chen",
      "update_time": "2025-02-26",
      "abstract": "Achieving high subject-independent accuracy in functional near-infrared spectroscopy (fNIRS)-based brain-computer interfaces (BCIs) remains a challenge, particularly when minimizing the number of channels. This study proposes a novel feature extraction scheme and a Pearson correlation-based channel selection algorithm to enhance classification accuracy while reducing hardware complexity. Using an open-access fNIRS dataset, our method improved average accuracy by 28.09% compared to existing approaches, achieving a peak subject-independent accuracy of 95.98% with only two channels. These results demonstrate the potential of our optimized feature extraction and channel selection methods for developing efficient, subject-independent fNIRS-based BCI systems."
    },
    "2502.18594v1": {
      "title": "Error-related Potential driven Reinforcement Learning for adaptive Brain-Computer Interfaces",
      "url": "http://arxiv.org/abs/2502.18594v1",
      "authors": "Aline Xavier Fid\u00eancio, Felix Gr\u00fcn, Christian Klaes, Ioannis Iossifidis",
      "update_time": "2025-02-25",
      "abstract": "Brain-computer interfaces (BCIs) provide alternative communication methods for individuals with motor disabilities by allowing control and interaction with external devices. Non-invasive BCIs, especially those using electroencephalography (EEG), are practical and safe for various applications. However, their performance is often hindered by EEG non-stationarities, caused by changing mental states or device characteristics like electrode impedance. This challenge has spurred research into adaptive BCIs that can handle such variations. In recent years, interest has grown in using error-related potentials (ErrPs) to enhance BCI performance. ErrPs, neural responses to errors, can be detected non-invasively and have been integrated into different BCI paradigms to improve performance through error correction or adaptation.   This research introduces a novel adaptive ErrP-based BCI approach using reinforcement learning (RL). We demonstrate the feasibility of an RL-driven adaptive framework incorporating ErrPs and motor imagery. Utilizing two RL agents, the framework adapts dynamically to EEG non-stationarities. Validation was conducted using a publicly available motor imagery dataset and a fast-paced game designed to boost user engagement. Results show the framework's promise, with RL agents learning control policies from user interactions and achieving robust performance across datasets. However, a critical insight from the game-based protocol revealed that motor imagery in a high-speed interaction paradigm was largely ineffective for participants, highlighting task design limitations in real-time BCI applications. These findings underscore the potential of RL for adaptive BCIs while pointing out practical constraints related to task complexity and user responsiveness."
    },
    "2502.17873v1": {
      "title": "EEGM2: An Efficient Mamba-2-Based Self-Supervised Framework for Long-Sequence EEG Modeling",
      "url": "http://arxiv.org/abs/2502.17873v1",
      "authors": "Jiazhen Hong, Geoffrey Mackellar, Soheila Ghane",
      "update_time": "2025-02-25",
      "abstract": "Deep learning has achieved significant progress in the development of electroencephalogram (EEG) foundation models, with Transformer-based architectures excelling at capturing long-range dependencies. However, their quadratic computational complexity presents challenges in memory efficiency, training, and inference speed, limiting their scalability and generalizability as a foundation model. In this paper, we propose EEGM2, a self-supervised framework based on structured state space duality (SSD) that overcomes these limitations. EEGM2 introduces three key innovations: (1) a reconstruction-based framework that captures both local and global EEG features through Mamba-2 structured state space models, (2) a spatiotemporal-aware loss function that enhances robustness to noise and preserves spectral information, and (3) a multi-branch receptive field input embedding strategy that improves cross-subject generalization and stability for EEG sequences of varying lengths. In comparison to traditional pretraining methods, on raw EEG or latent representation spaces, EEGM2 shows superior performance on long-sequence tasks, where conventional models struggle. Our experimental results on six EEG datasets validate that EEGM2 not only achieves state-of-the-art cross-domain accuracy but also reduces computational overhead, making it a more efficient solution for deployment on resource-constrained BCI devices.",
      "code_url": "https://github.com/Jiazhen-Hong/EEGM2"
    }
  },
  "fMRI": {
    "2503.15218v2": {
      "title": "Functional Correspondences in the Human and Marmoset Visual Cortex During Movie Watching: Insights from Correlation, Redundancy, and Synergy",
      "url": "http://arxiv.org/abs/2503.15218v2",
      "authors": "Qiang Li, Ting Xu, Vince D. Calhoun",
      "update_time": "2025-03-20",
      "abstract": "The world of beauty is deeply connected to the visual cortex, as perception often begins with vision in both humans and marmosets. Quantifying functional correspondences in the visual cortex across species can help us understand how information is processed in the primate visual cortex, while also providing deeper insights into human visual cortex functions through the study of marmosets. In this study, we measured pairwise and beyond pairwise correlation, redundancy, and synergy in movie-driven fMRI data across species. Our first key finding was that humans and marmosets exhibited significant overlaps in functional synergy. Second, we observed that the strongest functional correspondences between the human peri-entorhinal and entorhinal cortex (PeEc) and the occipitotemporal higher-level visual regions in the marmoset during movie watching reflected a functional synergistic relationship. These regions are known to correspond to face-selective areas in both species. Third, redundancy measures maintained stable high-order hubs, indicating a steady core of shared information processing, while synergy measures revealed a dynamic shift from low- to high-level visual regions as interaction increased, reflecting adaptive integration. This highlights distinct patterns of information processing across the visual hierarchy. Ultimately, our results reveal the marmoset as a compelling model for investigating visual perception, distinguished by its remarkable functional parallels to the human visual cortex."
    },
    "2503.15012v1": {
      "title": "Benchmarking Brain Connectivity Graph Inference: A Novel Validation Approach",
      "url": "http://arxiv.org/abs/2503.15012v1",
      "authors": "Alice Chevaux, Ali Fahkar, K\u00e9vin Polisano, Ir\u00e8ne Gannaz, Sophie Achard",
      "update_time": "2025-03-19",
      "abstract": "Inferring a binary connectivity graph from resting-state fMRI data for a single subject requires making several methodological choices and assumptions that can significantly affect the results. In this study, we investigate the robustness of existing edge detection methods when relaxing a common assumption: the sparsity of the graph. We propose a new pipeline to generate synthetic data and to benchmark the state of the art in graph inference. Simulated correlation matrices are designed to have a set of given zeros and a constraint on the signal-to-noise ratio. We compare approaches based on covariance or precision matrices, emphasizing their implications for connectivity inference. This framework allows us to assess the sensitivity of connectivity estimations and edge detection methods to different parameters."
    },
    "2503.14700v1": {
      "title": "From Density to Void: Why Brain Networks Fail to Reveal Complex Higher-Order Structures",
      "url": "http://arxiv.org/abs/2503.14700v1",
      "authors": "Moo K. Chung, Anass B. El-Yaagoubi, Anqi Qiu, Hernando Ombao",
      "update_time": "2025-03-18",
      "abstract": "In brain network analysis using resting-state fMRI, there is growing interest in modeling higher-order interactions beyond simple pairwise connectivity via persistent homology. Despite the promise of these advanced topological tools, robust and consistently observed higher-order interactions over time remain elusive. In this study, we investigate why conventional analyses often fail to reveal complex higher-order structures - such as interactions involving four or more nodes - and explore whether such interactions truly exist in functional brain networks. We utilize a simplicial complex framework often used in persistent homology to address this question."
    },
    "2503.14655v1": {
      "title": "Core-Periphery Principle Guided State Space Model for Functional Connectome Classification",
      "url": "http://arxiv.org/abs/2503.14655v1",
      "authors": "Minheng Chen, Xiaowei Yu, Jing Zhang, Tong Chen, Chao Cao, Yan Zhuang, Yanjun Lyu, Lu Zhang, Tianming Liu, Dajiang Zhu",
      "update_time": "2025-03-18",
      "abstract": "Understanding the organization of human brain networks has become a central focus in neuroscience, particularly in the study of functional connectivity, which plays a crucial role in diagnosing neurological disorders. Advances in functional magnetic resonance imaging and machine learning techniques have significantly improved brain network analysis. However, traditional machine learning approaches struggle to capture the complex relationships between brain regions, while deep learning methods, particularly Transformer-based models, face computational challenges due to their quadratic complexity in long-sequence modeling. To address these limitations, we propose a Core-Periphery State-Space Model (CP-SSM), an innovative framework for functional connectome classification. Specifically, we introduce Mamba, a selective state-space model with linear complexity, to effectively capture long-range dependencies in functional brain networks. Furthermore, inspired by the core-periphery (CP) organization, a fundamental characteristic of brain networks that enhances efficient information transmission, we design CP-MoE, a CP-guided Mixture-of-Experts that improves the representation learning of brain connectivity patterns. We evaluate CP-SSM on two benchmark fMRI datasets: ABIDE and ADNI. Experimental results demonstrate that CP-SSM surpasses Transformer-based models in classification performance while significantly reducing computational complexity. These findings highlight the effectiveness and efficiency of CP-SSM in modeling brain functional connectivity, offering a promising direction for neuroimaging-based neurological disease diagnosis."
    },
    "2503.13981v1": {
      "title": "Neural Constraints on Cognitive Experience and Mental Health",
      "url": "http://arxiv.org/abs/2503.13981v1",
      "authors": "Bita Shariatpanahi, Erfan Nozari, Soroush Daftarian, Fahimeh Arab, Mina Kheirkhah, Felix P. Bernhard, Shiva Khodadadi, Erik J. Giltay, Kaat Hebbrecht, Stefan G. Hofmann, Tim Hahn, Hamidreza Jamalabadi",
      "update_time": "2025-03-18",
      "abstract": "Understanding how neural dynamics shape cognitive experiences remains a central challenge in neuroscience and psychiatry. Here, we present a novel framework leveraging state-to-output controllability from dynamical systems theory to model the interplay between cognitive perturbations, neural activity, and subjective experience. We demonstrate that large-scale fMRI signals are constrained to low-dimensional manifolds, where affective and cognitive states are naturally organized. Furthermore, we provide a theoretically robust method to estimate the controllability Gramian from steady-state neural responses, offering a direct measure of the energy required to steer cognitive outcomes. In five healthy participants viewing 2,185 emotionally evocative short videos, our analyses reveal a strong alignment between neural activations and affective ratings, with an average correlation of $r \\approx 0.7$. In a clinical cohort of 255 patients with major depressive disorder, biweekly Hamilton Rating Scale trajectories over 11 weeks significantly mapped onto these manifolds, explaining approximately 20% more variance than chance ($p < 10^{-10}$, numerically better than chance in 93% reaching statistical significance in one-third of subjects). Our work bridges dynamical systems theory and clinical neuroscience, providing a principled approach to optimize mental health treatments by targeting the most efficient neural pathways for cognitive change."
    },
    "2503.11283v1": {
      "title": "Brain Effective Connectivity Estimation via Fourier Spatiotemporal Attention",
      "url": "http://arxiv.org/abs/2503.11283v1",
      "authors": "Wen Xiong, Jinduo Liu, Junzhong Ji, Fenglong Ma",
      "update_time": "2025-03-14",
      "abstract": "Estimating brain effective connectivity (EC) from functional magnetic resonance imaging (fMRI) data can aid in comprehending the neural mechanisms underlying human behavior and cognition, providing a foundation for disease diagnosis. However, current spatiotemporal attention modules handle temporal and spatial attention separately, extracting temporal and spatial features either sequentially or in parallel. These approaches overlook the inherent spatiotemporal correlations present in real world fMRI data. Additionally, the presence of noise in fMRI data further limits the performance of existing methods. In this paper, we propose a novel brain effective connectivity estimation method based on Fourier spatiotemporal attention (FSTA-EC), which combines Fourier attention and spatiotemporal attention to simultaneously capture inter-series (spatial) dynamics and intra-series (temporal) dependencies from high-noise fMRI data. Specifically, Fourier attention is designed to convert the high-noise fMRI data to frequency domain, and map the denoised fMRI data back to physical domain, and spatiotemporal attention is crafted to simultaneously learn spatiotemporal dynamics. Furthermore, through a series of proofs, we demonstrate that incorporating learnable filter into fast Fourier transform and inverse fast Fourier transform processes is mathematically equivalent to performing cyclic convolution. The experimental results on simulated and real-resting-state fMRI datasets demonstrate that the proposed method exhibits superior performance when compared to state-of-the-art methods.",
      "code_url": "https://github.com/XiongWenXww/FSTA"
    },
    "2503.11167v1": {
      "title": "Neurons: Emulating the Human Visual Cortex Improves Fidelity and Interpretability in fMRI-to-Video Reconstruction",
      "url": "http://arxiv.org/abs/2503.11167v1",
      "authors": "Haonan Wang, Qixiang Zhang, Lehan Wang, Xuanqi Huang, Xiaomeng Li",
      "update_time": "2025-03-14",
      "abstract": "Decoding visual stimuli from neural activity is essential for understanding the human brain. While fMRI methods have successfully reconstructed static images, fMRI-to-video reconstruction faces challenges due to the need for capturing spatiotemporal dynamics like motion and scene transitions. Recent approaches have improved semantic and perceptual alignment but struggle to integrate coarse fMRI data with detailed visual features. Inspired by the hierarchical organization of the visual system, we propose NEURONS, a novel framework that decouples learning into four correlated sub-tasks: key object segmentation, concept recognition, scene description, and blurry video reconstruction. This approach simulates the visual cortex's functional specialization, allowing the model to capture diverse video content. In the inference stage, NEURONS generates robust conditioning signals for a pre-trained text-to-video diffusion model to reconstruct the videos. Extensive experiments demonstrate that NEURONS outperforms state-of-the-art baselines, achieving solid improvements in video consistency (26.6%) and semantic-level accuracy (19.1%). Notably, NEURONS shows a strong functional correlation with the visual cortex, highlighting its potential for brain-computer interfaces and clinical applications. Code and model weights will be available at: https://github.com/xmed-lab/NEURONS."
    },
    "2503.06940v1": {
      "title": "CineBrain: A Large-Scale Multi-Modal Brain Dataset During Naturalistic Audiovisual Narrative Processing",
      "url": "http://arxiv.org/abs/2503.06940v1",
      "authors": "Jianxiong Gao, Yichang Liu, Baofeng Yang, Jianfeng Feng, Yanwei Fu",
      "update_time": "2025-03-10",
      "abstract": "In this paper, we introduce CineBrain, the first large-scale dataset featuring simultaneous EEG and fMRI recordings during dynamic audiovisual stimulation. Recognizing the complementary strengths of EEG's high temporal resolution and fMRI's deep-brain spatial coverage, CineBrain provides approximately six hours of narrative-driven content from the popular television series The Big Bang Theory for each of six participants. Building upon this unique dataset, we propose CineSync, an innovative multimodal decoding framework integrates a Multi-Modal Fusion Encoder with a diffusion-based Neural Latent Decoder. Our approach effectively fuses EEG and fMRI signals, significantly improving the reconstruction quality of complex audiovisual stimuli. To facilitate rigorous evaluation, we introduce Cine-Benchmark, a comprehensive evaluation protocol that assesses reconstructions across semantic and perceptual dimensions. Experimental results demonstrate that CineSync achieves state-of-the-art video reconstruction performance and highlight our initial success in combining fMRI and EEG for reconstructing both video and audio stimuli. Project Page: https://jianxgao.github.io/CineBrain."
    },
    "2503.06286v1": {
      "title": "A 7T fMRI dataset of synthetic images for out-of-distribution modeling of vision",
      "url": "http://arxiv.org/abs/2503.06286v1",
      "authors": "Alessandro T. Gifford, Radoslaw M. Cichy, Thomas Naselaris, Kendrick Kay",
      "update_time": "2025-03-08",
      "abstract": "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD) are boosting NeuroAI research by enabling computational models of the brain with performances beyond what was possible just a decade ago. However, these datasets lack out-of-distribution (OOD) components, which are crucial for the development of more robust models. Here, we address this limitation by releasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the eight NSD subjects for 284 carefully controlled synthetic images. We show that NSD-synthetic's fMRI responses reliably encode stimulus-related information and are OOD with respect to NSD. Furthermore, OOD generalization tests on NSD-synthetic reveal differences between models of the brain that are not detected with NSD - specifically, self-supervised deep neural networks better explain neural responses than their task-supervised counterparts. These results showcase how NSD-synthetic enables OOD generalization tests that facilitate the development of more robust models of visual processing, and the formulation of more accurate theories of human vision."
    },
    "2503.04205v1": {
      "title": "Learning 3D Medical Image Models From Brain Functional Connectivity Network Supervision For Mental Disorder Diagnosis",
      "url": "http://arxiv.org/abs/2503.04205v1",
      "authors": "Xingcan Hu, Wei Wang, Li Xiao",
      "update_time": "2025-03-06",
      "abstract": "In MRI-based mental disorder diagnosis, most previous studies focus on functional connectivity network (FCN) derived from functional MRI (fMRI). However, the small size of annotated fMRI datasets restricts its wide application. Meanwhile, structural MRIs (sMRIs), such as 3D T1-weighted (T1w) MRI, which are commonly used and readily accessible in clinical settings, are often overlooked. To integrate the complementary information from both function and structure for improved diagnostic accuracy, we propose CINP (Contrastive Image-Network Pre-training), a framework that employs contrastive learning between sMRI and FCN. During pre-training, we incorporate masked image modeling and network-image matching to enhance visual representation learning and modality alignment. Since the CINP facilitates knowledge transfer from FCN to sMRI, we introduce network prompting. It utilizes only sMRI from suspected patients and a small amount of FCNs from different patient classes for diagnosing mental disorders, which is practical in real-world clinical scenario. The competitive performance on three mental disorder diagnosis tasks demonstrate the effectiveness of the CINP in integrating multimodal MRI information, as well as the potential of incorporating sMRI into clinical diagnosis using network prompting."
    }
  },
  "MEG": {
    "2503.10965v1": {
      "title": "Auditing language models for hidden objectives",
      "url": "http://arxiv.org/abs/2503.10965v1",
      "authors": "Samuel Marks, Johannes Treutlein, Trenton Bricken, Jack Lindsey, Jonathan Marcus, Siddharth Mishra-Sharma, Daniel Ziegler, Emmanuel Ameisen, Joshua Batson, Tim Belonax, Samuel R. Bowman, Shan Carter, Brian Chen, Hoagy Cunningham, Carson Denison, Florian Dietz, Satvik Golechha, Akbir Khan, Jan Kirchner, Jan Leike, Austin Meek, Kei Nishimura-Gasparian, Euan Ong, Christopher Olah, Adam Pearce, Fabien Roger, Jeanne Salle, Andy Shih, Meg Tong, Drake Thomas, Kelley Rivoire, Adam Jermyn, Monte MacDiarmid, Tom Henighan, Evan Hubinger",
      "update_time": "2025-03-14",
      "abstract": "We study the feasibility of conducting alignment audits: investigations into whether models have undesired objectives. As a testbed, we train a language model with a hidden objective. Our training pipeline first teaches the model about exploitable errors in RLHF reward models (RMs), then trains the model to exploit some of these errors. We verify via out-of-distribution evaluations that the model generalizes to exhibit whatever behaviors it believes RMs rate highly, including ones not reinforced during training. We leverage this model to study alignment audits in two ways. First, we conduct a blind auditing game where four teams, unaware of the model's hidden objective or training, investigate it for concerning behaviors and their causes. Three teams successfully uncovered the model's hidden objective using techniques including interpretability with sparse autoencoders (SAEs), behavioral attacks, and training data analysis. Second, we conduct an unblinded follow-up study of eight techniques for auditing the model, analyzing their strengths and limitations. Overall, our work provides a concrete example of using alignment audits to discover a model's hidden objective and proposes a methodology for practicing and validating progress in alignment auditing."
    },
    "2503.06086v1": {
      "title": "Characterizing optimal monitoring edge-geodetic sets for some structured graph classes",
      "url": "http://arxiv.org/abs/2503.06086v1",
      "authors": "Florent Foucaud, Arti Pandey, Kaustav Paul",
      "update_time": "2025-03-08",
      "abstract": "Given a graph $G=(V,E)$, a set $S\\subseteq V$ is said to be a monitoring edge-geodetic set if the deletion of any edge in the graph results in a change in the distance between at least one pair of vertices in $S$. The minimum size of such a set in $G$ is called the monitoring edge-geodetic number of $G$ and is denoted by $meg(G)$.   In this work, we compute the monitoring edge-geodetic number efficiently for the following graph classes: distance-hereditary graphs, $P_4$-sparse graphs, bipartite permutation graphs, and strongly chordal graphs. The algorithms follow from structural characterizations of the optimal monitoring edge-geodetic sets for these graph classes in terms of \\emph{mandatory vertices} (those that need to be in every solution). This extends previous results from the literature for cographs, interval graphs and block graphs."
    },
    "2503.05211v1": {
      "title": "Language-specific Tonal Features Drive Speaker-Listener Neural Synchronization",
      "url": "http://arxiv.org/abs/2503.05211v1",
      "authors": "Chen Hong, Xiangbin Teng, Yu Li, Shen-Mou Hsu, Feng-Ming Tsao, Patrick C. M. Wong, Gangyi Feng",
      "update_time": "2025-03-07",
      "abstract": "Verbal communication transmits information across diverse linguistic levels, with neural synchronization (NS) between speakers and listeners emerging as a putative mechanism underlying successful exchange. However, the specific speech features driving this synchronization and how language-specific versus universal characteristics facilitate information transfer remain poorly understood. We developed a novel content-based interbrain encoding model to disentangle the contributions of acoustic and linguistic features to speaker-listener NS during Mandarin storytelling and listening, as measured via magnetoencephalography (MEG). Results revealed robust NS throughout frontotemporal-parietal networks with systematic time lags between speech production and perception. Crucially, suprasegmental lexical tone features (tone categories, pitch height, and pitch contour), essential for lexical meaning in Mandarin, contributed more significantly to NS than either acoustic elements or universal segmental units (consonants and vowels). These tonal features generated distinctive spatiotemporal NS patterns, creating language-specific neural \"communication channels\" that facilitated efficient representation sharing between interlocutors. Furthermore, the strength and patterns of NS driven by these language-specific features predicted communication success. These findings demonstrate the neural mechanisms underlying shared representations during verbal exchange and highlight how language-specific features can shape neural coupling to optimize information transfer during human communication."
    },
    "2502.16797v1": {
      "title": "Forecasting Rare Language Model Behaviors",
      "url": "http://arxiv.org/abs/2502.16797v1",
      "authors": "Erik Jones, Meg Tong, Jesse Mu, Mohammed Mahfoud, Jan Leike, Roger Grosse, Jared Kaplan, William Fithian, Ethan Perez, Mrinank Sharma",
      "update_time": "2025-02-24",
      "abstract": "Standard language model evaluations can fail to capture risks that emerge only at deployment scale. For example, a model may produce safe responses during a small-scale beta test, yet reveal dangerous information when processing billions of requests at deployment. To remedy this, we introduce a method to forecast potential risks across orders of magnitude more queries than we test during evaluation. We make forecasts by studying each query's elicitation probability -- the probability the query produces a target behavior -- and demonstrate that the largest observed elicitation probabilities predictably scale with the number of queries. We find that our forecasts can predict the emergence of diverse undesirable behaviors -- such as assisting users with dangerous chemical synthesis or taking power-seeking actions -- across up to three orders of magnitude of query volume. Our work enables model developers to proactively anticipate and patch rare failures before they manifest during large-scale deployments."
    },
    "2502.17480v1": {
      "title": "Brain-to-Text Decoding: A Non-invasive Approach via Typing",
      "url": "http://arxiv.org/abs/2502.17480v1",
      "authors": "Jarod L\u00e9vy, Mingfang Zhang, Svetlana Pinet, J\u00e9r\u00e9my Rapin, Hubert Banville, St\u00e9phane d'Ascoli, Jean-R\u00e9mi King",
      "update_time": "2025-02-18",
      "abstract": "Modern neuroprostheses can now restore communication in patients who have lost the ability to speak or move. However, these invasive devices entail risks inherent to neurosurgery. Here, we introduce a non-invasive method to decode the production of sentences from brain activity and demonstrate its efficacy in a cohort of 35 healthy volunteers. For this, we present Brain2Qwerty, a new deep learning architecture trained to decode sentences from either electro- (EEG) or magneto-encephalography (MEG), while participants typed briefly memorized sentences on a QWERTY keyboard. With MEG, Brain2Qwerty reaches, on average, a character-error-rate (CER) of 32% and substantially outperforms EEG (CER: 67%). For the best participants, the model achieves a CER of 19%, and can perfectly decode a variety of sentences outside of the training set. While error analyses suggest that decoding depends on motor processes, the analysis of typographical errors suggests that it also involves higher-level cognitive factors. Overall, these results narrow the gap between invasive and non-invasive methods and thus open the path for developing safe brain-computer interfaces for non-communicating patients."
    },
    "2502.07429v2": {
      "title": "From Thought to Action: How a Hierarchy of Neural Dynamics Supports Language Production",
      "url": "http://arxiv.org/abs/2502.07429v2",
      "authors": "Mingfang Zhang, Jarod L\u00e9vy, St\u00e9phane d'Ascoli, J\u00e9r\u00e9my Rapin, F. -Xavier Alario, Pierre Bourdillon, Svetlana Pinet, Jean-R\u00e9mi King",
      "update_time": "2025-02-18",
      "abstract": "Humans effortlessly communicate their thoughts through intricate sequences of motor actions. Yet, the neural processes that coordinate language production remain largely unknown, in part because speech artifacts limit the use of neuroimaging. To elucidate the unfolding of language production in the brain, we investigate with magnetoencephalography (MEG) and electroencephalography (EEG) the neurophysiological activity of 35 skilled typists, while they typed sentences on a keyboard. This approach confirms the hierarchical predictions of linguistic theories: the neural activity preceding the production of each word is marked by the sequential rise and fall of context-, word-, syllable-, and letter-level representations. Remarkably, each of these neural representations is maintained over long time periods within each level of the language hierarchy. This phenomenon results in a superposition of successive representations that is supported by a hierarchy of dynamic neural codes. Overall, these findings provide a precise computational breakdown of the neural dynamics that coordinate the production of language in the human brain."
    },
    "2502.05161v1": {
      "title": "Estimated Roadway Segment Traffic Data by Vehicle Class for the United States: A Machine Learning Approach",
      "url": "http://arxiv.org/abs/2502.05161v1",
      "authors": "Brittany Antonczak, Meg Fay, Aviral Chawla, Gregory Rowangould",
      "update_time": "2025-02-07",
      "abstract": "The Highway Performance Monitoring System, managed by the Federal Highway Administration, provides essential data on average annual daily traffic across U.S. roadways, but it has limited representation of medium- and heavy-duty vehicles on non-interstate roads. This gap limits research and policy analysis on the impacts of truck traffic, especially concerning air quality and public health. To address this, we use random forest regression to estimate medium- and heavy-duty vehicle traffic volumes in areas with sparse data. This results in a more comprehensive dataset, which enables the estimation of traffic density at the census block level as a proxy for traffic-related air pollution exposure. Our high-resolution spatial data products, rigorously validated, provide a more accurate representation of truck traffic and its environmental and health impacts. These datasets are valuable for transportation planning, public health research, and policy decisions aimed at mitigating the effects of truck traffic on vulnerable communities exposed to air pollution."
    },
    "2502.04658v1": {
      "title": "Shifting Attention to You: Personalized Brain-Inspired AI Models",
      "url": "http://arxiv.org/abs/2502.04658v1",
      "authors": "Stephen Chong Zhao, Yang Hu, Jason Lee, Andrew Bender, Trisha Mazumdar, Mark Wallace, David A. Tovar",
      "update_time": "2025-02-07",
      "abstract": "The integration of human and artificial intelligence represents a scientific opportunity to advance our understanding of information processing, as each system offers unique computational insights that can enhance and inform the other. The synthesis of human cognitive principles with artificial intelligence has the potential to produce more interpretable and functionally aligned computational models, while simultaneously providing a formal framework for investigating the neural mechanisms underlying perception, learning, and decision-making through systematic model comparisons and representational analyses. In this study, we introduce personalized brain-inspired modeling that integrates human behavioral embeddings and neural data to align with cognitive processes. We took a stepwise approach, fine-tuning the Contrastive Language-Image Pre-training (CLIP) model with large-scale behavioral decisions, group-level neural data, and finally, participant-level neural data within a broader framework that we have named CLIP-Human-Based Analysis (CLIP-HBA). We found that fine-tuning on behavioral data enhances its ability to predict human similarity judgments while indirectly aligning it with dynamic representations captured via MEG. To further gain mechanistic insights into the temporal evolution of cognitive processes, we introduced a model specifically fine-tuned on millisecond-level MEG neural dynamics (CLIP-HBA-MEG). This model resulted in enhanced temporal alignment with human neural processing while still showing improvement on behavioral alignment. Finally, we trained individualized models on participant-specific neural data, effectively capturing individualized neural dynamics and highlighting the potential for personalized AI systems. These personalized systems have far-reaching implications for the fields of medicine, cognitive research, human-computer interfaces, and AI development."
    },
    "2502.04258v1": {
      "title": "Detecting Mild Traumatic Brain Injury with MEG Scan Data: One-vs-K-Sample Tests",
      "url": "http://arxiv.org/abs/2502.04258v1",
      "authors": "Jian Zhang, Gary Green",
      "update_time": "2025-02-06",
      "abstract": "Magnetoencephalography (MEG) scanner has been shown to be more accurate than other medical devices in detecting mild traumatic brain injury (mTBI). However, MEG scan data in certain spectrum ranges can be skewed, multimodal and heterogeneous which can mislead the conventional case-control analysis that requires the data to be homogeneous and normally distributed within the control group. To meet this challenge, we propose a flexible one-vs-K-sample testing procedure for detecting brain injury for a single-case versus heterogeneous controls. The new procedure begins with source magnitude imaging using MEG scan data in frequency domain, followed by region-wise contrast tests for abnormality between the case and controls. The critical values for these tests are automatically determined by cross-validation. We adjust the testing results for heterogeneity effects by similarity analysis. An asymptotic theory is established for the proposed test statistic. By simulated and real data analyses in the context of neurotrauma, we show that the proposed test outperforms commonly used nonparametric methods in terms of overall accuracy and ability in accommodating data non-normality and subject-heterogeneity."
    },
    "2501.18837v1": {
      "title": "Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming",
      "url": "http://arxiv.org/abs/2501.18837v1",
      "authors": "Mrinank Sharma, Meg Tong, Jesse Mu, Jerry Wei, Jorrit Kruthoff, Scott Goodfriend, Euan Ong, Alwin Peng, Raj Agarwal, Cem Anil, Amanda Askell, Nathan Bailey, Joe Benton, Emma Bluemke, Samuel R. Bowman, Eric Christiansen, Hoagy Cunningham, Andy Dau, Anjali Gopal, Rob Gilson, Logan Graham, Logan Howard, Nimit Kalra, Taesung Lee, Kevin Lin, Peter Lofgren, Francesco Mosconi, Clare O'Hara, Catherine Olsson, Linda Petrini, Samir Rajani, Nikhil Saxena, Alex Silverstein, Tanya Singh, Theodore Sumers, Leonard Tang, Kevin K. Troy, Constantin Weisser, Ruiqi Zhong, Giulio Zhou, Jan Leike, Jared Kaplan, Ethan Perez",
      "update_time": "2025-01-31",
      "abstract": "Large language models (LLMs) are vulnerable to universal jailbreaks-prompting strategies that systematically bypass model safeguards and enable users to carry out harmful processes that require many model interactions, like manufacturing illegal substances at scale. To defend against these attacks, we introduce Constitutional Classifiers: safeguards trained on synthetic data, generated by prompting LLMs with natural language rules (i.e., a constitution) specifying permitted and restricted content. In over 3,000 estimated hours of red teaming, no red teamer found a universal jailbreak that could extract information from an early classifier-guarded LLM at a similar level of detail to an unguarded model across most target queries. On automated evaluations, enhanced classifiers demonstrated robust defense against held-out domain-specific jailbreaks. These classifiers also maintain deployment viability, with an absolute 0.38% increase in production-traffic refusals and a 23.7% inference overhead. Our work demonstrates that defending against universal jailbreaks while maintaining practical deployment viability is tractable."
    }
  },
  "neuroAI": {
    "2503.06286v1": {
      "title": "A 7T fMRI dataset of synthetic images for out-of-distribution modeling of vision",
      "url": "http://arxiv.org/abs/2503.06286v1",
      "authors": "Alessandro T. Gifford, Radoslaw M. Cichy, Thomas Naselaris, Kendrick Kay",
      "update_time": "2025-03-08",
      "abstract": "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD) are boosting NeuroAI research by enabling computational models of the brain with performances beyond what was possible just a decade ago. However, these datasets lack out-of-distribution (OOD) components, which are crucial for the development of more robust models. Here, we address this limitation by releasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the eight NSD subjects for 284 carefully controlled synthetic images. We show that NSD-synthetic's fMRI responses reliably encode stimulus-related information and are OOD with respect to NSD. Furthermore, OOD generalization tests on NSD-synthetic reveal differences between models of the brain that are not detected with NSD - specifically, self-supervised deep neural networks better explain neural responses than their task-supervised counterparts. These results showcase how NSD-synthetic enables OOD generalization tests that facilitate the development of more robust models of visual processing, and the formulation of more accurate theories of human vision."
    },
    "2502.16238v1": {
      "title": "Brain-Model Evaluations Need the NeuroAI Turing Test",
      "url": "http://arxiv.org/abs/2502.16238v1",
      "authors": "Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi",
      "update_time": "2025-02-22",
      "abstract": "What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \\emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development."
    },
    "2501.02402v1": {
      "title": "Asynchronous Hebbian/anti-Hebbian networks",
      "url": "http://arxiv.org/abs/2501.02402v1",
      "authors": "Henrique Reis Aguiar, Matthias H. Hennig",
      "update_time": "2025-01-04",
      "abstract": "Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.",
      "code_url": "https://github.com/henri-edinb/async_learning"
    },
    "2411.18526v1": {
      "title": "NeuroAI for AI Safety",
      "url": "http://arxiv.org/abs/2411.18526v1",
      "authors": "Patrick Mineault, Niccol\u00f2 Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias",
      "update_time": "2024-11-27",
      "abstract": "As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety."
    },
    "2411.14633v1": {
      "title": "Evaluating Representational Similarity Measures from the Lens of Functional Correspondence",
      "url": "http://arxiv.org/abs/2411.14633v1",
      "authors": "Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla",
      "update_time": "2024-11-21",
      "abstract": "Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research."
    },
    "2410.19315v1": {
      "title": "A prescriptive theory for brain-like inference",
      "url": "http://arxiv.org/abs/2410.19315v1",
      "authors": "Hadi Vafaii, Dekel Galor, Jacob L. Yates",
      "update_time": "2024-10-25",
      "abstract": "The Evidence Lower Bound (ELBO) is a widely used objective for training deep generative models, such as Variational Autoencoders (VAEs). In the neuroscience literature, an identical objective is known as the variational free energy, hinting at a potential unified framework for brain function and machine learning. Despite its utility in interpreting generative models, including diffusion models, ELBO maximization is often seen as too broad to offer prescriptive guidance for specific architectures in neuroscience or machine learning. In this work, we show that maximizing ELBO under Poisson assumptions for general sequence data leads to a spiking neural network that performs Bayesian posterior inference through its membrane potential dynamics. The resulting model, the iterative Poisson VAE (iP-VAE), has a closer connection to biological neurons than previous brain-inspired predictive coding models based on Gaussian assumptions. Compared to amortized and iterative VAEs, iP-VAElearns sparser representations and exhibits superior generalization to out-of-distribution samples. These findings suggest that optimizing ELBO, combined with Poisson assumptions, provides a solid foundation for developing prescriptive theories in NeuroAI."
    },
    "2409.05771v1": {
      "title": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models",
      "url": "http://arxiv.org/abs/2409.05771v1",
      "authors": "Emily Cheng, Richard J. Antonello",
      "update_time": "2024-09-09",
      "abstract": "Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first \"composition\" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties."
    },
    "2407.04117v2": {
      "title": "Predictive Coding Networks and Inference Learning: Tutorial and Survey",
      "url": "http://arxiv.org/abs/2407.04117v2",
      "authors": "Bj\u00f6rn van Zwol, Ro Jefferson, Egon L. van den Broek",
      "update_time": "2024-07-22",
      "abstract": "Recent years have witnessed a growing call for renewed emphasis on neuroscience-inspired approaches in artificial intelligence research, under the banner of NeuroAI. A prime example of this is predictive coding networks (PCNs), based on the neuroscientific framework of predictive coding. This framework views the brain as a hierarchical Bayesian inference model that minimizes prediction errors through feedback connections. Unlike traditional neural networks trained with backpropagation (BP), PCNs utilize inference learning (IL), a more biologically plausible algorithm that explains patterns of neural activity that BP cannot. Historically, IL has been more computationally intensive, but recent advancements have demonstrated that it can achieve higher efficiency than BP with sufficient parallelization. Furthermore, PCNs can be mathematically considered a superset of traditional feedforward neural networks (FNNs), significantly extending the range of trainable architectures. As inherently probabilistic (graphical) latent variable models, PCNs provide a versatile framework for both supervised learning and unsupervised (generative) modeling that goes beyond traditional artificial neural networks. This work provides a comprehensive review and detailed formal specification of PCNs, particularly situating them within the context of modern ML methods. Additionally, we introduce a Python library (PRECO) for practical implementation. This positions PC as a promising framework for future ML innovations."
    },
    "2306.10168v3": {
      "title": "Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis",
      "url": "http://arxiv.org/abs/2306.10168v3",
      "authors": "Mitchell Ostrow, Adam Eisen, Leo Kozachkov, Ila Fiete",
      "update_time": "2023-10-29",
      "abstract": "How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.",
      "code_url": "https://github.com/mitchellostrow/dsa"
    },
    "2305.11275v2": {
      "title": "Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture",
      "url": "http://arxiv.org/abs/2305.11275v2",
      "authors": "Galen Pogoncheff, Jacob Granley, Michael Beyeler",
      "update_time": "2023-05-25",
      "abstract": "Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence."
    }
  },
  "medical": {
    "2503.15454v1": {
      "title": "Evaluating Bias in Retrieval-Augmented Medical Question-Answering Systems",
      "url": "http://arxiv.org/abs/2503.15454v1",
      "authors": "Yuelyu Ji, Hang Zhang, Yanshan Wang",
      "update_time": "2025-03-19",
      "abstract": "Medical QA systems powered by Retrieval-Augmented Generation (RAG) models support clinical decision-making but may introduce biases related to race, gender, and social determinants of health. We systematically evaluate biases in RAG-based LLM by examining demographic-sensitive queries and measuring retrieval discrepancies. Using datasets like MMLU and MedMCQA, we analyze retrieval overlap and correctness disparities. Our findings reveal substantial demographic disparities within RAG pipelines, emphasizing the critical need for retrieval methods that explicitly account for fairness to ensure equitable clinical decision-making."
    },
    "2503.15390v1": {
      "title": "FedSCA: Federated Tuning with Similarity-guided Collaborative Aggregation for Heterogeneous Medical Image Segmentation",
      "url": "http://arxiv.org/abs/2503.15390v1",
      "authors": "Yumin Zhang, Yan Gao, Haoran Duan, Hanqing Guo, Tejal Shah, Rajiv Ranjan, Bo Wei",
      "update_time": "2025-03-19",
      "abstract": "Transformer-based foundation models (FMs) have recently demonstrated remarkable performance in medical image segmentation. However, scaling these models is challenging due to the limited size of medical image datasets within isolated hospitals, where data centralization is restricted due to privacy concerns. These constraints, combined with the data-intensive nature of FMs, hinder their broader application. Integrating federated learning (FL) with foundation models (FLFM) fine-tuning offers a potential solution to these challenges by enabling collaborative model training without data sharing, thus allowing FMs to take advantage of a diverse pool of sensitive medical image data across hospitals/clients. However, non-independent and identically distributed (non-IID) data among clients, paired with computational and communication constraints in federated environments, presents an additional challenge that limits further performance improvements and remains inadequately addressed in existing studies. In this work, we propose a novel FLFM fine-tuning framework, \\underline{\\textbf{Fed}}erated tuning with \\underline{\\textbf{S}}imilarity-guided \\underline{\\textbf{C}}ollaborative \\underline{\\textbf{A}}ggregation (FedSCA), encompassing all phases of the FL process. This includes (1) specially designed parameter-efficient fine-tuning (PEFT) for local client training to enhance computational efficiency; (2) partial low-level adapter transmission for communication efficiency; and (3) similarity-guided collaborative aggregation (SGCA) on the server side to address non-IID issues. Extensive experiments on three FL benchmarks for medical image segmentation demonstrate the effectiveness of our proposed FedSCA, establishing new SOTA performance."
    },
    "2503.15383v1": {
      "title": "Material Decomposition in Photon-Counting Computed Tomography with Diffusion Models: Comparative Study and Hybridization with Variational Regularizers",
      "url": "http://arxiv.org/abs/2503.15383v1",
      "authors": "Corentin Vazia, Thore Dassow, Alexandre Bousse, Jacques Froment, B\u00e9atrice Vedel, Franck Vermet, Alessandro Perelli, Jean-Pierre Tasu, Dimitris Visvikis",
      "update_time": "2025-03-19",
      "abstract": "Photon-counting computed tomography (PCCT) has emerged as a promising imaging technique, enabling spectral imaging and material decomposition (MD). However, images typically suffer from a low signal-to-noise ratio due to constraints such as low photon counts and sparse-view settings. Variational methods minimize a data-fit function coupled with handcrafted regularizers but are highly dependent on the choice of the regularizers. Artificial intelligence (AI)-based approaches and more particularly convolutional neural networks (CNNs) are now considered the state-of-the-art approach and can be used as an end-to-end method for MD or to implicitly learn an a priori. In the last few years, diffusion models (DMs) became predominant in the field of generative models where a distribution function is learned. This distribution function can be used as a prior for solving inverse problems. This work investigates the use of DMs as regularizers for MD tasks in PCCT. MD by diffusion posterior sampling (DPS) can be achieved. Three DPS-based approaches -- image-domain two-step DPS (im-TDPS), projection-domain two-step DPS (proj-TDPS), and one-step DPS (ODPS) -- are evaluated. The first two methods perform MD in two steps: im-TDPS samples spectral images by DPS then performs image-based MD, while proj-TDPS performs projection-based MD then samples material images by DPS. The last method, ODPS, samples the material images directly from the measurement data. The results indicate that ODPS achieves superior performance compared to im-TDPS and proj-TDPS, providing sharper, noise-free and crosstalk-free images. Furthermore, we introduce a novel hybrid ODPS method combining DM priors with standard variational regularizers for scenarios involving materials absent from the training dataset. This hybrid method demonstrates improved material reconstruction quality compared to a standard variational method."
    },
    "2503.15382v1": {
      "title": "The information mismatch, and how to fix it",
      "url": "http://arxiv.org/abs/2503.15382v1",
      "authors": "Samuel J. Weisenthal, Amit K. Chowdhry",
      "update_time": "2025-03-19",
      "abstract": "We live in unprecedented times in terms of our ability to use evidence to inform medical care. For example, we can perform data-driven post-test probability calculations. However, there is work to do. In current studies, sensitivity and specificity, which play a key role in post-test probability calculations, are defined as unadjusted for patient covariates. In light of this, there have been multiple recommendations that sensitivity and specificity be adjusted for covariates. However, there is less work on the downstream clinical impact of unadjusted sensitivity and specificity. We discuss this here. We argue that unadjusted sensitivity and specificity can lead to a post-test probability that contains an ``information mismatch.'' We write the equations behind such an information mismatch and discuss the steps that can be taken to fix it."
    },
    "2503.15374v1": {
      "title": "Real-world validation of a multimodal LLM-powered pipeline for High-Accuracy Clinical Trial Patient Matching leveraging EHR data",
      "url": "http://arxiv.org/abs/2503.15374v1",
      "authors": "Anatole Callies, Quentin Bodinier, Philippe Ravaud, Kourosh Davarpanah",
      "update_time": "2025-03-19",
      "abstract": "Background: Patient recruitment in clinical trials is hindered by complex eligibility criteria and labor-intensive chart reviews. Prior research using text-only models have struggled to address this problem in a reliable and scalable way due to (1) limited reasoning capabilities, (2) information loss from converting visual records to text, and (3) lack of a generic EHR integration to extract patient data.   Methods: We introduce a broadly applicable, integration-free, LLM-powered pipeline that automates patient-trial matching using unprocessed documents extracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm, enabling the assessment of even the most complex criteria, (2) visual capabilities of latest LLMs to interpret medical records without lossy image-to-text conversions, and (3) multimodal embeddings for efficient medical record search. The pipeline was validated on the n2c2 2018 cohort selection dataset (288 diabetic patients) and a real-world dataset composed of 485 patients from 30 different sites matched against 36 diverse trials.   Results: On the n2c2 dataset, our method achieved a new state-of-the-art criterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an accuracy of 87\\%, undermined by the difficulty to replicate human decision-making when medical records lack sufficient information. Nevertheless, users were able to review overall eligibility in under 9 minutes per patient on average, representing an 80\\% improvement over traditional manual chart reviews.   Conclusion: This pipeline demonstrates robust performance in clinical trial patient matching without requiring custom integration with site systems or trial-specific tailoring, thereby enabling scalable deployment across sites seeking to leverage AI for patient matching."
    },
    "2503.15260v1": {
      "title": "DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation",
      "url": "http://arxiv.org/abs/2503.15260v1",
      "authors": "Lei Shi, Xi Fang, Naiyu Wang, Junxing Zhang",
      "update_time": "2025-03-19",
      "abstract": "Automatic medical image segmentation plays a crucial role in computer aided diagnosis. However, fully supervised learning approaches often require extensive and labor-intensive annotation efforts. To address this challenge, weakly supervised learning methods, particularly those using extreme points as supervisory signals, have the potential to offer an effective solution. In this paper, we introduce Deep Extreme Point Tracing (DEPT) integrated with Feature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image segmentation. Notably, our method generates pseudo labels by identifying the lowest-cost path that connects all extreme points on the feature map-based cost matrix. Additionally, an iterative training strategy is proposed to refine pseudo labels progressively, enabling continuous network improvement. Experimental results on two public datasets demonstrate the effectiveness of our proposed method. The performance of our method approaches that of the fully supervised method and outperforms several existing weakly supervised methods."
    },
    "2503.15221v1": {
      "title": "A Foundation Model for Patient Behavior Monitoring and Suicide Detection",
      "url": "http://arxiv.org/abs/2503.15221v1",
      "authors": "Rodrigo Oliver, Josu\u00e9 P\u00e9rez-Sabater, Leire Paz-Arbaizar, Alejandro Lancho, Antonio Art\u00e9s, Pablo M. Olmos",
      "update_time": "2025-03-19",
      "abstract": "Foundation models (FMs) have achieved remarkable success across various domains, yet their adoption in healthcare remains limited. While significant advances have been made in medical imaging, genetic biomarkers, and time series from electronic health records, the potential of FMs for patient behavior monitoring through wearable devices remains underexplored. These datasets are inherently heterogeneous, multisource, and often exhibit high rates of missing data, posing unique challenges. This paper introduces a novel FM based on a modified vector quantized variational autoencoder (VQ-VAE), specifically designed to process real-world data from wearable devices. We demonstrate that our pretrained FM, trained on a broad cohort of psychiatric patients, performs downstream tasks via its latent representation without fine-tuning on a held-out cohort of suicidal patients. To illustrate this, we develop a probabilistic change-point detection algorithm for suicide detection and demonstrate the FM's effectiveness in predicting emotional states. Our results show that the discrete latent structure of the VQ-VAE outperforms a state-of-the-art Informer architecture in unsupervised suicide detection, while matching its performance in supervised emotion prediction when the latent dimensionality is increased, though at the cost of reduced unsupervised accuracy. This trade-off highlights the need for future FMs to integrate hybrid discrete-continuous structures for balanced performance across tasks."
    },
    "2503.15212v1": {
      "title": "Context-Aware Vision Language Foundation Models for Ocular Disease Screening in Retinal Images",
      "url": "http://arxiv.org/abs/2503.15212v1",
      "authors": "Lucie Berger, Mathieu Lamard, Philippe Zhang, Laurent Borderie, Alexandre Le Guilcher, Pascale Massin, B\u00e9atrice Cochener, Gwenol\u00e9 Quellec, Sarah Matta",
      "update_time": "2025-03-19",
      "abstract": "Foundation models are large-scale versatile systems trained on vast quantities of diverse data to learn generalizable representations. Their adaptability with minimal fine-tuning makes them particularly promising for medical imaging, where data variability and domain shifts are major challenges. Currently, two types of foundation models dominate the literature: self-supervised models and more recent vision-language models. In this study, we advance the application of vision-language foundation (VLF) models for ocular disease screening using the OPHDIAT dataset, which includes nearly 700,000 fundus photographs from a French diabetic retinopathy (DR) screening network. This dataset provides extensive clinical data (patient-specific information such as diabetic health conditions, and treatments), labeled diagnostics, ophthalmologists text-based findings, and multiple retinal images for each examination. Building on the FLAIR model $\\unicode{x2013}$ a VLF model for retinal pathology classification $\\unicode{x2013}$ we propose novel context-aware VLF models (e.g jointly analyzing multiple images from the same visit or taking advantage of past diagnoses and contextual data) to fully leverage the richness of the OPHDIAT dataset and enhance robustness to domain shifts. Our approaches were evaluated on both in-domain (a testing subset of OPHDIAT) and out-of-domain data (public datasets) to assess their generalization performance. Our model demonstrated improved in-domain performance for DR grading, achieving an area under the curve (AUC) ranging from 0.851 to 0.9999, and generalized well to ocular disease detection on out-of-domain data (AUC: 0.631-0.913)."
    },
    "2503.15161v1": {
      "title": "UltraFlwr -- An Efficient Federated Medical and Surgical Object Detection Framework",
      "url": "http://arxiv.org/abs/2503.15161v1",
      "authors": "Yang Li, Soumya Snigdha Kundu, Maxence Boels, Toktam Mahmoodi, Sebastien Ourselin, Tom Vercauteren, Prokar Dasgupta, Jonathan Shapey, Alejandro Granados",
      "update_time": "2025-03-19",
      "abstract": "Object detection shows promise for medical and surgical applications such as cell counting and tool tracking. However, its faces multiple real-world edge deployment challenges including limited high-quality annotated data, data sharing restrictions, and computational constraints. In this work, we introduce UltraFlwr, a framework for federated medical and surgical object detection. By leveraging Federated Learning (FL), UltraFlwr enables decentralized model training across multiple sites without sharing raw data. To further enhance UltraFlwr's efficiency, we propose YOLO-PA, a set of novel Partial Aggregation (PA) strategies specifically designed for YOLO models in FL. YOLO-PA significantly reduces communication overhead by up to 83% per round while maintaining performance comparable to Full Aggregation (FA) strategies. Our extensive experiments on BCCD and m2cai16-tool-locations datasets demonstrate that YOLO-PA not only provides better client models compared to client-wise centralized training and FA strategies, but also facilitates efficient training and deployment across resource-constrained edge devices. Further, we also establish one of the first benchmarks in federated medical and surgical object detection. This paper advances the feasibility of training and deploying detection models on the edge, making federated object detection more practical for time-critical and resource-constrained medical and surgical applications. UltraFlwr is publicly available at https://github.com/KCL-BMEIS/UltraFlwr."
    },
    "2503.15153v1": {
      "title": "Improving PET scanner time-of-flight resolution using additional prompt photon",
      "url": "http://arxiv.org/abs/2503.15153v1",
      "authors": "Lech Raczynski, Wojciech Krzemien, Konrad Klimaszewski",
      "update_time": "2025-03-19",
      "abstract": "Positronium Imaging requires two classes of events: double-coincidences originated from pair of back-to-back annihilation photons and triple-coincidences comprised with two annihilation photons and one additional prompt photon. The standard reconstruction of the emission position along the line-of-response of triple-coincidence event is the same as in the case of double-coincidence event; an information introduced by the high-energetic prompt photon is ignored. In this study, we propose to extend the reconstruction of position of triple-coincidence event by taking into account the time and position of prompt photon. We incorporate the knowledge about the positronium lifetime distribution and discuss the limitations of the method based on the simulation data. We highlight that the uncertainty of the estimate provided by prompt photon alone is much higher than the standard deviation estimated based on two annihilation photons. We finally demonstrate the extent of resolution improvement that can be obtained when estimated using three photons."
    }
  }
}