{
  "Brain": {
    "2512.04052v1": {
      "title": "Post-Cold War Diaspora of Russian Particle Physicists",
      "url": "http://arxiv.org/abs/2512.04052v1",
      "authors": "Vladimir Shiltsev",
      "update_time": "2025-12-03",
      "abstract": "While the migration of scientists from the Soviet Union to the West occurred at a modest pace during the 1970s and 1980s, the dissolution of the USSR in 1991 and the ensuing economic and social hardships precipitated a massive exodus that amounted to a true brain drain. The international physics community, particularly in Europe and the United States, absorbed a substantial influx of specialists in nuclear, high-energy, and accelerator physics, including both seasoned scientists and engineers as well as promising graduate students and postdoctoral fellows. Many of these emigre researchers went on to assume leadership positions, drive major experimental and theoretical initiatives, and achieve scientific distinction that equaled or even surpassed their accomplishments at home. In this article we explore the defining features of this post Cold War scientific diaspora, assess its impact on Russia research infrastructure and capabilities, and evaluate its enduring contributions to global particle physics collaborations and discoveries.",
      "code_url": null
    },
    "2512.04014v1": {
      "title": "Construction of irreducible integrity basis for anisotropic hyperelasticity via structural tensors",
      "url": "http://arxiv.org/abs/2512.04014v1",
      "authors": "Brain M. Riemer, J\u00f6rg Brummund, Karl A. Kalina, Abel H. G. Milor, Franz Damma\u00df, Markus K\u00e4stner",
      "update_time": "2025-12-03",
      "abstract": "We present a straightforward analytical-numerical methodology for determining polynomially complete and irreducible scalar-valued invariant sets for anisotropic hyperelasticity. By applying the proposed technique, we obtain irreducible integrity bases for all common anisotropies in hyperelasticity via the structural tensor concept, i.e., invariants are formed from a measure of deformation (symmetric 2nd order tensor) and a set of structural tensors describing the material's symmetry. Our work covers results for the 11 types of anisotropy that arise from the classical 7 crystal systems, as well as findings for 4 additional non-crystal anisotropies derived from the cylindrical, spherical, and icosahedral symmetry systems. Polynomial completeness and irreducibility of the proposed integrity bases are proven using the Molien series and, in addition, with established results for scalar-valued invariant sets from the literature. Furthermore, we derive relationships between a set of multiple structural tensors that specify a symmetry group and a description using only a single structural tensor. Both can be used to construct irreducible integrity bases by applying the proposed analytical-numerical method. The provided invariant sets are of great importance for modeling anisotropic materials via the structural tensor concept using both classical models as well as modern approaches based on machine learning. Alongside the results presented, this article also aims to provide an introductory overview of the complex field of modeling anisotropic materials.",
      "code_url": null
    },
    "2512.03912v1": {
      "title": "Parsimonious Clustering of Covariance Matrices",
      "url": "http://arxiv.org/abs/2512.03912v1",
      "authors": "Yixi Xu, Yi Zhao",
      "update_time": "2025-12-03",
      "abstract": "Functional connectivity (FC) derived from functional magnetic resonance imaging (fMRI) data offers vital insights for understanding brain function and neurological and psychiatric disorders. Unsupervised clustering methods are desired to group individuals based on shared features, facilitating clinical diagnosis. In this study, a parsimonious clustering model is proposed, which integrates the Mixture-of-Experts (MoE) and covariance regression framework, to cluster individuals based on FC captured by data covariance matrices in resting-state fMRI studies. The model assumes common linear projections across covariance matrices and a generalized linear model with covariates, allowing for flexible yet interpretable projection-specific clustering solutions. To evaluate the performance of the proposed framework, extensive simulation studies are conducted to assess clustering accuracy and robustness. The approach is applied to resting-state fMRI data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Subgroups are identified based on brain coherence and simultaneously uncover the association with demographic factors and cognitive functions.",
      "code_url": null
    },
    "2512.03907v1": {
      "title": "Emergent Spatiotemporal Dynamics in Large-Scale Brain Networks with Next Generation Neural Mass Models",
      "url": "http://arxiv.org/abs/2512.03907v1",
      "authors": "Rosa Maria Delicado, Gemma Huguet, Pau Clusella",
      "update_time": "2025-12-03",
      "abstract": "Understanding the dynamics of large-scale brain models remains a central challenge due to the inherent complexity of these systems. In this work, we explore the emergence of complex spatiotemporal patterns in a large scale-brain model composed of 90 interconnected brain regions coupled through empirically derived anatomical connectivity. An important aspect of our formulation is that the local dynamics of each brain region are described by a next-generation neural mass model, which explicitly captures the macroscopic gamma activity of coupled excitatory and inhibitory neural populations (PING mechanism). We first identify the system's homogeneous states-both resting and oscillatory-and analyze their stability under uniform perturbations. Then, we determine the stability against non-uniform perturbations by obtaining dispersion relations for the perturbation growth rate. This analysis enables us to link unstable directions of the homogeneous solutions to the emergence of rich spatiotemporal patterns, that we characterize by means of Lyapunov exponents and frequency spectrum analysis. Our results show that, compared to previous studies with classical neural mass models, next-generation neural mass models provide a broader dynamical repertoire, both within homogeneous states and in the heterogeneous regime. Additionally, we identify a key role for anatomical connectivity in cross-frequency coupling, allowing for the emergence of gamma oscillations with amplitude modulated by slower rhythms. These findings suggest that such models are not only more biophysically grounded but also particularly well-suited to capture the full complexity of large-scale brain dynamics. Overall, our study advances the analytical understanding of emerging spatiotemporal patterns in whole-brain models.",
      "code_url": null
    },
    "2512.03784v1": {
      "title": "Sleep Modulation: The Challenge of Transitioning from Open Loop to Closed Loop",
      "url": "http://arxiv.org/abs/2512.03784v1",
      "authors": "Guisong Liu, Jiansong Zhang, Yinpei Luo, Guoliang Wei, Shuqing Sun, Shiyang Deng, Pengfei Wei, Nanxi Chen",
      "update_time": "2025-12-03",
      "abstract": "Sleep disorders have emerged as a critical global health issue, highlighting the urgent need for effective and widely accessible intervention technologies. Non-invasive brain stimulation has garnered attention as it enables direct or indirect modulation of neural activity, thereby promoting sleep enhancement in a safe and unobtrusive manner. This class of approaches is collectively referred to as sleep modulation. To date, the majority of sleep modulation research relies on open-loop paradigms with empirically determined parameters, while achieving individual adaptation and modulation accuracy remains a distant objective. The paradigm-specific constraints inherent to open-loop designs represent a major obstacle to clinical translation and large-scale deployment in home environments. In this paper, we delineate fundamental paradigms of sleep modulation, critically examine the intrinsic limitations of open-loop approaches, and formally conceptualize sleep closed-loop modulation. We further provide a comprehensive synthesis of prior studies involving five commonly employed modulation techniques, evaluating their potential integration within a closed-loop framework. Finally, we identify three primary challenges in constructing an effective sleep closed-loop modulation system: sensor solution selection, monitoring model design, and modulation strategy design, while also proposing potential solutions. Collectively, this work aims to advance the paradigm shift of sleep modulation from open-loop toward closed-loop systems.",
      "code_url": null
    },
    "2512.03751v1": {
      "title": "Research on Brain Tumor Classification Method Based on Improved ResNet34 Network",
      "url": "http://arxiv.org/abs/2512.03751v1",
      "authors": "Yufeng Li, Wenchao Zhao, Bo Dang, Weimin Wang",
      "update_time": "2025-12-03",
      "abstract": "Previously, image interpretation in radiology relied heavily on manual methods. However, manual classification of brain tumor medical images is time-consuming and labor-intensive. Even with shallow convolutional neural network models, the accuracy is not ideal. To improve the efficiency and accuracy of brain tumor image classification, this paper proposes a brain tumor classification model based on an improved ResNet34 network. This model uses the ResNet34 residual network as the backbone network and incorporates multi-scale feature extraction. It uses a multi-scale input module as the first layer of the ResNet34 network and an Inception v2 module as the residual downsampling layer. Furthermore, a channel attention mechanism module assigns different weights to different channels of the image from a channel domain perspective, obtaining more important feature information. The results after a five-fold crossover experiment show that the average classification accuracy of the improved network model is approximately 98.8%, which is not only 1% higher than ResNet34, but also only 80% of the number of parameters of the original model. Therefore, the improved network model not only improves accuracy but also reduces clutter, achieving a classification effect with fewer parameters and higher accuracy.",
      "code_url": null
    },
    "2512.03458v1": {
      "title": "A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses",
      "url": "http://arxiv.org/abs/2512.03458v1",
      "authors": "Maryam Maghsoudi, Mohsen Rezaeizadeh, Shihab Shamma",
      "update_time": "2025-12-03",
      "abstract": "Decoding imagined speech engages complex neural processes that are difficult to interpret due to uncertainty in timing and the limited availability of imagined-response datasets. In this study, we present a Magnetoencephalography (MEG) dataset collected from trained musicians as they imagined and listened to musical and poetic stimuli. We show that both imagined and perceived brain responses contain consistent, condition-specific information. Using a sliding-window ridge regression model, we first mapped imagined responses to listened responses at the single-subject level, but found limited generalization across subjects. At the group level, we developed an encoder-decoder convolutional neural network with a subject-specific calibration layer that produced stable and generalizable mappings. The CNN consistently outperformed the null model, yielding significantly higher correlations between predicted and true listened responses for nearly all held-out subjects. Our findings demonstrate that imagined neural activity can be transformed into perception-like responses, providing a foundation for future brain-computer interface applications involving imagined speech and music.",
      "code_url": null
    },
    "2512.03409v1": {
      "title": "Optimal Griffiths Phase in Heterogeneous Human Brain Networks: Brain Criticality Embracing Stability and Flexibility across Individuals",
      "url": "http://arxiv.org/abs/2512.03409v1",
      "authors": "Kejian Wu, Dante R. Chialvo, Changsong Zhou, Lianchun Yu",
      "update_time": "2025-12-03",
      "abstract": "A prominent hypothesis in neuroscience proposes that brains achieve optimal performance by operating near a critical point. However, this framework, which often assumes a universal critical point, fails to account for the extensive individual variability observed in neural dynamics and cognitive functions. These variabilities are not noise but rather an inherent manifestation of a fundamental systems-biology principle: the necessary trade-off between robustness and flexibility in human populations. Here, we propose that the Griffiths phase (GP), an extended critical regime synergically induced by two kinds of heterogeneities in brain network region and connectivity, offers a unified framework for brain criticality that better reconciles robustness and flexibility and accounts for individual variability. Using Human Connectome Project data and whole-brain modeling, we demonstrated that the synergic interplay between structural network modularity and regional heterogeneity in local excitability yields biologically viable GP featured with widely extended global excitability ranges, with an embedded optimal point that balances global/local information transmission. Crucially, an individua's position within the GP gives rise to unique global network dynamics, which in turn confer a distinctive cognitive profile via flexible configuration of functional connectivity for segregation, integration, and balance between them. These results establish GP as an evolved adaptive mechanism resolving the robustness-flexibility trade-off, fulfilling diverse cognitive demands through individualized criticality landscapes, providing a new framework of brain criticality.",
      "code_url": null
    },
    "2512.03394v1": {
      "title": "VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing",
      "url": "http://arxiv.org/abs/2512.03394v1",
      "authors": "Hamed Poursiami, Shay Snyder, Guojing Cong, Thomas Potok, Maryam Parsa",
      "update_time": "2025-12-03",
      "abstract": "Graph classification is a fundamental task in domains ranging from molecular property prediction to materials design. While graph neural networks (GNNs) achieve strong performance by learning expressive representations via message passing, they incur high computational costs, limiting their scalability and deployment on resource-constrained devices. Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures (VSA), offers a lightweight, brain-inspired alternative, yet existing HDC-based graph methods typically struggle to match the predictive performance of GNNs. In this work, we propose VS-Graph, a vector-symbolic graph learning framework that narrows the gap between the efficiency of HDC and the expressive power of message passing. VS-Graph introduces a Spike Diffusion mechanism for topology-driven node identification and an Associative Message Passing scheme for multi-hop neighborhood aggregation entirely within the high-dimensional vector space. Without gradient-based optimization or backpropagation, our method achieves competitive accuracy with modern GNNs, outperforming the prior HDC baseline by 4-5% on standard benchmarks such as MUTAG and DD. It also matches or exceeds the performance of the GNN baselines on several datasets while accelerating the training by a factor of up to 450x. Furthermore, VS-Graph maintains high accuracy even with the hypervector dimensionality reduced to D=128, demonstrating robustness under aggressive dimension compression and paving the way for ultra-efficient execution on edge and neuromorphic hardware.",
      "code_url": null
    },
    "2512.03345v1": {
      "title": "HalluGen: Synthesizing Realistic and Controllable Hallucinations for Evaluating Image Restoration",
      "url": "http://arxiv.org/abs/2512.03345v1",
      "authors": "Seunghoi Kim, Henry F. J. Tregidgo, Chen Jin, Matteo Figini, Daniel C. Alexander",
      "update_time": "2025-12-03",
      "abstract": "Generative models are prone to hallucinations: plausible but incorrect structures absent in the ground truth. This issue is problematic in image restoration for safety-critical domains such as medical imaging, industrial inspection, and remote sensing, where such errors undermine reliability and trust. For example, in low-field MRI, widely used in resource-limited settings, restoration models are essential for enhancing low-quality scans, yet hallucinations can lead to serious diagnostic errors. Progress has been hindered by a circular dependency: evaluating hallucinations requires labeled data, yet such labels are costly and subjective. We introduce HalluGen, a diffusion-based framework that synthesizes realistic hallucinations with controllable type, location, and severity, producing perceptually realistic but semantically incorrect outputs (segmentation IoU drops from 0.86 to 0.36). Using HalluGen, we construct the first large-scale hallucination dataset comprising 4,350 annotated images derived from 1,450 brain MR images for low-field enhancement, enabling systematic evaluation of hallucination detection and mitigation. We demonstrate its utility in two applications: (1) benchmarking image quality metrics and developing Semantic Hallucination Assessment via Feature Evaluation (SHAFE), a feature-based metric with soft-attention pooling that improves hallucination sensitivity over traditional metrics; and (2) training reference-free hallucination detectors that generalize to real restoration failures. Together, HalluGen and its open dataset establish the first scalable foundation for evaluating hallucinations in safety-critical image restoration.",
      "code_url": null
    }
  },
  "EEG": {
    "2512.02978v1": {
      "title": "Rethinking Generalized BCIs: Benchmarking 340,000+ Unique Algorithmic Configurations for EEG Mental Command Decoding",
      "url": "http://arxiv.org/abs/2512.02978v1",
      "authors": "Paul Barbaste, Olivier Oullier, Xavier Vasques",
      "update_time": "2025-12-02",
      "abstract": "Robust decoding and classification of brain patterns measured with electroencephalography (EEG) remains a major challenge for real-world (i.e. outside scientific lab and medical facilities) brain-computer interface (BCI) applications due to well documented inter- and intra-participant variability. Here, we present a large-scale benchmark evaluating over 340,000+ unique combinations of spatial and nonlinear EEG classification. Our methodological pipeline consists in combinations of Common Spatial Patterns (CSP), Riemannian geometry, functional connectivity, and fractal- or entropy-based features across three open-access EEG datasets. Unlike prior studies, our analysis operates at the per-participant level and across multiple frequency bands (8-15 Hz and 8-30 Hz), enabling direct assessment of both group-level performance and individual variability. Covariance tangent space projection (cov-tgsp) and CSP consistently achieved the highest average classification accuracies. However, their effectiveness was strongly dataset-dependent, and marked participant-level differences persisted, particularly in the most heterogeneous of the datasets. Importantly, nonlinear methods outperformed spatial approaches for specific individuals, underscoring the need for personalized pipeline selection. Our findings highlight that no universal 'one-size-fits-all' method can optimally decode EEG motor imagery patterns across all users or datasets. Future work will require adaptive, multimodal, and possibly novel approaches to fully address neurophysiological variability in practical BCI applications where the system can automatically adapt to what makes each user unique.",
      "code_url": null
    },
    "2512.02626v1": {
      "title": "Adapting Tensor Kernel Machines to Enable Efficient Transfer Learning for Seizure Detection",
      "url": "http://arxiv.org/abs/2512.02626v1",
      "authors": "Seline J. S. de Rooij, Borb\u00e1la Hunyadi",
      "update_time": "2025-12-02",
      "abstract": "Transfer learning aims to optimize performance in a target task by learning from a related source problem. In this work, we propose an efficient transfer learning method using a tensor kernel machine. Our method takes inspiration from the adaptive SVM and hence transfers 'knowledge' from the source to the 'adapted' model via regularization. The main advantage of using tensor kernel machines is that they leverage low-rank tensor networks to learn a compact non-linear model in the primal domain. This allows for a more efficient adaptation without adding more parameters to the model. To demonstrate the effectiveness of our approach, we apply the adaptive tensor kernel machine (Adapt-TKM) to seizure detection on behind-the-ear EEG. By personalizing patient-independent models with a small amount of patient-specific data, the patient-adapted model (which utilizes the Adapt-TKM), achieves better performance compared to the patient-independent and fully patient-specific models. Notably, it is able to do so while requiring around 100 times fewer parameters than the adaptive SVM model, leading to a correspondingly faster inference speed. This makes the Adapt-TKM especially useful for resource-constrained wearable devices.",
      "code_url": null
    },
    "2512.00574v1": {
      "title": "GCMCG: A Clustering-Aware Graph Attention and Expert Fusion Network for Multi-Paradigm, Multi-task, and Cross-Subject EEG Decoding",
      "url": "http://arxiv.org/abs/2512.00574v1",
      "authors": "Yiqiao Chen, Zijian Huang, Juchi He, Fazheng Xu, Zhenghui Feng",
      "update_time": "2025-11-29",
      "abstract": "Brain-Computer Interfaces (BCIs) based on Motor Execution (ME) and Motor Imagery (MI) electroencephalogram (EEG) signals offer a direct pathway for human-machine interaction. However, developing robust decoding models remains challenging due to the complex spatio-temporal dynamics of EEG, its low signal-to-noise ratio, and the limited generalizability of many existing approaches across subjects and paradigms. To address these issues, this paper proposes Graph-guided Clustering Mixture-of-Experts CNN-GRU (GCMCG), a novel unified framework for MI-ME EEG decoding. Our approach integrates a robust preprocessing stage using Independent Component Analysis and Wavelet Transform (ICA-WT) for effective denoising. We further introduce a pre-trainable graph tokenization module that dynamically models electrode relationships via a Graph Attention Network (GAT), followed by unsupervised spectral clustering to decompose signals into interpretable functional brain regions. Each region is processed by a dedicated CNN-GRU expert network, and a gated fusion mechanism with L1 regularization adaptively combines these local features with a global expert. This Mixture-of-Experts (MoE) design enables deep spatio-temporal fusion and enhances representational capacity. A three-stage training strategy incorporating focal loss and progressive sampling is employed to improve cross-subject generalization and handle class imbalance. Evaluated on three public datasets of varying complexity (EEGmmidb-BCI2000, BCI-IV 2a, and M3CV), GCMCG achieves overall accuracies of 86.60%, 98.57%, and 99.61%, respectively, which demonstrates its superior effectiveness and strong generalization capability for practical BCI applications.",
      "code_url": null
    },
    "2512.00194v1": {
      "title": "AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI",
      "url": "http://arxiv.org/abs/2512.00194v1",
      "authors": "Zag ElSayed, Grace Westerkamp, Gavin Gammoh, Yanchen Liu, Peyton Siekierski, Craig Erickson, Ernest Pedapati",
      "update_time": "2025-11-28",
      "abstract": "We introduce EEG Autoclean Vision Language AI (ICVision) a first-of-its-kind system that emulates expert-level EEG ICA component classification through AI-agent vision and natural language reasoning. Unlike conventional classifiers such as ICLabel, which rely on handcrafted features, ICVision directly interprets ICA dashboard visualizations topography, time series, power spectra, and ERP plots, using a multimodal large language model (GPT-4 Vision). This allows the AI to see and explain EEG components the way trained neurologists do, making it the first scientific implementation of AI-agent visual cognition in neurophysiology. ICVision classifies each component into one of six canonical categories (brain, eye, heart, muscle, channel noise, and other noise), returning both a confidence score and a human-like explanation. Evaluated on 3,168 ICA components from 124 EEG datasets, ICVision achieved k = 0.677 agreement with expert consensus, surpassing MNE ICLabel, while also preserving clinically relevant brain signals in ambiguous cases. Over 97% of its outputs were rated as interpretable and actionable by expert reviewers. As a core module of the open-source EEG Autoclean platform, ICVision signals a paradigm shift in scientific AI, where models do not just classify, but see, reason, and communicate. It opens the door to globally scalable, explainable, and reproducible EEG workflows, marking the emergence of AI agents capable of expert-level visual decision-making in brain science and beyond.",
      "code_url": null
    },
    "2511.23384v2": {
      "title": "Improving motor imagery decoding methods for an EEG-based mobile brain-computer interface in the context of the 2024 Cybathlon",
      "url": "http://arxiv.org/abs/2511.23384v2",
      "authors": "Isabel Whiteley Tscherniak, Niels Christopher Thiemann, Ana McWhinnie-Fern\u00e1ndez, Iustin Curcean, Leon Jokinen, Sadat Hodzic, Thomas E. Huber, Daniel Pavlov, Manuel Methasani, Pietro Marcolongo, Glenn Viktor Krafczyk, Oscar Osvaldo Soto Rivera, Thien Le, Flaminia Pallotti, Enrico A. Fazzi, neuroTUM e.",
      "update_time": "2025-12-01",
      "abstract": "Motivated by the Cybathlon 2024 competition, we developed a modular, online EEG-based brain-computer interface to address these challenges, increasing accessibility for individuals with severe mobility impairments. Our system uses three mental and motor imagery classes to control up to five control signals. The pipeline consists of four modules: data acquisition, preprocessing, classification, and the transfer function to map classification output to control dimensions. We use three diagonalized structured state-space sequence layers as a deep learning classifier. We developed a training game for our pilot where the mental tasks control the game during quick-time events. We implemented a mobile web application for live user feedback. The components were designed with a human-centred approach in collaboration with the tetraplegic user. We achieve up to 84% classification accuracy in offline analysis using an S4D-layer-based model. In a competition setting, our pilot successfully completed one task; we attribute the reduced performance in this context primarily to factors such as stress and the challenging competition environment. Following the Cybathlon, we further validated our pipeline with the original pilot and an additional participant, achieving a success rate of 73% in real-time gameplay. We also compare our model to the EEGEncoder, which is slower in training but has a higher performance. The S4D model outperforms the reference machine learning models. We provide insights into developing a framework for portable BCIs, bridging the gap between the laboratory and daily life. Specifically, our framework integrates modular design, real-time data processing, user-centred feedback, and low-cost hardware to deliver an accessible and adaptable BCI solution, addressing critical gaps in current BCI applications.",
      "code_url": null
    },
    "2511.23162v1": {
      "title": "Estimating the Event-Related Potential from Few EEG Trials",
      "url": "http://arxiv.org/abs/2511.23162v1",
      "authors": "Anders Vestergaard N\u00f8rskov, Kasper J\u00f8rgensen, Alexander Neergaard Zahid, Morten M\u00f8rup",
      "update_time": "2025-11-28",
      "abstract": "Event-related potentials (ERP) are measurements of brain activity with wide applications in basic and clinical neuroscience, that are typically estimated using the average of many trials of electroencephalography signals (EEG) to sufficiently reduce noise and signal variability. We introduce EEG2ERP, a novel uncertainty-aware autoencoder approach that maps an arbitrary number of EEG trials to their associated ERP. To account for the ERP uncertainty we use bootstrapped training targets and introduce a separate variance decoder to model the uncertainty of the estimated ERP. We evaluate our approach in the challenging zero-shot scenario of generalizing to new subjects considering three different publicly available data sources; i) the comprehensive ERP CORE dataset that includes over 50,000 EEG trials across six ERP paradigms from 40 subjects, ii) the large P300 Speller BCI dataset, and iii) a neuroimaging dataset on face perception consisting of both EEG and magnetoencephalography (MEG) data. We consistently find that our method in the few trial regime provides substantially better ERP estimates than commonly used conventional and robust averaging procedures. EEG2ERP is the first deep learning approach to map EEG signals to their associated ERP, moving toward reducing the number of trials necessary for ERP research. Code is available at https://github.com/andersxa/EEG2ERP",
      "code_url": null
    },
    "2511.23142v1": {
      "title": "Adapting Neural Audio Codecs to EEG",
      "url": "http://arxiv.org/abs/2511.23142v1",
      "authors": "Ard Kastrati, Luca Lanzend\u00f6rfer, Riccardo Rigoni, John Staib Matilla, Roger Wattenhofer",
      "update_time": "2025-11-28",
      "abstract": "EEG and audio are inherently distinct modalities, differing in sampling rate, channel structure, and scale. Yet, we show that pretrained neural audio codecs can serve as effective starting points for EEG compression, provided that the data are preprocessed to be suitable to the codec's input constraints. Using DAC, a state-of-the-art neural audio codec as our base, we demonstrate that raw EEG can be mapped into the codec's stride-based framing, enabling direct reuse of the audio-pretrained encoder-decoder. Even without modification, this setup yields stable EEG reconstructions, and fine-tuning on EEG data further improves fidelity and generalization compared to training from scratch. We systematically explore compression-quality trade-offs by varying residual codebook depth, codebook (vocabulary) size, and input sampling rate. To capture spatial dependencies across electrodes, we propose DAC-MC, a multi-channel extension with attention-based cross-channel aggregation and channel-specific decoding, while retaining the audio-pretrained initialization. Evaluations on the TUH Abnormal and Epilepsy datasets show that the adapted codecs preserve clinically relevant information, as reflected in spectrogram-based reconstruction loss and downstream classification accuracy.",
      "code_url": null
    },
    "2511.22030v1": {
      "title": "Calibration-Free EEG-based Driver Drowsiness Detection with Online Test-Time Adaptation",
      "url": "http://arxiv.org/abs/2511.22030v1",
      "authors": "Geun-Deok Jang, Dong-Kyun Han, Seo-Hyeon Park, Seong-Whan Lee",
      "update_time": "2025-11-27",
      "abstract": "Drowsy driving is a growing cause of traffic accidents, prompting recent exploration of electroencephalography (EEG)-based drowsiness detection systems. However, the inherent variability of EEG signals due to psychological and physical factors necessitates a cumbersome calibration process. In particular, the inter-subject variability of EEG signals leads to a domain shift problem, which makes it challenging to generalize drowsiness detection models to unseen target subjects. To address these issues, we propose a novel driver drowsiness detection framework that leverages online test-time adaptation (TTA) methods to dynamically adjust to target subject distributions. Our proposed method updates the learnable parameters in batch normalization (BN) layers, while preserving pretrained normalization statistics, resulting in a modified configuration that ensures effective adaptation during test time. We incorporate a memory bank that dynamically manages streaming EEG segments, selecting samples based on their reliability determined by negative energy scores and persistence time. In addition, we introduce prototype learning to ensure robust predictions against distribution shifts over time. We validated our method on the sustained-attention driving dataset collected in a simulated environment, where drowsiness was estimated from delayed reaction times during monotonous lane-keeping tasks. Our experiments show that our method outperforms all baselines, achieving an average F1-score of 81.73\\%, an improvement of 11.73\\% over the best TTA baseline. This demonstrates that our proposed method significantly enhances the adaptability of EEG-based drowsiness detection systems in non-i.i.d. scenarios.",
      "code_url": null
    },
    "2511.21940v1": {
      "title": "Deep Learning Architectures for Code-Modulated Visual Evoked Potentials Detection",
      "url": "http://arxiv.org/abs/2511.21940v1",
      "authors": "Kiran Nair, Hubert Cecotti",
      "update_time": "2025-11-26",
      "abstract": "Non-invasive Brain-Computer Interfaces (BCIs) based on Code-Modulated Visual Evoked Potentials (C-VEPs) require highly robust decoding methods to address temporal variability and session-dependent noise in EEG signals. This study proposes and evaluates several deep learning architectures, including convolutional neural networks (CNNs) for 63-bit m-sequence reconstruction and classification, and Siamese networks for similarity-based decoding, alongside canonical correlation analysis (CCA) baselines. EEG data were recorded from 13 healthy adults under single-target flicker stimulation. The proposed deep models significantly outperformed traditional approaches, with distance-based decoding using Earth Mover's Distance (EMD) and constrained EMD showing greater robustness to latency variations than Euclidean and Mahalanobis metrics. Temporal data augmentation with small shifts further improved generalization across sessions. Among all models, the multi-class Siamese network achieved the best overall performance with an average accuracy of 96.89%, demonstrating the potential of data-driven deep architectures for reliable, single-trial C-VEP decoding in adaptive non-invasive BCI systems.",
      "code_url": null
    },
    "2511.20990v1": {
      "title": "Meditative absorption shifts brain dynamics toward criticality",
      "url": "http://arxiv.org/abs/2511.20990v1",
      "authors": "Jonas Mago, Joshua Brahinsky, Mark Miller, Charlotte Maschke, Heleen A. Slagter, Shaila Catherine, Ruben E. Laukkonen, B. Rael Cahn, Matthew D. Sacchet, Wangmo Dixey, Richard Dixey, Soham Rej, Michael Lifshitz",
      "update_time": "2025-11-26",
      "abstract": "Criticality describes a regime between order and chaos that supports flexible yet stable information processing. Here we examine whether neural dynamics can be volitionally shifted toward criticality through the self-regulation of attention. We examined ten experienced practitioners of meditation during a 10-day retreat, comparing refined states of meditative absorption, called the jhanas, to regular mindfulness of breathing. We collected electroencephalography (EEG) and physiological data during these practices and quantified the signal's dynamical properties using Lempel-Ziv complexity, signal entropy, chaoticity and long-range temporal correlations. In addition, we estimated perturbational sensitivity using a global auditory oddball mismatch negativity (MMN) during meditation. Relative to mindfulness, jhana was associated with pronounced self-reported sensory fading, slower respiration, higher neural signal diversity across multiple measures, reduced chaoticity, and enhanced MMN amplitude over frontocentral sites. Spectral analyses showed a flatter aperiodic one over f component and a frequency-specific reorganization of long-range temporal correlations. Together, increased diversity with reduced chaoticity and heightened deviance detection indicate a shift toward a metastable, near-critical regime during jhana. We propose an overlap of the phenomenology of jhana with minimal phenomenal experiences in terms of progressive attenuation of sensory content with preserved tonic alertness. Accordingly, our findings suggest that criticality is a candidate neurophysiological marker of the absorptive, minimal-content dimension of the minimal phenomenal experience.",
      "code_url": null
    }
  },
  "BCI": {
    "2512.02978v1": {
      "title": "Rethinking Generalized BCIs: Benchmarking 340,000+ Unique Algorithmic Configurations for EEG Mental Command Decoding",
      "url": "http://arxiv.org/abs/2512.02978v1",
      "authors": "Paul Barbaste, Olivier Oullier, Xavier Vasques",
      "update_time": "2025-12-02",
      "abstract": "Robust decoding and classification of brain patterns measured with electroencephalography (EEG) remains a major challenge for real-world (i.e. outside scientific lab and medical facilities) brain-computer interface (BCI) applications due to well documented inter- and intra-participant variability. Here, we present a large-scale benchmark evaluating over 340,000+ unique combinations of spatial and nonlinear EEG classification. Our methodological pipeline consists in combinations of Common Spatial Patterns (CSP), Riemannian geometry, functional connectivity, and fractal- or entropy-based features across three open-access EEG datasets. Unlike prior studies, our analysis operates at the per-participant level and across multiple frequency bands (8-15 Hz and 8-30 Hz), enabling direct assessment of both group-level performance and individual variability. Covariance tangent space projection (cov-tgsp) and CSP consistently achieved the highest average classification accuracies. However, their effectiveness was strongly dataset-dependent, and marked participant-level differences persisted, particularly in the most heterogeneous of the datasets. Importantly, nonlinear methods outperformed spatial approaches for specific individuals, underscoring the need for personalized pipeline selection. Our findings highlight that no universal 'one-size-fits-all' method can optimally decode EEG motor imagery patterns across all users or datasets. Future work will require adaptive, multimodal, and possibly novel approaches to fully address neurophysiological variability in practical BCI applications where the system can automatically adapt to what makes each user unique.",
      "code_url": null
    },
    "2512.00574v1": {
      "title": "GCMCG: A Clustering-Aware Graph Attention and Expert Fusion Network for Multi-Paradigm, Multi-task, and Cross-Subject EEG Decoding",
      "url": "http://arxiv.org/abs/2512.00574v1",
      "authors": "Yiqiao Chen, Zijian Huang, Juchi He, Fazheng Xu, Zhenghui Feng",
      "update_time": "2025-11-29",
      "abstract": "Brain-Computer Interfaces (BCIs) based on Motor Execution (ME) and Motor Imagery (MI) electroencephalogram (EEG) signals offer a direct pathway for human-machine interaction. However, developing robust decoding models remains challenging due to the complex spatio-temporal dynamics of EEG, its low signal-to-noise ratio, and the limited generalizability of many existing approaches across subjects and paradigms. To address these issues, this paper proposes Graph-guided Clustering Mixture-of-Experts CNN-GRU (GCMCG), a novel unified framework for MI-ME EEG decoding. Our approach integrates a robust preprocessing stage using Independent Component Analysis and Wavelet Transform (ICA-WT) for effective denoising. We further introduce a pre-trainable graph tokenization module that dynamically models electrode relationships via a Graph Attention Network (GAT), followed by unsupervised spectral clustering to decompose signals into interpretable functional brain regions. Each region is processed by a dedicated CNN-GRU expert network, and a gated fusion mechanism with L1 regularization adaptively combines these local features with a global expert. This Mixture-of-Experts (MoE) design enables deep spatio-temporal fusion and enhances representational capacity. A three-stage training strategy incorporating focal loss and progressive sampling is employed to improve cross-subject generalization and handle class imbalance. Evaluated on three public datasets of varying complexity (EEGmmidb-BCI2000, BCI-IV 2a, and M3CV), GCMCG achieves overall accuracies of 86.60%, 98.57%, and 99.61%, respectively, which demonstrates its superior effectiveness and strong generalization capability for practical BCI applications.",
      "code_url": null
    },
    "2511.23384v2": {
      "title": "Improving motor imagery decoding methods for an EEG-based mobile brain-computer interface in the context of the 2024 Cybathlon",
      "url": "http://arxiv.org/abs/2511.23384v2",
      "authors": "Isabel Whiteley Tscherniak, Niels Christopher Thiemann, Ana McWhinnie-Fern\u00e1ndez, Iustin Curcean, Leon Jokinen, Sadat Hodzic, Thomas E. Huber, Daniel Pavlov, Manuel Methasani, Pietro Marcolongo, Glenn Viktor Krafczyk, Oscar Osvaldo Soto Rivera, Thien Le, Flaminia Pallotti, Enrico A. Fazzi, neuroTUM e.",
      "update_time": "2025-12-01",
      "abstract": "Motivated by the Cybathlon 2024 competition, we developed a modular, online EEG-based brain-computer interface to address these challenges, increasing accessibility for individuals with severe mobility impairments. Our system uses three mental and motor imagery classes to control up to five control signals. The pipeline consists of four modules: data acquisition, preprocessing, classification, and the transfer function to map classification output to control dimensions. We use three diagonalized structured state-space sequence layers as a deep learning classifier. We developed a training game for our pilot where the mental tasks control the game during quick-time events. We implemented a mobile web application for live user feedback. The components were designed with a human-centred approach in collaboration with the tetraplegic user. We achieve up to 84% classification accuracy in offline analysis using an S4D-layer-based model. In a competition setting, our pilot successfully completed one task; we attribute the reduced performance in this context primarily to factors such as stress and the challenging competition environment. Following the Cybathlon, we further validated our pipeline with the original pilot and an additional participant, achieving a success rate of 73% in real-time gameplay. We also compare our model to the EEGEncoder, which is slower in training but has a higher performance. The S4D model outperforms the reference machine learning models. We provide insights into developing a framework for portable BCIs, bridging the gap between the laboratory and daily life. Specifically, our framework integrates modular design, real-time data processing, user-centred feedback, and low-cost hardware to deliver an accessible and adaptable BCI solution, addressing critical gaps in current BCI applications.",
      "code_url": null
    },
    "2511.23162v1": {
      "title": "Estimating the Event-Related Potential from Few EEG Trials",
      "url": "http://arxiv.org/abs/2511.23162v1",
      "authors": "Anders Vestergaard N\u00f8rskov, Kasper J\u00f8rgensen, Alexander Neergaard Zahid, Morten M\u00f8rup",
      "update_time": "2025-11-28",
      "abstract": "Event-related potentials (ERP) are measurements of brain activity with wide applications in basic and clinical neuroscience, that are typically estimated using the average of many trials of electroencephalography signals (EEG) to sufficiently reduce noise and signal variability. We introduce EEG2ERP, a novel uncertainty-aware autoencoder approach that maps an arbitrary number of EEG trials to their associated ERP. To account for the ERP uncertainty we use bootstrapped training targets and introduce a separate variance decoder to model the uncertainty of the estimated ERP. We evaluate our approach in the challenging zero-shot scenario of generalizing to new subjects considering three different publicly available data sources; i) the comprehensive ERP CORE dataset that includes over 50,000 EEG trials across six ERP paradigms from 40 subjects, ii) the large P300 Speller BCI dataset, and iii) a neuroimaging dataset on face perception consisting of both EEG and magnetoencephalography (MEG) data. We consistently find that our method in the few trial regime provides substantially better ERP estimates than commonly used conventional and robust averaging procedures. EEG2ERP is the first deep learning approach to map EEG signals to their associated ERP, moving toward reducing the number of trials necessary for ERP research. Code is available at https://github.com/andersxa/EEG2ERP",
      "code_url": null
    },
    "2511.21940v1": {
      "title": "Deep Learning Architectures for Code-Modulated Visual Evoked Potentials Detection",
      "url": "http://arxiv.org/abs/2511.21940v1",
      "authors": "Kiran Nair, Hubert Cecotti",
      "update_time": "2025-11-26",
      "abstract": "Non-invasive Brain-Computer Interfaces (BCIs) based on Code-Modulated Visual Evoked Potentials (C-VEPs) require highly robust decoding methods to address temporal variability and session-dependent noise in EEG signals. This study proposes and evaluates several deep learning architectures, including convolutional neural networks (CNNs) for 63-bit m-sequence reconstruction and classification, and Siamese networks for similarity-based decoding, alongside canonical correlation analysis (CCA) baselines. EEG data were recorded from 13 healthy adults under single-target flicker stimulation. The proposed deep models significantly outperformed traditional approaches, with distance-based decoding using Earth Mover's Distance (EMD) and constrained EMD showing greater robustness to latency variations than Euclidean and Mahalanobis metrics. Temporal data augmentation with small shifts further improved generalization across sessions. Among all models, the multi-class Siamese network achieved the best overall performance with an average accuracy of 96.89%, demonstrating the potential of data-driven deep architectures for reliable, single-trial C-VEP decoding in adaptive non-invasive BCI systems.",
      "code_url": null
    },
    "2511.20835v1": {
      "title": "Symbiotic Brain-Machine Drawing via Visual Brain-Computer Interfaces",
      "url": "http://arxiv.org/abs/2511.20835v1",
      "authors": "Gao Wang, Yingying Huang, Lars Muckli, Daniele Faccio",
      "update_time": "2025-11-25",
      "abstract": "Brain-computer interfaces (BCIs) are evolving from research prototypes into clinical, assistive, and performance enhancement technologies. Despite the rapid rise and promise of implantable technologies, there is a need for better and more capable wearable and non-invasive approaches whilst also minimising hardware requirements. We present a non-invasive BCI for mind-drawing that iteratively infers a subject's internal visual intent by adaptively presenting visual stimuli (probes) on a screen encoded at different flicker-frequencies and analyses the steady-state visual evoked potentials (SSVEPs). A Gabor-inspired or machine-learned policies dynamically update the spatial placement of the visual probes on the screen to explore the image space and reconstruct simple imagined shapes within approximately two minutes or less using just single-channel EEG data. Additionally, by leveraging stable diffusion models, reconstructed mental images can be transformed into realistic and detailed visual representations. Whilst we expect that similar results might be achievable with e.g. eye-tracking techniques, our work shows that symbiotic human-AI interaction can significantly increase BCI bit-rates by more than a factor 5x, providing a platform for future development of AI-augmented BCI.",
      "code_url": null
    },
    "2511.19312v1": {
      "title": "Human-AI Teaming Under Deception: An Implicit BCI Safeguards Drone Team Performance in Virtual Reality",
      "url": "http://arxiv.org/abs/2511.19312v1",
      "authors": "Christopher Baker, Stephen Hinton, Akashdeep Nijjar, Riccardo Poli, Caterina Cinel, Tom Reed, Stephen Fairclough",
      "update_time": "2025-11-24",
      "abstract": "Human-AI teams can be vulnerable to catastrophic failure when feedback from the AI is incorrect, especially under high cognitive workload. Traditional team aggregation methods, such as voting, are susceptible to these AI errors, which can actively bias the behaviour of each individual and inflate the likelihood of an erroneous group decision. We hypothesised that a collaborative Brain-Computer Interface (cBCI) using neural activity collected before a behavioural decision is made can provide a source of information that is decoupled from this biased behaviour, thereby protecting the team from the deleterious influence of AI error. We tested this in a VR drone surveillance task where teams of operators faced high workload and systematically misleading AI cues, comparing traditional behaviour-based team strategies against a purely Neuro-Decoupled Team (NDT) that used only BCI confidence scores derived from pre-response EEG. Under AI deception, behaviour-based teams catastrophically failed, with Majority Vote accuracy collapsing to 44%. The NDT, however, maintained 98% accuracy, a statistically significant synergistic gain over even the team's best individual performer (p < .001). This was explained by a neuro-behavioural decoupling, where the BCI's predictions remained highly accurate while the operator's subjective confidence became an unreliable signal. We conclude that an implicit BCI provides resilience by learning to adapt its neural strategy, shifting from relying on signals of efficient, autopilot processing in simple conditions to interpreting signatures of effortful deliberation when confronted with cognitive conflict. This demonstrates a system that leverages the context of the neural signal to defend against AI-induced error in high-stakes environments.",
      "code_url": null
    },
    "2511.18940v1": {
      "title": "Geometry-Aware Deep Congruence Networks for Manifold Learning in Cross-Subject Motor Imagery",
      "url": "http://arxiv.org/abs/2511.18940v1",
      "authors": "Sanjeev Manivannan, Chandrashekar Lakshminarayan",
      "update_time": "2025-11-24",
      "abstract": "Cross-subject motor-imagery decoding remains a major challenge in EEG-based brain-computer interfaces due to strong subject variability and the curved geometry of covariance matrices on the symmetric positive definite (SPD) manifold. We address the zero-shot cross-subject setting, where no target-subject labels or adaptation are allowed, by introducing novel geometry-aware preprocessing modules and deep congruence networks that operate directly on SPD covariance matrices. Our preprocessing modules, DCR and RiFU, extend Riemannian Alignment by improving action separation while reducing subject-specific distortions. We further propose two manifold classifiers, SPD-DCNet and RiFUNet, which use hierarchical congruence transforms to learn discriminative, subject-invariant covariance representations. On the BCI-IV 2a benchmark, our framework improves cross-subject accuracy by 3-4% over the strongest classical baselines, demonstrating the value of geometry-aware transformations for robust EEG decoding.",
      "code_url": null
    },
    "2511.20696v1": {
      "title": "Prototype-Guided Non-Exemplar Continual Learning for Cross-subject EEG Decoding",
      "url": "http://arxiv.org/abs/2511.20696v1",
      "authors": "Dan Li, Hye-Bin Shin, Yeon-Woo Choi",
      "update_time": "2025-11-24",
      "abstract": "Due to the significant variability in electroencephalogram (EEG) signals across individuals, knowledge acquired from previous subjects is often overwritten as new subjects are introduced in continual EEG decoding task. Current works mainly rely on storing the historical data of seen subjects as a replay buffer to prevent forgetting. However, privacy concerns or memory constraints make keeping such data impractical. Instead, we propose a Prototype-guided Non-Exemplar Continual Learning (ProNECL)framework that preserves prior knowledge without accessing any historical EEG samples. ProNECL constructs class-level prototypes to summarize discriminative representations from each subject and incrementally aligns new feature spaces with the global prototype memory through cross-subject feature alignment and knowledge distillation. Validated on the BCI Competition IV 2a and 2b datasets, our framework effectively balances knowledge retention and adaptability, achieving superior performance in cross-subject continual EEG decoding tasks.",
      "code_url": null
    },
    "2511.18294v1": {
      "title": "MultiDiffNet: A Multi-Objective Diffusion Framework for Generalizable Brain Decoding",
      "url": "http://arxiv.org/abs/2511.18294v1",
      "authors": "Mengchun Zhang, Kateryna Shapovalenko, Yucheng Shao, Eddie Guo, Parusha Pradhan",
      "update_time": "2025-11-23",
      "abstract": "Neural decoding from electroencephalography (EEG) remains fundamentally limited by poor generalization to unseen subjects, driven by high inter-subject variability and the lack of large-scale datasets to model it effectively. Existing methods often rely on synthetic subject generation or simplistic data augmentation, but these strategies fail to scale or generalize reliably. We introduce \\textit{MultiDiffNet}, a diffusion-based framework that bypasses generative augmentation entirely by learning a compact latent space optimized for multiple objectives. We decode directly from this space and achieve state-of-the-art generalization across various neural decoding tasks using subject and session disjoint evaluation. We also curate and release a unified benchmark suite spanning four EEG decoding tasks of increasing complexity (SSVEP, Motor Imagery, P300, and Imagined Speech) and an evaluation protocol that addresses inconsistent split practices in prior EEG research. Finally, we develop a statistical reporting framework tailored for low-trial EEG settings. Our work provides a reproducible and open-source foundation for subject-agnostic EEG decoding in real-world BCI systems.",
      "code_url": null
    }
  },
  "fMRI": {
    "2512.03912v1": {
      "title": "Parsimonious Clustering of Covariance Matrices",
      "url": "http://arxiv.org/abs/2512.03912v1",
      "authors": "Yixi Xu, Yi Zhao",
      "update_time": "2025-12-03",
      "abstract": "Functional connectivity (FC) derived from functional magnetic resonance imaging (fMRI) data offers vital insights for understanding brain function and neurological and psychiatric disorders. Unsupervised clustering methods are desired to group individuals based on shared features, facilitating clinical diagnosis. In this study, a parsimonious clustering model is proposed, which integrates the Mixture-of-Experts (MoE) and covariance regression framework, to cluster individuals based on FC captured by data covariance matrices in resting-state fMRI studies. The model assumes common linear projections across covariance matrices and a generalized linear model with covariates, allowing for flexible yet interpretable projection-specific clustering solutions. To evaluate the performance of the proposed framework, extensive simulation studies are conducted to assess clustering accuracy and robustness. The approach is applied to resting-state fMRI data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Subgroups are identified based on brain coherence and simultaneously uncover the association with demographic factors and cognitive functions.",
      "code_url": null
    },
    "2512.02503v1": {
      "title": "Individual-specific precision neuroimaging of learning-related plasticity",
      "url": "http://arxiv.org/abs/2512.02503v1",
      "authors": "Simon Leipold, Ryssa Moffat",
      "update_time": "2025-12-02",
      "abstract": "Studying learning-related plasticity is central to understanding the acquisition of complex skills, for example learning to master a musical instrument. Over the past three decades, conventional group-based functional magnetic resonance imaging (fMRI) studies have advanced our understanding of how humans' neural representations change during skill acquisition. However, group-based fMRI studies average across heterogeneous learners and often rely on coarse pre- versus post-training comparisons, limiting the spatial and temporal precision with which neural changes can be estimated. Here, we outline an individual-specific precision approach that tracks neural changes within individuals by collecting high-quality neuroimaging data frequently over the course of training, mapping brain function in each person's own anatomical space, and gathering detailed behavioral measures of learning, allowing neural trajectories to be directly linked to individual learning progress. Complementing fMRI with mobile neuroimaging methods, such as functional near-infrared spectroscopy (fNIRS), will enable researchers to track plasticity during naturalistic practice and across extended time scales. This multi-modal approach will enhance sensitivity to individual learning trajectories and will offer more nuanced insights into how neural representations change with training. We also discuss how findings can be generalized beyond individuals, including through statistical methods based on replication in additional individuals. Together, this approach allows researchers to design highly informative longitudinal training studies that advance a mechanistic, personalized account of skill learning in the human brain.",
      "code_url": null
    },
    "2512.01513v1": {
      "title": "Dynamic functional brain connectivity results depend on modeling assumptions: comparing frequentist and Bayesian hypothesis tests",
      "url": "http://arxiv.org/abs/2512.01513v1",
      "authors": "Hester Huijsdens, Linda Geerligs, Max Hinne",
      "update_time": "2025-12-01",
      "abstract": "Understanding the temporal dynamics of functional brain connectivity is important for addressing various questions in network neuroscience, such as how connectivity affects cognition and changes with disease. A fundamental challenge is to evaluate whether connectivity truly exhibits dynamics, or simply is static. The most common frequentist approach uses sliding-window methods to model functional connectivity over time, but this requires defining appropriate sampling distributions and hyperparameters, such as window length, which imposes specific assumptions on the dynamics. Here, we explore how these assumptions influence the detection of dynamic connectivity, and introduce an alternative approach based on Bayesian hypothesis testing with Wishart processes. This framework encodes assumptions through prior distributions, allowing prior knowledge on the time-dependent structure of connectivity to be incorporated into the model. Moreover, this framework provides evidence for both dynamic and static connectivity, offering additional information. Using simulations, we compare the frequentist and Bayesian approaches and demonstrate how different assumptions affect the detection of dynamic connectivity. Finally, by applying both approaches to an fMRI working-memory task, we find that conclusions at the individual level vary with modeling choices, while group-level results are more robust. Our work highlights the importance of carefully considering modeling assumptions when evaluating dynamic connectivity.",
      "code_url": null
    },
    "2512.01189v1": {
      "title": "fMRI2GES: Co-speech Gesture Reconstruction from fMRI Signal with Dual Brain Decoding Alignment",
      "url": "http://arxiv.org/abs/2512.01189v1",
      "authors": "Chunzheng Zhu, Jialin Shao, Jianxin Lin, Yijun Wang, Jing Wang, Jinhui Tang, Kenli Li",
      "update_time": "2025-12-01",
      "abstract": "Understanding how the brain responds to external stimuli and decoding this process has been a significant challenge in neuroscience. While previous studies typically concentrated on brain-to-image and brain-to-language reconstruction, our work strives to reconstruct gestures associated with speech stimuli perceived by brain. Unfortunately, the lack of paired \\{brain, speech, gesture\\} data hinders the deployment of deep learning models for this purpose. In this paper, we introduce a novel approach, \\textbf{fMRI2GES}, that allows training of fMRI-to-gesture reconstruction networks on unpaired data using \\textbf{Dual Brain Decoding Alignment}. This method relies on two key components: (i) observed texts that elicit brain responses, and (ii) textual descriptions associated with the gestures. Then, instead of training models in a completely supervised manner to find a mapping relationship among the three modalities, we harness an fMRI-to-text model, a text-to-gesture model with paired data and an fMRI-to-gesture model with unpaired data, establishing dual fMRI-to-gesture reconstruction patterns. Afterward, we explicitly align two outputs and train our model in a self-supervision way. We show that our proposed method can reconstruct expressive gestures directly from fMRI recordings. We also investigate fMRI signals from different ROIs in the cortex and how they affect generation results. Overall, we provide new insights into decoding co-speech gestures, thereby advancing our understanding of neuroscience and cognitive science.",
      "code_url": null
    },
    "2511.22870v1": {
      "title": "Scalable Diffusion Transformer for Conditional 4D fMRI Synthesis",
      "url": "http://arxiv.org/abs/2511.22870v1",
      "authors": "Jungwoo Seo, David Keetae Park, Shinjae Yoo, Jiook Cha",
      "update_time": "2025-11-28",
      "abstract": "Generating whole-brain 4D fMRI sequences conditioned on cognitive tasks remains challenging due to the high-dimensional, heterogeneous BOLD dynamics across subjects/acquisitions and the lack of neuroscience-grounded validation. We introduce the first diffusion transformer for voxelwise 4D fMRI conditional generation, combining 3D VQ-GAN latent compression with a CNN-Transformer backbone and strong task conditioning via AdaLN-Zero and cross-attention. On HCP task fMRI, our model reproduces task-evoked activation maps, preserves the inter-task representational structure observed in real data (RSA), achieves perfect condition specificity, and aligns ROI time-courses with canonical hemodynamic responses. Performance improves predictably with scale, reaching task-evoked map correlation of 0.83 and RSA of 0.98, consistently surpassing a U-Net baseline on all metrics. By coupling latent diffusion with a scalable backbone and strong conditioning, this work establishes a practical path to conditional 4D fMRI synthesis, paving the way for future applications such as virtual experiments, cross-site harmonization, and principled augmentation for downstream neuroimaging models.",
      "code_url": null
    },
    "2511.22178v1": {
      "title": "Enhanced Graph Convolutional Network with Chebyshev Spectral Graph and Graph Attention for Autism Spectrum Disorder Classification",
      "url": "http://arxiv.org/abs/2511.22178v1",
      "authors": "Adnan Ferdous Ashrafi, Hasanul Kabir",
      "update_time": "2025-11-27",
      "abstract": "ASD is a complicated neurodevelopmental disorder marked by variation in symptom presentation and neurological underpinnings, making early and objective diagnosis extremely problematic. This paper presents a Graph Convolutional Network (GCN) model, incorporating Chebyshev Spectral Graph Convolution and Graph Attention Networks (GAT), to increase the classification accuracy of ASD utilizing multimodal neuroimaging and phenotypic data. Leveraging the ABIDE I dataset, which contains resting-state functional MRI (rs-fMRI), structural MRI (sMRI), and phenotypic variables from 870 patients, the model leverages a multi-branch architecture that processes each modality individually before merging them via concatenation. Graph structure is encoded using site-based similarity to generate a population graph, which helps in understanding relationship connections across individuals. Chebyshev polynomial filters provide localized spectral learning with lower computational complexity, whereas GAT layers increase node representations by attention-weighted aggregation of surrounding information. The proposed model is trained using stratified five-fold cross-validation with a total input dimension of 5,206 features per individual. Extensive trials demonstrate the enhanced model's superiority, achieving a test accuracy of 74.82\\% and an AUC of 0.82 on the entire dataset, surpassing multiple state-of-the-art baselines, including conventional GCNs, autoencoder-based deep neural networks, and multimodal CNNs.",
      "code_url": null
    },
    "2511.21605v1": {
      "title": "Detecting absence: A dedicated prediction-error signal emerging in the auditory thalamus",
      "url": "http://arxiv.org/abs/2511.21605v1",
      "authors": "Alejandro Tabas, Heike S\u00f6nnichsen, Sandeep Kaur, Marco Meixner, Katharina von Kriegstein",
      "update_time": "2025-11-26",
      "abstract": "How does the brain know what is out there and what is not? Living organisms cannot rely solely on sensory signals for perception because they are noisy and ambiguous. To transform sensory signals into stable percepts, the brain uses its prior knowledge or beliefs. Current theories describe perceptual beliefs as probability distributions over the features of the stimuli, summarised by their mean and variance. Beliefs are updated by feature prediction errors: the mismatch between expected and observed feature values. This framework explains how the brain encodes unexpected changes in stimulus features (e.g., higher or lower pitch, stronger or weaker motion). How the brain updates beliefs about a stimulus' presence or absence is, however, unclear.   We propose that the detection of absence relies on a distinct form of prediction error dedicated to reducing the beliefs on stimulus occurrence. We call this signal absence prediction error. Using the human auditory system as a model for sensory processing, we developed a paradigm designed to test this hypothesis. fMRI results showed that absence prediction error is encoded in the auditory thalamus and cortex, indicating that absence is explicitly represented in subcortical sensory pathways. Moreover, while feature prediction error is already encoded in the auditory midbrain, absence prediction error was not, implying that absence-related error signals are supported by a different circuit.   These results identify a neural mechanism for the detection of sensory absence. Such mechanisms may be disrupted in conditions such as psychosis, where predictions about absence and presence are impaired.",
      "code_url": null
    },
    "2511.21760v1": {
      "title": "fMRI-LM: Towards a Universal Foundation Model for Language-Aligned fMRI Understanding",
      "url": "http://arxiv.org/abs/2511.21760v1",
      "authors": "Yuxiang Wei, Yanteng Zhang, Xi Xiao, Chengxuan Qian, Tianyang Wang, Vince D. Calhoun",
      "update_time": "2025-11-24",
      "abstract": "Recent advances in multimodal large language models (LLMs) have enabled unified reasoning across images, audio, and video, but extending such capability to brain imaging remains largely unexplored. Bridging this gap is essential to link neural activity with semantic cognition and to develop cross-modal brain representations. To this end, we present fMRI-LM, a foundational model that bridges functional MRI (fMRI) and language through a three-stage framework. In Stage 1, we learn a neural tokenizer that maps fMRI into discrete tokens embedded in a language-consistent space. In Stage 2, a pretrained LLM is adapted to jointly model fMRI tokens and text, treating brain activity as a sequence that can be temporally predicted and linguistically described. To overcome the lack of natural fMRI-text pairs, we construct a large descriptive corpus that translates diverse imaging-based features into structured textual descriptors, capturing the low-level organization of fMRI signals. In Stage 3, we perform multi-task, multi-paradigm instruction tuning to endow fMRI-LM with high-level semantic understanding, supporting diverse downstream applications. Across various benchmarks, fMRI-LM achieves strong zero-shot and few-shot performance, and adapts efficiently with parameter-efficient tuning (LoRA), establishing a scalable pathway toward a language-aligned, universal model for structural and semantic understanding of fMRI.",
      "code_url": null
    },
    "2512.00063v1": {
      "title": "Exploring the changes in brain network SC-FC coupling patterns of partial sleep deprivation based on DTI-fMRI fusion analysis",
      "url": "http://arxiv.org/abs/2512.00063v1",
      "authors": "Mengyuan Liu, Jing Hu, Zhenzhen Ru, Ruomeng Quan, Xu Zhang, Ning Qiang, Jin Li",
      "update_time": "2025-11-24",
      "abstract": "Sleep disorder is a serious global public health issue, with cognitive-emotional dysfunction being a core symptom. The analysis of multimodal MRI data provides an effective method for detecting sleep deprivation-induced neural network abnormalities. The structure-function coupling (SC-FC) integrates functional connectivity with white matter structural information, which can enable comprehensive detection of brain network abnormalities and offer quantitative measures of sleep deprivation-induced neural damage. This study integrates diffusion tensor imaging (DTI) and resting-state fMRI (rs-fMRI) to systematically investigate brain network reorganization and their relationship with emotional functions in partial sleep deprivation (PSD). Our methodology employed DTI to construct structural connectivity (SC) networks and rs-fMRI to establish functional connectivity (FC) networks, then construct SC-FC coupling model . The experiment included 16 healthy controls (HC) and 20 PSD patients, with comprehensive whole-brain and nodal-level SC-FC analyses performed. The results show that (1) severe FC disruptions in PSD patients involving the limbic system, default mode network, sensorimotor network, and visual networks; (2) altered SC in default mode, sensorimotor, visual, language, and auditory networks; (3) significant SC-FC decoupling in these networks; and (4) strong correlations between these neural changes and clinical measures (KSQ and HADS scores). The SC-FC coupling approach achieved comprehensive detection of PSD-related network abnormalities. Compared to single-modal approaches, this integrated SC-FC analysis provides more comprehensive biomarkers for sleep-related emotional dysregulation. This innovative multimodal neuroimaging approach elucidates the neural mechanisms of SC-FC imbalance induced by PSD, establishing novel biomarkers for sleep-mediated emotional dysregulation.",
      "code_url": null
    },
    "2511.18781v1": {
      "title": "A Novel Dual-Stream Framework for dMRI Tractography Streamline Classification with Joint dMRI and fMRI Data",
      "url": "http://arxiv.org/abs/2511.18781v1",
      "authors": "Haotian Yan, Bocheng Guo, Jianzhong He, Nir A. Sochen, Ofer Pasternak, Lauren J O'Donnell, Fan Zhang",
      "update_time": "2025-11-24",
      "abstract": "Streamline classification is essential to identify anatomically meaningful white matter tracts from diffusion MRI (dMRI) tractography. However, current streamline classification methods rely primarily on the geometric features of the streamline trajectory, failing to distinguish between functionally distinct fiber tracts with similar pathways. To address this, we introduce a novel dual-stream streamline classification framework that jointly analyzes dMRI and functional MRI (fMRI) data to enhance the functional coherence of tract parcellation. We design a novel network that performs streamline classification using a pretrained backbone model for full streamline trajectories, while augmenting with an auxiliary network that processes fMRI signals from fiber endpoint regions. We demonstrate our method by parcellating the corticospinal tract (CST) into its four somatotopic subdivisions. Experimental results from ablation studies and comparisons with state-of-the-art methods demonstrate our approach's superior performance.",
      "code_url": null
    }
  },
  "MEG": {
    "2512.03458v1": {
      "title": "A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses",
      "url": "http://arxiv.org/abs/2512.03458v1",
      "authors": "Maryam Maghsoudi, Mohsen Rezaeizadeh, Shihab Shamma",
      "update_time": "2025-12-03",
      "abstract": "Decoding imagined speech engages complex neural processes that are difficult to interpret due to uncertainty in timing and the limited availability of imagined-response datasets. In this study, we present a Magnetoencephalography (MEG) dataset collected from trained musicians as they imagined and listened to musical and poetic stimuli. We show that both imagined and perceived brain responses contain consistent, condition-specific information. Using a sliding-window ridge regression model, we first mapped imagined responses to listened responses at the single-subject level, but found limited generalization across subjects. At the group level, we developed an encoder-decoder convolutional neural network with a subject-specific calibration layer that produced stable and generalizable mappings. The CNN consistently outperformed the null model, yielding significantly higher correlations between predicted and true listened responses for nearly all held-out subjects. Our findings demonstrate that imagined neural activity can be transformed into perception-like responses, providing a foundation for future brain-computer interface applications involving imagined speech and music.",
      "code_url": null
    },
    "2512.01443v1": {
      "title": "MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification",
      "url": "http://arxiv.org/abs/2512.01443v1",
      "authors": "Xabier de Zuazo, Ibon Saratxaga, Eva Navas",
      "update_time": "2025-12-01",
      "abstract": "We present Conformer-based decoders for the LibriBrain 2025 PNPL competition, targeting two foundational MEG tasks: Speech Detection and Phoneme Classification. Our approach adapts a compact Conformer to raw 306-channel MEG signals, with a lightweight convolutional projection layer and task-specific heads. For Speech Detection, a MEG-oriented SpecAugment provided a first exploration of MEG-specific augmentation. For Phoneme Classification, we used inverse-square-root class weighting and a dynamic grouping loader to handle 100-sample averaged examples. In addition, a simple instance-level normalization proved critical to mitigate distribution shifts on the holdout split. Using the official Standard track splits and F1-macro for model selection, our best systems achieved 88.9% (Speech) and 65.8% (Phoneme) on the leaderboard, surpassing the competition baselines and ranking within the top-10 in both tasks. For further implementation details, the technical documentation, source code, and checkpoints are available at https://github.com/neural2speech/libribrain-experiments.",
      "code_url": null
    },
    "2511.23162v1": {
      "title": "Estimating the Event-Related Potential from Few EEG Trials",
      "url": "http://arxiv.org/abs/2511.23162v1",
      "authors": "Anders Vestergaard N\u00f8rskov, Kasper J\u00f8rgensen, Alexander Neergaard Zahid, Morten M\u00f8rup",
      "update_time": "2025-11-28",
      "abstract": "Event-related potentials (ERP) are measurements of brain activity with wide applications in basic and clinical neuroscience, that are typically estimated using the average of many trials of electroencephalography signals (EEG) to sufficiently reduce noise and signal variability. We introduce EEG2ERP, a novel uncertainty-aware autoencoder approach that maps an arbitrary number of EEG trials to their associated ERP. To account for the ERP uncertainty we use bootstrapped training targets and introduce a separate variance decoder to model the uncertainty of the estimated ERP. We evaluate our approach in the challenging zero-shot scenario of generalizing to new subjects considering three different publicly available data sources; i) the comprehensive ERP CORE dataset that includes over 50,000 EEG trials across six ERP paradigms from 40 subjects, ii) the large P300 Speller BCI dataset, and iii) a neuroimaging dataset on face perception consisting of both EEG and magnetoencephalography (MEG) data. We consistently find that our method in the few trial regime provides substantially better ERP estimates than commonly used conventional and robust averaging procedures. EEG2ERP is the first deep learning approach to map EEG signals to their associated ERP, moving toward reducing the number of trials necessary for ERP research. Code is available at https://github.com/andersxa/EEG2ERP",
      "code_url": null
    },
    "2511.08679v1": {
      "title": "Hunting for Neutrino Texture Zeros with Muon and Tau Flavor Violation",
      "url": "http://arxiv.org/abs/2511.08679v1",
      "authors": "Lorenzo Calibbi, Xiyuan Gao, Man Yuan",
      "update_time": "2025-11-11",
      "abstract": "We revisit the minimal type II seesaw mechanism generating the Majorana neutrino mass matrix $M^\u03bd$, under the assumption that two entries of $M^\u03bd$ vanish. Such flavor structures are known as two-zero textures. Processes with charged lepton flavor violation (CLFV), absent in the Standard Model (SM), can have sizable rates in this framework and are directly linked to the flavor structure of $M^\u03bd$. For each allowed two-zero texture, we quantify the predicted correlations among various CLFV observables using current neutrino oscillation data and show that they lead to distinctive patterns of CLFV processes that could be discriminated between at running and upcoming experiments. In addition, together with information from colliders, the sensitivity of these correlations to renormalization group (RG) effects could shed light on the potentially ultra-high scale where new dynamics (e.g. some underlying flavor symmetry) give rise to the two-zero texture. Furthermore, we find that certain zero textures, although not third-generation specific, can suppress $\u03bc\\to e$ transitions while allowing the rate of the process $\u03c4\\to \\bar\u03bcee$ to be within the future experimental sensitivity, even when the RG evolution is taken into account. The lowest possible cut-off scale of the effective theory, constructed by treating the two-zero flavor structure of $M^\u03bd$ as a CLFV spurion, can therefore reach $5-6$ TeV. Our results provide further motivation for searches for $\u03c4$ CLFV at Belle II, as probes of new physics complementary to MEG II and the upcoming Mu3e, COMET, and Mu2e experiments, as well as for collider searches for doubly charged scalar bosons.",
      "code_url": null
    },
    "2511.07082v1": {
      "title": "The Use of O2 in Gas Mixtures for Drift Chambers",
      "url": "http://arxiv.org/abs/2511.07082v1",
      "authors": "A. M. Baldini, L. Bianco, H. Benmansour, G. Cavoto, F. Cei, M. Chiappini, A. Corvaglia, M. Francesconi, E. Gabbrielli, L. Galli, G. Gallucci, F. Grancagnolo, E. G. Grandoni, M. Grassi, F. Leonetti, D. Nicolo', M. Panareo, D. Pasciuto, A. Papa, F. Renga, S. Scarpellini, A. Venturini, C. Voena",
      "update_time": "2025-11-10",
      "abstract": "The use of Oxygen in gas mixtures for drift chambers is highly discouraged because Oxygen, being strongly electronegative, is generally believed to lead, even in very small quantities, to extremely reduced drift electron attachment values, thus preventing the detector's operation.The drift chamber of the MEG II experiment at PSI has been operating for several years with a gas mixture that mainly contains He:Isobutane in relative proportions of 90:10% by molar concentration, in addition to 1.5% Isopropanol and 0.5% Oxygen. Oxygen and Isopropanol are essential for the proper functioning of the chamber. The electron attachment in the mixture used has proven negligible for the proper operation of the chamber and agrees well with the Garfield++ simulation after correctly accounting for the three-body attachment simulation.",
      "code_url": null
    },
    "2511.04078v1": {
      "title": "Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment",
      "url": "http://arxiv.org/abs/2511.04078v1",
      "authors": "Zehui Feng, Chenqi Zhang, Mingru Wang, Minuo Wei, Shiwei Cheng, Cuntai Guan, Ting Han",
      "update_time": "2025-11-06",
      "abstract": "Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI remains a fundamental challenge due to subject variability and the entangled nature of visual features. Existing approaches primarily align neural activity directly with visual embeddings, but visual-only representations often fail to capture latent semantic dimensions, limiting interpretability and deep robustness. To address these limitations, we propose Bratrix, the first end-to-end framework to achieve multimodal Language-Anchored Vision-Brain alignment. Bratrix decouples visual stimuli into hierarchical visual and linguistic semantic components, and projects both visual and brain representations into a shared latent space, enabling the formation of aligned visual-language and brain-language embeddings. To emulate human-like perceptual reliability and handle noisy neural signals, Bratrix incorporates a novel uncertainty perception module that applies uncertainty-aware weighting during alignment. By leveraging learnable language-anchored semantic matrices to enhance cross-modal correlations and employing a two-stage training strategy of single-modality pretraining followed by multimodal fine-tuning, Bratrix-M improves alignment precision. Extensive experiments on EEG, MEG, and fMRI benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and captioning performance compared to state-of-the-art methods, specifically surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.",
      "code_url": null
    },
    "2511.01784v1": {
      "title": "Variational Representational Similarity Analysis (vRSA) for M/EEG",
      "url": "http://arxiv.org/abs/2511.01784v1",
      "authors": "Alex Lepauvre, Lucia Melloni, Karl Friston, Peter Zeidman",
      "update_time": "2025-11-03",
      "abstract": "This paper introduces variational representational similarity analysis RSA (vRSA) for electromagnetic recordings of neural responses (e.g., EEG, MEG, ECoG or LFP). Variational RSA is a Bayesian approach for testing whether the similarity of stimuli or experimental conditions is expressed in univariate or multivariate neural recordings. Extending an approach previously introduced in the context of functional MRI, vRSA decomposes the condition-by-condition data covariance matrix into hypothesised effects and observation noise, thereby casting RSA as a covariance component estimation problem. In this context, peristimulus time may be treated as an experimental factor, enabling one to test for the probability that different experimental effects are expressed in data at different times. Variational Bayesian methods are used for model estimation and model comparison, which confer a number of advantages over classical approaches, including statistically efficient hypothesis testing, quantification of uncertainty using Bayesian credible intervals and computational efficiency. After introducing the theory, we provide a worked example using openly available EEG data. Software functions implementing vRSA for the SPM software package accompany this paper, together with exemplar analysis scripts.",
      "code_url": null
    },
    "2511.00697v1": {
      "title": "Smooth Models of Fibered Partially Hyperbolic Systems",
      "url": "http://arxiv.org/abs/2511.00697v1",
      "authors": "Jonathan DeWitt, Meg Doucette, Oliver Wang",
      "update_time": "2025-11-01",
      "abstract": "We study fibered partially hyperbolic diffeomorphisms. We show that as long as certain topological obstructions vanish and as long as homological minimum expansion dominates the distortion on the fibers that a fibered partially hyperbolic system can be homotoped to a fibered partially hyperbolic system with a $C^{\\infty}$-center fibering. In addition, we study obstructions to the existence of smooth lifts of Anosov diffeomorphisms to bundles. In particular, we give an example of smooth topologically trivial bundle over a torus, where an Anosov diffeomorphism can lift continuously but not smoothly to the bundle.",
      "code_url": null
    },
    "2510.25913v1": {
      "title": "Risk-Aware Safety Filters with Poisson Safety Functions and Laplace Guidance Fields",
      "url": "http://arxiv.org/abs/2510.25913v1",
      "authors": "Gilbert Bahati, Ryan M. Bena, Meg Wilkinson, Pol Mestres, Ryan K. Cosner, Aaron D. Ames",
      "update_time": "2025-10-29",
      "abstract": "Robotic systems navigating in real-world settings require a semantic understanding of their environment to properly determine safe actions. This work aims to develop the mathematical underpinnings of such a representation -- specifically, the goal is to develop safety filters that are risk-aware. To this end, we take a two step approach: encoding an understanding of the environment via Poisson's equation, and associated risk via Laplace guidance fields. That is, we first solve a Dirichlet problem for Poisson's equation to generate a safety function that encodes system safety as its 0-superlevel set. We then separately solve a Dirichlet problem for Laplace's equation to synthesize a safe \\textit{guidance field} that encodes variable levels of caution around obstacles -- by enforcing a tunable flux boundary condition. The safety function and guidance fields are then combined to define a safety constraint and used to synthesize a risk-aware safety filter which, given a semantic understanding of an environment with associated risk levels of environmental features, guarantees safety while prioritizing avoidance of higher risk obstacles. We demonstrate this method in simulation and discuss how \\textit{a priori} understandings of obstacle risk can be directly incorporated into the safety filter to generate safe behaviors that are risk-aware.",
      "code_url": null
    },
    "2510.23742v1": {
      "title": "Molecular Gas in Major Mergers Hosting Dual and Single AGN at <10 kpc Nuclear Separations",
      "url": "http://arxiv.org/abs/2510.23742v1",
      "authors": "Makoto A. Johnstone, Ezequiel Treister, Franz E. Bauer, Chin-Shin Chang, Claudia Cicone, Michael J. Koss, Ignacio del Moral-Castro, Francisco Muller-Sanchez, George C. Privon, Claudio Ricci, Nick Scoville, Giacomo Venturi, Loreto Barcos-Mu\u00f1oz, Lee Armus, Laura Blecha, Caitlin Casey, Julia Comerford, Aaron Evans, Taiki Kawamuro, Anne M. Medling, Hugo Messias, Neil Nagar, Alejandra Rojas, David Sanders, Benny Trakhtenbrot, Vivian U, Meg Urry",
      "update_time": "2025-10-27",
      "abstract": "We present high-resolution ($\\sim$50$-$100 pc) Atacama Large Millimeter Array (ALMA) observations of $^{12}$CO(2-1) or $^{12}$CO(1-0) emission in seven local ($z$ $\\lesssim$ 0.05) major mergers -- five of which are dual active galactic nuclei (AGN) systems, and two of which are single AGN systems. We model the molecular gas kinematics through rotating disk profiles using a Bayesian Markov chain Monte Carlo approach. The residuals were then used to isolate non-rotating components of the molecular gas -- the most likely contributor to future SMBH growth. We find that more massive SMBHs have higher surface densities of non-rotating molecular gas within their sphere of influence. This potential molecular gas supply, however, does not correlate with the current accretion efficiency of the SMBHs, suggesting that only a fraction of the observed non-rotating gas is currently reaching the SMBH. Finally, we tentatively find no significant differences in the nuclear molecular gas masses of single AGN and dual AGN hosts, both within the SMBH sphere of influence and within the central kiloparsec. Our results indicate that the probability of occurrence of the dual AGN phenomenon is likely dependent on AGN variability and/or obscuration rather than the availability of molecular gas in the nuclear regions.",
      "code_url": null
    }
  },
  "neuroAI": {
    "2511.19548v1": {
      "title": "When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics",
      "url": "http://arxiv.org/abs/2511.19548v1",
      "authors": "Yiven, Zhu",
      "update_time": "2025-11-24",
      "abstract": "Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, \"brain-based\" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies \"true\" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.",
      "code_url": null
    },
    "2510.22178v1": {
      "title": "Dopamine-driven synaptic credit assignment in neural networks",
      "url": "http://arxiv.org/abs/2510.22178v1",
      "authors": "Saranraj Nambusubramaniyan, Shervin Safavi, Raja Guru, Andreas Knoblauch",
      "update_time": "2025-10-25",
      "abstract": "Solving the synaptic Credit Assignment Problem(CAP) is central to learning in both biological and artificial neural systems. Finding an optimal solution for synaptic CAP means setting the synaptic weights that assign credit to each neuron for influencing the final output and behavior of neural networks or animals. Gradient-based methods solve this problem in artificial neural networks using back-propagation, however, not in the most efficient way. For instance, back-propagation requires a chain of top-down gradient computations. This leads to an expensive optimization process in terms of computing power and memory linked with well-known weight transport and update locking problems. To address these shortcomings, we take a NeuroAI approach and draw inspiration from neural Reinforcement Learning to develop a derivative-free optimizer for training neural networks, Dopamine. Dopamine is developed for Weight Perturbation (WP) learning that exploits stochastic updating of weights towards optima. It achieves this by minimizing the regret, a form of Reward Prediction Error (RPE) between the expected outcome from the perturbed model and the actual outcome from the unperturbed model. We use this RPE to adjust the learning rate in the network (i.e., creating an adaptive learning rate strategy, similar to the role of dopamine in the brain). We tested the Dopamine optimizer for training multi-layered perceptrons for XOR tasks, and recurrent neural networks for chaotic time series forecasting. Dopamine-trained models demonstrate accelerated convergence and outperform standard WP, and give comparable performance to gradient-based algorithms, while consuming significantly less computation and memory. Overall, the Dopamine optimizer not only finds robust solutions and comparable performance to the state-of-the-art Machine Learning optimizers but is also neurobiologically more plausible.",
      "code_url": null
    },
    "2509.23896v2": {
      "title": "A Computational Perspective on NeuroAI and Synthetic Biological Intelligence",
      "url": "http://arxiv.org/abs/2509.23896v2",
      "authors": "Dhruvik Patel, Md Sayed Tanveer, Jesus Gonzalez-Ferrer, Alon Loeffler, Brett J. Kagan, Mohammed A. Mostajo-Radji, Ge Wang",
      "update_time": "2025-10-09",
      "abstract": "NeuroAI is an emerging field at the intersection of neuroscience and artificial intelligence, where insights from brain function guide the design of intelligent systems. A central area within this field is synthetic biological intelligence (SBI), which combines the adaptive learning properties of biological neural networks with engineered hardware and software. SBI systems provide a platform for modeling neural computation, developing biohybrid architectures, and enabling new forms of embodied intelligence. In this review, we organize the NeuroAI landscape into three interacting domains: hardware, software, and wetware. We outline computational frameworks that integrate biological and non-biological systems and highlight recent advances in organoid intelligence, neuromorphic computing, and neuro-symbolic learning. These developments collectively point toward a new class of systems that compute through interactions between living neural tissue and digital algorithms.",
      "code_url": null
    },
    "2507.06645v2": {
      "title": "Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers",
      "url": "http://arxiv.org/abs/2507.06645v2",
      "authors": "Thomas Klein, Sascha Meyen, Wieland Brendel, Felix A. Wichmann, Kristof Meding",
      "update_time": "2025-11-07",
      "abstract": "Benchmarking models is a key factor for the rapid progress in machine learning (ML) research. Thus, further progress depends on improving benchmarking metrics. A standard metric to measure the behavioral alignment between ML models and human observers is error consistency (EC). EC allows for more fine-grained comparisons of behavior than other metrics such as accuracy, and has been used in the influential Brain-Score benchmark to rank different DNNs by their behavioral consistency with humans. Previously, EC values have been reported without confidence intervals. However, empirically measured EC values are typically noisy -- thus, without confidence intervals, valid benchmarking conclusions are problematic. Here we improve on standard EC in two ways: First, we show how to obtain confidence intervals for EC using a bootstrapping technique, allowing us to derive significance tests for EC. Second, we propose a new computational model relating the EC between two classifiers to the implicit probability that one of them copies responses from the other. This view of EC allows us to give practical guidance to scientists regarding the number of trials required for sufficiently powerful, conclusive experiments. Finally, we use our methodology to revisit popular NeuroAI-results. We find that while the general trend of behavioral differences between humans and machines holds up to scrutiny, many reported differences between deep vision models are statistically insignificant. Our methodology enables researchers to design adequately powered experiments that can reliably detect behavioral differences between models, providing a foundation for more rigorous benchmarking of behavioral alignment.",
      "code_url": null
    },
    "2507.02103v1": {
      "title": "What Neuroscience Can Teach AI About Learning in Continuously Changing Environments",
      "url": "http://arxiv.org/abs/2507.02103v1",
      "authors": "Daniel Durstewitz, Bruno Averbeck, Georgia Koppe",
      "update_time": "2025-07-02",
      "abstract": "Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.",
      "code_url": null
    },
    "2506.04536v3": {
      "title": "NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models",
      "url": "http://arxiv.org/abs/2506.04536v3",
      "authors": "Luca Ghafourpour, Valentin Duruisseaux, Bahareh Tolooshams, Philip H. Wong, Costas A. Anastassiou, Anima Anandkumar",
      "update_time": "2025-10-27",
      "abstract": "Characterizing the cellular properties of neurons is fundamental to understanding their function in the brain. In this quest, the generation of bio-realistic models is central towards integrating multimodal cellular data sets and establishing causal relationships. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. The deterministic formalism of bio-realistic models currently precludes accounting for the natural variability observed experimentally. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on synthetic data generated from bio-realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE enables the efficient generation of synthetic neurons that closely resemble experimental data and exhibit trial-to-trial variability, offering a $4200\\times$ speedup over the numerical solver. NOBLE is the first scaled-up deep learning framework that validates its generalization with real experimental data. To this end, NOBLE captures fundamental neural properties in a unique and emergent manner that opens the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.",
      "code_url": null
    },
    "2505.16080v1": {
      "title": "SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation",
      "url": "http://arxiv.org/abs/2505.16080v1",
      "authors": "Jiayue Liu, Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Kuo Yang, Xu Wang, Yang Wang",
      "update_time": "2025-05-21",
      "abstract": "Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.",
      "code_url": null
    },
    "2502.16238v1": {
      "title": "Brain-Model Evaluations Need the NeuroAI Turing Test",
      "url": "http://arxiv.org/abs/2502.16238v1",
      "authors": "Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi",
      "update_time": "2025-02-22",
      "abstract": "What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \\emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.",
      "code_url": null
    },
    "2501.02402v1": {
      "title": "Asynchronous Hebbian/anti-Hebbian networks",
      "url": "http://arxiv.org/abs/2501.02402v1",
      "authors": "Henrique Reis Aguiar, Matthias H. Hennig",
      "update_time": "2025-01-04",
      "abstract": "Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.",
      "code_url": null
    },
    "2411.18526v2": {
      "title": "NeuroAI for AI Safety",
      "url": "http://arxiv.org/abs/2411.18526v2",
      "authors": "Patrick Mineault, Niccol\u00f2 Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador",
      "update_time": "2025-04-03",
      "abstract": "As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.",
      "code_url": null
    }
  },
  "medical": {
    "2512.04034v1": {
      "title": "Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions",
      "url": "http://arxiv.org/abs/2512.04034v1",
      "authors": "Hong Yang, Devroop Kar, Qi Yu, Alex Ororbia, Travis Desell",
      "update_time": "2025-12-03",
      "abstract": "Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models.",
      "code_url": null
    },
    "2512.03994v1": {
      "title": "Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs",
      "url": "http://arxiv.org/abs/2512.03994v1",
      "authors": "Oren Rachmil, Roy Betser, Itay Gershon, Omer Hofman, Nitay Yakoby, Yuval Meron, Idan Yankelev, Asaf Shabtai, Yuval Elovici, Roman Vainshtein",
      "update_time": "2025-12-03",
      "abstract": "Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection",
      "code_url": null
    },
    "2512.03878v1": {
      "title": "Adhera: A Human-Centered Health Informatics Solution for Reducing Informal Caregiver Burden through Improved Medication Adherence",
      "url": "http://arxiv.org/abs/2512.03878v1",
      "authors": "Zhiyin Zhou",
      "update_time": "2025-12-03",
      "abstract": "The growing global population of older adults, combined with ongoing healthcare workforce shortages, has increased reliance on informal caregivers, including family members and friends who provide unpaid support to individuals with chronic illnesses. Among their daily responsibilities, medication management remains one of the most demanding and error-prone tasks. Non-adherence to prescribed regimens not only undermines patient outcomes but also intensifies caregiver stress, anxiety, and fatigue. Although digital health technologies have proliferated to address adherence, most solutions focus exclusively on patients and neglect the informational and emotional needs of caregivers. This paper introduces Adhera, a caregiver-inclusive health informatics system designed to support medication adherence while reducing caregiver burden. Using a mixed-methods research design that included fifteen semi-structured caregiver interviews, sixty-five survey responses, and five pharmacist consultations, this study identified three primary challenges: caregiver stress related to uncertainty about medication intake, fragmented communication with healthcare professionals, and distrust in existing digital tools. Informed by the CeHRes Roadmap 2.0 and the Triple Bottom Line by Design and Culture (TBLD+C) framework, as well as recent co-design studies involving caregivers, Adhera integrates a sensor-equipped smart pill organizer with a mobile companion application that records intake events, sends real-time reminders, and provides caregivers with synchronized adherence data. Preliminary evaluation suggests that Adhera enhances visibility, improves caregiver confidence, and streamlines medication routines. This study contributes to the field of health informatics by demonstrating how human-centered design and collaborative frameworks can align technical innovation with empathy-driven care.",
      "code_url": null
    },
    "2512.03838v1": {
      "title": "Training and Evaluation of Guideline-Based Medical Reasoning in LLMs",
      "url": "http://arxiv.org/abs/2512.03838v1",
      "authors": "Michael Staniek, Artem Sokolov, Stefan Riezler",
      "update_time": "2025-12-03",
      "abstract": "Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.",
      "code_url": null
    },
    "2512.03804v1": {
      "title": "EfficientECG: Cross-Attention with Feature Fusion for Efficient Electrocardiogram Classification",
      "url": "http://arxiv.org/abs/2512.03804v1",
      "authors": "Hanhui Deng, Xinglin Li, Jie Luo, Zhanpeng Jin, Di Wu",
      "update_time": "2025-12-03",
      "abstract": "Electrocardiogram is a useful diagnostic signal that can detect cardiac abnormalities by measuring the electrical activity generated by the heart. Due to its rapid, non-invasive, and richly informative characteristics, ECG has many emerging applications. In this paper, we study novel deep learning technologies to effectively manage and analyse ECG data, with the aim of building a diagnostic model, accurately and quickly, that can substantially reduce the burden on medical workers. Unlike the existing ECG models that exhibit a high misdiagnosis rate, our deep learning approaches can automatically extract the features of ECG data through end-to-end training. Specifically, we first devise EfficientECG, an accurate and lightweight classification model for ECG analysis based on the existing EfficientNet model, which can effectively handle high-frequency long-sequence ECG data with various leading types. On top of that, we next propose a cross-attention-based feature fusion model of EfficientECG for analysing multi-lead ECG data with multiple features (e.g., gender and age). Our evaluations on representative ECG datasets validate the superiority of our model against state-of-the-art works in terms of high precision, multi-feature fusion, and lightweights.",
      "code_url": null
    },
    "2512.03751v1": {
      "title": "Research on Brain Tumor Classification Method Based on Improved ResNet34 Network",
      "url": "http://arxiv.org/abs/2512.03751v1",
      "authors": "Yufeng Li, Wenchao Zhao, Bo Dang, Weimin Wang",
      "update_time": "2025-12-03",
      "abstract": "Previously, image interpretation in radiology relied heavily on manual methods. However, manual classification of brain tumor medical images is time-consuming and labor-intensive. Even with shallow convolutional neural network models, the accuracy is not ideal. To improve the efficiency and accuracy of brain tumor image classification, this paper proposes a brain tumor classification model based on an improved ResNet34 network. This model uses the ResNet34 residual network as the backbone network and incorporates multi-scale feature extraction. It uses a multi-scale input module as the first layer of the ResNet34 network and an Inception v2 module as the residual downsampling layer. Furthermore, a channel attention mechanism module assigns different weights to different channels of the image from a channel domain perspective, obtaining more important feature information. The results after a five-fold crossover experiment show that the average classification accuracy of the improved network model is approximately 98.8%, which is not only 1% higher than ResNet34, but also only 80% of the number of parameters of the original model. Therefore, the improved network model not only improves accuracy but also reduces clutter, achieving a classification effect with fewer parameters and higher accuracy.",
      "code_url": null
    },
    "2512.03737v1": {
      "title": "AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation",
      "url": "http://arxiv.org/abs/2512.03737v1",
      "authors": "Chuyue Wang, Jie Feng, Yuxi Wu, Hang Zhang, Zhiguo Fan, Bing Cheng, Wei Lin",
      "update_time": "2025-12-03",
      "abstract": "Accurate and reliable search on online healthcare platforms is critical for user safety and service efficacy. Traditional methods, however, often fail to comprehend complex and nuanced user queries, limiting their effectiveness. Large language models (LLMs) present a promising solution, offering powerful semantic understanding to bridge this gap. Despite their potential, deploying LLMs in this high-stakes domain is fraught with challenges, including factual hallucinations, specialized knowledge gaps, and high operational costs. To overcome these barriers, we introduce \\textbf{AR-Med}, a novel framework for \\textbf{A}utomated \\textbf{R}elevance assessment for \\textbf{Med}ical search that has been successfully deployed at scale on the Online Medical Delivery Platforms. AR-Med grounds LLM reasoning in verified medical knowledge through a retrieval-augmented approach, ensuring high accuracy and reliability. To enable efficient online service, we design a practical knowledge distillation scheme that compresses large teacher models into compact yet powerful student models. We also introduce LocalQSMed, a multi-expert annotated benchmark developed to guide model iteration and ensure strong alignment between offline and online performance. Extensive experiments show AR-Med achieves an offline accuracy of over 93\\%, a 24\\% absolute improvement over the original online system, and delivers significant gains in online relevance and user satisfaction. Our work presents a practical and scalable blueprint for developing trustworthy, LLM-powered systems in real-world healthcare applications.",
      "code_url": null
    },
    "2512.03671v1": {
      "title": "Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context",
      "url": "http://arxiv.org/abs/2512.03671v1",
      "authors": "Beatrice Savoldi, Giuseppe Attanasio, Olga Gorodetskaya, Marta Marchiori Manerba, Elisa Bassignana, Silvia Casola, Matteo Negri, Tommaso Caselli, Luisa Bentivogli, Alan Ramponi, Arianna Muti, Nicoletta Balbo, Debora Nozza",
      "update_time": "2025-12-03",
      "abstract": "The rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain.",
      "code_url": null
    },
    "2512.03597v1": {
      "title": "HBFormer: A Hybrid-Bridge Transformer for Microtumor and Miniature Organ Segmentation",
      "url": "http://arxiv.org/abs/2512.03597v1",
      "authors": "Fuchen Zheng, Xinyi Chen, Weixuan Li, Quanjun Li, Junhua Zhou, Xiaojiao Guo, Xuhang Chen, Chi-Man Pun, Shoujun Zhou",
      "update_time": "2025-12-03",
      "abstract": "Medical image segmentation is a cornerstone of modern clinical diagnostics. While Vision Transformers that leverage shifted window-based self-attention have established new benchmarks in this field, they are often hampered by a critical limitation: their localized attention mechanism struggles to effectively fuse local details with global context. This deficiency is particularly detrimental to challenging tasks such as the segmentation of microtumors and miniature organs, where both fine-grained boundary definition and broad contextual understanding are paramount. To address this gap, we propose HBFormer, a novel Hybrid-Bridge Transformer architecture. The 'Hybrid' design of HBFormer synergizes a classic U-shaped encoder-decoder framework with a powerful Swin Transformer backbone for robust hierarchical feature extraction. The core innovation lies in its 'Bridge' mechanism, a sophisticated nexus for multi-scale feature integration. This bridge is architecturally embodied by our novel Multi-Scale Feature Fusion (MFF) decoder. Departing from conventional symmetric designs, the MFF decoder is engineered to fuse multi-scale features from the encoder with global contextual information. It achieves this through a synergistic combination of channel and spatial attention modules, which are constructed from a series of dilated and depth-wise convolutions. These components work in concert to create a powerful feature bridge that explicitly captures long-range dependencies and refines object boundaries with exceptional precision. Comprehensive experiments on challenging medical image segmentation datasets, including multi-organ, liver tumor, and bladder tumor benchmarks, demonstrate that HBFormer achieves state-of-the-art results, showcasing its outstanding capabilities in microtumor and miniature organ segmentation. Code and models are available at: https://github.com/lzeeorno/HBFormer.",
      "code_url": null
    },
    "2512.03541v1": {
      "title": "Toward AI-Ready Medical Imaging Data",
      "url": "http://arxiv.org/abs/2512.03541v1",
      "authors": "Milen Nikolov, Edilberto Amorim, J Harry Caufield, Nayoon Gim, Nomi L Harris, Jared Houghtaling, Xiang Li, Danielle Morrison, Ana\u00efs Rameau, Jamie Shaffer, Hari Trivedi, Monica C Munoz-Torres",
      "update_time": "2025-12-03",
      "abstract": "Medical imaging data plays a vital role in disease diagnosis, monitoring, and clinical research discovery. Biomedical data managers and clinical researchers must navigate a complex landscape of medical imaging infrastructure, input/output tools and data reliability workflow configurations taking months to operationalize.   While standard formats exist for medical imaging data, standard operating procedures (SOPs) for data management are lacking. These data management SOPs are key for developing Findable, Accessible, Interoperable, and Reusable (FAIR) data, a prerequisite for AI-ready datasets.   The National Institutes of Health (NIH) Bridge to Artificial Intelligence (Bridge2AI) Standards Working Group members and domain-expert stakeholders from the Bridge2AI Grand Challenges teams developed data management SOPs for the Digital Imaging and Communications in Medicine (DICOM) format.   We describe novel SOPs applying to both static and cutting edge video imaging modalities. We emphasize steps required for centralized data aggregation, validation, and de-identification, including a review of new defacing methods for facial DICOM scans, anticipating adversarial AI/ML data re-identification methods. Data management vignettes based on Bridge2AI datasets include example parameters for efficient capture of a wide modality spectrum, including datasets from new ophthalmology retinal scans DICOM modalities.",
      "code_url": null
    }
  }
}