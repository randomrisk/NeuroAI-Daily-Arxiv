{
  "Brain": {
    "2406.04328v1": {
      "title": "The Brain's Bitter Lesson: Scaling Speech Decoding With Self-Supervised Learning",
      "url": "http://arxiv.org/abs/2406.04328v1",
      "authors": "Dulhan Jayalath, Gilad Landau, Brendan Shillingford, Mark Woolrich, Oiwi Parker Jones",
      "update_time": "2024-06-06",
      "abstract": "The past few years have produced a series of spectacular advances in the decoding of speech from brain activity. The engine of these advances has been the acquisition of labelled data, with increasingly large datasets acquired from single subjects. However, participants exhibit anatomical and other individual differences, and datasets use varied scanners and task designs. As a result, prior work has struggled to leverage data from multiple subjects, multiple datasets, multiple tasks, and unlabelled datasets. In turn, the field has not benefited from the rapidly growing number of open neural data repositories to exploit large-scale data and deep learning. To address this, we develop an initial set of neuroscience-inspired self-supervised objectives, together with a neural architecture, for representation learning from heterogeneous and unlabelled neural recordings. Experimental results show that representations learned with these objectives generalise across subjects, datasets, and tasks, and are also learned faster than using only labelled data. In addition, we set new benchmarks for two foundational speech decoding tasks. Taken together, these methods now unlock the potential for training speech decoding models with orders of magnitude more existing data."
    },
    "2406.04318v1": {
      "title": "Adaptive Sampling of k-Space in Magnetic Resonance for Rapid Pathology Prediction",
      "url": "http://arxiv.org/abs/2406.04318v1",
      "authors": "Chen-Yu Yen, Raghav Singhal, Umang Sharma, Rajesh Ranganath, Sumit Chopra, Lerrel Pinto",
      "update_time": "2024-06-06",
      "abstract": "Magnetic Resonance (MR) imaging, despite its proven diagnostic utility, remains an inaccessible imaging modality for disease surveillance at the population level. A major factor rendering MR inaccessible is lengthy scan times. An MR scanner collects measurements associated with the underlying anatomy in the Fourier space, also known as the k-space. Creating a high-fidelity image requires collecting large quantities of such measurements, increasing the scan time. Traditionally to accelerate an MR scan, image reconstruction from under-sampled k-space data is the method of choice. However, recent works show the feasibility of bypassing image reconstruction and directly learning to detect disease directly from a sparser learned subset of the k-space measurements. In this work, we propose Adaptive Sampling for MR (ASMR), a sampling method that learns an adaptive policy to sequentially select k-space samples to optimize for target disease detection. On 6 out of 8 pathology classification tasks spanning the Knee, Brain, and Prostate MR scans, ASMR reaches within 2% of the performance of a fully sampled classifier while using only 8% of the k-space, as well as outperforming prior state-of-the-art work in k-space sampling such as EMRT, LOUPE, and DPS."
    },
    "2406.04026v1": {
      "title": "Quantification of Collateral Supply with Local-AIF Dynamic Susceptibility Contrast MRI Predicts Infarct Growth",
      "url": "http://arxiv.org/abs/2406.04026v1",
      "authors": "Mira M. Liu, Niloufar Saadat, Steven P. Roth, Marek A. Niekrasz, Mihai Giurcanu, Timothy J. Carroll, Gregory A. Christoforidis",
      "update_time": "2024-06-06",
      "abstract": "In ischemic stroke, leptomeningeal collaterals can provide compensatory blood flow to tissue at risk despite an occlusion, and impact treatment response and infarct growth. The purpose of this work is to test the hypothesis that local perfusion with an appropriate Local Arterial Input Function (AIF) is needed to quantify the degree of collateral blood supply in tissue distal to an occlusion. Seven experiments were conducted in a pre-clinical middle cerebral artery occlusion model. Magnetic resonance dynamic susceptibility contrast (DSC) was imaged and post-processed as cerebral blood flow maps with both a traditionally chosen single arterial input function (AIF) applied globally to the whole brain (i.e. \"Global-AIF\") and a novel automatic delay and dispersion corrected AIF (i.e. \"Local AIF\") that is sensitive to retrograde flow. Pial collateral recruitment was assessed from x-ray angiograms and infarct growth via serially acquired diffusion weighted MRI scans both blinded to DSC. The degree of collateralization at x-ray correlated strongly with quantitative perfusion determined using the Local AIF in the ischemic penumbra (R2=0.81) compared to a traditionally chosen Global-AIF (R2=0.05). Quantitative perfusion calculated using a Local-AIF was negatively correlated (less infarct progression as local perfusion increased) with infarct growth (R2 = 0.79) compared to Global-AIF (R2=0.02). Local DSC perfusion with a Local-AIF is more accurate for assessing tissue status and degree of leptomeningeal collateralization than traditionally chosen AIFs. These findings support use of a Local-AIF in determining quantitative tissue perfusion with collateral supply in occlusive disease."
    },
    "2406.03762v1": {
      "title": "CORTEX: Large-Scale Brain Simulator Utilizing Indegree Sub-Graph Decomposition on Fugaku Supercomputer",
      "url": "http://arxiv.org/abs/2406.03762v1",
      "authors": "Tianxiang Lyu, Mitsuhisa Sato, Shigeki Aoki, Ryutaro Himeno, Zhe Sun",
      "update_time": "2024-06-06",
      "abstract": "We introduce CORTEX, an algorithmic framework designed for large-scale brain simulation. Leveraging the computational capacity of the Fugaku Supercomputer, CORTEX maximizes available problem size and processing performance. Our primary innovation, Indegree Sub-Graph Decomposition, along with a suite of parallel algorithms, facilitates efficient domain decomposition by segmenting the global graph structure into smaller, identically structured sub-graphs. This segmentation allows for parallel processing of synaptic interactions without inter-process dependencies, effectively eliminating data racing at the thread level without necessitating mutexes or atomic operations. Additionally, this strategy enhances the overlap of communication and computation. Benchmark tests conducted on spiking neural networks, characterized by biological parameters, have demonstrated significant enhancements in both problem size and simulation performance, surpassing the capabilities of the current leading open-source solution, the NEST Simulator. Our work offers a powerful new tool for the field of neuromorphic computing and understanding brain function."
    },
    "2406.03605v1": {
      "title": "Towards the Development of a Tendon-Actuated Galvanometer for Endoscopic Surgical Laser Scanning",
      "url": "http://arxiv.org/abs/2406.03605v1",
      "authors": "Kent K. Yamamoto, Tanner J. Zachem, Behnam Moradkhani, Yash Chitalia, Patrick J. Codd",
      "update_time": "2024-06-05",
      "abstract": "There is a need for precision pathological sensing, imaging, and tissue manipulation in neurosurgical procedures, such as brain tumor resection. Precise tumor margin identification and resection can prevent further growth and protect critical structures. Surgical lasers with small laser diameters and steering capabilities can allow for new minimally invasive procedures by traversing through complex anatomy, then providing energy to sense, visualize, and affect tissue. In this paper, we present the design of a small-scale tendon-actuated galvanometer (TAG) that can serve as an end-effector tool for a steerable surgical laser. The galvanometer sensor design, fabrication, and kinematic modeling are presented and derived. It can accurately rotate up to 30.14 degrees (or a laser reflection angle of 60.28 degrees). A kinematic mapping of input tendon stroke to output galvanometer angle change and a forward-kinematics model relating the end of the continuum joint to the laser end-point are derived and validated."
    },
    "2406.03583v1": {
      "title": "Towards robust radiomics and radiogenomics predictive models for brain tumor characterization",
      "url": "http://arxiv.org/abs/2406.03583v1",
      "authors": "Maria Nadeem, Asma Shaheen, Muhammad F. A. Chaudhary, Hassan Mohy-ud-Din",
      "update_time": "2024-06-05",
      "abstract": "In the context of brain tumor characterization, we focused on two key questions: (a) stability of radiomics features to variability in multiregional segmentation masks obtained with fully-automatic deep segmentation methods and (b) subsequent impact on predictive performance on downstream tasks: IDH prediction and Overall Survival (OS) classification. We further constrained our study to limited computational resources setting which are found in underprivileged, remote, and (or) resource-starved clinical sites in developing countries. We employed seven SOTA CNNs which can be trained with limited computational resources and have demonstrated superior segmentation performance on BraTS challenge. Subsequent selection of discriminatory features was done with RFE-SVM and MRMR. Our study revealed that highly stable radiomics features were: (1) predominantly texture features (79.1%), (2) mainly extracted from WT region (96.1%), and (3) largely representing T1Gd (35.9%) and T1 (28%) sequences. Shape features and radiomics features extracted from the ENC subregion had the lowest average stability. Stability filtering minimized non-physiological variability in predictive models as indicated by an order-of-magnitude decrease in the relative standard deviation of AUCs. The non-physiological variability is attributed to variability in multiregional segmentation maps obtained with fully-automatic CNNs. Stability filtering significantly improved predictive performance on the two downstream tasks substantiating the inevitability of learning novel radiomics and radiogenomics models with stable discriminatory features. The study (implicitly) demonstrates the importance of suboptimal deep segmentation networks which can be exploited as auxiliary networks for subsequent identification of radiomics features stable to variability in automatically generated multiregional segmentation maps."
    },
    "2406.03396v1": {
      "title": "Noisy Data Visualization using Functional Data Analysis",
      "url": "http://arxiv.org/abs/2406.03396v1",
      "authors": "Haozhe Chen, Andres Felipe Duque Correa, Guy Wolf, Kevin R. Moon",
      "update_time": "2024-06-05",
      "abstract": "Data visualization via dimensionality reduction is an important tool in exploratory data analysis. However, when the data are noisy, many existing methods fail to capture the underlying structure of the data. The method called Empirical Intrinsic Geometry (EIG) was previously proposed for performing dimensionality reduction on high dimensional dynamical processes while theoretically eliminating all noise. However, implementing EIG in practice requires the construction of high-dimensional histograms, which suffer from the curse of dimensionality. Here we propose a new data visualization method called Functional Information Geometry (FIG) for dynamical processes that adapts the EIG framework while using approaches from functional data analysis to mitigate the curse of dimensionality. We experimentally demonstrate that the resulting method outperforms a variant of EIG designed for visualization in terms of capturing the true structure, hyperparameter robustness, and computational speed. We then use our method to visualize EEG brain measurements of sleep activity."
    },
    "2406.03385v1": {
      "title": "Discrete Autoregressive Switching Processes in Sparse Graphical Modeling of Multivariate Time Series Data",
      "url": "http://arxiv.org/abs/2406.03385v1",
      "authors": "Beniamino Hadj-Amar, Aaron M. Bornstein, Michele Guindani, Marina Vannucci",
      "update_time": "2024-06-05",
      "abstract": "We propose a flexible Bayesian approach for sparse Gaussian graphical modeling of multivariate time series. We account for temporal correlation in the data by assuming that observations are characterized by an underlying and unobserved hidden discrete autoregressive process. We assume multivariate Gaussian emission distributions and capture spatial dependencies by modeling the state-specific precision matrices via graphical horseshoe priors. We characterize the mixing probabilities of the hidden process via a cumulative shrinkage prior that accommodates zero-inflated parameters for non-active components, and further incorporate a sparsity-inducing Dirichlet prior to estimate the effective number of states from the data. For posterior inference, we develop a sampling procedure that allows estimation of the number of discrete autoregressive lags and the number of states, and that cleverly avoids having to deal with the changing dimensions of the parameter space. We thoroughly investigate performance of our proposed methodology through several simulation studies. We further illustrate the use of our approach for the estimation of dynamic brain connectivity based on fMRI data collected on a subject performing a task-based experiment on latent learning"
    },
    "2406.03380v1": {
      "title": "Maximal information at the edge of stability in excitatory-inhibitory neural populations",
      "url": "http://arxiv.org/abs/2406.03380v1",
      "authors": "Giacomo Barzon, Daniel Maria Busiello, Giorgio Nicoletti",
      "update_time": "2024-06-05",
      "abstract": "Understanding how the complex connectivity structure of the brain shapes its information-processing capabilities is a long-standing question. Here, by focusing on a paradigmatic architecture, we study how the neural activity of excitatory and inhibitory populations encodes information on external signals. We show that information is maximized at the edge of stability, where excitation is balanced by inhibition. When the input switches among different stimuli, this maximum corresponds to the entropy of the external switching dynamics. By analyzing the case of a prolonged stimulus, we find that stronger inhibition is needed to maximize the instantaneous sensitivity, revealing an intrinsic trade-off between short-time responses and long-time accuracy. In agreement with recent experimental findings, our results open the avenue for a complete information-theoretic understanding of how and why inhibition strength should be tuned to optimize information-processing capabilities."
    },
    "2406.03287v1": {
      "title": "SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms",
      "url": "http://arxiv.org/abs/2406.03287v1",
      "authors": "Xingrun Xing, Zheng Zhang, Ziyi Ni, Shitao Xiao, Yiming Ju, Siqi Fan, Yequan Wang, Jiajun Zhang, Guoqi Li",
      "update_time": "2024-06-05",
      "abstract": "Towards energy-efficient artificial intelligence similar to the human brain, the bio-inspired spiking neural networks (SNNs) have advantages of biological plausibility, event-driven sparsity, and binary activation. Recently, large-scale language models exhibit promising generalization capability, making it a valuable issue to explore more general spike-driven models. However, the binary spikes in existing SNNs fail to encode adequate semantic information, placing technological challenges for generalization. This work proposes the first fully spiking mechanism for general language tasks, including both discriminative and generative ones. Different from previous spikes with {0,1} levels, we propose a more general spike formulation with bi-directional, elastic amplitude, and elastic frequency encoding, while still maintaining the addition nature of SNNs. In a single time step, the spike is enhanced by direction and amplitude information; in spike frequency, a strategy to control spike firing rate is well designed. We plug this elastic bi-spiking mechanism in language modeling, named SpikeLM. It is the first time to handle general language tasks with fully spike-driven models, which achieve much higher accuracy than previously possible. SpikeLM also greatly bridges the performance gap between SNNs and ANNs in language modeling. Our code is available at https://github.com/Xingrun-Xing/SpikeLM.",
      "code_url": "https://github.com/xingrun-xing/spikelm"
    }
  },
  "EEG": {
    "2406.03396v1": {
      "title": "Noisy Data Visualization using Functional Data Analysis",
      "url": "http://arxiv.org/abs/2406.03396v1",
      "authors": "Haozhe Chen, Andres Felipe Duque Correa, Guy Wolf, Kevin R. Moon",
      "update_time": "2024-06-05",
      "abstract": "Data visualization via dimensionality reduction is an important tool in exploratory data analysis. However, when the data are noisy, many existing methods fail to capture the underlying structure of the data. The method called Empirical Intrinsic Geometry (EIG) was previously proposed for performing dimensionality reduction on high dimensional dynamical processes while theoretically eliminating all noise. However, implementing EIG in practice requires the construction of high-dimensional histograms, which suffer from the curse of dimensionality. Here we propose a new data visualization method called Functional Information Geometry (FIG) for dynamical processes that adapts the EIG framework while using approaches from functional data analysis to mitigate the curse of dimensionality. We experimentally demonstrate that the resulting method outperforms a variant of EIG designed for visualization in terms of capturing the true structure, hyperparameter robustness, and computational speed. We then use our method to visualize EEG brain measurements of sleep activity."
    },
    "2406.03115v2": {
      "title": "GET: A Generative EEG Transformer for Continuous Context-Based Neural Signals",
      "url": "http://arxiv.org/abs/2406.03115v2",
      "authors": "Omair Ali, Muhammad Saif-ur-Rehman, Marita Metzler, Tobias Glasmachers, Ioannis Iossifidis, Christian Klaes",
      "update_time": "2024-06-06",
      "abstract": "Generating continuous electroencephalography (EEG) signals through advanced artificial neural networks presents a novel opportunity to enhance brain-computer interface (BCI) technology. This capability has the potential to significantly enhance applications ranging from simulating dynamic brain activity and data augmentation to improving real-time epilepsy detection and BCI inference. By harnessing generative transformer neural networks, specifically designed for EEG signal generation, we can revolutionize the interpretation and interaction with neural data. Generative AI has demonstrated significant success across various domains, from natural language processing (NLP) and computer vision to content creation in visual arts and music. It distinguishes itself by using large-scale datasets to construct context windows during pre-training, a technique that has proven particularly effective in NLP, where models are fine-tuned for specific downstream tasks after extensive foundational training. However, the application of generative AI in the field of BCIs, particularly through the development of continuous, context-rich neural signal generators, has been limited. To address this, we introduce the Generative EEG Transformer (GET), a model leveraging transformer architecture tailored for EEG data. The GET model is pre-trained on diverse EEG datasets, including motor imagery and alpha wave datasets, enabling it to produce high-fidelity neural signals that maintain contextual integrity. Our empirical findings indicate that GET not only faithfully reproduces the frequency spectrum of the training data and input prompts but also robustly generates continuous neural signals. By adopting the successful training strategies of the NLP domain for BCIs, the GET sets a new standard for the development and application of neural signal generation technologies."
    },
    "2406.02360v1": {
      "title": "A Practical Approach for Exploring Granger Connectivity in High-Dimensional Networks of Time Series",
      "url": "http://arxiv.org/abs/2406.02360v1",
      "authors": "Sipan Aslan, Hernando Ombao",
      "update_time": "2024-06-04",
      "abstract": "This manuscript presents a novel method for discovering effective connectivity between specified pairs of nodes in a high-dimensional network of time series. To accurately perform Granger causality analysis from the first node to the second node, it is essential to eliminate the influence of all other nodes within the network. The approach proposed is to create a low-dimensional representation of all other nodes in the network using frequency-domain-based dynamic principal component analysis (spectral DPCA). The resulting scores are subsequently removed from the first and second nodes of interest, thus eliminating the confounding effect of other nodes within the high-dimensional network. To conduct hypothesis testing on Granger causality, we propose a permutation-based causality test. This test enhances the accuracy of our findings when the error structures are non-Gaussian. The approach has been validated in extensive simulation studies, which demonstrate the efficacy of the methodology as a tool for causality analysis in complex time series networks. The proposed methodology has also been demonstrated to be both expedient and viable on real datasets, with particular success observed on multichannel EEG networks."
    },
    "2406.02001v1": {
      "title": "Higher-order Common Information",
      "url": "http://arxiv.org/abs/2406.02001v1",
      "authors": "Jan \u00d8stergaard",
      "update_time": "2024-06-04",
      "abstract": "We present a new notion $R_\\ell$ of higher-order common information, which quantifies the information that $\\ell\\geq 2$ arbitrarily distributed random variables have in common. We provide analytical lower bounds on $R_3$ and $R_4$ for jointly Gaussian distributed sources and provide computable lower bounds for $R_\\ell$ for any $\\ell$ and any sources. We also provide a practical method to estimate the lower bounds on, e.g., real-world time-series data. As an example, we consider EEG data acquired in a setup with competing acoustic stimuli. We demonstrate that $R_3$ has descriptive properties that is not in $R_2$. Moreover, we observe a linear relationship between the amount of common information $R_3$ communicated from the acoustic stimuli and to the brain and the corresponding cortical activity in terms of neural tracking of the envelopes of the stimuli."
    },
    "2406.01834v1": {
      "title": "Multi-Task Learning for Arousal and Sleep Stage Detection Using Fully Convolutional Networks",
      "url": "http://arxiv.org/abs/2406.01834v1",
      "authors": "Hasan Zan, Abdulnasir Yildiz",
      "update_time": "2024-06-03",
      "abstract": "Objective. Sleep is a critical physiological process that plays a vital role in maintaining physical and mental health. Accurate detection of arousals and sleep stages is essential for the diagnosis of sleep disorders, as frequent and excessive occurrences of arousals disrupt sleep stage patterns and lead to poor sleep quality, negatively impacting physical and mental health. Polysomnography is a traditional method for arousal and sleep stage detection that is time-consuming and prone to high variability among experts. Approach. In this paper, we propose a novel multi-task learning approach for arousal and sleep stage detection using fully convolutional neural networks. Our model, FullSleepNet, accepts a full-night single-channel EEG signal as input and produces segmentation masks for arousal and sleep stage labels. FullSleepNet comprises four modules: a convolutional module to extract local features, a recurrent module to capture long-range dependencies, an attention mechanism to focus on relevant parts of the input, and a segmentation module to output final predictions. Main results. By unifying the two interrelated tasks as segmentation problems and employing a multi-task learning approach, FullSleepNet achieves state-of-the-art performance for arousal detection with an area under the precision-recall curve of 0.70 on Sleep Heart Health Study and Multi-Ethnic Study of Atherosclerosis datasets. For sleep stage classification, FullSleepNet obtains comparable performance on both datasets, achieving an accuracy of 0.88 on the former and an accuracy of 0.83 on the latter. Significance. Our results demonstrate that FullSleepNet offers improved practicality, efficiency, and accuracy for the detection of arousal and classification of sleep stages using raw EEG signals as input."
    },
    "2406.01512v1": {
      "title": "MAD: Multi-Alignment MEG-to-Text Decoding",
      "url": "http://arxiv.org/abs/2406.01512v1",
      "authors": "Yiqian Yang, Hyejeong Jo, Yiqun Duan, Qiang Zhang, Jinni Zhou, Won Hee Lee, Renjing Xu, Hui Xiong",
      "update_time": "2024-06-03",
      "abstract": "Deciphering language from brain activity is a crucial task in brain-computer interface (BCI) research. Non-invasive cerebral signaling techniques including electroencephalography (EEG) and magnetoencephalography (MEG) are becoming increasingly popular due to their safety and practicality, avoiding invasive electrode implantation. However, current works under-investigated three points: 1) a predominant focus on EEG with limited exploration of MEG, which provides superior signal quality; 2) poor performance on unseen text, indicating the need for models that can better generalize to diverse linguistic contexts; 3) insufficient integration of information from other modalities, which could potentially constrain our capacity to comprehensively understand the intricate dynamics of brain activity.   This study presents a novel approach for translating MEG signals into text using a speech-decoding framework with multiple alignments. Our method is the first to introduce an end-to-end multi-alignment framework for totally unseen text generation directly from MEG signals. We achieve an impressive BLEU-1 score on the $\\textit{GWilliams}$ dataset, significantly outperforming the baseline from 5.49 to 10.44 on the BLEU-1 metric. This improvement demonstrates the advancement of our model towards real-world applications and underscores its potential in advancing BCI research. Code is available at $\\href{https://github.com/NeuSpeech/MAD-MEG2text}{https://github.com/NeuSpeech/MAD-MEG2text}$.",
      "code_url": "https://github.com/neuspeech/mad-meg2text"
    },
    "2406.01162v1": {
      "title": "Conditional Gumbel-Softmax for constrained feature selection with application to node selection in wireless sensor networks",
      "url": "http://arxiv.org/abs/2406.01162v1",
      "authors": "Thomas Strypsteen, Alexander Bertrand",
      "update_time": "2024-06-03",
      "abstract": "In this paper, we introduce Conditional Gumbel-Softmax as a method to perform end-to-end learning of the optimal feature subset for a given task and deep neural network (DNN) model, while adhering to certain pairwise constraints between the features. We do this by conditioning the selection of each feature in the subset on another feature. We demonstrate how this approach can be used to select the task-optimal nodes composing a wireless sensor network (WSN) while ensuring that none of the nodes that require communication between one another have too large of a distance between them, limiting the required power spent on this communication. We validate this approach on an emulated Wireless Electroencephalography (EEG) Sensor Network (WESN) solving a motor execution task. We analyze how the performance of the WESN varies as the constraints are made more stringent and how well the Conditional Gumbel-Softmax performs in comparison with a heuristic, greedy selection method. While the application focus of this paper is on wearable brain-computer interfaces, the proposed methodology is generic and can readily be applied to node deployment in wireless sensor networks and constrained feature selection in other applications as well."
    },
    "2406.00470v1": {
      "title": "MI 2 MI: Training Dyad with Collaborative Brain-Computer Interface and Cooperative Motor Imagery Tasks for Better BCI Performance",
      "url": "http://arxiv.org/abs/2406.00470v1",
      "authors": "Shiwei Cheng, Jialing Wang",
      "update_time": "2024-06-01",
      "abstract": "Collaborative brain-computer interface (cBCI) that conduct motor imagery (MI) among multiple users has the potential not only to improve overall BCI performance by integrating information from multiple users, but also to leverage individuals' performance in decision-making or control. However, existed research mostly focused on the brain signals changes through a single user, not noticing the possible interaction between users during the collaboration. In this work, we utilized cBCI and designed a cooperative four-classes MI task to train the dyad. A humanoid robot would stimulate the dyad to conduct both left/right hand and tongue/foot MI. Single user was asked to conduct single MI task before and after the cooperative MI task. The experiment results showed that our training could activate better performance (e.g., high quality of EEG /MI classification accuracy) for the single user than single MI task, and the single user also obtained better single MI performance after cooperative MI training."
    },
    "2405.18765v1": {
      "title": "Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI",
      "url": "http://arxiv.org/abs/2405.18765v1",
      "authors": "Wei-Bang Jiang, Li-Ming Zhao, Bao-Liang Lu",
      "update_time": "2024-05-29",
      "abstract": "The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs). We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training. Then the models can be fine-tuned for different downstream tasks. However, compared to text data, the volume of EEG datasets is generally small and the format varies widely. For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. To overcome these challenges, we propose a unified foundation model for EEG called Large Brain Model (LaBraM). LaBraM enables cross-dataset learning by segmenting the EEG signals into EEG channel patches. Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes. We then pre-train neural Transformers by predicting the original neural codes for the masked EEG channel patches. The LaBraMs were pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple different types of downstream tasks. Experiments on abnormal detection, event type classification, emotion recognition, and gait prediction show that our LaBraM outperforms all compared SOTA methods in their respective fields. Our code is available at https://github.com/935963004/LaBraM.",
      "code_url": "https://github.com/935963004/labram"
    },
    "2405.19373v1": {
      "title": "Multi-modal Mood Reader: Pre-trained Model Empowers Cross-Subject Emotion Recognition",
      "url": "http://arxiv.org/abs/2405.19373v1",
      "authors": "Yihang Dong, Xuhang Chen, Yanyan Shen, Michael Kwok-Po Ng, Tao Qian, Shuqiang Wang",
      "update_time": "2024-05-28",
      "abstract": "Emotion recognition based on Electroencephalography (EEG) has gained significant attention and diversified development in fields such as neural signal processing and affective computing. However, the unique brain anatomy of individuals leads to non-negligible natural differences in EEG signals across subjects, posing challenges for cross-subject emotion recognition. While recent studies have attempted to address these issues, they still face limitations in practical effectiveness and model framework unity. Current methods often struggle to capture the complex spatial-temporal dynamics of EEG signals and fail to effectively integrate multimodal information, resulting in suboptimal performance and limited generalizability across subjects. To overcome these limitations, we develop a Pre-trained model based Multimodal Mood Reader for cross-subject emotion recognition that utilizes masked brain signal modeling and interlinked spatial-temporal attention mechanism. The model learns universal latent representations of EEG signals through pre-training on large scale dataset, and employs Interlinked spatial-temporal attention mechanism to process Differential Entropy(DE) features extracted from EEG data. Subsequently, a multi-level fusion layer is proposed to integrate the discriminative features, maximizing the advantages of features across different dimensions and modalities. Extensive experiments on public datasets demonstrate Mood Reader's superior performance in cross-subject emotion recognition tasks, outperforming state-of-the-art methods. Additionally, the model is dissected from attention perspective, providing qualitative analysis of emotion-related brain areas, offering valuable insights for affective research in neural signal processing."
    }
  },
  "BCI": {
    "2406.03115v2": {
      "title": "GET: A Generative EEG Transformer for Continuous Context-Based Neural Signals",
      "url": "http://arxiv.org/abs/2406.03115v2",
      "authors": "Omair Ali, Muhammad Saif-ur-Rehman, Marita Metzler, Tobias Glasmachers, Ioannis Iossifidis, Christian Klaes",
      "update_time": "2024-06-06",
      "abstract": "Generating continuous electroencephalography (EEG) signals through advanced artificial neural networks presents a novel opportunity to enhance brain-computer interface (BCI) technology. This capability has the potential to significantly enhance applications ranging from simulating dynamic brain activity and data augmentation to improving real-time epilepsy detection and BCI inference. By harnessing generative transformer neural networks, specifically designed for EEG signal generation, we can revolutionize the interpretation and interaction with neural data. Generative AI has demonstrated significant success across various domains, from natural language processing (NLP) and computer vision to content creation in visual arts and music. It distinguishes itself by using large-scale datasets to construct context windows during pre-training, a technique that has proven particularly effective in NLP, where models are fine-tuned for specific downstream tasks after extensive foundational training. However, the application of generative AI in the field of BCIs, particularly through the development of continuous, context-rich neural signal generators, has been limited. To address this, we introduce the Generative EEG Transformer (GET), a model leveraging transformer architecture tailored for EEG data. The GET model is pre-trained on diverse EEG datasets, including motor imagery and alpha wave datasets, enabling it to produce high-fidelity neural signals that maintain contextual integrity. Our empirical findings indicate that GET not only faithfully reproduces the frequency spectrum of the training data and input prompts but also robustly generates continuous neural signals. By adopting the successful training strategies of the NLP domain for BCIs, the GET sets a new standard for the development and application of neural signal generation technologies."
    },
    "2406.01512v1": {
      "title": "MAD: Multi-Alignment MEG-to-Text Decoding",
      "url": "http://arxiv.org/abs/2406.01512v1",
      "authors": "Yiqian Yang, Hyejeong Jo, Yiqun Duan, Qiang Zhang, Jinni Zhou, Won Hee Lee, Renjing Xu, Hui Xiong",
      "update_time": "2024-06-03",
      "abstract": "Deciphering language from brain activity is a crucial task in brain-computer interface (BCI) research. Non-invasive cerebral signaling techniques including electroencephalography (EEG) and magnetoencephalography (MEG) are becoming increasingly popular due to their safety and practicality, avoiding invasive electrode implantation. However, current works under-investigated three points: 1) a predominant focus on EEG with limited exploration of MEG, which provides superior signal quality; 2) poor performance on unseen text, indicating the need for models that can better generalize to diverse linguistic contexts; 3) insufficient integration of information from other modalities, which could potentially constrain our capacity to comprehensively understand the intricate dynamics of brain activity.   This study presents a novel approach for translating MEG signals into text using a speech-decoding framework with multiple alignments. Our method is the first to introduce an end-to-end multi-alignment framework for totally unseen text generation directly from MEG signals. We achieve an impressive BLEU-1 score on the $\\textit{GWilliams}$ dataset, significantly outperforming the baseline from 5.49 to 10.44 on the BLEU-1 metric. This improvement demonstrates the advancement of our model towards real-world applications and underscores its potential in advancing BCI research. Code is available at $\\href{https://github.com/NeuSpeech/MAD-MEG2text}{https://github.com/NeuSpeech/MAD-MEG2text}$.",
      "code_url": "https://github.com/neuspeech/mad-meg2text"
    },
    "2406.00470v1": {
      "title": "MI 2 MI: Training Dyad with Collaborative Brain-Computer Interface and Cooperative Motor Imagery Tasks for Better BCI Performance",
      "url": "http://arxiv.org/abs/2406.00470v1",
      "authors": "Shiwei Cheng, Jialing Wang",
      "update_time": "2024-06-01",
      "abstract": "Collaborative brain-computer interface (cBCI) that conduct motor imagery (MI) among multiple users has the potential not only to improve overall BCI performance by integrating information from multiple users, but also to leverage individuals' performance in decision-making or control. However, existed research mostly focused on the brain signals changes through a single user, not noticing the possible interaction between users during the collaboration. In this work, we utilized cBCI and designed a cooperative four-classes MI task to train the dyad. A humanoid robot would stimulate the dyad to conduct both left/right hand and tongue/foot MI. Single user was asked to conduct single MI task before and after the cooperative MI task. The experiment results showed that our training could activate better performance (e.g., high quality of EEG /MI classification accuracy) for the single user than single MI task, and the single user also obtained better single MI performance after cooperative MI training."
    },
    "2405.18765v1": {
      "title": "Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI",
      "url": "http://arxiv.org/abs/2405.18765v1",
      "authors": "Wei-Bang Jiang, Li-Ming Zhao, Bao-Liang Lu",
      "update_time": "2024-05-29",
      "abstract": "The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs). We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training. Then the models can be fine-tuned for different downstream tasks. However, compared to text data, the volume of EEG datasets is generally small and the format varies widely. For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. To overcome these challenges, we propose a unified foundation model for EEG called Large Brain Model (LaBraM). LaBraM enables cross-dataset learning by segmenting the EEG signals into EEG channel patches. Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes. We then pre-train neural Transformers by predicting the original neural codes for the masked EEG channel patches. The LaBraMs were pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple different types of downstream tasks. Experiments on abnormal detection, event type classification, emotion recognition, and gait prediction show that our LaBraM outperforms all compared SOTA methods in their respective fields. Our code is available at https://github.com/935963004/LaBraM.",
      "code_url": "https://github.com/935963004/labram"
    },
    "2405.17024v1": {
      "title": "Beware of Overestimated Decoding Performance Arising from Temporal Autocorrelations in Electroencephalogram Signals",
      "url": "http://arxiv.org/abs/2405.17024v1",
      "authors": "Xiran Xu, Bo Wang, Boda Xiao, Yadong Niu, Yiwen Wang, Xihong Wu, Jing Chen",
      "update_time": "2024-05-27",
      "abstract": "Researchers have reported high decoding accuracy (>95%) using non-invasive Electroencephalogram (EEG) signals for brain-computer interface (BCI) decoding tasks like image decoding, emotion recognition, auditory spatial attention detection, etc. Since these EEG data were usually collected with well-designed paradigms in labs, the reliability and robustness of the corresponding decoding methods were doubted by some researchers, and they argued that such decoding accuracy was overestimated due to the inherent temporal autocorrelation of EEG signals. However, the coupling between the stimulus-driven neural responses and the EEG temporal autocorrelations makes it difficult to confirm whether this overestimation exists in truth. Furthermore, the underlying pitfalls behind overestimated decoding accuracy have not been fully explained due to a lack of appropriate formulation. In this work, we formulate the pitfall in various EEG decoding tasks in a unified framework. EEG data were recorded from watermelons to remove stimulus-driven neural responses. Labels were assigned to continuous EEG according to the experimental design for EEG recording of several typical datasets, and then the decoding methods were conducted. The results showed the label can be successfully decoded as long as continuous EEG data with the same label were split into training and test sets. Further analysis indicated that high accuracy of various BCI decoding tasks could be achieved by associating labels with EEG intrinsic temporal autocorrelation features. These results underscore the importance of choosing the right experimental designs and data splits in BCI decoding tasks to prevent inflated accuracies due to EEG temporal autocorrelation."
    },
    "2405.16090v2": {
      "title": "EEG-DBNet: A Dual-Branch Network for Temporal-Spectral Decoding in Motor-Imagery Brain-Computer Interfaces",
      "url": "http://arxiv.org/abs/2405.16090v2",
      "authors": "Xicheng Lou, Xinwei Li, Hongying Meng, Jun Hu, Meili Xu, Yue Zhao, Jiazhang Yang, Zhangyong Li",
      "update_time": "2024-05-30",
      "abstract": "Motor imagery electroencephalogram (EEG)-based brain-computer interfaces (BCIs) offer significant advantages for individuals with restricted limb mobility. However, challenges such as low signal-to-noise ratio and limited spatial resolution impede accurate feature extraction from EEG signals, thereby affecting the classification accuracy of different actions. To address these challenges, this study proposes an end-to-end dual-branch network (EEG-DBNet) that decodes the temporal and spectral sequences of EEG signals in parallel through two distinct network branches. Each branch comprises a local convolutional block and a global convolutional block. The local convolutional block transforms the source signal from the temporal-spatial domain to the temporal-spectral domain. By varying the number of filters and convolution kernel sizes, the local convolutional blocks in different branches adjust the length of their respective dimension sequences. Different types of pooling layers are then employed to emphasize the features of various dimension sequences, setting the stage for subsequent global feature extraction. The global convolution block splits and reconstructs the feature of the signal sequence processed by the local convolution block in the same branch and further extracts features through the dilated causal convolutional neural networks. Finally, the outputs from the two branches are concatenated, and signal classification is completed via a fully connected layer. Our proposed method achieves classification accuracies of 85.84% and 91.60% on the BCI Competition 4-2a and BCI Competition 4-2b datasets, respectively, surpassing existing state-of-the-art models. The source code is available at https://github.com/xicheng105/EEG-DBNet.",
      "code_url": "https://github.com/xicheng105/eeg-dbnet"
    },
    "2405.14994v1": {
      "title": "Combining Euclidean Alignment and Data Augmentation for BCI decoding",
      "url": "http://arxiv.org/abs/2405.14994v1",
      "authors": "Gustavo H. Rodrigues, Bruno Aristimunha, Sylvain Chevallier, Raphael Y. de Camargo",
      "update_time": "2024-05-23",
      "abstract": "Automated classification of electroencephalogram (EEG) signals is complex due to their high dimensionality, non-stationarity, low signal-to-noise ratio, and variability between subjects. Deep neural networks (DNNs) have shown promising results for EEG classification, but the above challenges hinder their performance. Euclidean Alignment (EA) and Data Augmentation (DA) are two promising techniques for improving DNN training by permitting the use of data from multiple subjects, increasing the data, and regularizing the available data. In this paper, we perform a detailed evaluation of the combined use of EA and DA with DNNs for EEG decoding. We trained individual models and shared models with data from multiple subjects and showed that combining EA and DA generates synergies that improve the accuracy of most models and datasets. Also, the shared models combined with fine-tuning benefited the most, with an overall increase of 8.41\\% in classification accuracy."
    },
    "2405.13329v1": {
      "title": "High Performance P300 Spellers Using GPT2 Word Prediction With Cross-Subject Training",
      "url": "http://arxiv.org/abs/2405.13329v1",
      "authors": "Nithin Parthasarathy, James Soetedjo, Saarang Panchavati, Nitya Parthasarathy, Corey Arnold, Nader Pouratian, William Speier",
      "update_time": "2024-05-22",
      "abstract": "Amyotrophic lateral sclerosis (ALS) severely impairs patients' ability to communicate, often leading to a decline in their quality of life within a few years of diagnosis. The P300 speller brain-computer interface (BCI) offers an alternative communication method by interpreting a subject's EEG response to characters presented on a grid interface.   This paper addresses the common speed limitations encountered in training efficient P300-based multi-subject classifiers by introducing innovative \"across-subject\" classifiers. We leverage a combination of the second-generation Generative Pre-Trained Transformer (GPT2) and Dijkstra's algorithm to optimize stimuli and suggest word completion choices based on typing history. Additionally, we employ a multi-layered smoothing technique to accommodate out-of-vocabulary (OOV) words.   Through extensive simulations involving random sampling of EEG data from subjects, we demonstrate significant speed enhancements in typing passages containing rare and OOV words. These optimizations result in approximately 10% improvement in character-level typing speed and up to 40% improvement in multi-word prediction. We demonstrate that augmenting standard row/column highlighting techniques with layered word prediction yields close-to-optimal performance.   Furthermore, we explore both \"within-subject\" and \"across-subject\" training techniques, showing that speed improvements are consistent across both approaches."
    },
    "2405.11459v1": {
      "title": "Du-IN: Discrete units-guided mask modeling for decoding speech from Intracranial Neural signals",
      "url": "http://arxiv.org/abs/2405.11459v1",
      "authors": "Hui Zheng, Hai-Teng Wang, Wei-Bang Jiang, Zhong-Tao Chen, Li He, Pei-Yang Lin, Peng-Hu Wei, Guo-Guang Zhao, Yun-Zhe Liu",
      "update_time": "2024-05-19",
      "abstract": "Invasive brain-computer interfaces have garnered significant attention due to their high performance. The current intracranial stereoElectroEncephaloGraphy (sEEG) foundation models typically build univariate representations based on a single channel. Some of them further use Transformer to model the relationship among channels. However, due to the locality and specificity of brain computation, their performance on more difficult tasks, e.g., speech decoding, which demands intricate processing in specific brain regions, is yet to be fully investigated. We hypothesize that building multi-variate representations within certain brain regions can better capture the specific neural processing. To explore this hypothesis, we collect a well-annotated Chinese word-reading sEEG dataset, targeting language-related brain networks, over 12 subjects. Leveraging this benchmark dataset, we developed the Du-IN model that can extract contextual embeddings from specific brain regions through discrete codebook-guided mask modeling. Our model achieves SOTA performance on the downstream 61-word classification task, surpassing all baseline models. Model comparison and ablation analysis reveal that our design choices, including (i) multi-variate representation by fusing channels in vSMC and STG regions and (ii) self-supervision by discrete codebook-guided mask modeling, significantly contribute to these performances. Collectively, our approach, inspired by neuroscience findings, capitalizing on multi-variate neural representation from specific brain regions, is suitable for invasive brain modeling. It marks a promising neuro-inspired AI approach in BCI."
    },
    "2405.11163v1": {
      "title": "Domain Generalization for Zero-calibration BCIs with Knowledge Distillation-based Phase Invariant Feature Extraction",
      "url": "http://arxiv.org/abs/2405.11163v1",
      "authors": "Zilin Liang, Zheng Zheng, Weihai Chen, Xinzhi Ma, Zhongcai Pei, Xiantao Sun",
      "update_time": "2024-05-18",
      "abstract": "The distribution shift of electroencephalography (EEG) data causes poor generalization of braincomputer interfaces (BCIs) in unseen domains. Some methods try to tackle this challenge by collecting a portion of user data for calibration. However, it is time-consuming, mentally fatiguing, and user-unfriendly. To achieve zerocalibration BCIs, most studies employ domain generalization (DG) techniques to learn invariant features across different domains in the training set. However, they fail to fully explore invariant features within the same domain, leading to limited performance. In this paper, we present an novel method to learn domain-invariant features from both interdomain and intra-domain perspectives. For intra-domain invariant features, we propose a knowledge distillation framework to extract EEG phase-invariant features within one domain. As for inter-domain invariant features, correlation alignment is used to bridge distribution gaps across multiple domains. Experimental results on three public datasets validate the effectiveness of our method, showcasing stateof-the-art performance. To the best of our knowledge, this is the first domain generalization study that exploit Fourier phase information as an intra-domain invariant feature to facilitate EEG generalization. More importantly, the zerocalibration BCI based on inter- and intra-domain invariant features has significant potential to advance the practical applications of BCIs in real world.",
      "code_url": "https://github.com/ZilinL/KnIFE"
    }
  },
  "fMRI": {
    "2406.03385v1": {
      "title": "Discrete Autoregressive Switching Processes in Sparse Graphical Modeling of Multivariate Time Series Data",
      "url": "http://arxiv.org/abs/2406.03385v1",
      "authors": "Beniamino Hadj-Amar, Aaron M. Bornstein, Michele Guindani, Marina Vannucci",
      "update_time": "2024-06-05",
      "abstract": "We propose a flexible Bayesian approach for sparse Gaussian graphical modeling of multivariate time series. We account for temporal correlation in the data by assuming that observations are characterized by an underlying and unobserved hidden discrete autoregressive process. We assume multivariate Gaussian emission distributions and capture spatial dependencies by modeling the state-specific precision matrices via graphical horseshoe priors. We characterize the mixing probabilities of the hidden process via a cumulative shrinkage prior that accommodates zero-inflated parameters for non-active components, and further incorporate a sparsity-inducing Dirichlet prior to estimate the effective number of states from the data. For posterior inference, we develop a sampling procedure that allows estimation of the number of discrete autoregressive lags and the number of states, and that cleverly avoids having to deal with the changing dimensions of the parameter space. We thoroughly investigate performance of our proposed methodology through several simulation studies. We further illustrate the use of our approach for the estimation of dynamic brain connectivity based on fMRI data collected on a subject performing a task-based experiment on latent learning"
    },
    "2406.02659v1": {
      "title": "Neural Representations of Dynamic Visual Stimuli",
      "url": "http://arxiv.org/abs/2406.02659v1",
      "authors": "Jacob Yeung, Andrew F. Luo, Gabriel Sarch, Margaret M. Henderson, Deva Ramanan, Michael J. Tarr",
      "update_time": "2024-06-04",
      "abstract": "Humans experience the world through constantly changing visual stimuli, where scenes can shift and move, change in appearance, and vary in distance. The dynamic nature of visual perception is a fundamental aspect of our daily lives, yet the large majority of research on object and scene processing, particularly using fMRI, has focused on static stimuli. While studies of static image perception are attractive due to their computational simplicity, they impose a strong non-naturalistic constraint on our investigation of human vision. In contrast, dynamic visual stimuli offer a more ecologically-valid approach but present new challenges due to the interplay between spatial and temporal information, making it difficult to disentangle the representations of stable image features and motion. To overcome this limitation -- given dynamic inputs, we explicitly decouple the modeling of static image representations and motion representations in the human brain. Three results demonstrate the feasibility of this approach. First, we show that visual motion information as optical flow can be predicted (or decoded) from brain activity as measured by fMRI. Second, we show that this predicted motion can be used to realistically animate static images using a motion-conditioned video diffusion model (where the motion is driven by fMRI brain activity). Third, we show prediction in the reverse direction: existing video encoders can be fine-tuned to predict fMRI brain activity from video imagery, and can do so more effectively than image encoders. This foundational work offers a novel, extensible framework for interpreting how the human brain processes dynamic visual information."
    },
    "2406.02014v1": {
      "title": "Understanding Auditory Evoked Brain Signal via Physics-informed Embedding Network with Multi-Task Transformer",
      "url": "http://arxiv.org/abs/2406.02014v1",
      "authors": "Wanli Ma, Xuegang Tang, Jin Gu, Ying Wang, Yuling Xia",
      "update_time": "2024-06-04",
      "abstract": "In the fields of brain-computer interaction and cognitive neuroscience, effective decoding of auditory signals from task-based functional magnetic resonance imaging (fMRI) is key to understanding how the brain processes complex auditory information. Although existing methods have enhanced decoding capabilities, limitations remain in information utilization and model representation. To overcome these challenges, we propose an innovative multi-task learning model, Physics-informed Embedding Network with Multi-Task Transformer (PEMT-Net), which enhances decoding performance through physics-informed embedding and deep learning techniques. PEMT-Net consists of two principal components: feature augmentation and classification. For feature augmentation, we propose a novel approach by creating neural embedding graphs via node embedding, utilizing random walks to simulate the physical diffusion of neural information. This method captures both local and non-local information overflow and proposes a position encoding based on relative physical coordinates. In the classification segment, we propose adaptive embedding fusion to maximally capture linear and non-linear characteristics. Furthermore, we propose an innovative parameter-sharing mechanism to optimize the retention and learning of extracted features. Experiments on a specific dataset demonstrate PEMT-Net's significant performance in multi-task auditory signal decoding, surpassing existing methods and offering new insights into the brain's mechanisms for processing complex auditory information."
    },
    "2406.01538v1": {
      "title": "What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores",
      "url": "http://arxiv.org/abs/2406.01538v1",
      "authors": "Ebrahim Feghhi, Nima Hadidi, Bryan Song, Idan A. Blank, Jonathan C. Kao",
      "update_time": "2024-06-03",
      "abstract": "Given the remarkable capabilities of large language models (LLMs), there has been a growing interest in evaluating their similarity to the human brain. One approach towards quantifying this similarity is by measuring how well a model predicts neural signals, also called \"brain score\". Internal representations from LLMs achieve state-of-the-art brain scores, leading to speculation that they share computational principles with human language processing. This inference is only valid if the subset of neural activity predicted by LLMs reflects core elements of language processing. Here, we question this assumption by analyzing three neural datasets used in an impactful study on LLM-to-brain mappings, with a particular focus on an fMRI dataset where participants read short passages. We first find that when using shuffled train-test splits, as done in previous studies with these datasets, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain. We therefore use contiguous splits moving forward. Second, we explain the surprisingly high brain scores of untrained LLMs by showing they do not account for additional neural variance beyond two simple features: sentence length and sentence position. This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like. Third, we find that brain scores of trained LLMs on this dataset can largely be explained by sentence length, position, and pronoun-dereferenced static word embeddings; a small, additional amount is explained by sense-specific embeddings and contextual representations of sentence structure. We conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, and emphasize the importance of deconstructing what LLMs are mapping to in neural signals.",
      "code_url": "https://github.com/ebrahimfeghhi/beyond-brainscore"
    },
    "2406.00085v1": {
      "title": "Augmentation-based Unsupervised Cross-Domain Functional MRI Adaptation for Major Depressive Disorder Identification",
      "url": "http://arxiv.org/abs/2406.00085v1",
      "authors": "Yunling Ma, Chaojun Zhang, Xiaochuan Wang, Qianqian Wang, Liang Cao, Limei Zhang, Mingxia Liu",
      "update_time": "2024-05-31",
      "abstract": "Major depressive disorder (MDD) is a common mental disorder that typically affects a person's mood, cognition, behavior, and physical health. Resting-state functional magnetic resonance imaging (rs-fMRI) data are widely used for computer-aided diagnosis of MDD. While multi-site fMRI data can provide more data for training reliable diagnostic models, significant cross-site data heterogeneity would result in poor model generalizability. Many domain adaptation methods are designed to reduce the distributional differences between sites to some extent, but usually ignore overfitting problem of the model on the source domain. Intuitively, target data augmentation can alleviate the overfitting problem by forcing the model to learn more generalized features and reduce the dependence on source domain data. In this work, we propose a new augmentation-based unsupervised cross-domain fMRI adaptation (AUFA) framework for automatic diagnosis of MDD. The AUFA consists of 1) a graph representation learning module for extracting rs-fMRI features with spatial attention, 2) a domain adaptation module for feature alignment between source and target data, 3) an augmentation-based self-optimization module for alleviating model overfitting on the source domain, and 4) a classification module. Experimental results on 1,089 subjects suggest that AUFA outperforms several state-of-the-art methods in MDD identification. Our approach not only reduces data heterogeneity between different sites, but also localizes disease-related functional connectivity abnormalities and provides interpretability for the model."
    },
    "2405.18812v1": {
      "title": "MindSemantix: Deciphering Brain Visual Experiences with a Brain-Language Model",
      "url": "http://arxiv.org/abs/2405.18812v1",
      "authors": "Ziqi Ren, Jie Li, Xuetong Xue, Xin Li, Fan Yang, Zhicheng Jiao, Xinbo Gao",
      "update_time": "2024-05-29",
      "abstract": "Deciphering the human visual experience through brain activities captured by fMRI represents a compelling and cutting-edge challenge in the field of neuroscience research. Compared to merely predicting the viewed image itself, decoding brain activity into meaningful captions provides a higher-level interpretation and summarization of visual information, which naturally enhances the application flexibility in real-world situations. In this work, we introduce MindSemantix, a novel multi-modal framework that enables LLMs to comprehend visually-evoked semantic content in brain activity. Our MindSemantix explores a more ideal brain captioning paradigm by weaving LLMs into brain activity analysis, crafting a seamless, end-to-end Brain-Language Model. To effectively capture semantic information from brain responses, we propose Brain-Text Transformer, utilizing a Brain Q-Former as its core architecture. It integrates a pre-trained brain encoder with a frozen LLM to achieve multi-modal alignment of brain-vision-language and establish a robust brain-language correspondence. To enhance the generalizability of neural representations, we pre-train our brain encoder on a large-scale, cross-subject fMRI dataset using self-supervised learning techniques. MindSemantix provides more feasibility to downstream brain decoding tasks such as stimulus reconstruction. Conditioned by MindSemantix captioning, our framework facilitates this process by integrating with advanced generative models like Stable Diffusion and excels in understanding brain visual perception. MindSemantix generates high-quality captions that are deeply rooted in the visual and semantic information derived from brain activity. This approach has demonstrated substantial quantitative improvements over prior art. Our code will be released."
    },
    "2405.18808v1": {
      "title": "BRACTIVE: A Brain Activation Approach to Human Visual Brain Learning",
      "url": "http://arxiv.org/abs/2405.18808v1",
      "authors": "Xuan-Bac Nguyen, Hojin Jang, Xin Li, Samee U. Khan, Pawan Sinha, Khoa Luu",
      "update_time": "2024-05-29",
      "abstract": "The human brain is a highly efficient processing unit, and understanding how it works can inspire new algorithms and architectures in machine learning. In this work, we introduce a novel framework named Brain Activation Network (BRACTIVE), a transformer-based approach to studying the human visual brain. The main objective of BRACTIVE is to align the visual features of subjects with corresponding brain representations via fMRI signals. It allows us to identify the brain's Regions of Interest (ROI) of the subjects. Unlike previous brain research methods, which can only identify ROIs for one subject at a time and are limited by the number of subjects, BRACTIVE automatically extends this identification to multiple subjects and ROIs. Our experiments demonstrate that BRACTIVE effectively identifies person-specific regions of interest, such as face and body-selective areas, aligning with neuroscience findings and indicating potential applicability to various object categories. More importantly, we found that leveraging human visual brain activity to guide deep neural networks enhances performance across various benchmarks. It encourages the potential of BRACTIVE in both neuroscience and machine intelligence studies."
    },
    "2405.18726v1": {
      "title": "Reverse the auditory processing pathway: Coarse-to-fine audio reconstruction from fMRI",
      "url": "http://arxiv.org/abs/2405.18726v1",
      "authors": "Che Liu, Changde Du, Xiaoyu Chen, Huiguang He",
      "update_time": "2024-05-29",
      "abstract": "Drawing inspiration from the hierarchical processing of the human auditory system, which transforms sound from low-level acoustic features to high-level semantic understanding, we introduce a novel coarse-to-fine audio reconstruction method. Leveraging non-invasive functional Magnetic Resonance Imaging (fMRI) data, our approach mimics the inverse pathway of auditory processing. Initially, we utilize CLAP to decode fMRI data coarsely into a low-dimensional semantic space, followed by a fine-grained decoding into the high-dimensional AudioMAE latent space guided by semantic features. These fine-grained neural features serve as conditions for audio reconstruction through a Latent Diffusion Model (LDM). Validation on three public fMRI datasets-Brain2Sound, Brain2Music, and Brain2Speech-underscores the superiority of our coarse-to-fine decoding method over stand-alone fine-grained approaches, showcasing state-of-the-art performance in metrics like FD, FAD, and KL. Moreover, by employing semantic prompts during decoding, we enhance the quality of reconstructed audio when semantic features are suboptimal. The demonstrated versatility of our model across diverse stimuli highlights its potential as a universal brain-to-audio framework. This research contributes to the comprehension of the human auditory system, pushing boundaries in neural decoding and audio reconstruction methodologies."
    },
    "2405.17992v1": {
      "title": "fMRI predictors based on language models of increasing complexity recover brain left lateralization",
      "url": "http://arxiv.org/abs/2405.17992v1",
      "authors": "Laurent Bonnasse-Gahot, Christophe Pallier",
      "update_time": "2024-05-28",
      "abstract": "Over the past decade, studies of naturalistic language processing where participants are scanned while listening to continuous text have flourished. Using word embeddings at first, then large language models, researchers have created encoding models to analyze the brain signals. Presenting these models with the same text as the participants allows to identify brain areas where there is a significant correlation between the functional magnetic resonance imaging (fMRI) time series and the ones predicted by the models' artificial neurons. One intriguing finding from these studies is that they have revealed highly symmetric bilateral activation patterns, somewhat at odds with the well-known left lateralization of language processing. Here, we report analyses of an fMRI dataset where we manipulate the complexity of large language models, testing 28 pretrained models from 8 different families, ranging from 124M to 14.2B parameters. First, we observe that the performance of models in predicting brain responses follows a scaling law, where the fit with brain activity increases linearly with the logarithm of the number of parameters of the model (and its performance on natural language processing tasks). Second, we show that a left-right asymmetry gradually appears as model size increases, and that the difference in left-right brain correlations also follows a scaling law. Whereas the smallest models show no asymmetry, larger models fit better and better left hemispheric activations than right hemispheric ones. This finding reconciles computational analyses of brain activity using large language models with the classic observation from aphasic patients showing left hemisphere dominance for language."
    },
    "2405.17720v1": {
      "title": "MindFormer: A Transformer Architecture for Multi-Subject Brain Decoding via fMRI",
      "url": "http://arxiv.org/abs/2405.17720v1",
      "authors": "Inhwa Han, Jaayeon Lee, Jong Chul Ye",
      "update_time": "2024-05-28",
      "abstract": "Research efforts to understand neural signals have been ongoing for many years, with visual decoding from fMRI signals attracting considerable attention. Particularly, the advent of image diffusion models has advanced the reconstruction of images from fMRI data significantly. However, existing approaches often introduce inter- and intra- subject variations in the reconstructed images, which can compromise accuracy. To address current limitations in multi-subject brain decoding, we introduce a new Transformer architecture called MindFormer. This model is specifically designed to generate fMRI-conditioned feature vectors that can be used for conditioning Stable Diffusion model. More specifically, MindFormer incorporates two key innovations: 1) a novel training strategy based on the IP-Adapter to extract semantically meaningful features from fMRI signals, and 2) a subject specific token and linear layer that effectively capture individual differences in fMRI signals while synergistically combines multi subject fMRI data for training. Our experimental results demonstrate that Stable Diffusion, when integrated with MindFormer, produces semantically consistent images across different subjects. This capability significantly surpasses existing models in multi-subject brain decoding. Such advancements not only improve the accuracy of our reconstructions but also deepen our understanding of neural processing variations among individuals."
    }
  },
  "MEG": {
    "2406.01512v1": {
      "title": "MAD: Multi-Alignment MEG-to-Text Decoding",
      "url": "http://arxiv.org/abs/2406.01512v1",
      "authors": "Yiqian Yang, Hyejeong Jo, Yiqun Duan, Qiang Zhang, Jinni Zhou, Won Hee Lee, Renjing Xu, Hui Xiong",
      "update_time": "2024-06-03",
      "abstract": "Deciphering language from brain activity is a crucial task in brain-computer interface (BCI) research. Non-invasive cerebral signaling techniques including electroencephalography (EEG) and magnetoencephalography (MEG) are becoming increasingly popular due to their safety and practicality, avoiding invasive electrode implantation. However, current works under-investigated three points: 1) a predominant focus on EEG with limited exploration of MEG, which provides superior signal quality; 2) poor performance on unseen text, indicating the need for models that can better generalize to diverse linguistic contexts; 3) insufficient integration of information from other modalities, which could potentially constrain our capacity to comprehensively understand the intricate dynamics of brain activity.   This study presents a novel approach for translating MEG signals into text using a speech-decoding framework with multiple alignments. Our method is the first to introduce an end-to-end multi-alignment framework for totally unseen text generation directly from MEG signals. We achieve an impressive BLEU-1 score on the $\\textit{GWilliams}$ dataset, significantly outperforming the baseline from 5.49 to 10.44 on the BLEU-1 metric. This improvement demonstrates the advancement of our model towards real-world applications and underscores its potential in advancing BCI research. Code is available at $\\href{https://github.com/NeuSpeech/MAD-MEG2text}{https://github.com/NeuSpeech/MAD-MEG2text}$.",
      "code_url": "https://github.com/neuspeech/mad-meg2text"
    },
    "2405.19479v1": {
      "title": "Participation in the age of foundation models",
      "url": "http://arxiv.org/abs/2405.19479v1",
      "authors": "Harini Suresh, Emily Tseng, Meg Young, Mary L. Gray, Emma Pierson, Karen Levy",
      "update_time": "2024-05-29",
      "abstract": "Growing interest and investment in the capabilities of foundation models has positioned such systems to impact a wide array of public services. Alongside these opportunities is the risk that these systems reify existing power imbalances and cause disproportionate harm to marginalized communities. Participatory approaches hold promise to instead lend agency and decision-making power to marginalized stakeholders. But existing approaches in participatory AI/ML are typically deeply grounded in context - how do we apply these approaches to foundation models, which are, by design, disconnected from context? Our paper interrogates this question.   First, we examine existing attempts at incorporating participation into foundation models. We highlight the tension between participation and scale, demonstrating that it is intractable for impacted communities to meaningfully shape a foundation model that is intended to be universally applicable. In response, we develop a blueprint for participatory foundation models that identifies more local, application-oriented opportunities for meaningful participation. In addition to the \"foundation\" layer, our framework proposes the \"subfloor'' layer, in which stakeholders develop shared technical infrastructure, norms and governance for a grounded domain, and the \"surface'' layer, in which affected communities shape the use of a foundation model for a specific downstream task. The intermediate \"subfloor'' layer scopes the range of potential harms to consider, and affords communities more concrete avenues for deliberation and intervention. At the same time, it avoids duplicative effort by scaling input across relevant use cases. Through three case studies in clinical care, financial services, and journalism, we illustrate how this multi-layer model can create more meaningful opportunities for participation than solely intervening at the foundation layer."
    },
    "2405.17698v3": {
      "title": "BaboonLand Dataset: Tracking Primates in the Wild and Automating Behaviour Recognition from Drone Videos",
      "url": "http://arxiv.org/abs/2405.17698v3",
      "authors": "Isla Duporge, Maksim Kholiavchenko, Roi Harel, Scott Wolf, Dan Rubenstein, Meg Crofoot, Tanya Berger-Wolf, Stephen Lee, Julie Barreau, Jenna Kline, Michelle Ramirez, Charles Stewart",
      "update_time": "2024-06-03",
      "abstract": "Using drones to track multiple individuals simultaneously in their natural environment is a powerful approach for better understanding group primate behavior. Previous studies have demonstrated that it is possible to automate the classification of primate behavior from video data, but these studies have been carried out in captivity or from ground-based cameras. To understand group behavior and the self-organization of a collective, the whole troop needs to be seen at a scale where behavior can be seen in relation to the natural environment in which ecological decisions are made. This study presents a novel dataset from drone videos for baboon detection, tracking, and behavior recognition. The baboon detection dataset was created by manually annotating all baboons in drone videos with bounding boxes. A tiling method was subsequently applied to create a pyramid of images at various scales from the original 5.3K resolution images, resulting in approximately 30K images used for baboon detection. The tracking dataset is derived from the detection dataset, where all bounding boxes are assigned the same ID throughout the video. This process resulted in half an hour of very dense tracking data. The behavior recognition dataset was generated by converting tracks into mini-scenes, a video subregion centered on each animal; each mini-scene was manually annotated with 12 distinct behavior types, resulting in over 20 hours of data. Benchmark results show mean average precision (mAP) of 92.62\\% for the YOLOv8-X detection model, multiple object tracking precision (MOTA) of 63.81\\% for the BotSort tracking algorithm, and micro top-1 accuracy of 63.97\\% for the X3D behavior recognition model. Using deep learning to classify wildlife behavior from drone footage facilitates non-invasive insight into the collective behavior of an entire group."
    },
    "2405.13875v1": {
      "title": "On the Inapproximability of Finding Minimum Monitoring Edge-Geodetic Sets",
      "url": "http://arxiv.org/abs/2405.13875v1",
      "authors": "Davide Bil\u00f2, Giordano Colli, Luca Forlizzi, Stefano Leucci",
      "update_time": "2024-05-22",
      "abstract": "Given an undirected connected graph $G = (V(G), E(G))$ on $n$ vertices, the minimum Monitoring Edge-Geodetic Set (MEG-set) problem asks to find a subset $M \\subseteq V(G)$ of minimum cardinality such that, for every edge $e \\in E(G)$, there exist $x,y \\in M$ for which all shortest paths between $x$ and $y$ in $G$ traverse $e$.   We show that, for any constant $c < \\frac{1}{2}$, no polynomial-time $(c \\log n)$-approximation algorithm for the minimum MEG-set problem exists, unless $\\mathsf{P} = \\mathsf{NP}$."
    },
    "2405.01012v1": {
      "title": "Correcting Biased Centered Kernel Alignment Measures in Biological and Artificial Neural Networks",
      "url": "http://arxiv.org/abs/2405.01012v1",
      "authors": "Alex Murphy, Joel Zylberberg, Alona Fyshe",
      "update_time": "2024-05-02",
      "abstract": "Centred Kernel Alignment (CKA) has recently emerged as a popular metric to compare activations from biological and artificial neural networks (ANNs) in order to quantify the alignment between internal representations derived from stimuli sets (e.g. images, text, video) that are presented to both systems. In this paper we highlight issues that the community should take into account if using CKA as an alignment metric with neural data. Neural data are in the low-data high-dimensionality domain, which is one of the cases where (biased) CKA results in high similarity scores even for pairs of random matrices. Using fMRI and MEG data from the THINGS project, we show that if biased CKA is applied to representations of different sizes in the low-data high-dimensionality domain, they are not directly comparable due to biased CKA's sensitivity to differing feature-sample ratios and not stimuli-driven responses. This situation can arise both when comparing a pre-selected area of interest (e.g. ROI) to multiple ANN layers, as well as when determining to which ANN layer multiple regions of interest (ROIs) / sensor groups of different dimensionality are most similar. We show that biased CKA can be artificially driven to its maximum value when using independent random data of different sample-feature ratios. We further show that shuffling sample-feature pairs of real neural data does not drastically alter biased CKA similarity in comparison to unshuffled data, indicating an undesirable lack of sensitivity to stimuli-driven neural responses. Positive alignment of true stimuli-driven responses is only achieved by using debiased CKA. Lastly, we report findings that suggest biased CKA is sensitive to the inherent structure of neural data, only differing from shuffled data when debiased CKA detects stimuli-driven alignment.",
      "code_url": "https://github.com/Alxmrphi/correcting_CKA_alignment"
    },
    "2404.15588v1": {
      "title": "Minimal Evidence Group Identification for Claim Verification",
      "url": "http://arxiv.org/abs/2404.15588v1",
      "authors": "Xiangci Li, Sihao Chen, Rajvi Kapadia, Jessica Ouyang, Fan Zhang",
      "update_time": "2024-04-24",
      "abstract": "Claim verification in real-world settings (e.g. against a large collection of candidate evidences retrieved from the web) typically requires identifying and aggregating a complete set of evidence pieces that collectively provide full support to the claim. The problem becomes particularly challenging when there exists distinct sets of evidence that could be used to verify the claim from different perspectives. In this paper, we formally define and study the problem of identifying such minimal evidence groups (MEGs) for claim verification. We show that MEG identification can be reduced from Set Cover problem, based on entailment inference of whether a given evidence group provides full/partial support to a claim. Our proposed approach achieves 18.4% and 34.8% absolute improvements on the WiCE and SciFact datasets over LLM prompting. Finally, we demonstrate the benefits of MEGs in downstream applications such as claim generation."
    },
    "2404.10869v1": {
      "title": "Alpha rhythm slowing in temporal epilepsy across Scalp EEG and MEG",
      "url": "http://arxiv.org/abs/2404.10869v1",
      "authors": "Vytene Janiukstyte, Csaba Kozma, Thomas W. Owen, Umair J Chaudhury, Beate Diehl, Louis Lemieux, John S Duncan, Fergus Rugg-Gunn, Jane de Tisi, Yujiang Wang, Peter N. Taylor",
      "update_time": "2024-04-16",
      "abstract": "EEG slowing is reported in various neurological disorders including Alzheimer's, Parkinson's and Epilepsy. Here, we investigate alpha rhythm slowing in individuals with refractory temporal lobe epilepsy (TLE), compared to healthy controls, using scalp electroencephalography (EEG) and magnetoencephalography (MEG).   We retrospectively analysed data from 17,(46) healthy controls and 22,(24) individuals with TLE who underwent scalp EEG and (MEG) recordings as part of presurgical evaluation. Resting-state, eyes-closed recordings were source reconstructed using the standardized low-resolution brain electrographic tomography (sLORETA) method. We extracted low (slow) 6-9 Hz and high (fast) 10-11 Hz alpha relative band power and calculated the alpha power ratio by dividing low (slow) alpha by high (fast) alpha. This ratio was computed for all brain regions in all individuals.   Alpha oscillations were slower in individuals with TLE than controls (p<0.05). This effect was present in both the ipsilateral and contralateral hemispheres, and across widespread brain regions.   Alpha slowing in TLE was found in both EEG and MEG recordings. We interpret greater low (slow)-alpha as greater deviation from health."
    },
    "2404.09256v1": {
      "title": "Foundational GPT Model for MEG",
      "url": "http://arxiv.org/abs/2404.09256v1",
      "authors": "Richard Csaky, Mats W. J. van Es, Oiwi Parker Jones, Mark Woolrich",
      "update_time": "2024-04-14",
      "abstract": "Deep learning techniques can be used to first training unsupervised models on large amounts of unlabelled data, before fine-tuning the models on specific tasks. This approach has seen massive success for various kinds of data, e.g. images, language, audio, and holds the promise of improving performance in various downstream tasks (e.g. encoding or decoding brain data). However, there has been limited progress taking this approach for modelling brain signals, such as Magneto-/electroencephalography (M/EEG). Here we propose two classes of deep learning foundational models that can be trained using forecasting of unlabelled MEG. First, we consider a modified Wavenet; and second, we consider a modified Transformer-based (GPT2) model. The modified GPT2 includes a novel application of tokenisation and embedding methods, allowing a model developed initially for the discrete domain of language to be applied to continuous multichannel time series data. We also extend the forecasting framework to include condition labels as inputs, enabling better modelling (encoding) of task data. We compare the performance of these deep learning models with standard linear autoregressive (AR) modelling on MEG data. This shows that GPT2-based models provide better modelling capabilities than Wavenet and linear AR models, by better reproducing the temporal, spatial and spectral characteristics of real data and evoked activity in task data. We show how the GPT2 model scales well to multiple subjects, while adapting its model to each subject through subject embedding. Finally, we show how such a model can be useful in downstream decoding tasks through data simulation. All code is available on GitHub (https://github.com/ricsinaruto/MEG-transfer-decoding).",
      "code_url": "https://github.com/ricsinaruto/meg-transfer-decoding"
    },
    "2404.07839v1": {
      "title": "RecurrentGemma: Moving Past Transformers for Efficient Open Language Models",
      "url": "http://arxiv.org/abs/2404.07839v1",
      "authors": "Aleksandar Botev, Soham De, Samuel L Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, L\u00e9onard Hussenot, Johan Ferret, Sertan Girgin, Olivier Bachem, Alek Andreev, Kathleen Kenealy, Thomas Mesnard, Cassidy Hardin, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Armand Joulin, Noah Fiedel, Evan Senter, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, David Budden, Arnaud Doucet, Sharad Vikram, Adam Paszke, Trevor Gale, Sebastian Borgeaud, Charlie Chen, Andy Brock, Antonia Paterson, Jenny Brennan, Meg Risdal, Raj Gundluru, Nesh Devanathan, Paul Mooney, Nilay Chauhan, Phil Culliton, Luiz GUStavo Martins, Elisa Bandy, David Huntsperger, Glenn Cameron, Arthur Zucker, Tris Warkentin, Ludovic Peran, Minh Giang, Zoubin Ghahramani, Cl\u00e9ment Farabet, Koray Kavukcuoglu, Demis Hassabis, Raia Hadsell, Yee Whye Teh, Nando de Frietas",
      "update_time": "2024-04-11",
      "abstract": "We introduce RecurrentGemma, an open language model which uses Google's novel Griffin architecture. Griffin combines linear recurrences with local attention to achieve excellent performance on language. It has a fixed-sized state, which reduces memory use and enables efficient inference on long sequences. We provide a pre-trained model with 2B non-embedding parameters, and an instruction tuned variant. Both models achieve comparable performance to Gemma-2B despite being trained on fewer tokens.",
      "code_url": "https://github.com/google-deepmind/recurrentgemma"
    },
    "2404.01250v1": {
      "title": "Image Reconstruction from Electroencephalography Using Latent Diffusion",
      "url": "http://arxiv.org/abs/2404.01250v1",
      "authors": "Teng Fei, Virginia de Sa",
      "update_time": "2024-04-01",
      "abstract": "In this work, we have adopted the diffusion-based image reconstruction pipeline previously used for fMRI image reconstruction and applied it to Electroencephalography (EEG). The EEG encoding method is very simple, and forms a baseline from which more sophisticated EEG encoding methods can be compared. We have also evaluated the fidelity of the generated image using the same metrics used in the previous functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) works. Our results show that while the reconstruction from EEG recorded to rapidly presented images is not as good as reconstructions from fMRI to slower presented images, it holds a surprising amount of information that could be applied in specific use cases. Also, EEG-based image reconstruction works better in some categories-such as land animals and food-than others, shedding new light on previous findings of EEG's sensitivity to those categories and revealing potential for these methods to further understand EEG responses to human visual coding. More investigation should use longer-duration image stimulations to elucidate the later components that might be salient to the different image categories.",
      "code_url": "https://github.com/desa-lab/eeg-image-reconstruction"
    }
  },
  "neuroAI": {
    "2306.10168v3": {
      "title": "Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis",
      "url": "http://arxiv.org/abs/2306.10168v3",
      "authors": "Mitchell Ostrow, Adam Eisen, Leo Kozachkov, Ila Fiete",
      "update_time": "2023-10-29",
      "abstract": "How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.",
      "code_url": "https://github.com/mitchellostrow/dsa"
    },
    "2305.11275v2": {
      "title": "Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture",
      "url": "http://arxiv.org/abs/2305.11275v2",
      "authors": "Galen Pogoncheff, Jacob Granley, Michael Beyeler",
      "update_time": "2023-05-25",
      "abstract": "Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence."
    },
    "2301.09245v2": {
      "title": "Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks",
      "url": "http://arxiv.org/abs/2301.09245v2",
      "authors": "Feng-Lei Fan, Yingxin Li, Hanchuan Peng, Tieyong Zeng, Fei Wang",
      "update_time": "2023-03-11",
      "abstract": "Throughout history, the development of artificial intelligence, particularly artificial neural networks, has been open to and constantly inspired by the increasingly deepened understanding of the brain, such as the inspiration of neocognitron, which is the pioneering work of convolutional neural networks. Per the motives of the emerging field: NeuroAI, a great amount of neuroscience knowledge can help catalyze the next generation of AI by endowing a network with more powerful capabilities. As we know, the human brain has numerous morphologically and functionally different neurons, while artificial neural networks are almost exclusively built on a single neuron type. In the human brain, neuronal diversity is an enabling factor for all kinds of biological intelligent behaviors. Since an artificial network is a miniature of the human brain, introducing neuronal diversity should be valuable in terms of addressing those essential problems of artificial networks such as efficiency, interpretability, and memory. In this Primer, we first discuss the preliminaries of biological neuronal diversity and the characteristics of information transmission and processing in a biological neuron. Then, we review studies of designing new neurons for artificial networks. Next, we discuss what gains can neuronal diversity bring into artificial networks and exemplary applications in several important fields. Lastly, we discuss the challenges and future directions of neuronal diversity to explore the potential of NeuroAI."
    },
    "2212.04401v1": {
      "title": "A Rubric for Human-like Agents and NeuroAI",
      "url": "http://arxiv.org/abs/2212.04401v1",
      "authors": "Ida Momennejad",
      "update_time": "2022-12-08",
      "abstract": "Researchers across cognitive, neuro-, and computer sciences increasingly reference human-like artificial intelligence and neuroAI. However, the scope and use of the terms are often inconsistent. Contributed research ranges widely from mimicking behaviour, to testing machine learning methods as neurally plausible hypotheses at the cellular or functional levels, or solving engineering problems. However, it cannot be assumed nor expected that progress on one of these three goals will automatically translate to progress in others. Here a simple rubric is proposed to clarify the scope of individual contributions, grounded in their commitments to human-like behaviour, neural plausibility, or benchmark/engineering goals. This is clarified using examples of weak and strong neuroAI and human-like agents, and discussing the generative, corroborate, and corrective ways in which the three dimensions interact with one another. The author maintains that future progress in artificial intelligence will need strong interactions across the disciplines, with iterative feedback loops and meticulous validity tests, leading to both known and yet-unknown advances that may span decades to come."
    },
    "2210.08340v3": {
      "title": "Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolution",
      "url": "http://arxiv.org/abs/2210.08340v3",
      "authors": "Anthony Zador, Sean Escola, Blake Richards, Bence \u00d6lveczky, Yoshua Bengio, Kwabena Boahen, Matthew Botvinick, Dmitri Chklovskii, Anne Churchland, Claudia Clopath, James DiCarlo, Surya Ganguli, Jeff Hawkins, Konrad Koerding, Alexei Koulakov, Yann LeCun, Timothy Lillicrap, Adam Marblestone, Bruno Olshausen, Alexandre Pouget, Cristina Savin, Terrence Sejnowski, Eero Simoncelli, Sara Solla, David Sussillo, Andreas S. Tolias, Doris Tsao",
      "update_time": "2023-02-22",
      "abstract": "Neuroscience has long been an essential driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI. A core component of this is the embodied Turing test, which challenges AI animal models to interact with the sensorimotor world at skill levels akin to their living counterparts. The embodied Turing test shifts the focus from those capabilities like game playing and language that are especially well-developed or uniquely human to those capabilities, inherited from over 500 million years of evolution, that are shared with all animals. Building models that can pass the embodied Turing test will provide a roadmap for the next generation of AI."
    },
    "2112.15459v3": {
      "title": "Social Neuro AI: Social Interaction as the \"dark matter\" of AI",
      "url": "http://arxiv.org/abs/2112.15459v3",
      "authors": "Samuele Bolotta, Guillaume Dumas",
      "update_time": "2022-04-11",
      "abstract": "This article introduces a three-axis framework indicating how AI can be informed by biological examples of social learning mechanisms. We argue that the complex human cognitive architecture owes a large portion of its expressive power to its ability to engage in social and cultural learning. However, the field of AI has mostly embraced a solipsistic perspective on intelligence. We thus argue that social interactions not only are largely unexplored in this field but also are an essential element of advanced cognitive ability, and therefore constitute metaphorically the dark matter of AI. In the first section, we discuss how social learning plays a key role in the development of intelligence. We do so by discussing social and cultural learning theories and empirical findings from social neuroscience. Then, we discuss three lines of research that fall under the umbrella of Social NeuroAI and can contribute to developing socially intelligent embodied agents in complex environments. First, neuroscientific theories of cognitive architecture, such as the global workspace theory and the attention schema theory, can enhance biological plausibility and help us understand how we could bridge individual and social theories of intelligence. Second, intelligence occurs in time as opposed to over time, and this is naturally incorporated by dynamical systems. Third, embodiment has been demonstrated to provide more sophisticated array of communicative signals. To conclude, we discuss the example of active inference, which offers powerful insights for developing agents that possess biological realism, can self-organize in time, and are socially embodied."
    },
    "2011.07464v2": {
      "title": "Predictive Coding, Variational Autoencoders, and Biological Connections",
      "url": "http://arxiv.org/abs/2011.07464v2",
      "authors": "Joseph Marino",
      "update_time": "2021-10-23",
      "abstract": "This paper reviews predictive coding, from theoretical neuroscience, and variational autoencoders, from machine learning, identifying the common origin and mathematical framework underlying both areas. As each area is prominent within its respective field, more firmly connecting these areas could prove useful in the dialogue between neuroscience and machine learning. After reviewing each area, we discuss two possible correspondences implied by this perspective: cortical pyramidal dendrites as analogous to (non-linear) deep networks and lateral inhibition as analogous to normalizing flows. These connections may provide new directions for further investigations in each field."
    },
    "1909.02603v2": {
      "title": "Additive function approximation in the brain",
      "url": "http://arxiv.org/abs/1909.02603v2",
      "authors": "Kameron Decker Harris",
      "update_time": "2019-09-13",
      "abstract": "Many biological learning systems such as the mushroom body, hippocampus, and cerebellum are built from sparsely connected networks of neurons. For a new understanding of such networks, we study the function spaces induced by sparse random features and characterize what functions may and may not be learned. A network with $d$ inputs per neuron is found to be equivalent to an additive model of order $d$, whereas with a degree distribution the network combines additive terms of different orders. We identify three specific advantages of sparsity: additive function approximation is a powerful inductive bias that limits the curse of dimensionality, sparse networks are stable to outlier noise in the inputs, and sparse random features are scalable. Thus, even simple brain architectures can be powerful function approximators. Finally, we hope that this work helps popularize kernel theories of networks among computational neuroscientists.",
      "code_url": "https://github.com/kharris/sparse-random-features"
    }
  },
  "medical": {
    "2406.04026v1": {
      "title": "Quantification of Collateral Supply with Local-AIF Dynamic Susceptibility Contrast MRI Predicts Infarct Growth",
      "url": "http://arxiv.org/abs/2406.04026v1",
      "authors": "Mira M. Liu, Niloufar Saadat, Steven P. Roth, Marek A. Niekrasz, Mihai Giurcanu, Timothy J. Carroll, Gregory A. Christoforidis",
      "update_time": "2024-06-06",
      "abstract": "In ischemic stroke, leptomeningeal collaterals can provide compensatory blood flow to tissue at risk despite an occlusion, and impact treatment response and infarct growth. The purpose of this work is to test the hypothesis that local perfusion with an appropriate Local Arterial Input Function (AIF) is needed to quantify the degree of collateral blood supply in tissue distal to an occlusion. Seven experiments were conducted in a pre-clinical middle cerebral artery occlusion model. Magnetic resonance dynamic susceptibility contrast (DSC) was imaged and post-processed as cerebral blood flow maps with both a traditionally chosen single arterial input function (AIF) applied globally to the whole brain (i.e. \"Global-AIF\") and a novel automatic delay and dispersion corrected AIF (i.e. \"Local AIF\") that is sensitive to retrograde flow. Pial collateral recruitment was assessed from x-ray angiograms and infarct growth via serially acquired diffusion weighted MRI scans both blinded to DSC. The degree of collateralization at x-ray correlated strongly with quantitative perfusion determined using the Local AIF in the ischemic penumbra (R2=0.81) compared to a traditionally chosen Global-AIF (R2=0.05). Quantitative perfusion calculated using a Local-AIF was negatively correlated (less infarct progression as local perfusion increased) with infarct growth (R2 = 0.79) compared to Global-AIF (R2=0.02). Local DSC perfusion with a Local-AIF is more accurate for assessing tissue status and degree of leptomeningeal collateralization than traditionally chosen AIFs. These findings support use of a Local-AIF in determining quantitative tissue perfusion with collateral supply in occlusive disease."
    },
    "2406.03986v1": {
      "title": "On The Persona-based Summarization of Domain-Specific Documents",
      "url": "http://arxiv.org/abs/2406.03986v1",
      "authors": "Ankan Mullick, Sombit Bose, Rounak Saha, Ayan Kumar Bhowmick, Pawan Goyal, Niloy Ganguly, Prasenjit Dey, Ravi Kokku",
      "update_time": "2024-06-06",
      "abstract": "In an ever-expanding world of domain-specific knowledge, the increasing complexity of consuming, and storing information necessitates the generation of summaries from large information repositories. However, every persona of a domain has different requirements of information and hence their summarization. For example, in the healthcare domain, a persona-based (such as Doctor, Nurse, Patient etc.) approach is imperative to deliver targeted medical information efficiently. Persona-based summarization of domain-specific information by humans is a high cognitive load task and is generally not preferred. The summaries generated by two different humans have high variability and do not scale in cost and subject matter expertise as domains and personas grow. Further, AI-generated summaries using generic Large Language Models (LLMs) may not necessarily offer satisfactory accuracy for different domains unless they have been specifically trained on domain-specific data and can also be very expensive to use in day-to-day operations. Our contribution in this paper is two-fold: 1) We present an approach to efficiently fine-tune a domain-specific small foundation LLM using a healthcare corpus and also show that we can effectively evaluate the summarization quality using AI-based critiquing. 2) We further show that AI-based critiquing has good concordance with Human-based critiquing of the summaries. Hence, such AI-based pipelines to generate domain-specific persona-based summaries can be easily scaled to other domains such as legal, enterprise documents, education etc. in a very efficient and cost-effective manner.",
      "code_url": "https://github.com/ankan2/persona-healthcare"
    },
    "2406.03962v1": {
      "title": "3D Ultrasound Shear Wave Elastography for Musculoskeletal Tissue Assessment Under Compressive Load: A Feasibility Study",
      "url": "http://arxiv.org/abs/2406.03962v1",
      "authors": "Bryan J. Ranger, Kevin M. Moerman, Micha Feigin, Hugh M. Herr, Brian W. Anthony",
      "update_time": "2024-06-06",
      "abstract": "Given its real-time capability to quantify mechanical tissue properties, ultrasound shear wave elastography holds significant promise in clinical musculoskeletal imaging. However, existing shear wave elastography methods fall short in enabling full-limb analysis of 3D anatomical structures under diverse loading conditions, and may introduce measurement bias due to sonographer-applied force on the transducer. These limitations pose numerous challenges, particularly for 3D computational biomechanical tissue modeling in areas like prosthetic socket design. In this feasibility study, a clinical linear ultrasound transducer system with integrated shear wave elastography capabilities was utilized to scan both a calibrated phantom and human limbs in a water tank imaging setup. By conducting 2D and 3D scans under varying compressive loads, this study demonstrates the feasibility of volumetric ultrasound shear wave elastography of human limbs. Our preliminary results showcase a potential method for evaluating 3D spatially varying tissue properties, offering future extensions to computational biomechanical modeling of tissue for various clinical scenarios."
    },
    "2406.03949v1": {
      "title": "UltraMedical: Building Specialized Generalists in Biomedicine",
      "url": "http://arxiv.org/abs/2406.03949v1",
      "authors": "Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, Xingtai Lv, Hu Jinfang, Zhiyuan Liu, Bowen Zhou",
      "update_time": "2024-06-06",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains and are moving towards more specialized areas. Recent advanced proprietary models such as GPT-4 and Gemini have achieved significant advancements in biomedicine, which have also raised privacy and security challenges. The construction of specialized generalists hinges largely on high-quality datasets, enhanced by techniques like supervised fine-tuning and reinforcement learning from human or AI feedback, and direct preference optimization. However, these leading technologies (e.g., preference learning) are still significantly limited in the open source community due to the scarcity of specialized data. In this paper, we present the UltraMedical collections, which consist of high-quality manual and synthetic datasets in the biomedicine domain, featuring preference annotations across multiple advanced LLMs. By utilizing these datasets, we fine-tune a suite of specialized medical models based on Llama-3 series, demonstrating breathtaking capabilities across various medical benchmarks. Moreover, we develop powerful reward models skilled in biomedical and general reward benchmark, enhancing further online preference learning within the biomedical LLM community.",
      "code_url": "https://github.com/tsinghuac3i/ultramedical"
    },
    "2406.03902v1": {
      "title": "C^2RV: Cross-Regional and Cross-View Learning for Sparse-View CBCT Reconstruction",
      "url": "http://arxiv.org/abs/2406.03902v1",
      "authors": "Yiqun Lin, Jiewen Yang, Hualiang Wang, Xinpeng Ding, Wei Zhao, Xiaomeng Li",
      "update_time": "2024-06-06",
      "abstract": "Cone beam computed tomography (CBCT) is an important imaging technology widely used in medical scenarios, such as diagnosis and preoperative planning. Using fewer projection views to reconstruct CT, also known as sparse-view reconstruction, can reduce ionizing radiation and further benefit interventional radiology. Compared with sparse-view reconstruction for traditional parallel/fan-beam CT, CBCT reconstruction is more challenging due to the increased dimensionality caused by the measurement process based on cone-shaped X-ray beams. As a 2D-to-3D reconstruction problem, although implicit neural representations have been introduced to enable efficient training, only local features are considered and different views are processed equally in previous works, resulting in spatial inconsistency and poor performance on complicated anatomies. To this end, we propose C^2RV by leveraging explicit multi-scale volumetric representations to enable cross-regional learning in the 3D space. Additionally, the scale-view cross-attention module is introduced to adaptively aggregate multi-scale and multi-view features. Extensive experiments demonstrate that our C^2RV achieves consistent and significant improvement over previous state-of-the-art methods on datasets with diverse anatomy.",
      "code_url": "https://github.com/xmed-lab/C2RV-CBCT"
    },
    "2406.03901v1": {
      "title": "Polyp and Surgical Instrument Segmentation with Double Encoder-Decoder Networks",
      "url": "http://arxiv.org/abs/2406.03901v1",
      "authors": "Adrian Galdran",
      "update_time": "2024-06-06",
      "abstract": "This paper describes a solution for the MedAI competition, in which participants were required to segment both polyps and surgical instruments from endoscopic images. Our approach relies on a double encoder-decoder neural network which we have previously applied for polyp segmentation, but with a series of enhancements: a more powerful encoder architecture, an improved optimization procedure, and the post-processing of segmentations based on tempered model ensembling. Experimental results show that our method produces segmentations that show a good agreement with manual delineations provided by medical experts."
    },
    "2406.03855v1": {
      "title": "Performance of large language models in numerical vs. semantic medical knowledge: Benchmarking on evidence-based Q&As",
      "url": "http://arxiv.org/abs/2406.03855v1",
      "authors": "Eden Avnat, Michal Levy, Daniel Herstain, Elia Yanko, Daniel Ben Joya, Michal Tzuchman Katz, Dafna Eshel, Sahar Laros, Yael Dagan, Shahar Barami, Joseph Mermelstein, Shahar Ovadia, Noam Shomron, Varda Shalev, Raja-Elie E. Abdulnour",
      "update_time": "2024-06-06",
      "abstract": "Clinical problem-solving requires processing of semantic medical knowledge such as illness scripts and numerical medical knowledge of diagnostic tests for evidence-based decision-making. As large language models (LLMs) show promising results in many aspects of language-based clinical practice, their ability to generate non-language evidence-based answers to clinical questions is inherently limited by tokenization. Therefore, we evaluated LLMs' performance on two question types: numeric (correlating findings) and semantic (differentiating entities) while examining differences within and between LLMs in medical aspects and comparing their performance to humans. To generate straightforward multi-choice questions and answers (QAs) based on evidence-based medicine (EBM), we used a comprehensive medical knowledge graph (encompassed data from more than 50,00 peer-reviewed articles) and created the \"EBMQA\". EBMQA contains 105,000 QAs labeled with medical and non-medical topics and classified into numerical or semantic questions. We benchmarked this dataset using more than 24,500 QAs on two state-of-the-art LLMs: Chat-GPT4 and Claude3-Opus. We evaluated the LLMs accuracy on semantic and numerical question types and according to sub-labeled topics. For validation, six medical experts were tested on 100 numerical EBMQA questions. We found that both LLMs excelled more in semantic than numerical QAs, with Claude3 surpassing GPT4 in numerical QAs. However, both LLMs showed inter and intra gaps in different medical aspects and remained inferior to humans. Thus, their medical advice should be addressed carefully."
    },
    "2406.03823v1": {
      "title": "Views about ChatGPT: Are human decision making and human learning necessary?",
      "url": "http://arxiv.org/abs/2406.03823v1",
      "authors": "Eiji Yamamura, Fumio Ohtake",
      "update_time": "2024-06-06",
      "abstract": "Using individual-level survey data from 2024, this study investigated how respondent characteristics are associated with a subjective view of generative artificial intelligence (GAI). We asked 14 questions concerning respondents view about GAI, such as general view, faulty GAI, autonomous GEI, GAI replacing humans, and importance of human learning. Regression analysis based on the ordered logit model revealed that: (1) In some cases, the results of smartphone and computer usage times differed. Smartphone usage time was negatively correlated with the importance of human learning, whereas computer usage was not negatively correlated. (2) Managers and ordinary businesspeople have positive views of GAI. However, managers do not show a positive view about GAI being responsible for human decision making. (3) Teachers generally have a negative view about GAI replacing humans and no need of learning. They do not have negative views about GAI producing documents unless GAI is faulty. (4) Medical industry workers positively view GAI if it operates following their direction. However, they do not agree with the view that GAI replaces humans, and that human learning is unnecessary. (5) Females are less likely than men to have a positive view of GAI. In summary, views about GAI vary widely by the individual characteristics and condition of GAI, and by the question set."
    },
    "2406.03798v1": {
      "title": "Optical biomarker of metabolism for breast tumor diagnosis: Insights from subcellular dynamics",
      "url": "http://arxiv.org/abs/2406.03798v1",
      "authors": "Zichen Yin, Shuwei Zhang, Bin He, Houpu Yang, Zhengyu Chen, Zhangwei Hu, Yejiong Shi, Ruizhi Xue, Panqi Yang, Yuzhe Ying, Chengming Wang, Shu Wang, Ping Xue",
      "update_time": "2024-06-06",
      "abstract": "Label-free metabolic dynamics contrast is highly appealing but difficult to achieve in biomedical imaging. Interference offers a highly sensitive mechanism for capturing the metabolic dynamics of the subcellular scatterers. However, traditional interference detection methods fail to isolate pure metabolic dynamics, as the dynamic signals are coupled with scatterer reflectivity and other uncontrollable imaging factors. Here, we demonstrate active phase modulation-assisted dynamic full-field optical coherence tomography (APMD-FFOCT) that decouples and quantifies the metabolic dynamics by adding a reference movement for all interferential scatterers. This novel technique enables imaging and dynamic analysis of subcellular structures along with their changes during the apoptotic process in tumor tissues. Furthermore, the nucleus-to-cytoplasm dynamic intensity ratio could serve as an optical biomarker for breast tumor grading, enhancing intraoperative diagnosis."
    },
    "2406.03712v1": {
      "title": "A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions",
      "url": "http://arxiv.org/abs/2406.03712v1",
      "authors": "Lei Liu, Xiaoyan Yang, Junchi Lei, Xiaoyang Liu, Yue Shen, Zhiqiang Zhang, Peng Wei, Jinjie Gu, Zhixuan Chu, Zhan Qin, Kui Ren",
      "update_time": "2024-06-06",
      "abstract": "Large language models (LLMs), such as GPT series models, have received substantial attention due to their impressive capabilities for generating and understanding human-level language. More recently, LLMs have emerged as an innovative and powerful adjunct in the medical field, transforming traditional practices and heralding a new era of enhanced healthcare services. This survey provides a comprehensive overview of Medical Large Language Models (Med-LLMs), outlining their evolution from general to the medical-specific domain (i.e, Technology and Application), as well as their transformative impact on healthcare (e.g., Trustworthiness and Safety). Concretely, starting from the fundamental history and technology of LLMs, we first delve into the progressive adaptation and refinements of general LLM models in the medical domain, especially emphasizing the advanced algorithms that boost the LLMs' performance in handling complicated medical environments, including clinical reasoning, knowledge graph, retrieval-augmented generation, human alignment, and multi-modal learning. Secondly, we explore the extensive applications of Med-LLMs across domains such as clinical decision support, report generation, and medical education, illustrating their potential to streamline healthcare services and augment patient outcomes. Finally, recognizing the imperative and responsible innovation, we discuss the challenges of ensuring fairness, accountability, privacy, and robustness in Med-LLMs applications. Finally, we conduct a concise discussion for anticipating possible future trajectories of Med-LLMs, identifying avenues for the prudent expansion of Med-LLMs. By consolidating above-mentioned insights, this review seeks to provide a comprehensive investigation of the potential strengths and limitations of Med-LLMs for professionals and researchers, ensuring a responsible landscape in the healthcare setting."
    }
  }
}