{
  "Brain": {
    "2504.20744v1": {
      "title": "DB-GNN: Dual-Branch Graph Neural Network with Multi-Level Contrastive Learning for Jointly Identifying Within- and Cross-Frequency Coupled Brain Networks",
      "url": "http://arxiv.org/abs/2504.20744v1",
      "authors": "Xiang Wang, Hui Xu, Jing Cai, Ta Zhou, Xibei Yang, Wei Xue",
      "update_time": "2025-04-29",
      "abstract": "Within-frequency coupling (WFC) and cross-frequency coupling (CFC) in brain networks reflect neural synchronization within the same frequency band and cross-band oscillatory interactions, respectively. Their synergy provides a comprehensive understanding of neural mechanisms underlying cognitive states such as emotion. However, existing multi-channel EEG studies often analyze WFC or CFC separately, failing to fully leverage their complementary properties. This study proposes a dual-branch graph neural network (DB-GNN) to jointly identify within- and cross-frequency coupled brain networks. Firstly, DBGNN leverages its unique dual-branch learning architecture to efficiently mine global collaborative information and local cross-frequency and within-frequency coupling information. Secondly, to more fully perceive the global information of cross-frequency and within-frequency coupling, the global perception branch of DB-GNN adopts a Transformer architecture. To prevent overfitting of the Transformer architecture, this study integrates prior within- and cross-frequency coupling information into the Transformer inference process, thereby enhancing the generalization capability of DB-GNN. Finally, a multi-scale graph contrastive learning regularization term is introduced to constrain the global and local perception branches of DB-GNN at both graph-level and node-level, enhancing its joint perception ability and further improving its generalization performance. Experimental validation on the emotion recognition dataset shows that DB-GNN achieves a testing accuracy of 97.88% and an F1- score of 97.87%, reaching the state-of-the-art performance."
    },
    "2504.20554v1": {
      "title": "Small brains but big challenges: white matter tractography in early life samples",
      "url": "http://arxiv.org/abs/2504.20554v1",
      "authors": "Jessica Dubois, Mareike Grotheer, Joseph Yuan-Mou Yang, Jacques-Donald Tournier, Christian Beaulieu, Catherine Lebel",
      "update_time": "2025-04-29",
      "abstract": "In the human brain, white matter development is a complex and long-lasting process involving intermingling micro-and macrostructural mechanisms, such as fiber growth, pruning and myelination. Did you know that all these neurodevelopmental changes strongly affect MRI signals, with consequences on tractography performances and reliability? This communication aims to elaborate on these aspects, highlighting the importance of tracking and studying the developing connections with dedicated approaches."
    },
    "2504.20416v1": {
      "title": "Large-scale artificial intelligence with 41 million nanophotonic neurons on a metasurface",
      "url": "http://arxiv.org/abs/2504.20416v1",
      "authors": "Mingcheng Luo, Meirui Jiang, Bhavin J. Shastri, Nansen Zhou, Wenfei Guo, Jianmin Xiong, Dongliang Wang, Renjie Zhou, Chester Shu, Qi Dou, Chaoran Huang",
      "update_time": "2025-04-29",
      "abstract": "Conventional integrated circuits (ICs) struggle to meet the escalating demands of artificial intelligence (AI). This has sparked a renewed interest in an unconventional computing paradigm: neuromorphic (brain-inspired) computing. However, current neuromorphic systems face significant challenges in delivering a large number of parameters (i.e., weights) required for large-scale AI models. As a result, most neuromorphic hardware is limited to basic benchmark demonstrations, hindering its application to real-world AI challenges. Here, we present a large-scale optical neural network (ONN) for machine learning acceleration, featuring over 41 million photonic neurons. This system not only surpasses digital electronics in speed and energy efficiency but more importantly, closes the performance gap with large-scale AI models. Our ONN leverages an innovative optical metasurface device featuring numerous spatial modes. This device integrates over 41 million meta-atoms on a 10 mm$^2$ metasurface chip, enabling the processing of tens of millions of weights in a single operation. For the first time, we demonstrate that an ONN, utilizing a single-layer metasurface, can match the performance of deep and large-scale deep learning models, such as ResNet and Vision Transformer, across various benchmark tasks. Additionally, we show that our system can deliver high-performance solutions to real-world AI challenges through its unprecedented scale, such as accelerating the analysis of multi-gigapixel whole slide images (WSIs) for cancer detection by processing the million-pixel sub-image in a single shot. Our system reduces computing time and energy consumption by over 1,000 times compared to state-of-the-art graphic processing units (GPUs). This work presents a large-scale, low-power, and high-performance neuromorphic computing system, paving the way for future disruptive AI technologies."
    },
    "2504.20364v1": {
      "title": "Exploring internal representation of self-supervised networks: few-shot learning abilities and comparison with human semantics and recognition of objects",
      "url": "http://arxiv.org/abs/2504.20364v1",
      "authors": "Asaki Kataoka, Yoshihiro Nagano, Masafumi Oizumi",
      "update_time": "2025-04-29",
      "abstract": "Recent advances in self-supervised learning have attracted significant attention from both machine learning and neuroscience. This is primarily because self-supervised methods do not require annotated supervisory information, making them applicable to training artificial networks without relying on large amounts of curated data, and potentially offering insights into how the brain adapts to its environment in an unsupervised manner. Although several previous studies have elucidated the correspondence between neural representations in deep convolutional neural networks (DCNNs) and biological systems, the extent to which unsupervised or self-supervised learning can explain the human-like acquisition of categorically structured information remains less explored. In this study, we investigate the correspondence between the internal representations of DCNNs trained using a self-supervised contrastive learning algorithm and human semantics and recognition. To this end, we employ a few-shot learning evaluation procedure, which measures the ability of DCNNs to recognize novel concepts from limited exposure, to examine the inter-categorical structure of the learned representations. Two comparative approaches are used to relate the few-shot learning outcomes to human semantics and recognition, with results suggesting that the representations acquired through contrastive learning are well aligned with human cognition. These findings underscore the potential of self-supervised contrastive learning frameworks to model learning mechanisms similar to those of the human brain, particularly in scenarios where explicit supervision is unavailable, such as in human infants prior to language acquisition."
    },
    "2504.20244v1": {
      "title": "Brain pulsations enhance cerebrospinal fluid flow in perivascular spaces",
      "url": "http://arxiv.org/abs/2504.20244v1",
      "authors": "Gregory Holba, James P. Hague, Nigel Hoggard, Marc Pradas",
      "update_time": "2025-04-28",
      "abstract": "A novel approach to modelling cerebrospinal fluid (CSF) flow in arterial perivascular spaces (PVS) is adopted by proposing that the outer PVS boundary oscillates due to brain pulsations in addition to arterial wall motion driven by a blood pressure wave. Lubrication theory is employed to derive a mathematical model for the CSF flow, which is then solved numerically. A parametric analysis is undertaken to investigate the effect of the brain pulsations, which shows that pulsations magnify the net axial CSF flows created by the arterial wall motion. The findings suggest that net axial CSF flows are almost entirely positive (deeper into the brain), with arterial wall motion highly dependent on PVS-perforating artery configurations. Given the glymphatic hypothesis, the findings support the clinical practice of treating dilated PVS as indicators of an increased likelihood of neurodegenerative conditions, such as dementia."
    },
    "2504.19950v1": {
      "title": "Data-Driven Stabilization of Unknown Linear-Threshold Network Dynamics",
      "url": "http://arxiv.org/abs/2504.19950v1",
      "authors": "Xuan Wang, Duy Duong-Tran, Jorge Cort\u00e9s",
      "update_time": "2025-04-28",
      "abstract": "This paper studies the data-driven control of unknown linear-threshold network dynamics to stabilize the state to a reference value. We consider two types of controllers: (i) a state feedback controller with feed-forward reference input and (ii) an augmented feedback controller with error integration. The first controller features a simpler structure and is easier to design, while the second offers improved performance in the presence of system parameter changes and disturbances. Our design strategy employs state-input datasets to construct data-based representations of the closed-loop dynamics. Since these representations involve linear threshold functions, we rewrite them as switched linear systems, and formulate the design problem as that of finding a common controller for all the resulting modes. This gives rise to a set of linear matrix inequalities (LMIs) whose solutions corresponds to the controller gain matrices. We analyze the computational complexity of solving the LMIs and propose a simplified, sufficient set of conditions that scales linearly with the system state. Simulations on two case studies involving regulation of firing rate dynamics in rodent brains and of arousal level dynamics in humans demonstrate the effectiveness of the controller designs."
    },
    "2504.20127v2": {
      "title": "Learning Hierarchical Interaction for Accurate Molecular Property Prediction",
      "url": "http://arxiv.org/abs/2504.20127v2",
      "authors": "Huiyang Hong, Xinkai Wu, Hongyu Sun, Chaoyang Xie, Qi Wang, Yuquan Li",
      "update_time": "2025-04-30",
      "abstract": "Discovering molecules with desirable molecular properties, including ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) profiles, is of great importance in drug discovery. Existing approaches typically employ deep learning models, such as Graph Neural Networks (GNNs) and Transformers, to predict these molecular properties by learning from diverse chemical information. However, these models often fail to efficiently capture and utilize the hierarchical nature of molecular structures, and lack mechanisms for effective interaction among multi-level features. To address these limitations, we propose a Hierarchical Interaction Message Passing Mechanism, which serves as the foundation of our novel model, HimNet. Our method enables interaction-aware representation learning across atomic, motif, and molecular levels via hierarchical attention-guided message passing. This design allows HimNet to effectively balance global and local information, ensuring rich and task-relevant feature extraction for downstream property prediction tasks, such as Blood-Brain Barrier Permeability (BBBP). Extensive experiments on multiple benchmark datasets demonstrate that HimNet achieves the best or near-best performance in most molecular property prediction tasks. Furthermore, our method exhibits promising hierarchical interpretability, aligning well with chemical intuition on representative molecules. We believe that HimNet offers an accurate and efficient solution for molecular activity and ADMET property prediction, contributing significantly to advanced decision-making in the early stages of drug discovery."
    },
    "2504.19596v1": {
      "title": "Towards Robust Multimodal Physiological Foundation Models: Handling Arbitrary Missing Modalities",
      "url": "http://arxiv.org/abs/2504.19596v1",
      "authors": "Xi Fu, Wei-Bang Jiang, Yi Ding, Cuntai Guan",
      "update_time": "2025-04-28",
      "abstract": "Multimodal physiological signals, such as EEG, ECG, EOG, and EMG, are crucial for healthcare and brain-computer interfaces. While existing methods rely on specialized architectures and dataset-specific fusion strategies, they struggle to learn universal representations that generalize across datasets and handle missing modalities at inference time. To address these issues, we propose PhysioOmni, a foundation model for multimodal physiological signal analysis that models both homogeneous and heterogeneous features to decouple multimodal signals and extract generic representations while maintaining compatibility with arbitrary missing modalities. PhysioOmni trains a decoupled multimodal tokenizer, enabling masked signal pre-training via modality-invariant and modality-specific objectives. To ensure adaptability to diverse and incomplete modality combinations, the pre-trained encoders undergo resilient fine-tuning with prototype alignment on downstream datasets. Extensive experiments on four downstream tasks, emotion recognition, sleep stage classification, motor prediction, and mental workload detection, demonstrate that PhysioOmni achieves state-of-the-art performance while maintaining strong robustness to missing modalities. Our code and model weights will be released."
    },
    "2504.20109v1": {
      "title": "Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems",
      "url": "http://arxiv.org/abs/2504.20109v1",
      "authors": "Rajeev Gupta, Suhani Gupta, Ronak Parikh, Divya Gupta, Amir Javaheri, Jairaj Singh Shaktawat",
      "update_time": "2025-04-27",
      "abstract": "Artificial Intelligence has made remarkable advancements in recent years, primarily driven by increasingly large deep learning models. However, achieving true Artificial General Intelligence (AGI) demands fundamentally new architectures rather than merely scaling up existing models. Current approaches largely depend on expanding model parameters, which improves task-specific performance but falls short in enabling continuous, adaptable, and generalized learning. Achieving AGI capable of continuous learning and personalization on resource-constrained edge devices is an even bigger challenge.   This paper reviews the state of continual learning and neuroscience-inspired AI, and proposes a novel architecture for Personalized AGI that integrates brain-like learning mechanisms for edge deployment. We review literature on continuous lifelong learning, catastrophic forgetting, and edge AI, and discuss key neuroscience principles of human learning, including Synaptic Pruning, Hebbian plasticity, Sparse Coding, and Dual Memory Systems, as inspirations for AI systems. Building on these insights, we outline an AI architecture that features complementary fast-and-slow learning modules, synaptic self-optimization, and memory-efficient model updates to support on-device lifelong adaptation.   Conceptual diagrams of the proposed architecture and learning processes are provided. We address challenges such as catastrophic forgetting, memory efficiency, and system scalability, and present application scenarios for mobile AI assistants and embodied AI systems like humanoid robots. We conclude with key takeaways and future research directions toward truly continual, personalized AGI on the edge. While the architecture is theoretical, it synthesizes diverse findings and offers a roadmap for future implementation."
    },
    "2504.19253v1": {
      "title": "Quantitative evaluation of brain-inspired vision sensors in high-speed robotic perception",
      "url": "http://arxiv.org/abs/2504.19253v1",
      "authors": "Taoyi Wang, Lijian Wang, Yihan Lin, Mingtao Ou, Yuguo Chen, Xinglong Ji, Rong Zhao",
      "update_time": "2025-04-27",
      "abstract": "Perception systems in robotics encounter significant challenges in high-speed and dynamic conditions when relying on traditional cameras, where motion blur can compromise spatial feature integrity and task performance. Brain-inspired vision sensors (BVS) have recently gained attention as an alternative, offering high temporal resolution with reduced bandwidth and power requirements. Here, we present the first quantitative evaluation framework for two representative classes of BVSs in variable-speed robotic sensing, including event-based vision sensors (EVS) that detect asynchronous temporal contrasts, and the primitive-based sensor Tianmouc that employs a complementary mechanism to encode both spatiotemporal changes and intensity. A unified testing protocol is established, including crosssensor calibrations, standardized testing platforms, and quality metrics to address differences in data modality. From an imaging standpoint, we evaluate the effects of sensor non-idealities, such as motion-induced distortion, on the capture of structural information. For functional benchmarking, we examine task performance in corner detection and motion estimation under different rotational speeds. Results indicate that EVS performs well in highspeed, sparse scenarios and in modestly fast, complex scenes, but exhibits performance limitations in high-speed, cluttered settings due to pixel-level bandwidth variations and event rate saturation. In comparison, Tianmouc demonstrates consistent performance across sparse and complex scenarios at various speeds, supported by its global, precise, high-speed spatiotemporal gradient samplings. These findings offer valuable insights into the applicationdependent suitability of BVS technologies and support further advancement in this area."
    }
  },
  "EEG": {
    "2504.20744v1": {
      "title": "DB-GNN: Dual-Branch Graph Neural Network with Multi-Level Contrastive Learning for Jointly Identifying Within- and Cross-Frequency Coupled Brain Networks",
      "url": "http://arxiv.org/abs/2504.20744v1",
      "authors": "Xiang Wang, Hui Xu, Jing Cai, Ta Zhou, Xibei Yang, Wei Xue",
      "update_time": "2025-04-29",
      "abstract": "Within-frequency coupling (WFC) and cross-frequency coupling (CFC) in brain networks reflect neural synchronization within the same frequency band and cross-band oscillatory interactions, respectively. Their synergy provides a comprehensive understanding of neural mechanisms underlying cognitive states such as emotion. However, existing multi-channel EEG studies often analyze WFC or CFC separately, failing to fully leverage their complementary properties. This study proposes a dual-branch graph neural network (DB-GNN) to jointly identify within- and cross-frequency coupled brain networks. Firstly, DBGNN leverages its unique dual-branch learning architecture to efficiently mine global collaborative information and local cross-frequency and within-frequency coupling information. Secondly, to more fully perceive the global information of cross-frequency and within-frequency coupling, the global perception branch of DB-GNN adopts a Transformer architecture. To prevent overfitting of the Transformer architecture, this study integrates prior within- and cross-frequency coupling information into the Transformer inference process, thereby enhancing the generalization capability of DB-GNN. Finally, a multi-scale graph contrastive learning regularization term is introduced to constrain the global and local perception branches of DB-GNN at both graph-level and node-level, enhancing its joint perception ability and further improving its generalization performance. Experimental validation on the emotion recognition dataset shows that DB-GNN achieves a testing accuracy of 97.88% and an F1- score of 97.87%, reaching the state-of-the-art performance."
    },
    "2504.19596v1": {
      "title": "Towards Robust Multimodal Physiological Foundation Models: Handling Arbitrary Missing Modalities",
      "url": "http://arxiv.org/abs/2504.19596v1",
      "authors": "Xi Fu, Wei-Bang Jiang, Yi Ding, Cuntai Guan",
      "update_time": "2025-04-28",
      "abstract": "Multimodal physiological signals, such as EEG, ECG, EOG, and EMG, are crucial for healthcare and brain-computer interfaces. While existing methods rely on specialized architectures and dataset-specific fusion strategies, they struggle to learn universal representations that generalize across datasets and handle missing modalities at inference time. To address these issues, we propose PhysioOmni, a foundation model for multimodal physiological signal analysis that models both homogeneous and heterogeneous features to decouple multimodal signals and extract generic representations while maintaining compatibility with arbitrary missing modalities. PhysioOmni trains a decoupled multimodal tokenizer, enabling masked signal pre-training via modality-invariant and modality-specific objectives. To ensure adaptability to diverse and incomplete modality combinations, the pre-trained encoders undergo resilient fine-tuning with prototype alignment on downstream datasets. Extensive experiments on four downstream tasks, emotion recognition, sleep stage classification, motor prediction, and mental workload detection, demonstrate that PhysioOmni achieves state-of-the-art performance while maintaining strong robustness to missing modalities. Our code and model weights will be released."
    },
    "2504.18969v1": {
      "title": "Advancing Face-to-Face Emotion Communication: A Multimodal Dataset (AFFEC)",
      "url": "http://arxiv.org/abs/2504.18969v1",
      "authors": "Meisam J. Sekiavandi, Laurits Dixen, Jostein Fimland, Sree Keerthi Desu, Antonia-Bianca Zserai, Ye Sul Lee, Maria Barrett, Paolo Burre",
      "update_time": "2025-04-26",
      "abstract": "Emotion recognition has the potential to play a pivotal role in enhancing human-computer interaction by enabling systems to accurately interpret and respond to human affect. Yet, capturing emotions in face-to-face contexts remains challenging due to subtle nonverbal cues, variations in personal traits, and the real-time dynamics of genuine interactions. Existing emotion recognition datasets often rely on limited modalities or controlled conditions, thereby missing the richness and variability found in real-world scenarios.   In this work, we introduce Advancing Face-to-Face Emotion Communication (AFFEC), a multimodal dataset designed to address these gaps. AFFEC encompasses 84 simulated emotional dialogues across six distinct emotions, recorded from 73 participants over more than 5,000 trials and annotated with more than 20,000 labels. It integrates electroencephalography (EEG), eye-tracking, galvanic skin response (GSR), facial videos, and Big Five personality assessments. Crucially, AFFEC explicitly distinguishes between felt emotions (the participant's internal affect) and perceived emotions (the observer's interpretation of the stimulus).   Baseline analyses spanning unimodal features and straightforward multimodal fusion demonstrate that even minimal processing yields classification performance significantly above chance, especially for arousal. Incorporating personality traits further improves predictions of felt emotions, highlighting the importance of individual differences. By bridging controlled experimentation with more realistic face-to-face stimuli, AFFEC offers a unique resource for researchers aiming to develop context-sensitive, adaptive, and personalized emotion recognition models."
    },
    "2504.18345v1": {
      "title": "NRevisit: A Cognitive Behavioral Metric for Code Understandability Assessment",
      "url": "http://arxiv.org/abs/2504.18345v1",
      "authors": "Gao Hao, Haytham Hijazi, J\u00falio Medeiros, Jo\u00e3o Dur\u00e3es, Chan Tong Lam, Paulo de Carvalho, Henrique Madeira",
      "update_time": "2025-04-25",
      "abstract": "Measuring code understandability is both highly relevant and exceptionally challenging. This paper proposes a dynamic code understandability assessment method, which estimates a personalized code understandability score from the perspective of the specific programmer handling the code. The method consists of dynamically dividing the code unit under development or review in code regions (invisible to the programmer) and using the number of revisits (NRevisit) to each region as the primary feature for estimating the code understandability score. This approach removes the uncertainty related to the concept of a \"typical programmer\" assumed by static software code complexity metrics and can be easily implemented using a simple, low-cost, and non-intrusive desktop eye tracker or even a standard computer camera. This metric was evaluated using cognitive load measured through electroencephalography (EEG) in a controlled experiment with 35 programmers. Results show a very high correlation ranging from rs = 0.9067 to rs = 0.9860 (with p nearly 0) between the scores obtained with different alternatives of NRevisit and the ground truth represented by the EEG measurements of programmers' cognitive load, demonstrating the effectiveness of our approach in reflecting the cognitive effort required for code comprehension. The paper also discusses possible practical applications of NRevisit, including its use in the context of AI-generated code, which is already widely used today."
    },
    "2504.18095v1": {
      "title": "Subject-independent Classification of Meditative State from the Resting State using EEG",
      "url": "http://arxiv.org/abs/2504.18095v1",
      "authors": "Jerrin Thomas Panachakel, Pradeep Kumar G., Suryaa Seran, Kanishka Sharma, Ramakrishnan Angarai Ganesan",
      "update_time": "2025-04-25",
      "abstract": "While it is beneficial to objectively determine whether a subject is meditating, most research in the literature reports good results only in a subject-dependent manner. This study aims to distinguish the modified state of consciousness experienced during Rajyoga meditation from the resting state of the brain in a subject-independent manner using EEG data. Three architectures have been proposed and evaluated: The CSP-LDA Architecture utilizes common spatial pattern (CSP) for feature extraction and linear discriminant analysis (LDA) for classification. The CSP-LDA-LSTM Architecture employs CSP for feature extraction, LDA for dimensionality reduction, and long short-term memory (LSTM) networks for classification, modeling the binary classification problem as a sequence learning problem. The SVD-NN Architecture uses singular value decomposition (SVD) to select the most relevant components of the EEG signals and a shallow neural network (NN) for classification. The CSP-LDA-LSTM architecture gives the best performance with 98.2% accuracy for intra-subject classification. The SVD-NN architecture provides significant performance with 96.4\\% accuracy for inter-subject classification. This is comparable to the best-reported accuracies in the literature for intra-subject classification. Both architectures are capable of capturing subject-invariant EEG features for effectively classifying the meditative state from the resting state. The high intra-subject and inter-subject classification accuracies indicate these systems' robustness and their ability to generalize across different subjects."
    },
    "2504.17908v1": {
      "title": "The use of Multi-domain Electroencephalogram Representations in the building of Models based on Convolutional and Recurrent Neural Networks for Epilepsy Detection",
      "url": "http://arxiv.org/abs/2504.17908v1",
      "authors": "Luiz Antonio Nicolau Anghinoni, Gustavo Weber Denardin, Jadson Castro Gertrudes, Dalcimar Casanova, Jefferson Tales Oliva",
      "update_time": "2025-04-24",
      "abstract": "Epilepsy, affecting approximately 50 million people globally, is characterized by abnormal brain activity and remains challenging to treat. The diagnosis of epilepsy relies heavily on electroencephalogram (EEG) data, where specialists manually analyze epileptiform patterns across pre-ictal, ictal, post-ictal, and interictal periods. However, the manual analysis of EEG signals is prone to variability between experts, emphasizing the need for automated solutions. Although previous studies have explored preprocessing techniques and machine learning approaches for seizure detection, there is a gap in understanding how the representation of EEG data (time, frequency, or time-frequency domains) impacts the predictive performance of deep learning models. This work addresses this gap by systematically comparing deep neural networks trained on EEG data in these three domains. Through the use of statistical tests, we identify the optimal data representation and model architecture for epileptic seizure detection. The results demonstrate that frequency-domain data achieves detection metrics exceeding 97\\%, providing a robust foundation for more accurate and reliable seizure detection systems."
    },
    "2504.17888v1": {
      "title": "Seizure duration is associated with multiple timescales in interictal iEEG band power",
      "url": "http://arxiv.org/abs/2504.17888v1",
      "authors": "Mariella Panagiotopoulou, Gabrielle M. Schroeder, Jess Blickwedel, Fahmida A Chowdhury, Beate Diehl, Jane de Tisi, John S. Duncan, Alison Cronie, Jennifer Falconer, Ryan Faulder, Veronica Leach, Shona Livingstone, Rhys H. Thomas, Peter N. Taylor, Yujiang Wang",
      "update_time": "2025-04-24",
      "abstract": "Background Seizure severity can change from one seizure to the next within individual people with epilepsy. It is unclear if and how seizure severity is modulated over longer timescales. Characterising seizure severity variability over time could lead to tailored treatments. In this study, we test if continuously-recorded interictal intracranial EEG (iEEG) features encapsulate signatures of such modulations.   Methods We analysed 20 subjects with iEEG recordings of at least one day. We identified cycles on timescales of hours to days embedded in long-term iEEG band power and associated them with seizure severity, which we approximated using seizure duration. In order to quantify these associations, we created linear-circular statistical models of seizure duration that incorporated different band power cycles within each subject.   Findings In most subjects, seizure duration was weakly to moderately correlated with individual band power cycles. Combinations of multiple band power cycles significantly explained most of the variability in seizure duration. Specifically, we found 70% of the models had a higher than 60% adjusted $R^2$ across all subjects. From these models, around 80% were deemed to be above chance-level (p-value < 0.05) based on permutation tests. Models included cycles of ultradian, circadian and slower timescales in a subject-specific manner.   Interpretation These results suggest that seizure severity, as measured by seizure duration, may be modulated over timescales of minutes to days by subject-specific cycles in interictal iEEG signal properties. These cycles likely serve as markers of seizure modulating processes. Future work can investigate biological drivers of these detected fluctuations and may inform novel treatment strategies that minimise seizure severity."
    },
    "2504.17724v1": {
      "title": "Unsupervised EEG-based decoding of absolute auditory attention with canonical correlation analysis",
      "url": "http://arxiv.org/abs/2504.17724v1",
      "authors": "Nicolas Heintz, Tom Francart, Alexander Bertrand",
      "update_time": "2025-04-24",
      "abstract": "We propose a fully unsupervised algorithm that detects from encephalography (EEG) recordings when a subject actively listens to sound, versus when the sound is ignored. This problem is known as absolute auditory attention decoding (aAAD). We propose an unsupervised discriminative CCA model for feature extraction and combine it with an unsupervised classifier called minimally informed linear discriminant analysis (MILDA) for aAAD classification. Remarkably, the proposed unsupervised algorithm performs significantly better than a state-of-the-art supervised model. A key reason is that the unsupervised algorithm can successfully adapt to the non-stationary test data at a low computational cost. This opens the door to the analysis of the auditory attention of a subject using EEG signals with a model that automatically tunes itself to the subject without requiring an arduous supervised training session beforehand."
    },
    "2504.20069v1": {
      "title": "A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives",
      "url": "http://arxiv.org/abs/2504.20069v1",
      "authors": "Junhong Lai, Jiyu Wei, Lin Yao, Yueming Wang",
      "update_time": "2025-04-24",
      "abstract": "Electroencephalogram (EEG) signals play a crucial role in understanding brain activity and diagnosing neurological disorders. This review focuses on the recent development of EEG foundation models(EEG-FMs), which have shown great potential in processing and analyzing EEG data. We discuss various EEG-FMs, including their architectures, pre-training strategies, their pre-training and downstream datasets and other details. The review also highlights the challenges and future directions in this field, aiming to provide a comprehensive overview for researchers and practitioners interested in EEG analysis and related EEG-FMs."
    },
    "2504.17352v1": {
      "title": "The Riemannian Means Field Classifier for EEG-Based BCI Data",
      "url": "http://arxiv.org/abs/2504.17352v1",
      "authors": "Anton Andreev, Gr\u00e9goire Cattan, Marco Congedo",
      "update_time": "2025-04-24",
      "abstract": "A substantial amount of research has demonstrated the robustness and accuracy of the Riemannian minimum distance to mean (MDM) classifier for all kinds of EEG-based brain--computer interfaces (BCIs). This classifier is simple, fully deterministic, robust to noise, computationally efficient, and prone to transfer learning. Its training is very simple, requiring just the computation of a geometric mean of a symmetric positive-definite (SPD) matrix per class. We propose an improvement of the MDM involving a number of power means of SPD matrices instead of the sole geometric mean. By the analysis of 20 public databases, 10 for the motor-imagery BCI paradigm and 10 for the P300 BCI paradigm, comprising 587 individuals in total, we show that the proposed classifier clearly outperforms the MDM, approaching the state-of-the art in terms of performance while retaining the simplicity and the deterministic behavior. In order to promote reproducible research, our code will be released as open source."
    }
  },
  "BCI": {
    "2504.17352v1": {
      "title": "The Riemannian Means Field Classifier for EEG-Based BCI Data",
      "url": "http://arxiv.org/abs/2504.17352v1",
      "authors": "Anton Andreev, Gr\u00e9goire Cattan, Marco Congedo",
      "update_time": "2025-04-24",
      "abstract": "A substantial amount of research has demonstrated the robustness and accuracy of the Riemannian minimum distance to mean (MDM) classifier for all kinds of EEG-based brain--computer interfaces (BCIs). This classifier is simple, fully deterministic, robust to noise, computationally efficient, and prone to transfer learning. Its training is very simple, requiring just the computation of a geometric mean of a symmetric positive-definite (SPD) matrix per class. We propose an improvement of the MDM involving a number of power means of SPD matrices instead of the sole geometric mean. By the analysis of 20 public databases, 10 for the motor-imagery BCI paradigm and 10 for the P300 BCI paradigm, comprising 587 individuals in total, we show that the proposed classifier clearly outperforms the MDM, approaching the state-of-the art in terms of performance while retaining the simplicity and the deterministic behavior. In order to promote reproducible research, our code will be released as open source."
    },
    "2504.17111v1": {
      "title": "Transferring Spatial Filters via Tangent Space Alignment in Motor Imagery BCIs",
      "url": "http://arxiv.org/abs/2504.17111v1",
      "authors": "Tekin Gunasar, Virginia de Sa",
      "update_time": "2025-04-23",
      "abstract": "We propose a method to improve subject transfer in motor imagery BCIs by aligning covariance matrices on a Riemannian manifold, followed by computing a new common spatial patterns (CSP) based spatial filter. We explore various ways to integrate information from multiple subjects and show improved performance compared to standard CSP. Across three datasets, our method shows marginal improvements over standard CSP; however, when training data are limited, the improvements become more significant."
    },
    "2504.15984v1": {
      "title": "Neuroadaptive Haptics: Comparing Reinforcement Learning from Explicit Ratings and Neural Signals for Adaptive XR Systems",
      "url": "http://arxiv.org/abs/2504.15984v1",
      "authors": "Lukas Gehrke, Aleksandrs Koselevsk, Marius Klug, Klaus Gramann",
      "update_time": "2025-04-22",
      "abstract": "Neuroadaptive haptics offers a path to more immersive extended reality (XR) experiences by dynamically tuning multisensory feedback to user preferences. We present a neuroadaptive haptics system that adapts XR feedback through reinforcement learning (RL) from explicit user ratings and brain-decoded neural signals. In a user study, participants interacted with virtual objects in VR while Electroencephalography (EEG) data were recorded. An RL agent adjusted haptic feedback based either on explicit ratings or on outputs from a neural decoder. Results show that the RL agent's performance was comparable across feedback sources, suggesting that implicit neural feedback can effectively guide personalization without requiring active user input. The EEG-based neural decoder achieved a mean F1 score of 0.8, supporting reliable classification of user experience. These findings demonstrate the feasibility of combining brain-computer interfaces (BCI) and RL to autonomously adapt XR interactions, reducing cognitive load and enhancing immersion."
    },
    "2504.15101v1": {
      "title": "NeuGaze: Reshaping the future BCI",
      "url": "http://arxiv.org/abs/2504.15101v1",
      "authors": "Yiqian Yang",
      "update_time": "2025-04-21",
      "abstract": "Traditional brain-computer interfaces (BCIs), reliant on costly electroencephalography or invasive implants, struggle with complex human-computer interactions due to setup complexity and limited precision. We present NeuGaze, a novel webcam-based system that leverages eye gaze, head movements, and facial expressions to enable intuitive, real-time control using only a standard 30 Hz webcam, often pre-installed in laptops. Requiring minimal calibration, NeuGaze achieves performance comparable to conventional inputs, supporting precise cursor navigation, key triggering via an efficient skill wheel, and dynamic gaming interactions, such as defeating formidable opponents in first-person games. By harnessing preserved neck-up functionalities in motor-impaired individuals, NeuGaze eliminates the need for specialized hardware, offering a low-cost, accessible alternative to BCIs. This paradigm empowers diverse applications, from assistive technology to entertainment, redefining human-computer interaction for motor-impaired users. Project is at \\href{https://github.com/NeuSpeech/NeuGaze}{github.com/NeuSpeech/NeuGaze}.",
      "code_url": "https://github.com/neuspeech/neugaze"
    },
    "2504.14269v1": {
      "title": "Recognition of Frequencies of Short-Time SSVEP Signals Utilizing an SSCCA-Based Spatio-Spectral Feature Fusion Framework",
      "url": "http://arxiv.org/abs/2504.14269v1",
      "authors": "Saif Bashar, Samia Nasir Nira, Shabbir Mahmood, Md. Humaun Kabir, Sujit Roy, Iffat Farhana",
      "update_time": "2025-04-19",
      "abstract": "A brain-computer interface (BCI) facilitates direct communication between the brain and external equipment through EEG, which is preferred for its superior temporal resolution. Among EEG techniques, the steady-state visual evoked potential (SSVEP) is favored due to its robust signal-to-noise ratio, minimal training demands, and elevated information transmission rate. Frequency detection in SSVEP-based brain-computer interfaces commonly employs canonical correlation analysis (CCA). SSCCA (spatio-spectral canonical correlation analysis) augments CCA by refining spatial filtering. This paper presents a multistage feature fusion methodology for short-duration SSVEP frequency identification, employing SSCCA with template signals derived via leave-one-out cross-validation (LOOCV). A filterbank generates bandpass filters for stimulus frequencies and their harmonics, whereas SSCCA calculates correlation coefficients between subbands and templates. Two phases of non-linear weighting amalgamate these coefficients to discern the target stimulus. This multistage methodology surpasses traditional techniques, attaining a accuracy of 94.5%."
    },
    "2504.13220v1": {
      "title": "SSTAF: Spatial-Spectral-Temporal Attention Fusion Transformer for Motor Imagery Classification",
      "url": "http://arxiv.org/abs/2504.13220v1",
      "authors": "Ummay Maria Muna, Md. Mehedi Hasan Shawon, Md Jobayer, Sumaiya Akter, Saifur Rahman Sabuj",
      "update_time": "2025-04-17",
      "abstract": "Brain-computer interfaces (BCI) in electroencephalography (EEG)-based motor imagery classification offer promising solutions in neurorehabilitation and assistive technologies by enabling communication between the brain and external devices. However, the non-stationary nature of EEG signals and significant inter-subject variability cause substantial challenges for developing robust cross-subject classification models. This paper introduces a novel Spatial-Spectral-Temporal Attention Fusion (SSTAF) Transformer specifically designed for upper-limb motor imagery classification. Our architecture consists of a spectral transformer and a spatial transformer, followed by a transformer block and a classifier network. Each module is integrated with attention mechanisms that dynamically attend to the most discriminative patterns across multiple domains, such as spectral frequencies, spatial electrode locations, and temporal dynamics. The short-time Fourier transform is incorporated to extract features in the time-frequency domain to make it easier for the model to obtain a better feature distinction. We evaluated our SSTAF Transformer model on two publicly available datasets, the EEGMMIDB dataset, and BCI Competition IV-2a. SSTAF Transformer achieves an accuracy of 76.83% and 68.30% in the data sets, respectively, outperforms traditional CNN-based architectures and a few existing transformer-based approaches."
    },
    "2504.12578v1": {
      "title": "Sub-Scalp Brain-Computer Interface Device Design and Fabrication",
      "url": "http://arxiv.org/abs/2504.12578v1",
      "authors": "Timothy B. Mahoney, David B. Grayden, Sam E. John",
      "update_time": "2025-04-17",
      "abstract": "Current brain-computer interfaces (BCI) face limitations in signal acquisition. While sub-scalp EEG offers a potential solution, existing devices prioritize chronic seizure monitoring and lack features suited for BCI applications. This work addresses this gap by outlining key specifications for sub-scalp BCI devices, focusing on channel count, sampling rate, power efficiency, and form factor. We present the Set-And-Forget EEG (SAFE) system, a custom-built amplifier and wireless transmitter meeting these criteria. This compact (12x12 mm), six-channel device offers 1024 Hz sampling and Bluetooth Low Energy data transmission. Validation using generated sinusoids and electrocorticography recordings of visual evoked potentials in sheep models demonstrated low noise recording. Future animal studies will assess sub-scalp EEG signal quality for BCI applications. This data lays the groundwork for human trials, ultimately paving the way for chronic, in-home BCIs that empower individuals with physical disabilities."
    },
    "2504.11936v2": {
      "title": "Mind2Matter: Creating 3D Models from EEG Signals",
      "url": "http://arxiv.org/abs/2504.11936v2",
      "authors": "Xia Deng, Shen Chen, Jiale Zhou, Lei Li",
      "update_time": "2025-04-18",
      "abstract": "The reconstruction of 3D objects from brain signals has gained significant attention in brain-computer interface (BCI) research. Current research predominantly utilizes functional magnetic resonance imaging (fMRI) for 3D reconstruction tasks due to its excellent spatial resolution. Nevertheless, the clinical utility of fMRI is limited by its prohibitive costs and inability to support real-time operations. In comparison, electroencephalography (EEG) presents distinct advantages as an affordable, non-invasive, and mobile solution for real-time brain-computer interaction systems. While recent advances in deep learning have enabled remarkable progress in image generation from neural data, decoding EEG signals into structured 3D representations remains largely unexplored. In this paper, we propose a novel framework that translates EEG recordings into 3D object reconstructions by leveraging neural decoding techniques and generative models. Our approach involves training an EEG encoder to extract spatiotemporal visual features, fine-tuning a large language model to interpret these features into descriptive multimodal outputs, and leveraging generative 3D Gaussians with layout-guided control to synthesize the final 3D structures. Experiments demonstrate that our model captures salient geometric and semantic features, paving the way for applications in brain-computer interfaces (BCIs), virtual reality, and neuroprosthetics. Our code is available in https://github.com/sddwwww/Mind2Matter.",
      "code_url": "https://github.com/sddwwww/mind2matter"
    },
    "2504.13937v1": {
      "title": "Auditory Conversational BAI: A Feasibility Study",
      "url": "http://arxiv.org/abs/2504.13937v1",
      "authors": "Michal Robert \u017d\u00e1k, Moritz Grosse-Wentrup",
      "update_time": "2025-04-15",
      "abstract": "We introduce a novel auditory brain-computer interface (BCI) paradigm, Auditory Intention Decoding (AID), designed to enhance communication capabilities within the brain-AI interface (BAI) system EEGChat. AID enables users to select among multiple auditory options (intentions) by analyzing their brain responses, offering a pathway to construct a communication system that requires neither muscle movement nor syntactic formation. To evaluate the feasibility of this paradigm, we conducted a proof-of-concept study. The results demonstrated statistically significant decoding performance, validating the approach's potential. Despite these promising findings, further optimization is required to enhance system performance and realize the paradigm's practical application."
    },
    "2504.10973v1": {
      "title": "Early Detection of Cognitive Impairment in Elderly using a Passive FPVS-EEG BCI and Machine Learning -- Extended Version",
      "url": "http://arxiv.org/abs/2504.10973v1",
      "authors": "Tomasz M. Rutkowski, Stanis\u0142aw Nar\u0119bski, Mihoko Otake-Matsuura, Tomasz Komendzi\u0144ski",
      "update_time": "2025-04-15",
      "abstract": "Early dementia diagnosis requires biomarkers sensitive to both structural and functional brain changes. While structural neuroimaging biomarkers have progressed significantly, objective functional biomarkers of early cognitive decline remain a critical unmet need. Current cognitive assessments often rely on behavioral responses, making them susceptible to factors like effort, practice effects, and educational background, thereby hindering early and accurate detection. This work introduces a novel approach, leveraging a lightweight convolutional neural network (CNN) to infer cognitive impairment levels directly from electroencephalography (EEG) data. Critically, this method employs a passive fast periodic visual stimulation (FPVS) paradigm, eliminating the need for explicit behavioral responses or task comprehension from the participant. This passive approach provides an objective measure of working memory function, independent of confounding factors inherent in active cognitive tasks, and offers a promising new avenue for early and unbiased detection of cognitive decline."
    }
  },
  "fMRI": {
    "2504.16798v1": {
      "title": "4D Multimodal Co-attention Fusion Network with Latent Contrastive Alignment for Alzheimer's Diagnosis",
      "url": "http://arxiv.org/abs/2504.16798v1",
      "authors": "Yuxiang Wei, Yanteng Zhang, Xi Xiao, Tianyang Wang, Xiao Wang, Vince D. Calhoun",
      "update_time": "2025-04-23",
      "abstract": "Multimodal neuroimaging provides complementary structural and functional insights into both human brain organization and disease-related dynamics. Recent studies demonstrate enhanced diagnostic sensitivity for Alzheimer's disease (AD) through synergistic integration of neuroimaging data (e.g., sMRI, fMRI) with behavioral cognitive scores tabular data biomarkers. However, the intrinsic heterogeneity across modalities (e.g., 4D spatiotemporal fMRI dynamics vs. 3D anatomical sMRI structure) presents critical challenges for discriminative feature fusion. To bridge this gap, we propose M2M-AlignNet: a geometry-aware multimodal co-attention network with latent alignment for early AD diagnosis using sMRI and fMRI. At the core of our approach is a multi-patch-to-multi-patch (M2M) contrastive loss function that quantifies and reduces representational discrepancies via geometry-weighted patch correspondence, explicitly aligning fMRI components across brain regions with their sMRI structural substrates without one-to-one constraints. Additionally, we propose a latent-as-query co-attention module to autonomously discover fusion patterns, circumventing modality prioritization biases while minimizing feature redundancy. We conduct extensive experiments to confirm the effectiveness of our method and highlight the correspondance between fMRI and sMRI as AD biomarkers."
    },
    "2504.14796v1": {
      "title": "Edge-boosted graph learning for functional brain connectivity analysis",
      "url": "http://arxiv.org/abs/2504.14796v1",
      "authors": "David Yang, Mostafa Abdelmegeed, John Modl, Minjeong Kim",
      "update_time": "2025-04-21",
      "abstract": "Predicting disease states from functional brain connectivity is critical for the early diagnosis of severe neurodegenerative diseases such as Alzheimer's Disease and Parkinson's Disease. Existing studies commonly employ Graph Neural Networks (GNNs) to infer clinical diagnoses from node-based brain connectivity matrices generated through node-to-node similarities of regionally averaged fMRI signals. However, recent neuroscience studies found that such node-based connectivity does not accurately capture ``functional connections\" within the brain. This paper proposes a novel approach to brain network analysis that emphasizes edge functional connectivity (eFC), shifting the focus to inter-edge relationships. Additionally, we introduce a co-embedding technique to integrate edge functional connections effectively. Experimental results on the ADNI and PPMI datasets demonstrate that our method significantly outperforms state-of-the-art GNN methods in classifying functional brain networks."
    },
    "2504.13124v1": {
      "title": "Spatial Confidence Regions for Excursion Sets with False Discovery Rate Control",
      "url": "http://arxiv.org/abs/2504.13124v1",
      "authors": "Howon Ryu, Thomas Maullin-Sapey, Armin Schwartzman, Samuel Davenport",
      "update_time": "2025-04-17",
      "abstract": "Identifying areas where the signal is prominent is an important task in image analysis, with particular applications in brain mapping. In this work, we develop confidence regions for spatial excursion sets above and below a given level. We achieve this by treating the confidence procedure as a testing problem at the given level, allowing control of the False Discovery Rate (FDR). Methods are developed to control the FDR, separately for positive and negative excursions, as well as jointly over both. Furthermore, power is increased by incorporating a two-stage adaptive procedure. Simulation results with various signals show that our confidence regions successfully control the FDR under the nominal level. We showcase our methods with an application to functional magnetic resonance imaging (fMRI) data from the Human Connectome Project illustrating the improvement in statistical power over existing approaches."
    },
    "2504.11936v2": {
      "title": "Mind2Matter: Creating 3D Models from EEG Signals",
      "url": "http://arxiv.org/abs/2504.11936v2",
      "authors": "Xia Deng, Shen Chen, Jiale Zhou, Lei Li",
      "update_time": "2025-04-18",
      "abstract": "The reconstruction of 3D objects from brain signals has gained significant attention in brain-computer interface (BCI) research. Current research predominantly utilizes functional magnetic resonance imaging (fMRI) for 3D reconstruction tasks due to its excellent spatial resolution. Nevertheless, the clinical utility of fMRI is limited by its prohibitive costs and inability to support real-time operations. In comparison, electroencephalography (EEG) presents distinct advantages as an affordable, non-invasive, and mobile solution for real-time brain-computer interaction systems. While recent advances in deep learning have enabled remarkable progress in image generation from neural data, decoding EEG signals into structured 3D representations remains largely unexplored. In this paper, we propose a novel framework that translates EEG recordings into 3D object reconstructions by leveraging neural decoding techniques and generative models. Our approach involves training an EEG encoder to extract spatiotemporal visual features, fine-tuning a large language model to interpret these features into descriptive multimodal outputs, and leveraging generative 3D Gaussians with layout-guided control to synthesize the final 3D structures. Experiments demonstrate that our model captures salient geometric and semantic features, paving the way for applications in brain-computer interfaces (BCIs), virtual reality, and neuroprosthetics. Our code is available in https://github.com/sddwwww/Mind2Matter.",
      "code_url": "https://github.com/sddwwww/mind2matter"
    },
    "2504.10752v1": {
      "title": "Time-varying EEG spectral power predicts evoked and spontaneous fMRI motor brain activity",
      "url": "http://arxiv.org/abs/2504.10752v1",
      "authors": "Neil Mehta, Ines Goncalves, Alberto Montagna, Mathis Fleury, Gustavo Caetano, Ines Esteves, Athanasios Vourvopoulos, Pulkit Grover, Patricia Figueiredo",
      "update_time": "2025-04-14",
      "abstract": "Simultaneous EEG-fMRI recordings are increasingly used to investigate brain activity by leveraging the complementary high spatial and high temporal resolution of fMRI and EEG signals respectively. It remains unclear, however, to what degree these two imaging modalities capture shared information about neural activity. Here, we investigate whether it is possible to predict both task-evoked and spontaneous fMRI signals of motor brain networks from EEG time-varying spectral power using interpretable models trained for individual subjects with Sparse Group Lasso regularization. Critically, we test the trained models on data acquired from each subject on a different day and obtain statistical validation by comparison with appropriate null models as well as the conventional EEG sensorimotor rhythm. We find significant prediction results in most subjects, although less frequently for resting-state compared to task-based conditions. Furthermore, we interpret the model learned parameters to understand representations of EEG-fMRI coupling in terms of predictive EEG channels, frequencies, and haemodynamic delays. In conclusion, our work provides evidence of the ability to predict fMRI motor brain activity from EEG recordings alone across different days, in both task-evoked and spontaneous conditions, with statistical significance in individual subjects. These results present great potential for translation to EEG neurofeedback applications."
    },
    "2504.10252v1": {
      "title": "MapperEEG: A Topological Approach to Brain State Clustering in EEG Recordings",
      "url": "http://arxiv.org/abs/2504.10252v1",
      "authors": "Brittany Story, Zhibin Zhou, Ramesh Srinivasan, Piotr J. Franaszczuk",
      "update_time": "2025-04-14",
      "abstract": "Electrical potential scalp recordings (Electroencephalograms-EEGs) are a common tool used to investigate brain activity. EEG is routinely used in clinical applications as well as in research studies thanks to its noninvasive nature, relatively inexpensive equipment, and high temporal resolution. But, EEG is prone to contamination from movement artifacts and signals from external sources. Thus, it requires advanced signal processing and mathematical analysis methods in tasks requiring brain state identification. Recently, tools from topological data analysis have been used successfully across many domains, including brain research, however these uses have been limited to fMRI datasets. We introduce the topological tool MapperEEG (M-EEG) and provide an example of it's ability to separate different brain states during a simple finger tapping teaming task without any pre-labeling or prior knowledge. M-EEG uses the power spectral density applied to traditional EEG frequency bands combined with the Mapper algorithm from topological data analysis to capture the underlying structure of the data and represent that structure as a graph in two-dimensional space. This tool provides clear separation (clustering) of states during different conditions of the experiment (syncopated vs. synchronized) and we demonstrate that M-EEG outperforms other clustering methods when applied to EEG data."
    },
    "2504.10140v1": {
      "title": "The topology of synergy: linking topological and information-theoretic approaches to higher-order interactions in complex systems",
      "url": "http://arxiv.org/abs/2504.10140v1",
      "authors": "Thomas F. Varley, Pedro A. M. Mediano, Alice Patania, Josh Bongard",
      "update_time": "2025-04-14",
      "abstract": "The study of irreducible higher-order interactions has become a core topic of study in complex systems. Two of the most well-developed frameworks, topological data analysis and multivariate information theory, aim to provide formal tools for identifying higher-order interactions in empirical data. Despite similar aims, however, these two approaches are built on markedly different mathematical foundations and have been developed largely in parallel. In this study, we present a head-to-head comparison of topological data analysis and information-theoretic approaches to describing higher-order interactions in multivariate data; with the aim of assessing the similarities and differences between how the frameworks define ``higher-order structures.\" We begin with toy examples with known topologies, before turning to naturalistic data: fMRI signals collected from the human brain. We find that intrinsic, higher-order synergistic information is associated with three-dimensional cavities in a point cloud: shapes such as spheres are synergy-dominated. In fMRI data, we find strong correlations between synergistic information and both the number and size of three-dimensional cavities. Furthermore, we find that dimensionality reduction techniques such as PCA preferentially represent higher-order redundancies, and largely fail to preserve both higher-order information and topological structure, suggesting that common manifold-based approaches to studying high-dimensional data are systematically failing to identify important features of the data. These results point towards the possibility of developing a rich theory of higher-order interactions that spans topological and information-theoretic approaches while simultaneously highlighting the profound limitations of more conventional methods."
    },
    "2504.09463v1": {
      "title": "Comorbidity-Informed Transfer Learning for Neuro-developmental Disorder Diagnosis",
      "url": "http://arxiv.org/abs/2504.09463v1",
      "authors": "Xin Wen, Shijie Guo, Wenbo Ning, Rui Cao, Jie Xiang, Xiaobo Liu, Jintai Chen",
      "update_time": "2025-04-13",
      "abstract": "Neuro-developmental disorders are manifested as dysfunctions in cognition, communication, behaviour and adaptability, and deep learning-based computer-aided diagnosis (CAD) can alleviate the increasingly strained healthcare resources on neuroimaging. However, neuroimaging such as fMRI contains complex spatio-temporal features, which makes the corresponding representations susceptible to a variety of distractions, thus leading to less effective in CAD. For the first time, we present a Comorbidity-Informed Transfer Learning(CITL) framework for diagnosing neuro-developmental disorders using fMRI. In CITL, a new reinforced representation generation network is proposed, which first combines transfer learning with pseudo-labelling to remove interfering patterns from the temporal domain of fMRI and generates new representations using encoder-decoder architecture. The new representations are then trained in an architecturally simple classification network to obtain CAD model. In particular, the framework fully considers the comorbidity mechanisms of neuro-developmental disorders and effectively integrates them with semi-supervised learning and transfer learning, providing new perspectives on interdisciplinary. Experimental results demonstrate that CITL achieves competitive accuracies of 76.32% and 73.15% for detecting autism spectrum disorder and attention deficit hyperactivity disorder, respectively, which outperforms existing related transfer learning work for 7.2% and 0.5% respectively."
    },
    "2504.09179v1": {
      "title": "A Confounding Factors-Inhibition Adversarial Learning Framework for Multi-site fMRI Mental Disorder Identification",
      "url": "http://arxiv.org/abs/2504.09179v1",
      "authors": "Xin Wen, Shijie Guo, Wenbo Ning, Rui Cao, Yan Niu, Bin Wan, Peng Wei, Xiaobo Liu, Jie Xiang",
      "update_time": "2025-04-12",
      "abstract": "In open data sets of functional magnetic resonance imaging (fMRI), the heterogeneity of the data is typically attributed to a combination of factors, including differences in scanning procedures, the presence of confounding effects, and population diversities between multiple sites. These factors contribute to the diminished effectiveness of representation learning, which in turn affects the overall efficacy of subsequent classification procedures. To address these limitations, we propose a novel multi-site adversarial learning network (MSalNET) for fMRI-based mental disorder detection. Firstly, a representation learning module is introduced with a node information assembly (NIA) mechanism to better extract features from functional connectivity (FC). This mechanism aggregates edge information from both horizontal and vertical directions, effectively assembling node information. Secondly, to generalize the feature across sites, we proposed a site-level feature extraction module that can learn from individual FC data, which circumvents additional prior information. Lastly, an adversarial learning network is proposed as a means of balancing the trade-off between individual classification and site regression tasks, with the introduction of a novel loss function. The proposed method was evaluated on two multi-site fMRI datasets, i.e., Autism Brain Imaging Data Exchange (ABIDE) and ADHD-200. The results indicate that the proposed method achieves a better performance than other related algorithms with the accuracy of 75.56 and 68.92 in ABIDE and ADHD-200 datasets, respectively. Furthermore, the result of the site regression indicates that the proposed method reduces site variability from a data-driven perspective. The most discriminative brain regions revealed by NIA are consistent with statistical findings, uncovering the \"black box\" of deep learning to a certain extent."
    },
    "2504.09109v2": {
      "title": "Probability Distribution Alignment and Low-Rank Weight Decomposition for Source-Free Domain Adaptive Brain Decoding",
      "url": "http://arxiv.org/abs/2504.09109v2",
      "authors": "Ganxi Xu, Jinyi Long, Jia Zhang",
      "update_time": "2025-04-15",
      "abstract": "Brain decoding currently faces significant challenges in individual differences, modality alignment, and high-dimensional embeddings. To address individual differences, researchers often use source subject data, which leads to issues such as privacy leakage and heavy data storage burdens. In modality alignment, current works focus on aligning the softmax probability distribution but neglect the alignment of marginal probability distributions, resulting in modality misalignment. Additionally, images and text are aligned separately with fMRI without considering the complex interplay between images and text, leading to poor image reconstruction. Finally, the enormous dimensionality of CLIP embeddings causes significant computational costs. Although the dimensionality of CLIP embeddings can be reduced by ignoring the number of patches obtained from images and the number of tokens acquired from text, this comes at the cost of a significant drop in model performance, creating a dilemma. To overcome these limitations, we propose a source-free domain adaptation-based brain decoding framework."
    }
  },
  "MEG": {
    "2504.15711v1": {
      "title": "New limit on the \u03bc+->e+\u03b3decay with the MEG II experiment",
      "url": "http://arxiv.org/abs/2504.15711v1",
      "authors": "K. Afanaciev, A. M. Baldini, S. Ban, H. Benmansour, G. Boca, P. W. Cattaneo, G. Cavoto, F. Cei, M. Chiappini, A. Corvaglia, G. Dal Maso, A. De Bari, M. De Gerone, L. Ferrari Barusso, M. Francesconi, L. Galli, G. Gallucci, F. Gatti, L. Gerritzen, F. Grancagnolo, E. G. Grandoni, M. Grassi, D. N. Grigoriev, M. Hildebrandt, F. Ignatov, F. Ikeda, T. Iwamoto, S. Karpov, P. -R. Kettle, N. Khomutov, A. Kolesnikov, N. Kravchuk, V. Krylov, N. Kuchinskiy, F. Leonetti, W. Li, V. Malyshev, A. Matsushita, S. Mihara, W. Moltzon, Toshinori Mori, D. Nicolo', H. Nishiguchi, A. Ochi, H. Ogawa, W. Ootani, A. Oya, D. Palo, M. Panareo, A. Papa, D. Pasciuto, A. Popov, F. Renga, S. Ritt, M. Rossella, A. Rozhdestvensky, S. Scarpellini, G. Signorelli, H. Suzuki, M. Takahashi, Y. Uchiyama, R. Umakoshi, A. Venturini, B. Vitali, C. Voena, K. Yamamoto, R. Yokota, T. Yonemoto",
      "update_time": "2025-04-22",
      "abstract": "This letter reports the result of the search for the decay \\mu+->e+\\gamma undertaken at the Paul Scherrer Institut in Switzerland with the MEG II experiment using the data collected in the 2021- 2022 physics runs. The sensitivity of this search is 2.2x10-13, a factor of 2.4 better than that of the full MEG dataset and obtained in a data taking period of about one fourth that of MEG, thanks to the superior performances of the new detector. The result is consistent with the expected background, yielding an upper limit on the branching ratio of B(\\mu+->e+\\gamma)<1.5 x 10-13 (90 % C.L.). Additional improvements are expected with the data collected during the years 2023-2024. The data-taking will continue in the coming years."
    },
    "2504.11332v1": {
      "title": "Constraints from muon $g-2$ on a gauged non-universal $U(1)_{X}$ model with inverse see-saw neutrinos",
      "url": "http://arxiv.org/abs/2504.11332v1",
      "authors": "J. S. Alvarado, R. Martinez, Cristian Sierra",
      "update_time": "2025-04-15",
      "abstract": "We study the effects on a non-universal $U(1)_{X}$ extension of the Standard Model given the alternative value obtained by the Budapest-Marseille-Wuppertal (BMW) group for the anomalous magnetic moment of the muon $g-2$. The model explains the fermion mass hierarchy through the non-universality of the extra gauge symmetry and by an additional $\\mathbb{Z}_{2}$ discrete symmetry, where the heaviest fermions acquire their masses from two different scales determined by two Higgs doublets and one singlet, whereas the lightest fermions obtain their masses from radiative corrections. From cancellation of chiral anomalies, the model also includes heavy extra fermions, both charged and neutral. The latter are right-handed neutrinos that acquire masses via an inverse see-saw mechanism, reproducing the observed squared mass differences for the active neutrinos. Using the latest lattice calculation of the leading hadronic vacuum polarization (HVP) contribution to the muon $g-2$, we compute the dominant one-loop diagrams mediated by the $W$ and charged Higgs bosons, both with a heavy Majorana neutrino in the loop, setting bounds for masses of the new particles. We also provide predictions for observables that can probe our model in the future such as charged lepton flavor violating searches at Belle II like $\\tau\\to \\mu\\gamma$, $\\tau\\to e\\gamma$ and at MEG II for $\\mu\\to e\\gamma$."
    },
    "2503.16892v1": {
      "title": "Multifractal analysis based on the weak scaling exponent and applications to MEG recordings in neuroscience",
      "url": "http://arxiv.org/abs/2503.16892v1",
      "authors": "Patrice Abry, Phipippe Ciuciu, Merlin Dumeur, St\u00e9phane Jaffard, Guillaume Sa\u00ebs",
      "update_time": "2025-03-21",
      "abstract": "We develop the mathematical properties of a multifractal analysis of data based on the weak scaling exponent. The advantage of this analysis is that it does not require any a priori global regularity assumption on the analyzed signal, in contrast with the previously used H{\\\"o}lder or p-exponents. As an illustration, we show that this technique allows one to perform a multifractal analysis of MEG signals, which records electromagnetic brain activity, that was not theoretically valid using the formerly introduced methods based on H{\\\"o}lder or p-exponents."
    },
    "2503.10965v2": {
      "title": "Auditing language models for hidden objectives",
      "url": "http://arxiv.org/abs/2503.10965v2",
      "authors": "Samuel Marks, Johannes Treutlein, Trenton Bricken, Jack Lindsey, Jonathan Marcus, Siddharth Mishra-Sharma, Daniel Ziegler, Emmanuel Ameisen, Joshua Batson, Tim Belonax, Samuel R. Bowman, Shan Carter, Brian Chen, Hoagy Cunningham, Carson Denison, Florian Dietz, Satvik Golechha, Akbir Khan, Jan Kirchner, Jan Leike, Austin Meek, Kei Nishimura-Gasparian, Euan Ong, Christopher Olah, Adam Pearce, Fabien Roger, Jeanne Salle, Andy Shih, Meg Tong, Drake Thomas, Kelley Rivoire, Adam Jermyn, Monte MacDiarmid, Tom Henighan, Evan Hubinger",
      "update_time": "2025-03-28",
      "abstract": "We study the feasibility of conducting alignment audits: investigations into whether models have undesired objectives. As a testbed, we train a language model with a hidden objective. Our training pipeline first teaches the model about exploitable errors in RLHF reward models (RMs), then trains the model to exploit some of these errors. We verify via out-of-distribution evaluations that the model generalizes to exhibit whatever behaviors it believes RMs rate highly, including ones not reinforced during training. We leverage this model to study alignment audits in two ways. First, we conduct a blind auditing game where four teams, unaware of the model's hidden objective or training, investigate it for concerning behaviors and their causes. Three teams successfully uncovered the model's hidden objective using techniques including interpretability with sparse autoencoders (SAEs), behavioral attacks, and training data analysis. Second, we conduct an unblinded follow-up study of eight techniques for auditing the model, analyzing their strengths and limitations. Overall, our work provides a concrete example of using alignment audits to discover a model's hidden objective and proposes a methodology for practicing and validating progress in alignment auditing."
    },
    "2503.06086v1": {
      "title": "Characterizing optimal monitoring edge-geodetic sets for some structured graph classes",
      "url": "http://arxiv.org/abs/2503.06086v1",
      "authors": "Florent Foucaud, Arti Pandey, Kaustav Paul",
      "update_time": "2025-03-08",
      "abstract": "Given a graph $G=(V,E)$, a set $S\\subseteq V$ is said to be a monitoring edge-geodetic set if the deletion of any edge in the graph results in a change in the distance between at least one pair of vertices in $S$. The minimum size of such a set in $G$ is called the monitoring edge-geodetic number of $G$ and is denoted by $meg(G)$.   In this work, we compute the monitoring edge-geodetic number efficiently for the following graph classes: distance-hereditary graphs, $P_4$-sparse graphs, bipartite permutation graphs, and strongly chordal graphs. The algorithms follow from structural characterizations of the optimal monitoring edge-geodetic sets for these graph classes in terms of \\emph{mandatory vertices} (those that need to be in every solution). This extends previous results from the literature for cographs, interval graphs and block graphs."
    },
    "2503.05211v1": {
      "title": "Language-specific Tonal Features Drive Speaker-Listener Neural Synchronization",
      "url": "http://arxiv.org/abs/2503.05211v1",
      "authors": "Chen Hong, Xiangbin Teng, Yu Li, Shen-Mou Hsu, Feng-Ming Tsao, Patrick C. M. Wong, Gangyi Feng",
      "update_time": "2025-03-07",
      "abstract": "Verbal communication transmits information across diverse linguistic levels, with neural synchronization (NS) between speakers and listeners emerging as a putative mechanism underlying successful exchange. However, the specific speech features driving this synchronization and how language-specific versus universal characteristics facilitate information transfer remain poorly understood. We developed a novel content-based interbrain encoding model to disentangle the contributions of acoustic and linguistic features to speaker-listener NS during Mandarin storytelling and listening, as measured via magnetoencephalography (MEG). Results revealed robust NS throughout frontotemporal-parietal networks with systematic time lags between speech production and perception. Crucially, suprasegmental lexical tone features (tone categories, pitch height, and pitch contour), essential for lexical meaning in Mandarin, contributed more significantly to NS than either acoustic elements or universal segmental units (consonants and vowels). These tonal features generated distinctive spatiotemporal NS patterns, creating language-specific neural \"communication channels\" that facilitated efficient representation sharing between interlocutors. Furthermore, the strength and patterns of NS driven by these language-specific features predicted communication success. These findings demonstrate the neural mechanisms underlying shared representations during verbal exchange and highlight how language-specific features can shape neural coupling to optimize information transfer during human communication."
    },
    "2502.16797v1": {
      "title": "Forecasting Rare Language Model Behaviors",
      "url": "http://arxiv.org/abs/2502.16797v1",
      "authors": "Erik Jones, Meg Tong, Jesse Mu, Mohammed Mahfoud, Jan Leike, Roger Grosse, Jared Kaplan, William Fithian, Ethan Perez, Mrinank Sharma",
      "update_time": "2025-02-24",
      "abstract": "Standard language model evaluations can fail to capture risks that emerge only at deployment scale. For example, a model may produce safe responses during a small-scale beta test, yet reveal dangerous information when processing billions of requests at deployment. To remedy this, we introduce a method to forecast potential risks across orders of magnitude more queries than we test during evaluation. We make forecasts by studying each query's elicitation probability -- the probability the query produces a target behavior -- and demonstrate that the largest observed elicitation probabilities predictably scale with the number of queries. We find that our forecasts can predict the emergence of diverse undesirable behaviors -- such as assisting users with dangerous chemical synthesis or taking power-seeking actions -- across up to three orders of magnitude of query volume. Our work enables model developers to proactively anticipate and patch rare failures before they manifest during large-scale deployments."
    },
    "2502.17480v1": {
      "title": "Brain-to-Text Decoding: A Non-invasive Approach via Typing",
      "url": "http://arxiv.org/abs/2502.17480v1",
      "authors": "Jarod L\u00e9vy, Mingfang Zhang, Svetlana Pinet, J\u00e9r\u00e9my Rapin, Hubert Banville, St\u00e9phane d'Ascoli, Jean-R\u00e9mi King",
      "update_time": "2025-02-18",
      "abstract": "Modern neuroprostheses can now restore communication in patients who have lost the ability to speak or move. However, these invasive devices entail risks inherent to neurosurgery. Here, we introduce a non-invasive method to decode the production of sentences from brain activity and demonstrate its efficacy in a cohort of 35 healthy volunteers. For this, we present Brain2Qwerty, a new deep learning architecture trained to decode sentences from either electro- (EEG) or magneto-encephalography (MEG), while participants typed briefly memorized sentences on a QWERTY keyboard. With MEG, Brain2Qwerty reaches, on average, a character-error-rate (CER) of 32% and substantially outperforms EEG (CER: 67%). For the best participants, the model achieves a CER of 19%, and can perfectly decode a variety of sentences outside of the training set. While error analyses suggest that decoding depends on motor processes, the analysis of typographical errors suggests that it also involves higher-level cognitive factors. Overall, these results narrow the gap between invasive and non-invasive methods and thus open the path for developing safe brain-computer interfaces for non-communicating patients."
    },
    "2502.07429v2": {
      "title": "From Thought to Action: How a Hierarchy of Neural Dynamics Supports Language Production",
      "url": "http://arxiv.org/abs/2502.07429v2",
      "authors": "Mingfang Zhang, Jarod L\u00e9vy, St\u00e9phane d'Ascoli, J\u00e9r\u00e9my Rapin, F. -Xavier Alario, Pierre Bourdillon, Svetlana Pinet, Jean-R\u00e9mi King",
      "update_time": "2025-02-18",
      "abstract": "Humans effortlessly communicate their thoughts through intricate sequences of motor actions. Yet, the neural processes that coordinate language production remain largely unknown, in part because speech artifacts limit the use of neuroimaging. To elucidate the unfolding of language production in the brain, we investigate with magnetoencephalography (MEG) and electroencephalography (EEG) the neurophysiological activity of 35 skilled typists, while they typed sentences on a keyboard. This approach confirms the hierarchical predictions of linguistic theories: the neural activity preceding the production of each word is marked by the sequential rise and fall of context-, word-, syllable-, and letter-level representations. Remarkably, each of these neural representations is maintained over long time periods within each level of the language hierarchy. This phenomenon results in a superposition of successive representations that is supported by a hierarchy of dynamic neural codes. Overall, these findings provide a precise computational breakdown of the neural dynamics that coordinate the production of language in the human brain."
    },
    "2502.05161v3": {
      "title": "Estimated Roadway Segment Traffic Data by Vehicle Class for the United States: A Machine Learning Approach",
      "url": "http://arxiv.org/abs/2502.05161v3",
      "authors": "Brittany Antonczak, Meg Fay, Aviral Chawla, Gregory Rowangould",
      "update_time": "2025-04-10",
      "abstract": "The Highway Performance Monitoring System, managed by the Federal Highway Administration, provides essential data on average annual daily traffic across U.S. roadways, but it has limited representation of medium- and heavy-duty vehicles on non-interstate roads. This gap limits research and policy analysis on the impacts of truck traffic, especially concerning air quality and public health. To address this, we use random forest regression to estimate medium- and heavy-duty vehicle traffic volumes in areas with sparse data. This results in a more comprehensive dataset, which enables the estimation of traffic density at the census block level as a proxy for traffic-related air pollution exposure. Our high-resolution spatial data products, rigorously validated, provide a more accurate representation of truck traffic and its environmental and health impacts. These datasets are valuable for transportation planning, public health research, and policy decisions aimed at mitigating the effects of truck traffic on vulnerable communities exposed to air pollution."
    }
  },
  "neuroAI": {
    "2503.06286v1": {
      "title": "A 7T fMRI dataset of synthetic images for out-of-distribution modeling of vision",
      "url": "http://arxiv.org/abs/2503.06286v1",
      "authors": "Alessandro T. Gifford, Radoslaw M. Cichy, Thomas Naselaris, Kendrick Kay",
      "update_time": "2025-03-08",
      "abstract": "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD) are boosting NeuroAI research by enabling computational models of the brain with performances beyond what was possible just a decade ago. However, these datasets lack out-of-distribution (OOD) components, which are crucial for the development of more robust models. Here, we address this limitation by releasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the eight NSD subjects for 284 carefully controlled synthetic images. We show that NSD-synthetic's fMRI responses reliably encode stimulus-related information and are OOD with respect to NSD. Furthermore, OOD generalization tests on NSD-synthetic reveal differences between models of the brain that are not detected with NSD - specifically, self-supervised deep neural networks better explain neural responses than their task-supervised counterparts. These results showcase how NSD-synthetic enables OOD generalization tests that facilitate the development of more robust models of visual processing, and the formulation of more accurate theories of human vision."
    },
    "2502.16238v1": {
      "title": "Brain-Model Evaluations Need the NeuroAI Turing Test",
      "url": "http://arxiv.org/abs/2502.16238v1",
      "authors": "Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi",
      "update_time": "2025-02-22",
      "abstract": "What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \\emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development."
    },
    "2501.02402v1": {
      "title": "Asynchronous Hebbian/anti-Hebbian networks",
      "url": "http://arxiv.org/abs/2501.02402v1",
      "authors": "Henrique Reis Aguiar, Matthias H. Hennig",
      "update_time": "2025-01-04",
      "abstract": "Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.",
      "code_url": "https://github.com/henri-edinb/async_learning"
    },
    "2411.18526v2": {
      "title": "NeuroAI for AI Safety",
      "url": "http://arxiv.org/abs/2411.18526v2",
      "authors": "Patrick Mineault, Niccol\u00f2 Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador",
      "update_time": "2025-04-03",
      "abstract": "As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety."
    },
    "2411.14633v1": {
      "title": "Evaluating Representational Similarity Measures from the Lens of Functional Correspondence",
      "url": "http://arxiv.org/abs/2411.14633v1",
      "authors": "Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla",
      "update_time": "2024-11-21",
      "abstract": "Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research."
    },
    "2410.19315v1": {
      "title": "A prescriptive theory for brain-like inference",
      "url": "http://arxiv.org/abs/2410.19315v1",
      "authors": "Hadi Vafaii, Dekel Galor, Jacob L. Yates",
      "update_time": "2024-10-25",
      "abstract": "The Evidence Lower Bound (ELBO) is a widely used objective for training deep generative models, such as Variational Autoencoders (VAEs). In the neuroscience literature, an identical objective is known as the variational free energy, hinting at a potential unified framework for brain function and machine learning. Despite its utility in interpreting generative models, including diffusion models, ELBO maximization is often seen as too broad to offer prescriptive guidance for specific architectures in neuroscience or machine learning. In this work, we show that maximizing ELBO under Poisson assumptions for general sequence data leads to a spiking neural network that performs Bayesian posterior inference through its membrane potential dynamics. The resulting model, the iterative Poisson VAE (iP-VAE), has a closer connection to biological neurons than previous brain-inspired predictive coding models based on Gaussian assumptions. Compared to amortized and iterative VAEs, iP-VAElearns sparser representations and exhibits superior generalization to out-of-distribution samples. These findings suggest that optimizing ELBO, combined with Poisson assumptions, provides a solid foundation for developing prescriptive theories in NeuroAI."
    },
    "2409.05771v1": {
      "title": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models",
      "url": "http://arxiv.org/abs/2409.05771v1",
      "authors": "Emily Cheng, Richard J. Antonello",
      "update_time": "2024-09-09",
      "abstract": "Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first \"composition\" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties."
    },
    "2407.04117v2": {
      "title": "Predictive Coding Networks and Inference Learning: Tutorial and Survey",
      "url": "http://arxiv.org/abs/2407.04117v2",
      "authors": "Bj\u00f6rn van Zwol, Ro Jefferson, Egon L. van den Broek",
      "update_time": "2024-07-22",
      "abstract": "Recent years have witnessed a growing call for renewed emphasis on neuroscience-inspired approaches in artificial intelligence research, under the banner of NeuroAI. A prime example of this is predictive coding networks (PCNs), based on the neuroscientific framework of predictive coding. This framework views the brain as a hierarchical Bayesian inference model that minimizes prediction errors through feedback connections. Unlike traditional neural networks trained with backpropagation (BP), PCNs utilize inference learning (IL), a more biologically plausible algorithm that explains patterns of neural activity that BP cannot. Historically, IL has been more computationally intensive, but recent advancements have demonstrated that it can achieve higher efficiency than BP with sufficient parallelization. Furthermore, PCNs can be mathematically considered a superset of traditional feedforward neural networks (FNNs), significantly extending the range of trainable architectures. As inherently probabilistic (graphical) latent variable models, PCNs provide a versatile framework for both supervised learning and unsupervised (generative) modeling that goes beyond traditional artificial neural networks. This work provides a comprehensive review and detailed formal specification of PCNs, particularly situating them within the context of modern ML methods. Additionally, we introduce a Python library (PRECO) for practical implementation. This positions PC as a promising framework for future ML innovations."
    },
    "2306.10168v3": {
      "title": "Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis",
      "url": "http://arxiv.org/abs/2306.10168v3",
      "authors": "Mitchell Ostrow, Adam Eisen, Leo Kozachkov, Ila Fiete",
      "update_time": "2023-10-29",
      "abstract": "How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.",
      "code_url": "https://github.com/mitchellostrow/dsa"
    },
    "2305.11275v2": {
      "title": "Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture",
      "url": "http://arxiv.org/abs/2305.11275v2",
      "authors": "Galen Pogoncheff, Jacob Granley, Michael Beyeler",
      "update_time": "2023-05-25",
      "abstract": "Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence."
    }
  },
  "medical": {
    "2504.20980v1": {
      "title": "Jekyll-and-Hyde Tipping Point in an AI's Behavior",
      "url": "http://arxiv.org/abs/2504.20980v1",
      "authors": "Neil F. Johnson, Frank Yingjie Huo",
      "update_time": "2025-04-29",
      "abstract": "Trust in AI is undermined by the fact that there is no science that predicts -- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is likely to tip mid-response to become wrong, misleading, irrelevant or dangerous. With deaths and trauma already being blamed on LLMs, this uncertainty is even pushing people to treat their 'pet' LLM more politely to 'dissuade' it (or its future Artificial General Intelligence offspring) from suddenly turning on them. Here we address this acute need by deriving from first principles an exact formula for when a Jekyll-and-Hyde tipping point occurs at LLMs' most basic level. Requiring only secondary school mathematics, it shows the cause to be the AI's attention spreading so thin it suddenly snaps. This exact formula provides quantitative predictions for how the tipping-point can be delayed or prevented by changing the prompt and the AI's training. Tailored generalizations will provide policymakers and the public with a firm platform for discussing any of AI's broader uses and risks, e.g. as a personal counselor, medical advisor, decision-maker for when to use force in a conflict situation. It also meets the need for clear and transparent answers to questions like ''should I be polite to my LLM?''"
    },
    "2504.20970v1": {
      "title": "SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features",
      "url": "http://arxiv.org/abs/2504.20970v1",
      "authors": "Mete Erdogan, Sebnem Demirtas",
      "update_time": "2025-04-29",
      "abstract": "Accurate and early diagnosis of pneumonia through X-ray imaging is essential for effective treatment and improved patient outcomes. Recent advancements in machine learning have enabled automated diagnostic tools that assist radiologists in making more reliable and efficient decisions. In this work, we propose a Singular Value Decomposition-based Least Squares (SVD-LS) framework for multi-class pneumonia classification, leveraging powerful feature representations from state-of-the-art self-supervised and transfer learning models. Rather than relying on computationally expensive gradient based fine-tuning, we employ a closed-form, non-iterative classification approach that ensures efficiency without compromising accuracy. Experimental results demonstrate that SVD-LS achieves competitive performance while offering significantly reduced computational costs, making it a viable alternative for real-time medical imaging applications."
    },
    "2504.20930v1": {
      "title": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification",
      "url": "http://arxiv.org/abs/2504.20930v1",
      "authors": "Ziqing Fan, Cheng Liang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie",
      "update_time": "2025-04-29",
      "abstract": "Recent advances in reasoning-enhanced large language models (LLMs) and multimodal LLMs (MLLMs) have significantly improved performance in complex tasks, yet medical AI models often overlook the structured reasoning processes inherent in clinical practice. In this work, we present ChestX-Reasoner, a radiology diagnosis MLLM designed to leverage process supervision mined directly from clinical reports, reflecting the step-by-step reasoning followed by radiologists. We construct a large dataset by extracting and refining reasoning chains from routine radiology reports. Our two-stage training framework combines supervised fine-tuning and reinforcement learning guided by process rewards to better align model reasoning with clinical standards. We introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual question answering samples with 301K clinically validated reasoning steps, and propose RadRScore, a metric evaluating reasoning factuality, completeness, and effectiveness. ChestX-Reasoner outperforms existing medical and general-domain MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical MLLM, the best general MLLM, and its base model, respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All resources are open-sourced to facilitate further research in medical reasoning MLLMs."
    },
    "2504.20921v1": {
      "title": "Leveraging Generative AI Through Prompt Engineering and Rigorous Validation to Create Comprehensive Synthetic Datasets for AI Training in Healthcare",
      "url": "http://arxiv.org/abs/2504.20921v1",
      "authors": "Polycarp Nalela",
      "update_time": "2025-04-29",
      "abstract": "Access to high-quality medical data is often restricted due to privacy concerns, posing significant challenges for training artificial intelligence (AI) algorithms within Electronic Health Record (EHR) applications. In this study, prompt engineering with the GPT-4 API was employed to generate high-quality synthetic datasets aimed at overcoming this limitation. The generated data encompassed a comprehensive array of patient admission information, including healthcare provider details, hospital departments, wards, bed assignments, patient demographics, emergency contacts, vital signs, immunizations, allergies, medical histories, appointments, hospital visits, laboratory tests, diagnoses, treatment plans, medications, clinical notes, visit logs, discharge summaries, and referrals. To ensure data quality and integrity, advanced validation techniques were implemented utilizing models such as BERT's Next Sentence Prediction for sentence coherence, GPT-2 for overall plausibility, RoBERTa for logical consistency, autoencoders for anomaly detection, and conducted diversity analysis. Synthetic data that met all validation criteria were integrated into a comprehensive PostgreSQL database, serving as the data management system for the EHR application. This approach demonstrates that leveraging generative AI models with rigorous validation can effectively produce high-quality synthetic medical data, facilitating the training of AI algorithms while addressing privacy concerns associated with real patient data."
    },
    "2504.20874v1": {
      "title": "Diagnostic performance of echocardiography in detecting and differentiating cardiac amyloidosis: a meta-analysis",
      "url": "http://arxiv.org/abs/2504.20874v1",
      "authors": "Zihang Zhang, Yunjie Chen, Yuanzhou Cao, Xinyi Xie, Kangming Ji, Chuang Yang, Lijun Qian",
      "update_time": "2025-04-29",
      "abstract": "Aims: This meta-analysis aimed to evaluate the diagnostic performance of echocardiographic parameters for cardiac amyloidosis (CA), with a focus on subtype stratification and comparisons with healthy controls. Methods and Results: A comprehensive search identified 26 studies published before February 2025, encompassing 3,802 patients. Compared to healthy individuals, CA patients demonstrated significant echocardiographic abnormalities, including reduced left ventricular ejection fraction (LVEF; WMD = -10.65, 95% CI: [-11.84, -9.46]), increased left atrial volume index (WMD = +15.87, 95% CI: [14.35, 17.38]), and thickened posterior wall (WMD = +5.14, 95% CI: [4.85, 5.42]). Subtype analyses revealed that transthyretin cardiac amyloidosis (ATTR-CA) was associated with more pronounced systolic dysfunction than light-chain cardiac amyloidosis (AL-CA), evidenced by lower global longitudinal strain (WMD = -2.02, 95% CI: [-2.66, -1.37]), reduced LVEF (WMD = -5.31, 95% CI: [-6.63, -3.99]), and diminished tricuspid annular plane systolic excursion (WMD = -1.59, 95% CI: [-2.23, -0.95]). Additionally, ATTR-CA patients exhibited greater ventricular wall thickening in both posterior wall (WMD = +1.87, 95% CI: [1.51, 2.23]) and interventricular septum (WMD = +2.24, 95% CI: [1.85, 2.63]). Conclusion: Echocardiography plays a pivotal role in diagnosing CA and distinguishing between AL-CA and ATTR-CA. Key indices such as LVEF and global longitudinal strain are especially valuable for early detection, while subtype-specific patterns highlight distinct underlying pathophysiologies, offering guidance for tailored diagnostic and therapeutic strategies."
    },
    "2504.20837v1": {
      "title": "RadSAM: Segmenting 3D radiological images with a 2D promptable model",
      "url": "http://arxiv.org/abs/2504.20837v1",
      "authors": "Julien Khlaut, Elodie Ferreres, Daniel Tordjman, H\u00e9l\u00e8ne Philippe, Tom Boeken, Pierre Manceron, Corentin Dancette",
      "update_time": "2025-04-29",
      "abstract": "Medical image segmentation is a crucial and time-consuming task in clinical care, where mask precision is extremely important. The Segment Anything Model (SAM) offers a promising approach, as it provides an interactive interface based on visual prompting and edition to refine an initial segmentation. This model has strong generalization capabilities, does not rely on predefined classes, and adapts to diverse objects; however, it is pre-trained on natural images and lacks the ability to process medical data effectively. In addition, this model is built for 2D images, whereas a whole medical domain is based on 3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging are based on 2D models, thus requiring one prompt per slice to segment 3D objects, making the segmentation process tedious. They also lack important features such as editing. To bridge this gap, we propose RadSAM, a novel method for segmenting 3D objects with a 2D model from a single prompt. In practice, we train a 2D model using noisy masks as initial prompts, in addition to bounding boxes and points. We then use this novel prompt type with an iterative inference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a benchmark to evaluate the model's ability to segment 3D objects in CT images from a single prompt and evaluate the models' out-of-domain transfer and edition capabilities. We demonstrate the effectiveness of our approach against state-of-the-art models on this benchmark using the AMOS abdominal organ segmentation dataset."
    },
    "2504.20741v1": {
      "title": "In defence of post-hoc explanations in medical AI",
      "url": "http://arxiv.org/abs/2504.20741v1",
      "authors": "Joshua Hatherley, Lauritz Munch, Jens Christian Bjerring",
      "update_time": "2025-04-29",
      "abstract": "Since the early days of the Explainable AI movement, post-hoc explanations have been praised for their potential to improve user understanding, promote trust, and reduce patient safety risks in black box medical AI systems. Recently, however, critics have argued that the benefits of post-hoc explanations are greatly exaggerated since they merely approximate, rather than replicate, the actual reasoning processes that black box systems take to arrive at their outputs. In this article, we aim to defend the value of post-hoc explanations against this recent critique. We argue that even if post-hoc explanations do not replicate the exact reasoning processes of black box systems, they can still improve users' functional understanding of black box systems, increase the accuracy of clinician-AI teams, and assist clinicians in justifying their AI-informed decisions. While post-hoc explanations are not a \"silver bullet\" solution to the black box problem in medical AI, we conclude that they remain a useful strategy for addressing the black box problem in medical AI."
    },
    "2504.20698v1": {
      "title": "A High-Granularity Proton CT Enhanced by Track Discrimination",
      "url": "http://arxiv.org/abs/2504.20698v1",
      "authors": "Huang-Chao Shi, Dan-Qi Wang, Mei-Qi Liu, Yi-Dong Yang, Bao-Hua Qi, Yong Liu, Yang-Hui Qin, Chang-Qing Feng, Yi Liu, Hong-Bo Zhu",
      "update_time": "2025-04-29",
      "abstract": "Proton Computed Tomography (pCT) provides a promising solution to enhance the accuracy of Relative Stopping Power (RSP) required for proton therapy planning. This research introduces a novel high-granularity pCT architecture that incorporates a silicon pixel tracking system and a calorimetric range telescope, which uniquely integrates range telescope functionality with track discrimination capabilities. The Bortfeld function fitting and Convolutional Neural Network (CNN) classifier algorithms are developed and applied for discrimination. In simulation studies, both approaches demonstrate the capability to reduce uncertainty in Water Equivalent Path Length (WEPL) determination for individual proton tracks to below 3~mm. The standard imaging protocol (3.2~mGy, $4\\times10^{8}$ protons) achieves sub-millimeter spatial resolution ($\\sim$0.5 mm) with sub-1\\% RSP accuracy. With proton count requirements reduced by track discrimination, an ultra-low-dose protocol (0.16~mGy, $2\\times10^{7}$~protons) is proposed with achieved sub-1\\% RSP accuracy and $\\sim$1.1~mm spatial resolution in simulation. This low-dose performance significantly expands clinical applicability, particularly for pediatric imaging or frequent imaging scenarios. Furthermore, the target 10 MHz proton detection rate suggests potential for real-time image guidance during radiotherapy. By circumventing the need for ultra-precise energy measurements, this design minimizes hardware complexity and provides a scalable foundation for future pCT systems."
    },
    "2504.20689v1": {
      "title": "DICOM Compatible, 3D Multimodality Image Encryption using Hyperchaotic Signal",
      "url": "http://arxiv.org/abs/2504.20689v1",
      "authors": "Anandik N Anand, Sishu Shankar Muni, Abhishek Kaushik",
      "update_time": "2025-04-29",
      "abstract": "Medical image encryption plays an important role in protecting sensitive health information from cyberattacks and unauthorized access. In this paper, we introduce a secure and robust encryption scheme that is multi-modality compatible and works with MRI, CT, X-Ray and Ultrasound images for different anatomical region of interest. The method utilizes hyperchaotic signals and multi-level diffusion methods. The encryption starts by taking DICOM image as input, then padding to increase the image area. Chaotic signals are produced by a logistic map and are used to carry out pixel random permutation. Then, multi-level diffusion is carried out by 4-bit, 8-bit, radial and adjacent diffusion to provide high randomness and immunity against statistical attacks. In addition, we propose a captcha-based authentication scheme to further improve security. An algorithm generates alphanumeric captcha-based image which is encrypted with the same chaotic and diffusion methods as the medical image. Both encrypted images(DICOM image and captcha image) are then superimposed to create a final encrypted output, essentially integrating dual-layer security. Upon decryption, the superimposed image is again decomposed back to original medical and captcha images, and inverse operations are performed to obtain the original unencrypted data. Experimental results show that the proposed method provides strong protection with no loss in image integrity, thereby reducing unauthorized data breaches to a significant level. The dual-encryption approach not only protects the confidentiality of the medical images but also enhances authentication by incorporating captcha."
    },
    "2504.20657v1": {
      "title": "Image deidentification in the XNAT ecosystem: use cases and solutions",
      "url": "http://arxiv.org/abs/2504.20657v1",
      "authors": "Alex Michie, Simon J Doran",
      "update_time": "2025-04-29",
      "abstract": "XNAT is a server-based data management platform widely used in academia for curating large databases of DICOM images for research projects. We describe in detail a deidentification workflow for DICOM data using facilities in XNAT, together with independent tools in the XNAT \"ecosystem\". We list different contexts in which deidentification might be needed, based on our prior experience. The starting point for participation in the Medical Image De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local methodologies, which were adapted during the validation phase of the challenge. Our result in the test phase was 97.91\\%, considerably lower than our peers, due largely to an arcane technical incompatibility of our methodology with the challenge's Synapse platform, which prevented us receiving feedback during the validation phase. Post-submission, additional discrepancy reports from the organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to improve this score significantly to 99.61\\%. An entirely rule-based approach was shown to be capable of removing all name-related information in the test corpus, but exhibited failures in dealing fully with address data. Initial experiments using published machine-learning models to remove addresses were partially successful but showed the models to be \"over-aggressive\" on other types of free-text data, leading to a slight overall degradation in performance to 99.54\\%. Future development will therefore focus on improving address-recognition capabilities, but also on better removal of identifiable data burned into the image pixels. Several technical aspects relating to the \"answer key\" are still under discussion with the challenge organisers, but we estimate that our percentage of genuine deidentification failures on the MIDI-B test corpus currently stands at 0.19\\%. (Abridged from original for arXiv submission)"
    }
  }
}