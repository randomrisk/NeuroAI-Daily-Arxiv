{
  "Brain": {
    "2602.04681v1": {
      "title": "HFMCA: Orthonormal Feature Learning for EEG-based Brain Decoding",
      "url": "http://arxiv.org/abs/2602.04681v1",
      "authors": "Yinghao Wang, Lintao Xu, Shujian Yu, Enzo Tartaglione, Van-Tam Nguyen",
      "update_time": "2026-02-04",
      "abstract": "Electroencephalography (EEG) analysis is critical for brain-computer interfaces and neuroscience, but the intrinsic noise and high dimensionality of EEG signals hinder effective feature learning. We propose a self-supervised framework based on the Hierarchical Functional Maximal Correlation Algorithm (HFMCA), which learns orthonormal EEG representations by enforcing feature decorrelation and reducing redundancy. This design enables robust capture of essential brain dynamics for various EEG recognition tasks. We validate HFMCA on two benchmark datasets, SEED and BCIC-2A, where pretraining with HFMCA consistently outperforms competitive self-supervised baselines, achieving notable gains in classification accuracy. Across diverse EEG tasks, our method demonstrates superior cross-subject generalization under leave-one-subject-out validation, advancing state-of-the-art by 2.71\\% on SEED emotion recognition and 2.57\\% on BCIC-2A motor imagery classification.",
      "code_url": null
    },
    "2602.04619v1": {
      "title": "Scalable platform enabling reservoir computing with nanoporous oxide memristors for image recognition and time series prediction",
      "url": "http://arxiv.org/abs/2602.04619v1",
      "authors": "Joshua Donald, Ben A. Johnson, Amir Mehrnejat, Alex Gabbitas, Arthur G. T. Coveney, Alexander G. Balanov, Sergey Savel'ev, Pavel Borisov",
      "update_time": "2026-02-04",
      "abstract": "Typical mammal brains have some form of random connectivity between neurons. Reservoir computing, a neural network approach, uses random weights within its processing layer along with built-in recurrent connections and short-term, fading memory, and is shown to be time and training efficient in processing spatiotemporal signals. Here we prepared a niobium oxide-based thin film memristor device with intrinsic structural in-homogeneity in the form of random nanopores and performed computational tasks of XOR operations, image recognition, and time series prediction and reconstruction. For the latter task we chose a complex three-dimensional chaotic Lorenz-63 time series. By applying three temporal voltage waveforms individually across the device and training the readout layer with electrical current signals from a three-output physical reservoir, we achieved satisfactory prediction and reconstruction accuracy in comparison to the case of no reservoir. This work highlights the potential for scalable, on-chip devices using all-oxide reservoir systems, paving the way for energy-efficient neuromorphic electronics dealing with time signals.",
      "code_url": null
    },
    "2602.04512v1": {
      "title": "BrainVista: Modeling Naturalistic Brain Dynamics as Multimodal Next-Token Prediction",
      "url": "http://arxiv.org/abs/2602.04512v1",
      "authors": "Xuanhua Yin, Runkai Zhao, Lina Yao, Weidong Cai",
      "update_time": "2026-02-04",
      "abstract": "Naturalistic fMRI characterizes the brain as a dynamic predictive engine driven by continuous sensory streams. However, modeling the causal forward evolution in realistic neural simulation is impeded by the timescale mismatch between multimodal inputs and the complex topology of cortical networks. To address these challenges, we introduce BrainVista, a multimodal autoregressive framework designed to model the causal evolution of brain states. BrainVista incorporates Network-wise Tokenizers to disentangle system-specific dynamics and a Spatial Mixer Head that captures inter-network information flow without compromising functional boundaries. Furthermore, we propose a novel Stimulus-to-Brain (S2B) masking mechanism to synchronize high-frequency sensory stimuli with hemodynamically filtered signals, enabling strict, history-only causal conditioning. We validate our framework on Algonauts 2025, CineBrain, and HAD, achieving state-of-the-art fMRI encoding performance. In long-horizon rollout settings, our model yields substantial improvements over baselines, increasing pattern correlation by 36.0\\% and 33.3\\% on relative to the strongest baseline Algonauts 2025 and CineBrain, respectively.",
      "code_url": null
    },
    "2602.04320v1": {
      "title": "A Domain-Specific Curated Benchmark for Entity and Document-Level Relation Extraction",
      "url": "http://arxiv.org/abs/2602.04320v1",
      "authors": "Marco Martinelli, Stefano Marchesin, Vanessa Bonato, Giorgio Maria Di Nunzio, Nicola Ferro, Ornella Irrera, Laura Menotti, Federica Vezzani, Gianmaria Silvello",
      "update_time": "2026-02-04",
      "abstract": "Information Extraction (IE), encompassing Named Entity Recognition (NER), Named Entity Linking (NEL), and Relation Extraction (RE), is critical for transforming the rapidly growing volume of scientific publications into structured, actionable knowledge. This need is especially evident in fast-evolving biomedical fields such as the gut-brain axis, where research investigates complex interactions between the gut microbiota and brain-related disorders. Existing biomedical IE benchmarks, however, are often narrow in scope and rely heavily on distantly supervised or automatically generated annotations, limiting their utility for advancing robust IE methods. We introduce GutBrainIE, a benchmark based on more than 1,600 PubMed abstracts, manually annotated by biomedical and terminological experts with fine-grained entities, concept-level links, and relations. While grounded in the gut-brain axis, the benchmark's rich schema, multiple tasks, and combination of highly curated and weakly supervised data make it broadly applicable to the development and evaluation of biomedical IE systems across domains.",
      "code_url": null
    },
    "2602.04227v1": {
      "title": "An Intuitionistic Fuzzy Logic Driven UNet architecture: Application to Brain Image segmentation",
      "url": "http://arxiv.org/abs/2602.04227v1",
      "authors": "Hanuman Verma, Kiho Im, Pranabesh Maji, Akshansh Gupta",
      "update_time": "2026-02-04",
      "abstract": "Accurate segmentation of MRI brain images is essential for image analysis, diagnosis of neuro-logical disorders and medical image computing. In the deep learning approach, the convolutional neural networks (CNNs), especially UNet, are widely applied in medical image segmentation. However, it is difficult to deal with uncertainty due to the partial volume effect in brain images. To overcome this limitation, we propose an enhanced framework, named UNet with intuitionistic fuzzy logic (IF-UNet), which incorporates intuitionistic fuzzy logic into UNet. The model processes input data in terms of membership, nonmembership, and hesitation degrees, allowing it to better address tissue ambiguity resulting from partial volume effects and boundary uncertainties. The proposed architecture is evaluated on the Internet Brain Segmentation Repository (IBSR) dataset, and its performance is computed using accuracy, Dice coefficient, and intersection over union (IoU). Experimental results confirm that IF-UNet improves segmentation quality with handling uncertainty in brain images.",
      "code_url": null
    },
    "2602.04095v1": {
      "title": "A computational account of dreaming: learning and memory consolidation",
      "url": "http://arxiv.org/abs/2602.04095v1",
      "authors": "Qi Zhang",
      "update_time": "2026-02-04",
      "abstract": "A number of studies have concluded that dreaming is mostly caused by randomly arriving internal signals because \"dream contents are random impulses\", and argued that dream sleep is unlikely to play an important part in our intellectual capacity. On the contrary, numerous functional studies have revealed that dream sleep does play an important role in our learning and other intellectual functions. Specifically, recent studies have suggested the importance of dream sleep in memory consolidation, following the findings of neural replaying of recent waking patterns in the hippocampus. The randomness has been the hurdle that divides dream theories into either functional or functionless. This study presents a cognitive and computational model of dream process. This model is simulated to perform the functions of learning and memory consolidation, which are two most popular dream functions that have been proposed. The simulations demonstrate that random signals may result in learning and memory consolidation. Thus, dreaming is proposed as a continuation of brain's waking activities that processes signals activated spontaneously and randomly from the hippocampus. The characteristics of the model are discussed and found in agreement with many characteristics concluded from various empirical studies.",
      "code_url": null
    },
    "2602.04081v1": {
      "title": "Abstraction Induces the Brain Alignment of Language and Speech Models",
      "url": "http://arxiv.org/abs/2602.04081v1",
      "authors": "Emily Cheng, Aditya R. Vaidya, Richard Antonello",
      "update_time": "2026-02-03",
      "abstract": "Research has repeatedly demonstrated that intermediate hidden states extracted from large language models and speech audio models predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most effective for this unique and highly general transfer task? We give evidence that the correspondence between speech and language models and the brain derives from shared meaning abstraction and not their next-word prediction properties. In particular, models construct higher-order linguistic features in their middle layers, cued by a peak in the layerwise intrinsic dimension, a measure of feature complexity. We show that a layer's intrinsic dimension strongly predicts how well it explains fMRI and ECoG signals; that the relation between intrinsic dimension and brain predictivity arises over model pre-training; and finetuning models to better predict the brain causally increases both representations' intrinsic dimension and their semantic content. Results suggest that semantic richness, high intrinsic dimension, and brain predictivity mirror each other, and that the key driver of model-brain similarity is rich meaning abstraction of the inputs, where language modeling is a task sufficiently complex (but perhaps not the only) to require it.",
      "code_url": null
    },
    "2602.04077v1": {
      "title": "Efficient Subgroup Analysis via Optimal Trees with Global Parameter Fusion",
      "url": "http://arxiv.org/abs/2602.04077v1",
      "authors": "Zhongming Xie, Joseph Giorgio, Jingshen Wang",
      "update_time": "2026-02-03",
      "abstract": "Identifying and making statistical inferences on differential treatment effects (commonly known as subgroup analysis in clinical research) is central to precision health. Subgroup analysis allows practitioners to pinpoint populations for whom a treatment is especially beneficial or protective, thereby advancing targeted interventions. Tree based recursive partitioning methods are widely used for subgroup analysis due to their interpretability. Nevertheless, these approaches encounter significant limitations, including suboptimal partitions induced by greedy heuristics and overfitting from locally estimated splits, especially under limited sample sizes. To address these limitations, we propose a fused optimal causal tree method that leverages mixed integer optimization (MIO) to facilitate precise subgroup identification. Our approach ensures globally optimal partitions and introduces a parameter fusion constraint to facilitate information sharing across related subgroups. This design substantially improves subgroup discovery accuracy and enhances statistical efficiency. We provide theoretical guarantees by rigorously establishing out of sample risk bounds and comparing them with those of classical tree based methods. Empirically, our method consistently outperforms popular baselines in simulations. Finally, we demonstrate its practical utility through a case study on the Health and Aging Brain Study Health Disparities (HABS-HD) dataset, where our approach yields clinically meaningful insights.",
      "code_url": null
    },
    "2602.04074v1": {
      "title": "Stroke Lesions as a Rosetta Stone for Language Model Interpretability",
      "url": "http://arxiv.org/abs/2602.04074v1",
      "authors": "Julius Fridriksson, Roger D. Newman-Norlund, Saeed Ahmadi, Regan Willis, Nadra Salman, Kalil Warren, Xiang Guan, Yong Yang, Srihari Nelakuditi, Rutvik Desai, Leonardo Bonilha, Jeff Charney, Chris Rorden",
      "update_time": "2026-02-03",
      "abstract": "Large language models (LLMs) have achieved remarkable capabilities, yet methods to verify which model components are truly necessary for language function remain limited. Current interpretability approaches rely on internal metrics and lack external validation. Here we present the Brain-LLM Unified Model (BLUM), a framework that leverages lesion-symptom mapping, the gold standard for establishing causal brain-behavior relationships for over a century, as an external reference structure for evaluating LLM perturbation effects. Using data from individuals with chronic post-stroke aphasia (N = 410), we trained symptom-to-lesion models that predict brain damage location from behavioral error profiles, applied systematic perturbations to transformer layers, administered identical clinical assessments to perturbed LLMs and human patients, and projected LLM error profiles into human lesion space. LLM error profiles were sufficiently similar to human error profiles that predicted lesions corresponded to actual lesions in error-matched humans above chance in 67% of picture naming conditions (p < 10^{-23}) and 68.3% of sentence completion conditions (p < 10^{-61}), with semantic-dominant errors mapping onto ventral-stream lesion patterns and phonemic-dominant errors onto dorsal-stream patterns. These findings open a new methodological avenue for LLM interpretability in which clinical neuroscience provides external validation, establishing human lesion-symptom mapping as a reference framework for evaluating artificial language systems and motivating direct investigation of whether behavioral alignment reflects shared computational principles.",
      "code_url": null
    },
    "2602.04018v1": {
      "title": "Cross-Frequency Bispectral EEG Analysis of Reach-to-Grasp Planning and Execution",
      "url": "http://arxiv.org/abs/2602.04018v1",
      "authors": "Sima Ghafoori, Anna Cetera, Ali Rabiee, MH Farhadi, Rahul Singh, Mariusz Furmanek, Yalda Shahriari, Reza Abiri",
      "update_time": "2026-02-03",
      "abstract": "Neural control of grasping arises from nonlinear interactions across multiple brain rhythms, yet EEG-based motor decoding has largely relied on linear, second-order spectral features. Here, we examine whether higher-order cross-frequency dynamics distinguish motor planning from execution during natural reach-to-grasp behavior. EEG was recorded in a cue-based paradigm during executed precision and power grips, enabling stage-resolved analysis of preparatory and execution-related neural activity.   Cross-frequency bispectral analysis was used to compute bicoherence matrices across canonical frequency band pairs, from which magnitude- and phase-based features were extracted. Classification, permutation-based feature selection, and within-subject statistical testing showed that execution is characterized by substantially stronger and more discriminative nonlinear coupling than planning, with dominant contributions from beta- and gamma-driven interactions. In contrast, decoding of precision versus power grips achieved comparable performance during planning and execution, indicating that grasp-type representations emerge during planning and persist into execution.   Spatial and spectral analyses further revealed that informative bispectral features reflect coordinated activity across prefrontal, central, and occipital regions. Despite substantial feature redundancy, effective dimensionality reduction preserved decoding performance. Together, these findings demonstrate that nonlinear cross-frequency coupling provides an interpretable and robust marker of motor planning and execution, extending bispectral EEG analysis to ecologically valid grasping and supporting its relevance for brain-computer interfaces and neuroprosthetic control.",
      "code_url": null
    }
  },
  "EEG": {
    "2602.04769v1": {
      "title": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Image",
      "url": "http://arxiv.org/abs/2602.04769v1",
      "authors": "Yan Chen, Jie Peng, Moajjem Hossain Chowdhury, Tianlong Chen, Yunmei Liu",
      "update_time": "2026-02-04",
      "abstract": "Accurate and timely seizure detection from Electroencephalography (EEG) is critical for clinical intervention, yet manual review of long-term recordings is labor-intensive. Recent efforts to encode EEG signals into large language models (LLMs) show promise in handling neural signals across diverse patients, but two significant challenges remain: (1) multi-channel heterogeneity, as seizure-relevant information varies substantially across EEG channels, and (2) computing inefficiency, as the EEG signals need to be encoded into a massive number of tokens for the prediction. To address these issues, we draw the EEG signal and propose the novel NeuroCanvas framework. Specifically, NeuroCanvas consists of two modules: (i) The Entropy-guided Channel Selector (ECS) selects the seizure-relevant channels input to LLM and (ii) the following Canvas of Neuron Signal (CNS) converts selected multi-channel heterogeneous EEG signals into structured visual representations. The ECS module alleviates the multi-channel heterogeneity issue, and the CNS uses compact visual tokens to represent the EEG signals that improve the computing efficiency. We evaluate NeuroCanvas across multiple seizure detection datasets, demonstrating a significant improvement of $20\\%$ in F1 score and reductions of $88\\%$ in inference latency. These results highlight NeuroCanvas as a scalable and effective solution for real-time and resource-efficient seizure detection in clinical practice.The code will be released at https://github.com/Yanchen30247/seizure_detect.",
      "code_url": null
    },
    "2602.04681v1": {
      "title": "HFMCA: Orthonormal Feature Learning for EEG-based Brain Decoding",
      "url": "http://arxiv.org/abs/2602.04681v1",
      "authors": "Yinghao Wang, Lintao Xu, Shujian Yu, Enzo Tartaglione, Van-Tam Nguyen",
      "update_time": "2026-02-04",
      "abstract": "Electroencephalography (EEG) analysis is critical for brain-computer interfaces and neuroscience, but the intrinsic noise and high dimensionality of EEG signals hinder effective feature learning. We propose a self-supervised framework based on the Hierarchical Functional Maximal Correlation Algorithm (HFMCA), which learns orthonormal EEG representations by enforcing feature decorrelation and reducing redundancy. This design enables robust capture of essential brain dynamics for various EEG recognition tasks. We validate HFMCA on two benchmark datasets, SEED and BCIC-2A, where pretraining with HFMCA consistently outperforms competitive self-supervised baselines, achieving notable gains in classification accuracy. Across diverse EEG tasks, our method demonstrates superior cross-subject generalization under leave-one-subject-out validation, advancing state-of-the-art by 2.71\\% on SEED emotion recognition and 2.57\\% on BCIC-2A motor imagery classification.",
      "code_url": null
    },
    "2602.04299v1": {
      "title": "A Multimodal fNIRS-EEG Dataset for Unilateral Limb Motor Imagery",
      "url": "http://arxiv.org/abs/2602.04299v1",
      "authors": "Lufeng Feng, Baomin Xu, Haoran Zhang, Bihai Lin, Zuxuan Deng, Sidi Tao, Chenyu Liu, Shifan Jia, Li Duan, Ziyu Jia",
      "update_time": "2026-02-04",
      "abstract": "Unilateral limb motor imagery (MI) plays an important role in upper-limb motor rehabilitation and precise control of external devices, and places higher demands on spatial resolution. However, most existing public datasets focus on binary- or four-class left-right limb paradigms that mainly exploit coarse hemispheric lateralization, and there is still a lack of multimodal datasets that simultaneously record EEG and fNIRS for unilateral multi-directional MI. To address this gap, we constructed MIND, a public motor imagery fNIRS-EEG dataset based on a four-class directional MI paradigm of the right upper limb. The dataset includes 64-channel EEG recordings (1000 Hz) and 51-channel fNIRS recordings (47.62 Hz) from 30 participants (12 females, 18 males; aged 19.0-25.0 years). We analyse the spatiotemporal characteristics of EEG spectral power and hemodynamic responses, and validate the potential advantages of hybrid fNIRS-EEG BCIs in terms of classification accuracy. We expect that this dataset will facilitate the evaluation and comparison of neuroimaging analysis and decoding methods.",
      "code_url": null
    },
    "2602.04018v1": {
      "title": "Cross-Frequency Bispectral EEG Analysis of Reach-to-Grasp Planning and Execution",
      "url": "http://arxiv.org/abs/2602.04018v1",
      "authors": "Sima Ghafoori, Anna Cetera, Ali Rabiee, MH Farhadi, Rahul Singh, Mariusz Furmanek, Yalda Shahriari, Reza Abiri",
      "update_time": "2026-02-03",
      "abstract": "Neural control of grasping arises from nonlinear interactions across multiple brain rhythms, yet EEG-based motor decoding has largely relied on linear, second-order spectral features. Here, we examine whether higher-order cross-frequency dynamics distinguish motor planning from execution during natural reach-to-grasp behavior. EEG was recorded in a cue-based paradigm during executed precision and power grips, enabling stage-resolved analysis of preparatory and execution-related neural activity.   Cross-frequency bispectral analysis was used to compute bicoherence matrices across canonical frequency band pairs, from which magnitude- and phase-based features were extracted. Classification, permutation-based feature selection, and within-subject statistical testing showed that execution is characterized by substantially stronger and more discriminative nonlinear coupling than planning, with dominant contributions from beta- and gamma-driven interactions. In contrast, decoding of precision versus power grips achieved comparable performance during planning and execution, indicating that grasp-type representations emerge during planning and persist into execution.   Spatial and spectral analyses further revealed that informative bispectral features reflect coordinated activity across prefrontal, central, and occipital regions. Despite substantial feature redundancy, effective dimensionality reduction preserved decoding performance. Together, these findings demonstrate that nonlinear cross-frequency coupling provides an interpretable and robust marker of motor planning and execution, extending bispectral EEG analysis to ecologically valid grasping and supporting its relevance for brain-computer interfaces and neuroprosthetic control.",
      "code_url": null
    },
    "2602.03624v1": {
      "title": "A Multi-decoder Neural Tracking Method for Accurately Predicting Speech Intelligibility",
      "url": "http://arxiv.org/abs/2602.03624v1",
      "authors": "Rien Sonck, Bernd Accou, Tom Francart, Jonas Vanthornhout",
      "update_time": "2026-02-03",
      "abstract": "Objective: EEG-based methods can predict speech intelligibility, but their accuracy and robustness lag behind behavioral tests, which typically show test-retest differences under 1 dB. We introduce the multi-decoder method to predict speech reception thresholds (SRTs) from EEG recordings, enabling objective assessment for populations unable to perform behavioral tests; such as those with disorders of consciousness or during hearing aid fitting. Approach: The method aggregates data from hundreds of decoders, each trained on different speech features and EEG preprocessing setups to quantify neural tracking (NT) of speech signals. Using data from 39 participants (ages 18-24), we recorded 29 minutes of EEG per person while they listened to speech at six signal-to-noise ratios and a quiet story. NT values were combined into a high-dimensional feature vector per subject, and a support vector regression model was trained to predict SRTs from these vectors. Main Result: Predictions correlated significantly with behavioral SRTs (r = 0.647, p < 0.001; NRMSE = 0.19), with all differences under 1 dB. SHAP analysis showed theta/delta bands and early lags had slightly greater influence. Using pretrained subject-independent decoders reduced required EEG data collection to 15 minutes (3 minutes of story, 12 minutes across six SNR conditions) without losing accuracy.",
      "code_url": null
    },
    "2602.03269v1": {
      "title": "Systematic review of self-supervised foundation models for brain network representation using electroencephalography",
      "url": "http://arxiv.org/abs/2602.03269v1",
      "authors": "Hannah Portmann, Yosuke Morishima",
      "update_time": "2026-02-03",
      "abstract": "Automated analysis of electroencephalography (EEG) has recently undergone a paradigm shift. The introduction of transformer architectures and self-supervised pretraining (SSL) has led to the development of EEG foundation models. These models are pretrained on large amounts of unlabeled data and can be adapted to a range of downstream tasks. This systematic review summarizes recent SSL-trained EEG foundation models that learn whole-brain representations from multichannel EEG rather than representations derived from a single channel. We searched PubMed, IEEE Xplore, Scopus, and arXiv through July 21, 2025. Nineteen preprints and peer-reviewed articles met inclusion criteria. We extracted information regarding pretraining datasets, model architectures, pretraining SSL objectives, and downstream task applications. While pretraining data heavily relied on the Temple University EEG corpus, there was significant heterogeneity in model architecture and training objectives across studies. Transformer architectures were identified as the predominant pretraining architecture with state-space models such as MAMBA and S4 as emerging alternatives. Concerning SSL objectives, masked auto-encoding was most common, and other studies incorporate contrastive learning. Downstream tasks varied widely and implemented diverse fine-tuning strategies, which made direct comparison challenging. Furthermore, most studies used single-task fine-tuning, and a generalizable EEG foundation model remains lacking. In conclusion, the field is advancing rapidly but still limited by limited dataset diversity and the absence of standardized benchmarks. Progress will likely depend on larger and more diverse pretraining datasets, standardized evaluation protocols, and multi-task validation. The development will advance EEG foundation models towards robust and general-purpose relevant to both basic and clinical applications.",
      "code_url": null
    },
    "2602.02784v1": {
      "title": "Cross-Temporal Attention Fusion (CTAF) for Multimodal Physiological Signals in Self-Supervised Learning",
      "url": "http://arxiv.org/abs/2602.02784v1",
      "authors": "Arian Khorasani, Th\u00e9ophile Demazure",
      "update_time": "2026-02-02",
      "abstract": "We study multimodal affect modeling when EEG and peripheral physiology are asynchronous, which most fusion methods ignore or handle with costly warping. We propose Cross-Temporal Attention Fusion (CTAF), a self-supervised module that learns soft bidirectional alignments between modalities and builds a robust clip embedding using time-aware cross attention, a lightweight fusion gate, and alignment-regularized contrastive objectives with optional weak supervision. On the K-EmoCon dataset, under leave-one-out cross-validation evaluation, CTAF yields higher cosine margins for matched pairs and better cross-modal token retrieval within one second, and it is competitive with the baseline on three-bin accuracy and macro-F1 while using few labels. Our contributions are a time-aware fusion mechanism that directly models correspondence, an alignment-driven self-supervised objective tailored to EEG and physiology, and an evaluation protocol that measures alignment quality itself. Our approach accounts for the coupling between the central and autonomic nervous systems in psychophysiological time series. These results indicate that CTAF is a strong step toward label-efficient, generalizable EEG-peripheral fusion under temporal asynchrony.",
      "code_url": null
    },
    "2602.02238v1": {
      "title": "Geometry- and Relation-Aware Diffusion for EEG Super-Resolution",
      "url": "http://arxiv.org/abs/2602.02238v1",
      "authors": "Laura Yao, Gengwei Zhang, Moajjem Chowdhury, Yunmei Liu, Tianlong Chen",
      "update_time": "2026-02-02",
      "abstract": "Recent electroencephalography (EEG) spatial super-resolution (SR) methods, while showing improved quality by either directly predicting missing signals from visible channels or adapting latent diffusion-based generative modeling to temporal data, often lack awareness of physiological spatial structure, thereby constraining spatial generation performance. To address this issue, we introduce TopoDiff, a geometry- and relation-aware diffusion model for EEG spatial super-resolution. Inspired by how human experts interpret spatial EEG patterns, TopoDiff incorporates topology-aware image embeddings derived from EEG topographic representations to provide global geometric context for spatial generation, together with a dynamic channel-relation graph that encodes inter-electrode relationships and evolves with temporal dynamics. This design yields a spatially grounded EEG spatial super-resolution framework with consistent performance improvements. Across multiple EEG datasets spanning diverse applications, including SEED/SEED-IV for emotion recognition, PhysioNet motor imagery (MI/MM), and TUSZ for seizure detection, our method achieves substantial gains in generation fidelity and leads to notable improvements in downstream EEG task performance.",
      "code_url": null
    },
    "2602.02086v1": {
      "title": "Neurophysiological effects of museum modalities on emotional engagement with real artworks",
      "url": "http://arxiv.org/abs/2602.02086v1",
      "authors": "Chen Feng, S\u00e9bastien Lugan, Karine Lasaracina, Midori Sugaya, Beno\u00eet Macq",
      "update_time": "2026-02-02",
      "abstract": "Museums increasingly rely on digital content to support visitors' understanding of artworks, yet little is known about how these formats shape the emotional engagement that underlies meaningful art experiences. This research presents an in-situ EEG study on how digital interpretive content modulate engagement during art viewing. Participants experienced three modalities: direct viewing of a Bruegel painting, a 180\u00b0 immersive interpretive projection, and a regular, display-based interpretive video. Frontal EEG markers of motivational orientation, internal involvement, perceptual drive, and arousal were extracted using eyes-open baselines and Z-normalized contrasts. Results show modality-specific engagement profiles: display-based interpretive video induced high arousal and fast-band activity, immersive projections promoted calm, presence-oriented absorption, and original artworks reflected internally regulated engagement. These findings, relying on lightweight EEG sensing in an operational cultural environment, suggest that digital interpretive content affects engagement style rather than quantity. This paves the way for new multimodal sensing approaches and enables museums to optimize the modalities and content of their interpretive media.",
      "code_url": null
    },
    "2602.01728v1": {
      "title": "Mutual-Guided Expert Collaboration for Cross-Subject EEG Classification",
      "url": "http://arxiv.org/abs/2602.01728v1",
      "authors": "Zhi Zhang, Yan Liu, Zhejing Hu, Gong Chen, Jiannong Cao, Shenghua Zhong, Sean Fontaine, Changhong Jing, Shuqiang Wang",
      "update_time": "2026-02-02",
      "abstract": "Decoding the human brain from electroencephalography (EEG) signals holds promise for understanding neurological activities. However, EEG data exhibit heterogeneity across subjects and sessions, limiting the generalization of existing methods. Representation learning approaches sacrifice subject-specific information for domain invariance, while ensemble learning methods risk error accumulation for unseen subjects. From a theoretical perspective, we reveal that the applicability of these paradigms depends on the reducibility cost of domain-specific functions to domain-invariant ones. Building on this insight, we propose a Mutual-Guided Expert Collaboration (MGEC) framework that employs distinct network structures aligned with domain-specific and domain-invariant functions. Shared expert-guided learning captures reducible domain-invariant functions. Routed expert-guided learning employs a mixture-of-experts architecture to model irreducible domain-specific functions. Mutual-guided learning enables collaborative regularization to prevent over-reduction and under-reduction. We validate our theoretical findings on synthetic datasets, and experiments on seven benchmarks demonstrate that MGEC outperforms state-of-the-art methods.",
      "code_url": null
    }
  },
  "BCI": {
    "2602.04299v1": {
      "title": "A Multimodal fNIRS-EEG Dataset for Unilateral Limb Motor Imagery",
      "url": "http://arxiv.org/abs/2602.04299v1",
      "authors": "Lufeng Feng, Baomin Xu, Haoran Zhang, Bihai Lin, Zuxuan Deng, Sidi Tao, Chenyu Liu, Shifan Jia, Li Duan, Ziyu Jia",
      "update_time": "2026-02-04",
      "abstract": "Unilateral limb motor imagery (MI) plays an important role in upper-limb motor rehabilitation and precise control of external devices, and places higher demands on spatial resolution. However, most existing public datasets focus on binary- or four-class left-right limb paradigms that mainly exploit coarse hemispheric lateralization, and there is still a lack of multimodal datasets that simultaneously record EEG and fNIRS for unilateral multi-directional MI. To address this gap, we constructed MIND, a public motor imagery fNIRS-EEG dataset based on a four-class directional MI paradigm of the right upper limb. The dataset includes 64-channel EEG recordings (1000 Hz) and 51-channel fNIRS recordings (47.62 Hz) from 30 participants (12 females, 18 males; aged 19.0-25.0 years). We analyse the spatiotemporal characteristics of EEG spectral power and hemodynamic responses, and validate the potential advantages of hybrid fNIRS-EEG BCIs in terms of classification accuracy. We expect that this dataset will facilitate the evaluation and comparison of neuroimaging analysis and decoding methods.",
      "code_url": null
    },
    "2602.01019v1": {
      "title": "Inter- and Intra-Subject Variability in EEG: A Systematic Survey",
      "url": "http://arxiv.org/abs/2602.01019v1",
      "authors": "Xuan-The Tran, Thien-Nhan Vo, Son-Tung Vu, Thoa-Thi Tran, Manh-Dat Nguyen, Thomas Do, Chin-Teng Lin",
      "update_time": "2026-02-01",
      "abstract": "Electroencephalography (EEG) underpins neuroscience, clinical neurophysiology, and brain-computer interfaces (BCIs), yet pronounced inter- and intra-subject variability limits reliability, reproducibility, and translation. This systematic review studies that quantified or modeled EEG variability across resting-state, event-related potentials (ERPs), and task-related/BCI paradigms (including motor imagery and SSVEP) in healthy and clinical cohorts. Across paradigms, inter-subject differences are typically larger than within-subject fluctuations, but both affect inference and model generalization. Stability is feature-dependent: alpha-band measures and individual alpha peak frequency are often relatively reliable, whereas higher-frequency and many connectivity-derived metrics show more heterogeneous reliability; ERP reliability varies by component, with P300 measures frequently showing moderate-to-good stability. We summarize major sources of variability (biological, state-related, technical, and analytical), review common quantification and modeling approaches (e.g., ICC, CV, SNR, generalizability theory, and multivariate/learning-based methods), and provide recommendations for study design, reporting, and harmonization. Overall, EEG variability should be treated as both a practical constraint to manage and a meaningful signal to leverage for precision neuroscience and robust neurotechnology.",
      "code_url": null
    },
    "2601.21965v1": {
      "title": "Cognitive Load Estimation Using Brain Foundation Models and Interpretability for BCIs",
      "url": "http://arxiv.org/abs/2601.21965v1",
      "authors": "Deeksha M. Shama, Dimitra Emmanouilidou, Ivan J. Tashev",
      "update_time": "2026-01-29",
      "abstract": "Accurately monitoring cognitive load in real time is critical for Brain-Computer Interfaces (BCIs) that adapt to user engagement and support personalized learning. Electroencephalography (EEG) offers a non-invasive, cost-effective modality for capturing neural activity, though traditional methods often struggle with cross-subject variability and task-specific preprocessing. We propose leveraging Brain Foundation Models (BFMs), large pre-trained neural networks, to extract generalizable EEG features for cognitive load estimation. We adapt BFMs for long-term EEG monitoring and show that fine-tuning a small subset of layers yields improved accuracy over the state-of-the-art. Despite their scale, BFMs allow for real-time inference with a longer context window. To address often-overlooked interpretability challenges, we apply Partition SHAP (SHapley Additive exPlanations) to quantify feature importance. Our findings reveal consistent emphasis on prefrontal regions linked to cognitive control, while longitudinal trends suggest learning progression. These results position BFMs as efficient and interpretable tools for continuous cognitive load monitoring in real-world BCIs.",
      "code_url": null
    },
    "2601.21203v1": {
      "title": "Rethinking Self-Training Based Cross-Subject Domain Adaptation for SSVEP Classification",
      "url": "http://arxiv.org/abs/2601.21203v1",
      "authors": "Weiguang Wang, Yong Liu, Yingjie Gao, Guangyuan Xu",
      "update_time": "2026-01-29",
      "abstract": "Steady-state visually evoked potentials (SSVEP)-based brain-computer interfaces (BCIs) are widely used due to their high signal-to-noise ratio and user-friendliness. Accurate decoding of SSVEP signals is crucial for interpreting user intentions in BCI applications. However, signal variability across subjects and the costly user-specific annotation limit recognition performance. Therefore, we propose a novel cross-subject domain adaptation method built upon the self-training paradigm. Specifically, a Filter-Bank Euclidean Alignment (FBEA) strategy is designed to exploit frequency information from SSVEP filter banks. Then, we propose a Cross-Subject Self-Training (CSST) framework consisting of two stages: Pre-Training with Adversarial Learning (PTAL), which aligns the source and target distributions, and Dual-Ensemble Self-Training (DEST), which refines pseudo-label quality. Moreover, we introduce a Time-Frequency Augmented Contrastive Learning (TFA-CL) module to enhance feature discriminability across multiple augmented views. Extensive experiments on the Benchmark and BETA datasets demonstrate that our approach achieves state-of-the-art performance across varying signal lengths, highlighting its superiority.",
      "code_url": null
    },
    "2601.20447v1": {
      "title": "Assembling the Mind's Mosaic: Towards EEG Semantic Intent Decoding",
      "url": "http://arxiv.org/abs/2601.20447v1",
      "authors": "Jiahe Li, Junru Chen, Fanqi Shen, Jialan Yang, Jada Li, Zhizhang Yuan, Baowen Cheng, Meng Li, Yang Yang",
      "update_time": "2026-01-28",
      "abstract": "Enabling natural communication through brain-computer interfaces (BCIs) remains one of the most profound challenges in neuroscience and neurotechnology. While existing frameworks offer partial solutions, they are constrained by oversimplified semantic representations and a lack of interpretability. To overcome these limitations, we introduce Semantic Intent Decoding (SID), a novel framework that translates neural activity into natural language by modeling meaning as a flexible set of compositional semantic units. SID is built on three core principles: semantic compositionality, continuity and expandability of semantic space, and fidelity in reconstruction. We present BrainMosaic, a deep learning architecture implementing SID. BrainMosaic decodes multiple semantic units from EEG/SEEG signals using set matching and then reconstructs coherent sentences through semantic-guided reconstruction. This approach moves beyond traditional pipelines that rely on fixed-class classification or unconstrained generation, enabling a more interpretable and expressive communication paradigm. Extensive experiments on multilingual EEG and clinical SEEG datasets demonstrate that SID and BrainMosaic offer substantial advantages over existing frameworks, paving the way for natural and effective BCI-mediated communication.",
      "code_url": null
    },
    "2601.19963v1": {
      "title": "Cross-Session Decoding of Neural Spiking Data via Task-Conditioned Latent Alignment",
      "url": "http://arxiv.org/abs/2601.19963v1",
      "authors": "Canyang Zhao, Bolin Peng, J. Patrick Mayo, Ce Ju, Bing Liu",
      "update_time": "2026-01-27",
      "abstract": "Cross-session nonstationarity in neural activity recorded by implanted electrodes is a major challenge for invasive Brain-computer interfaces (BCIs), as decoders trained on data from one session often fail to generalize to subsequent sessions. This issue is further exacerbated in practice, as retraining or adapting decoders becomes particularly challenging when only limited data are available from a new session. To address this challenge, we propose a Task-Conditioned Latent Alignment framework (TCLA) for cross-session neural decoding. Building upon an autoencoder architecture, TCLA first learns a low-dimensional representation of neural dynamics from a source session with sufficient data. For target sessions with limited data, TCLA then aligns target latent representations to the source in a task-conditioned manner, enabling effective transfer of learned neural dynamics. We evaluate TCLA on the macaque motor and oculomotor center-out dataset. Compared to baseline methods trained solely on target-session data, TCLA consistently improves decoding performance across datasets and decoding settings, with gains in the coefficient of determination of up to 0.386 for y coordinate velocity decoding in a motor dataset. These results suggest that TCLA provides an effective strategy for transferring knowledge from source to target sessions, enabling more robust neural decoding under conditions with limited data.",
      "code_url": null
    },
    "2601.19269v1": {
      "title": "A Personalized and Adaptable User Interface for a Speech and Cursor Brain-Computer Interface",
      "url": "http://arxiv.org/abs/2601.19269v1",
      "authors": "Hamza Peracha, Carrina Iacobacci, Tyler Singer-Clark, Leigh R. Hochberg, Sergey D. Stavisky, David M. Brandman, Nicholas S. Card",
      "update_time": "2026-01-27",
      "abstract": "Communication and computer interaction are important for autonomy in modern life. Unfortunately, these capabilities can be limited or inaccessible for the millions of people living with paralysis. While implantable brain-computer interfaces (BCIs) show promise for restoring these capabilities, little has been explored on designing BCI user interfaces (UIs) for sustained daily use. Here, we present a personalized UI for an intracortical BCI system that enables users with severe paralysis to communicate and interact with their computers independently. Through a 22-month longitudinal deployment with one participant, we used iterative co-design to develop a system for everyday at-home use and documented how it evolved to meet changing needs. Our findings highlight how personalization and adaptability enabled independence in daily life and provide design implications for developing future BCI assistive technologies.",
      "code_url": null
    },
    "2601.17883v1": {
      "title": "EEG Foundation Models: Progresses, Benchmarking, and Open Problems",
      "url": "http://arxiv.org/abs/2601.17883v1",
      "authors": "Dingkun Liu, Yuheng Chen, Zhu Chen, Zhenyao Cui, Yaozhi Wen, Jiayu An, Jingwei Luo, Dongrui Wu",
      "update_time": "2026-01-25",
      "abstract": "Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.",
      "code_url": null
    },
    "2601.17625v1": {
      "title": "BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation",
      "url": "http://arxiv.org/abs/2601.17625v1",
      "authors": "Yuhan Xie, Jinhan Liu, Xiaoyong Ni, Fei Tan, Icare Sakr, Thibault Collin, Shiqi Sun, Alejandro Rodriguez Guajardo, Demon Fanny, Charles-francois Vincent Latchoumane, Henri Lorach, Jocelyne Bloch, Gregoire Courtine, Mahsa Shoaran",
      "update_time": "2026-01-24",
      "abstract": "Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce BrainDistill, a novel implantable motor decoding pipeline that integrates an implantable neural decoder (IND) with a task-specific knowledge distillation (TSKD) framework. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. Across multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.",
      "code_url": null
    },
    "2601.12279v1": {
      "title": "HCFT: Hierarchical Convolutional Fusion Transformer for EEG Decoding",
      "url": "http://arxiv.org/abs/2601.12279v1",
      "authors": "Haodong Zhang, Jiapeng Zhu, Yitong Chen, Hongqi Li",
      "update_time": "2026-01-18",
      "abstract": "Electroencephalography (EEG) decoding requires models that can effectively extract and integrate complex temporal, spectral, and spatial features from multichannel signals. To address this challenge, we propose a lightweight and generalizable decoding framework named Hierarchical Convolutional Fusion Transformer (HCFT), which combines dual-branch convolutional encoders and hierarchical Transformer blocks for multi-scale EEG representation learning. Specifically, the model first captures local temporal and spatiotemporal dynamics through time-domain and time-space convolutional branches, and then aligns these features via a cross-attention mechanism that enables interaction between branches at each stage. Subsequently, a hierarchical Transformer fusion structure is employed to encode global dependencies across all feature stages, while a customized Dynamic Tanh normalization module is introduced to replace traditional Layer Normalization in order to enhance training stability and reduce redundancy. Extensive experiments are conducted on two representative benchmark datasets, BCI Competition IV-2b and CHB-MIT, covering both event-related cross-subject classification and continuous seizure prediction tasks. Results show that HCFT achieves 80.83% average accuracy and a Cohen's kappa of 0.6165 on BCI IV-2b, as well as 99.10% sensitivity, 0.0236 false positives per hour, and 98.82% specificity on CHB-MIT, consistently outperforming over ten state-of-the-art baseline methods. Ablation studies confirm that each core component of the proposed framework contributes significantly to the overall decoding performance, demonstrating HCFT's effectiveness in capturing EEG dynamics and its potential for real-world BCI applications.",
      "code_url": null
    }
  },
  "fMRI": {
    "2602.04512v1": {
      "title": "BrainVista: Modeling Naturalistic Brain Dynamics as Multimodal Next-Token Prediction",
      "url": "http://arxiv.org/abs/2602.04512v1",
      "authors": "Xuanhua Yin, Runkai Zhao, Lina Yao, Weidong Cai",
      "update_time": "2026-02-04",
      "abstract": "Naturalistic fMRI characterizes the brain as a dynamic predictive engine driven by continuous sensory streams. However, modeling the causal forward evolution in realistic neural simulation is impeded by the timescale mismatch between multimodal inputs and the complex topology of cortical networks. To address these challenges, we introduce BrainVista, a multimodal autoregressive framework designed to model the causal evolution of brain states. BrainVista incorporates Network-wise Tokenizers to disentangle system-specific dynamics and a Spatial Mixer Head that captures inter-network information flow without compromising functional boundaries. Furthermore, we propose a novel Stimulus-to-Brain (S2B) masking mechanism to synchronize high-frequency sensory stimuli with hemodynamically filtered signals, enabling strict, history-only causal conditioning. We validate our framework on Algonauts 2025, CineBrain, and HAD, achieving state-of-the-art fMRI encoding performance. In long-horizon rollout settings, our model yields substantial improvements over baselines, increasing pattern correlation by 36.0\\% and 33.3\\% on relative to the strongest baseline Algonauts 2025 and CineBrain, respectively.",
      "code_url": null
    },
    "2602.04081v1": {
      "title": "Abstraction Induces the Brain Alignment of Language and Speech Models",
      "url": "http://arxiv.org/abs/2602.04081v1",
      "authors": "Emily Cheng, Aditya R. Vaidya, Richard Antonello",
      "update_time": "2026-02-03",
      "abstract": "Research has repeatedly demonstrated that intermediate hidden states extracted from large language models and speech audio models predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most effective for this unique and highly general transfer task? We give evidence that the correspondence between speech and language models and the brain derives from shared meaning abstraction and not their next-word prediction properties. In particular, models construct higher-order linguistic features in their middle layers, cued by a peak in the layerwise intrinsic dimension, a measure of feature complexity. We show that a layer's intrinsic dimension strongly predicts how well it explains fMRI and ECoG signals; that the relation between intrinsic dimension and brain predictivity arises over model pre-training; and finetuning models to better predict the brain causally increases both representations' intrinsic dimension and their semantic content. Results suggest that semantic richness, high intrinsic dimension, and brain predictivity mirror each other, and that the key driver of model-brain similarity is rich meaning abstraction of the inputs, where language modeling is a task sufficiently complex (but perhaps not the only) to require it.",
      "code_url": null
    },
    "2602.03437v1": {
      "title": "Accelerated Electromagnetic Simulation of MRI RF Interactions with Graphene Microtransistor-Based Neural Probes for Electrophysiology-fMRI Integration",
      "url": "http://arxiv.org/abs/2602.03437v1",
      "authors": "Suchit Kumar, Alejandro Labastida Ramirez, Samuel M Flaherty, Anton Guimera Brunet, Nerea Alvarez de Eulate, Kostas Kostarelos, Ben Dickie, Rob C Wykes, Louis Lemieux",
      "update_time": "2026-02-03",
      "abstract": "Implementing electrophysiological recordings within an MRI environment is challenging due to complex interactions between recording probes and MRI-generated fields, which can affect both safety and data quality. This study aims to develop and evaluate a hybrid electromagnetic (EM) simulation framework for efficient and accurate assessment of such interactions. Methods: A hybrid EM strategy integrating the Huygens' Box (HB) method with sub-gridding was implemented in an FDTD solver (Sim4Life). RF coil models for mouse and rat head were simulated with and without intracortical (IC) and epicortical (EC) graphene-based micro-transistor arrays. Three-dimensional multi-layered probe models were reconstructed from two-dimensional layouts, and transmit field ($B_{1}^{+}$), electric field ($E$), and specific absorption rate (SAR) distributions were evaluated. Performance was benchmarked against conventional full-wave multi-port (MP) simulations using Bland-Altman analysis and voxel-wise percentage differences. Results: HB simulations reduced computational time by approximately 70-80%, while preserving spatial patterns of $|B_{1}^{+}|$, $|E|$, and SAR, including transmit-field symmetry and localized high-field regions. Deviations from MP were minimal for $|B_{1}^{+}|$ (median $\u0394$% 0.02-0.07% in mice, -3.7% to -1.7% in rats) and modest for $|E|$ and SAR, with absolute SAR values remaining well below human safety limits. Graphene-based arrays produced negligible effects on RF transmission and SAR deposition. Conclusion: The HB approach enables computationally efficient, high-resolution evaluation of EM interactions involving microscopic probes in MRI environments, supporting simulations that are otherwise impractical with full-wave MP modeling.",
      "code_url": null
    },
    "2602.03278v1": {
      "title": "A Pipeline for ADNI Resting-State Functional MRI Processing and Quality Control",
      "url": "http://arxiv.org/abs/2602.03278v1",
      "authors": "Saige Rutherford, Zeshawn Zahid, Robert C. Welsh, Andrea Avena-Koenigsberger, Vincent Koppelmans, Amanda F. Mejia",
      "update_time": "2026-02-03",
      "abstract": "The Alzheimer's Disease Neuroimaging Initiative (ADNI) provides a comprehensive multimodal neuroimaging resource for studying aging and Alzheimer's disease (AD). Since its second wave, ADNI has increasingly collected resting-state functional MRI (rs-fMRI), a valuable resource for discovering brain connectivity changes predictive of cognitive decline and AD. A major barrier to its use is the considerable variability in acquisition protocols and data quality, compounded by missing imaging sessions and inconsistencies in how functional scans temporally align with clinical assessments. As a result, many studies only utilize a small subset of the total rs-fMRI data, limiting statistical power, reproducibility, and the ability to study longitudinal functional brain changes at scale. Here, we describe a pipeline for ADNI rs-fMRI data that encompasses the download of necessary imaging and clinical data, temporally aligning the clinical and imaging data, preprocessing, and quality control. We integrate data curation and preprocessing across all ADNI sites and scanner types using a combination of open-source software (Clinica, fMRIPrep, and MRIQC) and bespoke tools. Quality metrics and reports are generated for each subject and session to facilitate rigorous data screening. All scripts and configuration files are available to enable reproducibility. The pipeline, which currently supports ADNI-GO, ADNI-2, and ADNI-3 data releases, outputs high-quality rs-fMRI time series data adhering to the BIDS-derivatives specification. This protocol provides a transparent and scalable framework for curating and utilizing ADNI fMRI data, empowering large-scale functional biomarker discovery and integrative multimodal analyses in Alzheimer's disease research.",
      "code_url": null
    },
    "2602.03240v1": {
      "title": "Estimating measures of information processing during cognitive tasks using functional magnetic resonance imaging",
      "url": "http://arxiv.org/abs/2602.03240v1",
      "authors": "Chetan Gohil, Oliver M. Cliff, James M. Shine, Ben D. Fulcher, Joseph T. Lizier",
      "update_time": "2026-02-03",
      "abstract": "Cognition is increasingly framed in terms of information processing, yet most fMRI analyses focus on activation or functional connectivity rather than quantifying how information is stored and transferred. To remedy this problem, we propose a framework for estimating measures of information processing: active information storage (AIS), transfer entropy (TE), and net synergy from task-based fMRI. AIS measures information maintained within a region, TE captures directed information flow, and net synergy contrasts higher-order synergistic to redundant interactions. Crucially, to enable this framework we utilised a recently developed approach for calculating information-theoretic measures: the cross mutual information. This approach combines resting-state and task data to address the challenges of limited sample size, non-stationarity and context in task-based fMRI. We applied this framework to the working memory (N-back) task from the Human Connectome Project (470 participants). Results show that AIS increases in fronto-parietal regions with working memory load, TE reveals enhanced directed information flows across control pathways, and net synergy indicates a global shift to redundancy. This work establishes a novel methodology for quantifying information processing in task-based fMRI.",
      "code_url": null
    },
    "2602.01551v1": {
      "title": "Bayesian brain mapping: population-informed individualized functional topography and connectivity",
      "url": "http://arxiv.org/abs/2602.01551v1",
      "authors": "Nohelia Da Silva Sanchez, Diego Derman, Damon D. Pham, Ellyn R. Butler, Mary Beth Nebel, Amanda F. Mejia",
      "update_time": "2026-02-02",
      "abstract": "The spatial topography of brain functional organization is increasingly recognized to play an important role in cognition and disease. Accounting for individual differences in functional topography is also crucial for accurately distinguishing spatial and temporal aspects of brain organization. Yet, accurate estimation of individual functional brain networks from functional magnetic resonance imaging (fMRI) without extensive scanning remains challenging, due to low signal-to-noise ratio. Here, we describe Bayesian brain mapping (BBM), a technique for individual functional topography and connectivity leveraging population information. Population-derived priors for both spatial topography and functional connectivity based on existing spatial templates, such as parcellations or continuous network maps, are used to guide subject-level estimation and combat noise. BBM is highly flexible, avoiding strong spatial or temporal constraints and allowing for overlap between networks and heterogeneous patterns of engagement. Unlike multi-subject hierarchical models, BBM is designed for single-subject analysis, making it highly computationally efficient and translatable to clinical settings. Here, we describe the BBM model and illustrate the use of the BayesBrainMap R package to construct population-derived priors, fit the model, and perform inference to identify engagements. A demo is provided in an accompanying Github repo. We also share priors derived from the Human Connectome Project database and provide code to support the construction of priors from different data sources, lowering the barrier to adoption of BBM for studies of individual brain organization.",
      "code_url": null
    },
    "2601.23090v1": {
      "title": "Omni-fMRI: A Universal Atlas-Free fMRI Foundation Model",
      "url": "http://arxiv.org/abs/2601.23090v1",
      "authors": "Mo Wang, Wenhao Ye, Junfeng Xia, Junxiang Zhang, Xuanye Pan, Minghao Xu, Haotian Deng, Hongkai Wen, Quanying Liu",
      "update_time": "2026-01-30",
      "abstract": "Self-supervised fMRI foundation models have shown promising transfer performance, yet most rely on predefined region-level parcellations that discard fine-grained voxel information and introduce atlas-dependent biases. We propose Omni-fMRI, an atlas-free foundation model that operates directly on voxel-level signals. To enable scalable pretraining on 49,497 fMRI sessions across nine datasets, Omni-fMRI introduces a dynamic patching mechanism that substantially reduces computational cost while preserving informative spatial structure. To support reproducibility and fair comparison, we establish a comprehensive benchmark suite spanning 11 datasets and a diverse set of resting-state and task-based fMRI tasks. Experimental results demonstrate that Omni-fMRI consistently outperforms existing foundation models, providing a scalable and reproducible framework for atlas-free brain representation learning. Code and logs are available.",
      "code_url": null
    },
    "2601.17857v1": {
      "title": "SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction",
      "url": "http://arxiv.org/abs/2601.17857v1",
      "authors": "Lan Yang, Minghan Yang, Ke Li, Honggang Zhang, Kaiyue Pang, Yi-Zhe Song",
      "update_time": "2026-01-25",
      "abstract": "Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.",
      "code_url": null
    },
    "2601.16423v1": {
      "title": "Quantum Sensing MRI for Noninvasive Detection of Neuronal Electrical Activity in Human Brains",
      "url": "http://arxiv.org/abs/2601.16423v1",
      "authors": "Yongxian Qian, Ying-Chia Lin, Seyedehsara Hejazi, Kamri Clarke, Kennedy Watson, Xingye Chen, Nahbila-Malikha Kumbella, Justin Quimbo, Abena Dinizulu, Simon Henin, Yulin Ge, Arjun Masurkar, Anli Liu, Yvonne W. Lui, Fernando E. Boada",
      "update_time": "2026-01-23",
      "abstract": "Neuronal electrical activity underlies human cognition, yet its direct, noninvasive measurement in the living human brain remains a fundamental challenge. Existing neuroimaging techniques, including EEG, MEG, and fMRI, are limited by trade-offs in sensitivity and spatial or temporal resolution. Here we propose quantum sensing MRI (qsMRI), a noninvasive approach that enables direct detection of neuronal firing-induced magnetic fields using a clinical MRI system. qsMRI exploits endogenous proton (1H) nuclear spins in water molecules as intrinsic quantum sensors and decodes time-resolved phase information from free induction decay (FID) signals to infer neuronal magnetic fields. We validate qsMRI through simulations, phantom experiments, and human studies at rest and during motor tasks, and provide open experimental procedures to facilitate independent validation. We further present a case study demonstrating potential applications to neurological disorders. qsMRI represents a first-in-human application of quantum sensing on a clinical MRI platform, establishes a non-BOLD functional imaging modality, and enables interrogation of neuronal firing dynamics in both cortical and deep brain regions.",
      "code_url": null
    },
    "2602.02511v1": {
      "title": "Training Data Governance for Brain Foundation Models",
      "url": "http://arxiv.org/abs/2602.02511v1",
      "authors": "Margot Hanley, Jiunn-Tyng Yeh, Ryan Rodriguez, Jack Pilkington, Nita Farahany",
      "update_time": "2026-01-23",
      "abstract": "Brain foundation models bring the foundation model paradigm to the field of neuroscience. Like language and image foundation models, they are general-purpose AI systems pretrained on large-scale datasets that adapt readily to downstream tasks. Unlike text-and-image based models, however, they train on brain data: large-datasets of EEG, fMRI, and other neural data types historically collected within tightly governed clinical and research settings. This paper contends that training foundation models on neural data opens new normative territory. Neural data carry stronger expectations of, and claims to, protection than text or images, given their body-derived nature and historical governance within clinical and research settings. Yet the foundation model paradigm subjects them to practices of large-scale repurposing, cross-context stitching, and open-ended downstream application. Furthermore, these practices are now accessible to a much broader range of actors, including commercial developers, against a backdrop of fragmented and unclear governance. To map this territory, we first describe brain foundation models' technical foundations and training-data ecosystem. We then draw on AI ethics, neuroethics, and bioethics to organize concerns across privacy, consent, bias, benefit sharing, and governance. For each, we propose both agenda-setting questions and baseline safeguards as the field matures.",
      "code_url": null
    }
  },
  "MEG": {
    "2602.04168v1": {
      "title": "Charged lepton flavor violating decays $Z\\to \\ell_\u03b1\\ell_\u03b2$ in the inverse seesaw",
      "url": "http://arxiv.org/abs/2602.04168v1",
      "authors": "Adri\u00e1n Gonz\u00e1lez-Quiterio, H\u00e9ctor Novales-S\u00e1nchez",
      "update_time": "2026-02-04",
      "abstract": "After confirmation of massiveness and mixing of neutrinos, by neutrino oscillation data, the origin of neutrino mass and the occurrence of charged-lepton-flavor non-conservation in nature have become two main objectives for the physics of elementary particles. Taking inspiration from both matters, we address the decays $Z\\to\\ell_\u03b1\\ell_\u03b2$, with $\\ell_\u03b1\\ne\\ell_\u03b2$, thus violating charged-lepton flavor. We calculate the set of contributing one-loop diagrams characterized by virtual neutral leptons, both light and heavy, emerged from the inverse seesaw mechanism for the generation of neutrino mass. By neglecting charged-lepton and light-neutrino masses, and then assuming that the mass spectrum of the heavy neutral leptons is degenerate, we find that a relation $\\textrm{Br}\\big( Z\\to\\ell_\u03b1\\ell_\u03b2\\big)\\propto\\big| \u03b7_{\u03b2\u03b1} \\big|^2$, with $\u03b7$ the matrix describing non-unitarity effects in light-lepton mixing, is fulfilled. Our quantitative analysis, which considers both scenarios of degenerate and non-degenerate masses of heavy neutral leptons, takes into account upper bounds on $\u03b7_{\u03bce}$, imposed by current constraints on the decay $\u03bc\\to e\u03b3$ from the MEG II experiment, while projected future sensitivity of this experiment is considered as well. We find that, even though current constraints on $Z\\to\\ell_\u03b1\\ell_\u03b2$, by the ATLAS Collaboration, remain far from inverse-seesaw contributions, improved sensitivity from in-plans machines, such as the Future Circular Collider and the Circular Electron Positron Collider, shall be able to probe this mass-generating mechanism through these decays.",
      "code_url": null
    },
    "2602.03288v1": {
      "title": "An Algorithm for Monitoring Edge-geodetic Sets in Chordal Graphs",
      "url": "http://arxiv.org/abs/2602.03288v1",
      "authors": "Nacim Oijid, Clara Marcille",
      "update_time": "2026-02-03",
      "abstract": "A monitoring edge-geodetic set (or meg-set for short) of a graph is a set of vertices $M$ such that if any edge is removed, then the distance between some two vertices of $M$ increases. This notion was introduced by Foucaud et al. in 2023 as a way to monitor networks for communication failures. As computing a minimal meg-set is hard in general, recent works aimed to find polynomial-time algorithms to compute minimal meg-sets when the input belongs to a restricted class of graphs. Most of these results are based on the property of some classes of graphs to admit a unique minimum meg-set, which is then easy to compute. In this work, we prove that chordal graphs also admit a unique minimal meg-set, answering a standing open question of Foucaud et al.",
      "code_url": null
    },
    "2602.02494v1": {
      "title": "MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training",
      "url": "http://arxiv.org/abs/2602.02494v1",
      "authors": "Dulhan Jayalath, Oiwi Parker Jones",
      "update_time": "2026-02-02",
      "abstract": "Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .",
      "code_url": null
    },
    "2601.20138v2": {
      "title": "Scaling Next-Brain-Token Prediction for MEG",
      "url": "http://arxiv.org/abs/2601.20138v2",
      "authors": "Richard Csaky",
      "update_time": "2026-01-29",
      "abstract": "We present a large autoregressive model for source-space MEG that scales next-token prediction to long context across datasets and scanners: handling a corpus of over 500 hours and thousands of sessions across the three largest MEG datasets. A modified SEANet-style vector-quantizer reduces multichannel MEG into a flattened token stream on which we train a Qwen2.5-VL backbone from scratch to predict the next brain token and to recursively generate minutes of MEG from up to a minute of context. To evaluate long-horizon generation, we introduce task-matched tests: (i) on-manifold stability via generated-only drift compared to the time-resolved distribution of real sliding windows, and (ii) conditional specificity via correct context versus prompt-swap controls using a neurophysiologically grounded metric set. We train on CamCAN and Omega and run all analyses on held-out MOUS, establishing cross-dataset generalization. Across metrics, generations remain relatively stable over long rollouts and are closer to the correct continuation than swapped controls. Code available at: https://github.com/ricsinaruto/brain-gen.",
      "code_url": null
    },
    "2601.18792v1": {
      "title": "MEGnifying Emotion: Sentiment Analysis from Annotated Brain Data",
      "url": "http://arxiv.org/abs/2601.18792v1",
      "authors": "Brian Liu, Oiwi Parker Jones",
      "update_time": "2026-01-26",
      "abstract": "Decoding emotion from brain activity could unlock a deeper understanding of the human experience. While a number of existing datasets align brain data with speech and with speech transcripts, no datasets have annotated brain data with sentiment. To bridge this gap, we explore the use of pre-trained Text-to-Sentiment models to annotate non invasive brain recordings, acquired using magnetoencephalography (MEG), while participants listened to audiobooks. Having annotated the text, we employ force-alignment of the text and audio to align our sentiment labels with the brain recordings. It is straightforward then to train Brainto-Sentiment models on these data. Experimental results show an improvement in balanced accuracy for Brain-to-Sentiment compared to baseline, supporting the proposed approach as a proof-of-concept for leveraging existing MEG datasets and learning to decode sentiment directly from the brain.",
      "code_url": null
    },
    "2601.18843v1": {
      "title": "Human Cardiac Measurements with Diamond Magnetometers",
      "url": "http://arxiv.org/abs/2601.18843v1",
      "authors": "Muhib Omar, Magnus Benke, Shaowen Zhang, Jixing Zhang, Michael Kuebler, Pouya Sharbati, Ara Rahimpour, Arno Gueck, Maryna Kapitonova, Devyani Kadam, Carlos Rene Izquierdo Geiser, Jens Haller, Arno Trautmann, Katharina Jag-Lauber, Robert Roelver, Thanh-Duc Nguyen, Leonardo Gizzi, Michelle Schweizer, Mena Abdelsayed, Ingo Wickenbrock, Andrew M. Edmonds, Matthew Markham, Peter A. Koss, Oliver Schnell, Ulrich G. Hofmann, Tonio Ball, Juergen Beck, Dmitry Budker, Joerg Wrachtrup, Arne Wickenbrock",
      "update_time": "2026-01-26",
      "abstract": "We demonstrate direct, non-invasive and non-contact detection of human cardiac magnetic signals using quantum sensors based on nitrogen-vacancy (NV) centers in diamond. Three configurations were employed recording magnetocardiography (MCG) signals in various shielded and unshielded environments. The signals were averaged over a few hundreds up to several thousands of heart beats to detect the MCG traces. The compact room-temperature NV sensors exhibit sensitivities of 6-26 pT/Hz^(1/2) with active sensing volumes below 0.5 mm^3, defining the performance level of the demonstrated MCG measurements. While the present signals are obtained by averaging, this performance already indicates a clear path toward single-shot MCG sensing. To move beyond shielded environments toward practical clinical use, strong noise suppression is required. To this end, we implement NV-based gradiometry and achieve efficient common-mode noise rejection, enabled by the intrinsically small sensing volume of NV sensors. Together, these multi-platform results obtained across diverse magnetic environments provide a solid foundation for translating quantum sensors into human medical diagnostics such as MCG and magnetoencephalography (MEG).",
      "code_url": null
    },
    "2601.16423v1": {
      "title": "Quantum Sensing MRI for Noninvasive Detection of Neuronal Electrical Activity in Human Brains",
      "url": "http://arxiv.org/abs/2601.16423v1",
      "authors": "Yongxian Qian, Ying-Chia Lin, Seyedehsara Hejazi, Kamri Clarke, Kennedy Watson, Xingye Chen, Nahbila-Malikha Kumbella, Justin Quimbo, Abena Dinizulu, Simon Henin, Yulin Ge, Arjun Masurkar, Anli Liu, Yvonne W. Lui, Fernando E. Boada",
      "update_time": "2026-01-23",
      "abstract": "Neuronal electrical activity underlies human cognition, yet its direct, noninvasive measurement in the living human brain remains a fundamental challenge. Existing neuroimaging techniques, including EEG, MEG, and fMRI, are limited by trade-offs in sensitivity and spatial or temporal resolution. Here we propose quantum sensing MRI (qsMRI), a noninvasive approach that enables direct detection of neuronal firing-induced magnetic fields using a clinical MRI system. qsMRI exploits endogenous proton (1H) nuclear spins in water molecules as intrinsic quantum sensors and decodes time-resolved phase information from free induction decay (FID) signals to infer neuronal magnetic fields. We validate qsMRI through simulations, phantom experiments, and human studies at rest and during motor tasks, and provide open experimental procedures to facilitate independent validation. We further present a case study demonstrating potential applications to neurological disorders. qsMRI represents a first-in-human application of quantum sensing on a clinical MRI platform, establishes a non-BOLD functional imaging modality, and enables interrogation of neuronal firing dynamics in both cortical and deep brain regions.",
      "code_url": null
    },
    "2601.15909v1": {
      "title": "Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech",
      "url": "http://arxiv.org/abs/2601.15909v1",
      "authors": "Soufiane Jhilal, St\u00e9phanie Martin, Anne-Lise Giraud",
      "update_time": "2026-01-22",
      "abstract": "Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.",
      "code_url": null
    },
    "2601.05923v1": {
      "title": "Cedalion Tutorial: A Python-based framework for comprehensive analysis of multimodal fNIRS & DOT from the lab to the everyday world",
      "url": "http://arxiv.org/abs/2601.05923v1",
      "authors": "E. Middell, L. Carlton, S. Moradi, T. Codina, T. Fischer, J. Cutler, S. Kelley, J. Behrendt, T. Dissanayake, N. Harmening, M. A. Y\u00fccel, D. A. Boas, A. von L\u00fchmann",
      "update_time": "2026-01-09",
      "abstract": "Functional near-infrared spectroscopy (fNIRS) and diffuse optical tomography (DOT) are rapidly evolving toward wearable, multimodal, and data-driven, AI-supported neuroimaging in the everyday world. However, current analytical tools are fragmented across platforms, limiting reproducibility, interoperability, and integration with modern machine learning (ML) workflows. Cedalion is a Python-based open-source framework designed to unify advanced model-based and data-driven analysis of multimodal fNIRS and DOT data within a reproducible, extensible, and community-driven environment. Cedalion integrates forward modelling, photogrammetric optode co-registration, signal processing, GLM Analysis, DOT image reconstruction, and ML-based data-driven methods within a single standardized architecture based on the Python ecosystem. It adheres to SNIRF and BIDS standards, supports cloud-executable Jupyter notebooks, and provides containerized workflows for scalable, fully reproducible analysis pipelines that can be provided alongside original research publications. Cedalion connects established optical-neuroimaging pipelines with ML frameworks such as scikit-learn and PyTorch, enabling seamless multimodal fusion with EEG, MEG, and physiological data. It implements validated algorithms for signal-quality assessment, motion correction, GLM modelling, and DOT reconstruction, complemented by modules for simulation, data augmentation, and multimodal physiology analysis. Automated documentation links each method to its source publication, and continuous-integration testing ensures robustness. This tutorial paper provides seven fully executable notebooks that demonstrate core features. Cedalion offers an open, transparent, and community extensible foundation that supports reproducible, scalable, cloud- and ML-ready fNIRS/DOT workflows for laboratory-based and real-world neuroimaging.",
      "code_url": null
    },
    "2601.00723v1": {
      "title": "Nematic-fluctuation-mediated superconductivity in CuxTiSe2",
      "url": "http://arxiv.org/abs/2601.00723v1",
      "authors": "Xingyu Lv, Yang Fu, Shangjie Tian, Ying Ma, Shouguo Wang, Cedomir Petrovic, Xiao Zhang, Hechang Lei",
      "update_time": "2026-01-02",
      "abstract": "The interplay among electronic nematicity, charge density wave, and superconductivity in correlated electronic systems has induced extensive research interest. Here, we discover the existence of nematic fluctuations in TiSe2 single crystal and investigate its evolution with Cu intercalation. It is observed that the elastoresistivity coefficient mEg exhibits a divergent temperature dependence following a Curie-Weiss law at high temperature. Upon Cu intercalation, the characteristic temperature T* of nematic fluctuation is progressively suppressed and becomes near zero when the superconductivity is optimized. Further intercalation of Cu leads to the sign change of T* and the suppression of superconductivity. These results strongly indicate that nematic phase transition may play a vital role in enhancing superconductivity in CuxTiSe2. Therefore, CuxTiSe2 provides a unique material platform to explore the nematic-fluctuation-mediated superconductivity.",
      "code_url": null
    }
  },
  "neuroAI": {
    "2602.01546v1": {
      "title": "NeuroAI Temporal Neural Networks (NeuTNNs): Microarchitecture and Design Framework for Specialized Neuromorphic Processing Units",
      "url": "http://arxiv.org/abs/2602.01546v1",
      "authors": "Shanmuga Venkatachalam, Prabhu Vellaisamy, Harideep Nair, Wei-Che Huang, Youngseok Na, Yuyang Kang, Quinn Jacobson, John Paul Shen",
      "update_time": "2026-02-02",
      "abstract": "Leading experts from both communities have suggested the need to (re)connect research in neuroscience and artificial intelligence (AI) to accelerate the development of next-generation AI innovations. They term this convergence as NeuroAI. Previous research has established temporal neural networks (TNNs) as a promising neuromorphic approach toward biological intelligence and efficiency. We fully embrace NeuroAI and propose a new category of TNNs we call NeuroAI TNNs (NeuTNNs) with greater capability and hardware efficiency by adopting neuroscience findings, including a neuron model with active dendrites and a hierarchy of distal and proximal segments. This work introduces a PyTorch-to-layout tool suite (NeuTNNGen) to design application-specific NeuTNNs. Compared to previous TNN designs, NeuTNNs achieve superior performance and efficiency. We demonstrate NeuTNNGen's capabilities using three example applications: 1) UCR time series benchmarks, 2) MNIST design exploration, and 3) Place Cells design for neocortical reference frames. We also explore using synaptic pruning to further reduce synapse counts and hardware costs by 30-50% while maintaining model precision across diverse sensory modalities. NeuTNNGen can facilitate the design of application-specific energy-efficient NeuTNNs for the next generation of NeuroAI computing systems.",
      "code_url": null
    },
    "2602.01503v1": {
      "title": "Governance at the Edge of Architecture: Regulating NeuroAI and Neuromorphic Systems",
      "url": "http://arxiv.org/abs/2602.01503v1",
      "authors": "Afifah Kashif, Abdul Muhsin Hameed, Asim Iqbal",
      "update_time": "2026-02-02",
      "abstract": "Current AI governance frameworks, including regulatory benchmarks for accuracy, latency, and energy efficiency, are built for static, centrally trained artificial neural networks on von Neumann hardware. NeuroAI systems, embodied in neuromorphic hardware and implemented via spiking neural networks, break these assumptions. This paper examines the limitations of current AI governance frameworks for NeuroAI, arguing that assurance and audit methods must co-evolve with these architectures, aligning traditional regulatory metrics with the physics, learning dynamics, and embodied efficiency of brain-inspired computation to enable technically grounded assurance.",
      "code_url": null
    },
    "2601.19955v1": {
      "title": "NeuroAI and Beyond",
      "url": "http://arxiv.org/abs/2601.19955v1",
      "authors": "Jean-Marc Fellous, Gert Cauwenberghs, Cornelia Ferm\u00fcller, Yulia Sandamisrkaya, Terrence Sejnowski",
      "update_time": "2026-01-27",
      "abstract": "Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.",
      "code_url": null
    },
    "2511.19548v1": {
      "title": "When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics",
      "url": "http://arxiv.org/abs/2511.19548v1",
      "authors": "Yiven, Zhu",
      "update_time": "2025-11-24",
      "abstract": "Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, \"brain-based\" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies \"true\" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.",
      "code_url": null
    },
    "2510.22178v1": {
      "title": "Dopamine-driven synaptic credit assignment in neural networks",
      "url": "http://arxiv.org/abs/2510.22178v1",
      "authors": "Saranraj Nambusubramaniyan, Shervin Safavi, Raja Guru, Andreas Knoblauch",
      "update_time": "2025-10-25",
      "abstract": "Solving the synaptic Credit Assignment Problem(CAP) is central to learning in both biological and artificial neural systems. Finding an optimal solution for synaptic CAP means setting the synaptic weights that assign credit to each neuron for influencing the final output and behavior of neural networks or animals. Gradient-based methods solve this problem in artificial neural networks using back-propagation, however, not in the most efficient way. For instance, back-propagation requires a chain of top-down gradient computations. This leads to an expensive optimization process in terms of computing power and memory linked with well-known weight transport and update locking problems. To address these shortcomings, we take a NeuroAI approach and draw inspiration from neural Reinforcement Learning to develop a derivative-free optimizer for training neural networks, Dopamine. Dopamine is developed for Weight Perturbation (WP) learning that exploits stochastic updating of weights towards optima. It achieves this by minimizing the regret, a form of Reward Prediction Error (RPE) between the expected outcome from the perturbed model and the actual outcome from the unperturbed model. We use this RPE to adjust the learning rate in the network (i.e., creating an adaptive learning rate strategy, similar to the role of dopamine in the brain). We tested the Dopamine optimizer for training multi-layered perceptrons for XOR tasks, and recurrent neural networks for chaotic time series forecasting. Dopamine-trained models demonstrate accelerated convergence and outperform standard WP, and give comparable performance to gradient-based algorithms, while consuming significantly less computation and memory. Overall, the Dopamine optimizer not only finds robust solutions and comparable performance to the state-of-the-art Machine Learning optimizers but is also neurobiologically more plausible.",
      "code_url": null
    },
    "2509.23896v2": {
      "title": "A Computational Perspective on NeuroAI and Synthetic Biological Intelligence",
      "url": "http://arxiv.org/abs/2509.23896v2",
      "authors": "Dhruvik Patel, Md Sayed Tanveer, Jesus Gonzalez-Ferrer, Alon Loeffler, Brett J. Kagan, Mohammed A. Mostajo-Radji, Ge Wang",
      "update_time": "2025-10-09",
      "abstract": "NeuroAI is an emerging field at the intersection of neuroscience and artificial intelligence, where insights from brain function guide the design of intelligent systems. A central area within this field is synthetic biological intelligence (SBI), which combines the adaptive learning properties of biological neural networks with engineered hardware and software. SBI systems provide a platform for modeling neural computation, developing biohybrid architectures, and enabling new forms of embodied intelligence. In this review, we organize the NeuroAI landscape into three interacting domains: hardware, software, and wetware. We outline computational frameworks that integrate biological and non-biological systems and highlight recent advances in organoid intelligence, neuromorphic computing, and neuro-symbolic learning. These developments collectively point toward a new class of systems that compute through interactions between living neural tissue and digital algorithms.",
      "code_url": null
    },
    "2507.06645v2": {
      "title": "Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers",
      "url": "http://arxiv.org/abs/2507.06645v2",
      "authors": "Thomas Klein, Sascha Meyen, Wieland Brendel, Felix A. Wichmann, Kristof Meding",
      "update_time": "2025-11-07",
      "abstract": "Benchmarking models is a key factor for the rapid progress in machine learning (ML) research. Thus, further progress depends on improving benchmarking metrics. A standard metric to measure the behavioral alignment between ML models and human observers is error consistency (EC). EC allows for more fine-grained comparisons of behavior than other metrics such as accuracy, and has been used in the influential Brain-Score benchmark to rank different DNNs by their behavioral consistency with humans. Previously, EC values have been reported without confidence intervals. However, empirically measured EC values are typically noisy -- thus, without confidence intervals, valid benchmarking conclusions are problematic. Here we improve on standard EC in two ways: First, we show how to obtain confidence intervals for EC using a bootstrapping technique, allowing us to derive significance tests for EC. Second, we propose a new computational model relating the EC between two classifiers to the implicit probability that one of them copies responses from the other. This view of EC allows us to give practical guidance to scientists regarding the number of trials required for sufficiently powerful, conclusive experiments. Finally, we use our methodology to revisit popular NeuroAI-results. We find that while the general trend of behavioral differences between humans and machines holds up to scrutiny, many reported differences between deep vision models are statistically insignificant. Our methodology enables researchers to design adequately powered experiments that can reliably detect behavioral differences between models, providing a foundation for more rigorous benchmarking of behavioral alignment.",
      "code_url": null
    },
    "2507.02103v1": {
      "title": "What Neuroscience Can Teach AI About Learning in Continuously Changing Environments",
      "url": "http://arxiv.org/abs/2507.02103v1",
      "authors": "Daniel Durstewitz, Bruno Averbeck, Georgia Koppe",
      "update_time": "2025-07-02",
      "abstract": "Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.",
      "code_url": null
    },
    "2506.04536v4": {
      "title": "NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models",
      "url": "http://arxiv.org/abs/2506.04536v4",
      "authors": "Luca Ghafourpour, Valentin Duruisseaux, Bahareh Tolooshams, Philip H. Wong, Costas A. Anastassiou, Anima Anandkumar",
      "update_time": "2026-02-03",
      "abstract": "Characterizing the cellular properties of neurons is fundamental to understanding their function in the brain. In this quest, the generation of bio-realistic models is central towards integrating multimodal cellular data sets and establishing causal relationships. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. The deterministic formalism of bio-realistic models currently precludes accounting for the natural variability observed experimentally. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on synthetic data generated from bio-realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE enables the efficient generation of synthetic neurons that closely resemble experimental data and exhibit trial-to-trial variability, offering a $4200\\times$ speedup over the numerical solver. NOBLE is the first scaled-up deep learning framework that validates its generalization with real experimental data. To this end, NOBLE captures fundamental neural properties in a unique and emergent manner that opens the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.",
      "code_url": null
    },
    "2506.11062v2": {
      "title": "Decoding Cortical Microcircuits: A Generative Model for Latent Space Exploration and Controlled Synthesis",
      "url": "http://arxiv.org/abs/2506.11062v2",
      "authors": "Xingyu Liu, Yubin Li, Guozhang Chen",
      "update_time": "2026-01-27",
      "abstract": "A central idea in understanding brains and building artificial intelligence is that structure determines function. Yet, how the brain's complex structure arises from a limited set of genetic instructions remains a key question. The ultra high-dimensional detail of neural connections vastly exceeds the information storage capacity of genes, suggesting a compact, low-dimensional blueprint must guide brain development. Our motivation is to uncover this blueprint. We introduce a generative model, to learn this underlying representation from detailed connectivity maps of mouse cortical microcircuits. Our model successfully captures the essential structural information of these circuits in a compressed latent space. We found that specific, interpretable directions within this space directly relate to understandable network properties. Building on this, we demonstrate a novel method to controllably generate new, synthetic microcircuits with desired structural features by navigating this latent space. This work offers a new way to investigate the design principles of neural circuits and explore how structure gives rise to function, potentially informing the development of more advanced artificial neural networks.",
      "code_url": null
    }
  },
  "medical": {
    "2602.04813v1": {
      "title": "Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents",
      "url": "http://arxiv.org/abs/2602.04813v1",
      "authors": "Shubham Vatsal, Harsh Dubey, Aditi Singh",
      "update_time": "2026-02-04",
      "abstract": "Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).",
      "code_url": null
    },
    "2602.04731v1": {
      "title": "Less Finetuning, Better Retrieval: Rethinking LLM Adaptation for Biomedical Retrievers via Synthetic Data and Model Merging",
      "url": "http://arxiv.org/abs/2602.04731v1",
      "authors": "Sameh Khattab, Jean-Philippe Corbeil, Osman Alperen Kora\u015f, Amin Dada, Julian Friedrich, Fran\u00e7ois Beaulieu, Paul Vozila, Jens Kleesiek",
      "update_time": "2026-02-04",
      "abstract": "Retrieval-augmented generation (RAG) has become the backbone of grounding Large Language Models (LLMs), improving knowledge updates and reducing hallucinations. Recently, LLM-based retriever models have shown state-of-the-art performance for RAG applications. However, several technical aspects remain underexplored on how to adapt general-purpose LLMs into effective domain-specific retrievers, especially in specialized domains such as biomedicine. We present Synthesize-Train-Merge (STM), a modular framework that enhances decoder-only LLMs with synthetic hard negatives, retrieval prompt optimization, and model merging. Experiments on a subset of 12 medical and general tasks from the MTEB benchmark show STM boosts task-specific experts by up to 23.5\\% (average 7.5\\%) and produces merged models that outperform both single experts and strong baselines without extensive pretraining. Our results demonstrate a scalable, efficient path for turning general LLMs into high-performing, domain-specialized retrievers, preserving general-domain capabilities while excelling on specialized tasks.",
      "code_url": null
    },
    "2602.04624v1": {
      "title": "A labeled dataset of simulated phlebotomy procedures for medical AI: polygon annotations for object detection and human-object interaction",
      "url": "http://arxiv.org/abs/2602.04624v1",
      "authors": "Ra\u00fal Jim\u00e9nez Cruz, C\u00e9sar Torres-Huitzil, Marco Franceschetti, Ronny Seiger, Luciano Garc\u00eda-Ba\u00f1uelos, Barbara Weber",
      "update_time": "2026-02-04",
      "abstract": "This data article presents a dataset of 11,884 labeled images documenting a simulated blood extraction (phlebotomy) procedure performed on a training arm. Images were extracted from high-definition videos recorded under controlled conditions and curated to reduce redundancy using Structural Similarity Index Measure (SSIM) filtering. An automated face-anonymization step was applied to all videos prior to frame selection. Each image contains polygon annotations for five medically relevant classes: syringe, rubber band, disinfectant wipe, gloves, and training arm. The annotations were exported in a segmentation format compatible with modern object detection frameworks (e.g., YOLOv8), ensuring broad usability. This dataset is partitioned into training (70%), validation (15%), and test (15%) subsets and is designed to advance research in medical training automation and human-object interaction. It enables multiple applications, including phlebotomy tool detection, procedural step recognition, workflow analysis, conformance checking, and the development of educational systems that provide structured feedback to medical trainees. The data and accompanying label files are publicly available on Zenodo.",
      "code_url": null
    },
    "2602.04618v1": {
      "title": "HoloHema: Digital Holographic Hematology Analyzer",
      "url": "http://arxiv.org/abs/2602.04618v1",
      "authors": "Andreas Erik Gejl Madsen",
      "update_time": "2026-02-04",
      "abstract": "This industrial Ph.D. project, carried out in collaboration between Radiometer Medical ApS and SDU Centre for Photonics Engineering at the University of Southern Denmark, explored the use of digital holographic microscopy (DHM) for the purposes of differential white blood cell counts (dWBCs) in point-of-care (PoC) devices for acute care settings. Two DHM prototypes were developed; an initial lens-based system serving as the foundation for algorithm development, and experimental validation of the approach, achieving 89.6% classification accuracy on a 3-part differential, and a subsequent lensless system for simplified design and increased field-of-view (FoV). Both prototypes employed convolutional neural networks (CNNs) for cell classification. With further optimizations, the lensless system achieved classification accuracies of 92.65% and 89.44% on the 3-part and 5-part differential, respectively. With the lensless system, the derivation of the monocyte distribution width (MDW), a biomarker for sepsis, was also demonstrated.   Additionally, pixel super-resolution and multi-wavelength DHM approaches were investigated to enhance the obtained cell information. Finally, a proof-of-principle physics-informed neural network (PINN) for holographic reconstruction was implemented, demonstrating the potential for machine learning (ML) reconstruction techniques.   In summary, this work represents an initial exploration of DHM for dWBC in PoC devices, laying the groundwork for future research.",
      "code_url": null
    },
    "2602.04617v1": {
      "title": "LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation",
      "url": "http://arxiv.org/abs/2602.04617v1",
      "authors": "Ruixiao Yang, Yuanhe Tian, Xu Yang, Huiqi Li, Yan Song",
      "update_time": "2026-02-04",
      "abstract": "Radiology Report Generation (RRG) aims to produce accurate and coherent diagnostics from medical images. Although large vision language models (LVLM) improve report fluency and accuracy, they exhibit hallucinations, generating plausible yet image-ungrounded pathological details. Existing methods primarily rely on external knowledge guidance to facilitate the alignment between generated text and visual information. However, these approaches often ignore the inherent decoding priors and vision-language alignment biases in pretrained models and lack robustness due to reliance on constructed guidance. In this paper, we propose Layer-wise Expert-aligned Decoding (LEAD), a novel method to inherently modify the LVLM decoding trajectory. A multiple experts module is designed for extracting distinct pathological features which are integrated into each decoder layer via a gating mechanism. This layer-wise architecture enables the LLM to consult expert features at every inference step via a learned gating function, thereby dynamically rectifying decoding biases and steering the generation toward factual consistency. Experiments conducted on multiple public datasets demonstrate that the LEAD method yields effective improvements in clinical accuracy metrics and mitigates hallucinations while preserving high generation quality.",
      "code_url": null
    },
    "2602.04547v1": {
      "title": "OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis",
      "url": "http://arxiv.org/abs/2602.04547v1",
      "authors": "Luca Zedda, Andrea Loddo, Cecilia Di Ruberto",
      "update_time": "2026-02-04",
      "abstract": "Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation.",
      "code_url": null
    },
    "2602.04461v1": {
      "title": "Human-organ-scale x-ray fluorescence ghost imaging for radioisotope-free diagnostics",
      "url": "http://arxiv.org/abs/2602.04461v1",
      "authors": "E. Levinson, R. H. Shukrun, N. Vigano, S. Shwartz",
      "update_time": "2026-02-04",
      "abstract": "A wide range of diagnostic information in medicine is currently obtained using radioactive tracers. While central to nuclear medicine, these methods are inherently constrained: radiation dose limits repeat examinations, short tracer half-lives and complex logistics restrict access and raise costs, and their relatively poor spatial resolution often necessitates complementary CT or MRI. Here we present a first proof-of-concept demonstration of a non-radioactive alternative based on x-ray fluorescence (XRF) computational ghost imaging (CGI) at the human-organ scale. Using a thyroid phantom filled with iodine solution as a model system, we show that structured illuminations combined with fluorescence detection reconstruct the iodine distribution with high fidelity. This approach eliminates the need for radioactive tracers while preserving image quality, and in principle can reach spatial resolution comparable to CT. Beyond this demonstration, XRF-CGI establishes a generalizable framework for non-radioactive tracer imaging, opening a route toward safer, repeatable, and more accessible diagnostics.",
      "code_url": null
    },
    "2602.04416v1": {
      "title": "Med-MMFL: A Multimodal Federated Learning Benchmark in Healthcare",
      "url": "http://arxiv.org/abs/2602.04416v1",
      "authors": "Aavash Chhetri, Bibek Niroula, Pratik Shrestha, Yash Raj Shrestha, Lesley A Anderson, Prashnna K Gyawali, Loris Bazzani, Binod Bhattarai",
      "update_time": "2026-02-04",
      "abstract": "Federated learning (FL) enables collaborative model training across decentralized medical institutions while preserving data privacy. However, medical FL benchmarks remain scarce, with existing efforts focusing mainly on unimodal or bimodal modalities and a limited range of medical tasks. This gap underscores the need for standardized evaluation to advance systematic understanding in medical MultiModal FL (MMFL). To this end, we introduce Med-MMFL, the first comprehensive MMFL benchmark for the medical domain, encompassing diverse modalities, tasks, and federation scenarios. Our benchmark evaluates six representative state-of-the-art FL algorithms, covering different aggregation strategies, loss formulations, and regularization techniques. It spans datasets with 2 to 4 modalities, comprising a total of 10 unique medical modalities, including text, pathology images, ECG, X-ray, radiology reports, and multiple MRI sequences. Experiments are conducted across naturally federated, synthetic IID, and synthetic non-IID settings to simulate real-world heterogeneity. We assess segmentation, classification, modality alignment (retrieval), and VQA tasks. To support reproducibility and fair comparison of future multimodal federated learning (MMFL) methods under realistic medical settings, we release the complete benchmark implementation, including data processing and partitioning pipelines, at https://github.com/bhattarailab/Med-MMFL-Benchmark .",
      "code_url": null
    },
    "2602.04400v1": {
      "title": "Unit Shiha Distribution and its Applications to Engineering and Medical Data",
      "url": "http://arxiv.org/abs/2602.04400v1",
      "authors": "F. A. Shiha",
      "update_time": "2026-02-04",
      "abstract": "There is a growing need for flexible statistical distributions that can accurately model data defined on the unit interval. This paper introduces a new unit distribution, termed the unit Shiha (USh) distribution, which is derived from the original Shiha (Sh) distribution through an inverse exponential transformation. The probability density function of the USh distribution is sufficiently flexible to model both left- and right-skewed data, while its hazard rate function is capable of capturing various failure-rate patterns, including increasing, bathtub-shaped, and J-shaped forms. Several statistical properties of the proposed distribution are investigated, including moments and related measures, the quantile function, entropy, and stress-strength reliability. Parameter estimation is carried out using the maximum likelihood method, and its performance is evaluated through a simulation study. The practical usefulness of the USh distribution is demonstrated using four real-life data sets, and its performance is compared with several well-known competing unit distributions. The comparative results indicate that the proposed model fits the data better than the competitive models applied in this study.",
      "code_url": null
    },
    "2602.04375v1": {
      "title": "Angle dependent dose transformer algorithm for fast proton therapy dose calculations",
      "url": "http://arxiv.org/abs/2602.04375v1",
      "authors": "Miko\u0142aj Stryja, Danny Lathouwers, Zolt\u00e1n Perk\u00f3",
      "update_time": "2026-02-04",
      "abstract": "Accurate 3D dose calculation for Pencil Beam Scanning Proton Therapy (PBSPT) is typically performed with Monte Carlo (MC) engines, but their runtimes limit adaptive workflows and repeated evaluations. Current deep-learning proton dose engines often require orthogonality between proton rays and the CT grid, forcing computationally expensive beamlet-wise 3D reinterpolation. We propose the Angle-dependent Dose Transformer Algorithm (ADoTA), which eliminates grid rotation by augmenting the model input with a fast analytical beamlet-shape projection that explicitly encodes beam direction. The model was trained on CT data from 108 patients to predict beamlet dose distributions for initial energies of $70$--$270\\,\\mathrm{MeV}$ over an $80\\times110\\,\\mathrm{mm}^2$ field, and tested on an independent cohort of 50 patients. On the test set, gamma pass rates $(1\\%,3\\,\\mathrm{mm})$ were $99.40\\pm0.86\\%$ (thorax) and $99.87\\pm0.23\\%$ (abdomen/pelvis). Single-beamlet inference took $1.72\\pm0.8\\,\\mathrm{ms}$. By avoiding reinterpolation, end-to-end 3D dose computation was reduced by $\\approx86\\%$ relative to the fastest published reinterpolation-based methods. For full treatment plans, gamma pass rates $\u0393(2\\%,2\\,\\mathrm{mm})$ with a 10\\% dose cut-off reached $98.4\\%$ (lung) and $98.9\\%$ (prostate). ADoTA provides an angle-aware deep-learning proton dose engine that preserves MC-level accuracy across heterogeneous anatomies while substantially reducing computational overhead.",
      "code_url": null
    }
  }
}