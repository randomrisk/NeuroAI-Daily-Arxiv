{
  "Brain": {
    "2508.08973v1": {
      "title": "Millisecond-scale Volatile Memory in HZO Ferroelectric Capacitors for Bio-inspired Temporal Computing",
      "url": "http://arxiv.org/abs/2508.08973v1",
      "authors": "Luca Fehlings, Thomas Mikolajick, Beatriz Noheda, Erika Covi",
      "update_time": "2025-08-12",
      "abstract": "With the broad recent research on ferroelectric hafnium oxide for non-volatile memory technology, depolarization effects in HfO2-based ferroelectric devices gained a lot of interest. Understanding the physical mechanisms regulating the retention of these devices provides an excellent opportunity for device optimization both towards non-volatile memory applications and towards real-time signal processing applications in which controlled time constants are of paramount importance. Indeed, we argue that ferroelectric devices, particularly HfO2-based, are an elegant solution to realize possibly arbitrary time constants in a single scaled memory device, which paves the way for temporal and brain-inspired computing in hardware. Here we present a ferroelectric capacitor stack realizing volatile memory due to its unique interface configuration. We provide electrical characterization of the device to motivate its use for realizing time constants in hardware, followed by an investigation of the electronic mechanisms and their possible relation to the observed retention times to facilitate further modeling of the retention process in HfO2-based ferroelectric capacitors. In the presented device, internal electric fields stabilize one polarization of the ferroelectric film, opening the possibility for unipolar operation with millisecond retention for the unstable polarization state. We show a dependence of the retention on both the polarization as well as the electrical stimuli, allowing us to exploit a range of time scales in a single device. Further, the intentionally defective interface in the presented material stack allows an insight into the interplay between retention loss in HfO2-based ferroelectric devices and the internal bias field, which we relate to the interface composition and the role of oxygen vacancies as a possible source of the internal bias fields.",
      "code_url": null
    },
    "2508.08762v1": {
      "title": "Bio-Inspired Artificial Neural Networks based on Predictive Coding",
      "url": "http://arxiv.org/abs/2508.08762v1",
      "authors": "Davide Casnici, Charlotte Frenkel, Justin Dauwels",
      "update_time": "2025-08-12",
      "abstract": "Backpropagation (BP) of errors is the backbone training algorithm for artificial neural networks (ANNs). It updates network weights through gradient descent to minimize a loss function representing the mismatch between predictions and desired outputs. BP uses the chain rule to propagate the loss gradient backward through the network hierarchy, allowing efficient weight updates. However, this process requires weight updates at every layer to rely on a global error signal generated at the network's output.   In contrast, the Hebbian model of synaptic plasticity states that weight updates are local, depending only on the activity of pre- and post-synaptic neurons. This suggests biological brains likely do not implement BP directly. Recently, Predictive Coding (PC) has gained interest as a biologically plausible alternative that updates weights using only local information. Originating from 1950s work on signal compression, PC was later proposed as a model of the visual cortex and formalized under the free energy principle, linking it to Bayesian inference and dynamical systems. PC weight updates rely solely on local information and provide theoretical advantages such as automatic scaling of gradients based on uncertainty.   This lecture notes column offers a novel, tutorial-style introduction to PC, focusing on its formulation, derivation, and connections to well-known optimization and signal processing algorithms such as BP and the Kalman Filter (KF). It aims to support existing literature by guiding readers from the mathematical foundations of PC to practical implementation, including Python examples using PyTorch.",
      "code_url": null
    },
    "2508.08681v1": {
      "title": "Multi-dimensional Neural Decoding with Orthogonal Representations for Brain-Computer Interfaces",
      "url": "http://arxiv.org/abs/2508.08681v1",
      "authors": "Kaixi Tian, Shengjia Zhao, Yuhan Zhang, Shan Yu",
      "update_time": "2025-08-12",
      "abstract": "Current brain-computer interfaces primarily decode single motor variables, limiting their ability to support natural, high-bandwidth neural control that requires simultaneous extraction of multiple correlated motor dimensions. We introduce Multi-dimensional Neural Decoding (MND), a task formulation that simultaneously extracts multiple motor variables (direction, position, velocity, acceleration) from single neural population recordings. MND faces two key challenges: cross-task interference when decoding correlated motor dimensions from shared cortical representations, and generalization issues across sessions, subjects, and paradigms. To address these challenges, we propose OrthoSchema, a multi-task framework inspired by cortical orthogonal subspace organization and cognitive schema reuse. OrthoSchema enforces representation orthogonality to eliminate cross-task interference and employs selective feature reuse transfer for few-shot cross-session, subject and paradigm adaptation. Experiments on macaque motor cortex datasets demonstrate that OrthoSchema significantly improves decoding accuracy in cross-session, cross-subject and challenging cross-paradigm generalization tasks, with larger performance improvements when fine-tuning samples are limited. Ablation studies confirm the synergistic effects of all components are crucial, with OrthoSchema effectively modeling cross-task features and capturing session relationships for robust transfer. Our results provide new insights into scalable and robust neural decoding for real-world BCI applications.",
      "code_url": null
    },
    "2508.08602v1": {
      "title": "Biomedical Signal Processing: EEG and ECG Classification with Discrete Wavelet Transforms, Energy Distribution, and Convolutional Neural Networks",
      "url": "http://arxiv.org/abs/2508.08602v1",
      "authors": "Justin London",
      "update_time": "2025-08-12",
      "abstract": "Biomedical signal processing extract meaningful information from physiological signals like electrocardiograms (ECGs), electroencephalograms (EEGs), and electromyograms (EMGs) to diagnose, monitor, and treat medical conditions and diseases such as seizures, cardiomyopathy, and neuromuscular disorders, respectively. Traditional manual physician analysis of electrical recordings is prone to human error as subtle anomolies may not be detected. Recently, advanced deep learning has significantly improved the accuracy of biomedical signal analysis. A multi-modal deep learning model is proposed that utilizes discrete wavelet transforms for signal pre-processing to reduce noise. A multi-modal image fusion and multimodal feature fusion framework is utilized that converts numeric biomedical signals into 2D and 3D images for image processing using Gramian angular fields, recurrency plots, and Markov transition fields. In this paper, deep learning models are applied to ECG, EEG, and human activity signals using actual medical datasets, brain, and heart recordings. The results demonstrate that using a multi-modal approach using wavelet transforms improves the accuracy of disease and disorder classification.",
      "code_url": null
    },
    "2508.08435v1": {
      "title": "Fast weight programming and linear transformers: from machine learning to neurobiology",
      "url": "http://arxiv.org/abs/2508.08435v1",
      "authors": "Kazuki Irie, Samuel J. Gershman",
      "update_time": "2025-08-11",
      "abstract": "Recent advances in artificial neural networks for machine learning, and language modeling in particular, have established a family of recurrent neural network (RNN) architectures that, unlike conventional RNNs with vector-form hidden states, use two-dimensional (2D) matrix-form hidden states. Such 2D-state RNNs, known as Fast Weight Programmers (FWPs), can be interpreted as a neural network whose synaptic weights (called fast weights) dynamically change over time as a function of input observations, and serve as short-term memory storage; corresponding synaptic weight modifications are controlled or programmed by another network (the programmer) whose parameters are trained (e.g., by gradient descent). In this Primer, we review the technical foundations of FWPs, their computational characteristics, and their connections to transformers and state space models. We also discuss connections between FWPs and models of synaptic plasticity in the brain, suggesting a convergence of natural and artificial intelligence.",
      "code_url": null
    },
    "2508.08418v1": {
      "title": "A hierarchical modelling approach for Bayesian Causal Forests on longitudinal data: A Case Study in Multiple Sclerosis Clinical Trials",
      "url": "http://arxiv.org/abs/2508.08418v1",
      "authors": "Emma Prevot, Dieter A. H\u00e4ring, Thomas E. Nichols, Chris C. Holmes, Habib Ganjgahi",
      "update_time": "2025-08-11",
      "abstract": "Long-running clinical trials offer a unique opportunity to study disease progression and treatment response over time, enabling questions about how and when interventions alter patient trajectories. However, drawing causal conclusions in this setting is challenging due to irregular follow-up, individual-level heterogeneity, and time-varying confounding. Bayesian Additive Regression Trees (BART) and their extension, Bayesian Causal Forests (BCF), have proven powerful for flexible causal inference in observational data, especially for heterogeneous treatment effects and non-linear outcome surfaces. Yet, both models assume independence across observations and are fundamentally limited in their ability to model within-individual correlation over time. This limits their use in real-world longitudinal settings where repeated measures are the norm. Motivated by the NO.MS dataset, the largest and most comprehensive clinical trial dataset in Multiple Sclerosis (MS), with more than 35,000 patients and up to 15 years follow-up, we develop BCFLong, a hierarchical model that preserves BART's strengths while extending it for longitudinal analysis. Inspired by BCF, we decompose the mean into prognostic and treatment effects, modelling the former on Image Quality Metrics (IQMs) to account for scanner effects, and introduce individual-specific random effects, including intercepts and slope, with a sparsity-inducing horseshoe prior. Simulations confirm BCFLong's superior performance and robustness to sparsity, significantly improving outcome and treatment effect estimation. On NO.MS, BCFLong captures clinically meaningful longitudinal patterns in brain volume change, which would have otherwise remained undetected. These findings highlight the importance of adaptively accounting for within-individual correlations and position BCFLong as a flexible framework for causal inference in longitudinal data.",
      "code_url": null
    },
    "2508.08216v1": {
      "title": "Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion",
      "url": "http://arxiv.org/abs/2508.08216v1",
      "authors": "Nicole Lai-Tan, Xiao Gu, Marios G. Philiastides, Fani Deligianni",
      "update_time": "2025-08-11",
      "abstract": "Personalised music-based interventions offer a powerful means of supporting motor rehabilitation by dynamically tailoring auditory stimuli to provide external timekeeping cues, modulate affective states, and stabilise gait patterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for adapting these interventions across individuals. However, inter-subject variability in EEG signals, further compounded by movement-induced artefacts and motor planning differences, hinders the generalisability of BCIs and results in lengthy calibration processes. We propose Individual Tangent Space Alignment (ITSA), a novel pre-alignment strategy incorporating subject-specific recentering, distribution matching, and supervised rotational alignment to enhance cross-subject generalisation. Our hybrid architecture fuses Regularised Common Spatial Patterns (RCSP) with Riemannian geometry in parallel and sequential configurations, improving class separability while maintaining the geometric structure of covariance matrices for robust statistical computation. Using leave-one-subject-out cross-validation, `ITSA' demonstrates significant performance improvements across subjects and conditions. The parallel fusion approach shows the greatest enhancement over its sequential counterpart, with robust performance maintained across varying data conditions and electrode configurations. The code will be made publicly available at the time of publication.",
      "code_url": null
    },
    "2508.08123v1": {
      "title": "A Physics-Driven Neural Network with Parameter Embedding for Generating Quantitative MR Maps from Weighted Images",
      "url": "http://arxiv.org/abs/2508.08123v1",
      "authors": "Lingjing Chen, Chengxiu Zhang, Yinqiao Yi, Yida Wang, Yang Song, Xu Yan, Shengfang Xu, Dalin Zhu, Mengqiu Cao, Yan Zhou, Chenglong Wang, Guang Yang",
      "update_time": "2025-08-11",
      "abstract": "We propose a deep learning-based approach that integrates MRI sequence parameters to improve the accuracy and generalizability of quantitative image synthesis from clinical weighted MRI. Our physics-driven neural network embeds MRI sequence parameters -- repetition time (TR), echo time (TE), and inversion time (TI) -- directly into the model via parameter embedding, enabling the network to learn the underlying physical principles of MRI signal formation. The model takes conventional T1-weighted, T2-weighted, and T2-FLAIR images as input and synthesizes T1, T2, and proton density (PD) quantitative maps. Trained on healthy brain MR images, it was evaluated on both internal and external test datasets. The proposed method achieved high performance with PSNR values exceeding 34 dB and SSIM values above 0.92 for all synthesized parameter maps. It outperformed conventional deep learning models in accuracy and robustness, including data with previously unseen brain structures and lesions. Notably, our model accurately synthesized quantitative maps for these unseen pathological regions, highlighting its superior generalization capability. Incorporating MRI sequence parameters via parameter embedding allows the neural network to better learn the physical characteristics of MR signals, significantly enhancing the performance and reliability of quantitative MRI synthesis. This method shows great potential for accelerating qMRI and improving its clinical utility.",
      "code_url": null
    },
    "2508.07842v1": {
      "title": "DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts",
      "url": "http://arxiv.org/abs/2508.07842v1",
      "authors": "Yutong Shen, Hangxu Liu, Penghui Liu, Ruizhe Xia, Tianyi Yao, Yitong Sun, Tongtong Feng",
      "update_time": "2025-08-11",
      "abstract": "Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex multi-step tasks that require continuous planning, sequential decision-making, and extended execution across domains to achieve the final goal. However, existing methods heavily rely on skill chaining by concatenating pre-trained subtasks, with environment observations and self-state tightly coupled, lacking the ability to generalize to new combinations of environments and skills, failing to complete various LH tasks across domains. To solve this problem, this paper presents DETACH, a cross-domain learning framework for LH tasks via biologically inspired dual-stream disentanglement. Inspired by the brain's \"where-what\" dual pathway mechanism, DETACH comprises two core modules: i) an environment learning module for spatial understanding, which captures object functions, spatial relationships, and scene semantics, achieving cross-domain transfer through complete environment-self disentanglement; ii) a skill learning module for task execution, which processes self-state information including joint degrees of freedom and motor patterns, enabling cross-skill transfer through independent motor pattern encoding. We conducted extensive experiments on various LH tasks in HSI scenes. Compared with existing methods, DETACH can achieve an average subtasks success rate improvement of 23% and average execution efficiency improvement of 29%.",
      "code_url": null
    },
    "2508.07838v1": {
      "title": "CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving",
      "url": "http://arxiv.org/abs/2508.07838v1",
      "authors": "Qi Xiang, Kunsong Shi, Zhigui Lin, Lei He",
      "update_time": "2025-08-11",
      "abstract": "Bird's Eye View (BEV) perception systems based on multi-sensor feature fusion have become a fundamental cornerstone for end-to-end autonomous driving. However, existing multi-modal BEV methods commonly suffer from limited input adaptability, constrained modeling capacity, and suboptimal generalization. To address these challenges, we propose a hierarchically decoupled Mixture-of-Experts architecture at the functional module level, termed Computing Brain DEvelopment System Mixture-of-Experts (CBDES MoE). CBDES MoE integrates multiple structurally heterogeneous expert networks with a lightweight Self-Attention Router (SAR) gating mechanism, enabling dynamic expert path selection and sparse, input-aware efficient inference. To the best of our knowledge, this is the first modular Mixture-of-Experts framework constructed at the functional module granularity within the autonomous driving domain. Extensive evaluations on the real-world nuScenes dataset demonstrate that CBDES MoE consistently outperforms fixed single-expert baselines in 3D object detection. Compared to the strongest single-expert model, CBDES MoE achieves a 1.6-point increase in mAP and a 4.1-point improvement in NDS, demonstrating the effectiveness and practical advantages of the proposed approach.",
      "code_url": null
    }
  },
  "EEG": {
    "2508.08724v1": {
      "title": "Hierarchical Variable Importance with Statistical Control for Medical Data-Based Prediction",
      "url": "http://arxiv.org/abs/2508.08724v1",
      "authors": "Joseph Paillard, Antoine Collas, Denis A. Engemann, Bertrand Thirion",
      "update_time": "2025-08-12",
      "abstract": "Recent advances in machine learning have greatly expanded the repertoire of predictive methods for medical imaging. However, the interpretability of complex models remains a challenge, which limits their utility in medical applications. Recently, model-agnostic methods have been proposed to measure conditional variable importance and accommodate complex non-linear models. However, they often lack power when dealing with highly correlated data, a common problem in medical imaging. We introduce Hierarchical-CPI, a model-agnostic variable importance measure that frames the inference problem as the discovery of groups of variables that are jointly predictive of the outcome. By exploring subgroups along a hierarchical tree, it remains computationally tractable, yet also enjoys explicit family-wise error rate control. Moreover, we address the issue of vanishing conditional importance under high correlation with a tree-based importance allocation mechanism. We benchmarked Hierarchical-CPI against state-of-the-art variable importance methods. Its effectiveness is demonstrated in two neuroimaging datasets: classifying dementia diagnoses from MRI data (ADNI dataset) and analyzing the Berger effect on EEG data (TDBRAIN dataset), identifying biologically plausible variables.",
      "code_url": null
    },
    "2508.08602v1": {
      "title": "Biomedical Signal Processing: EEG and ECG Classification with Discrete Wavelet Transforms, Energy Distribution, and Convolutional Neural Networks",
      "url": "http://arxiv.org/abs/2508.08602v1",
      "authors": "Justin London",
      "update_time": "2025-08-12",
      "abstract": "Biomedical signal processing extract meaningful information from physiological signals like electrocardiograms (ECGs), electroencephalograms (EEGs), and electromyograms (EMGs) to diagnose, monitor, and treat medical conditions and diseases such as seizures, cardiomyopathy, and neuromuscular disorders, respectively. Traditional manual physician analysis of electrical recordings is prone to human error as subtle anomolies may not be detected. Recently, advanced deep learning has significantly improved the accuracy of biomedical signal analysis. A multi-modal deep learning model is proposed that utilizes discrete wavelet transforms for signal pre-processing to reduce noise. A multi-modal image fusion and multimodal feature fusion framework is utilized that converts numeric biomedical signals into 2D and 3D images for image processing using Gramian angular fields, recurrency plots, and Markov transition fields. In this paper, deep learning models are applied to ECG, EEG, and human activity signals using actual medical datasets, brain, and heart recordings. The results demonstrate that using a multi-modal approach using wavelet transforms improves the accuracy of disease and disorder classification.",
      "code_url": null
    },
    "2508.08216v1": {
      "title": "Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion",
      "url": "http://arxiv.org/abs/2508.08216v1",
      "authors": "Nicole Lai-Tan, Xiao Gu, Marios G. Philiastides, Fani Deligianni",
      "update_time": "2025-08-11",
      "abstract": "Personalised music-based interventions offer a powerful means of supporting motor rehabilitation by dynamically tailoring auditory stimuli to provide external timekeeping cues, modulate affective states, and stabilise gait patterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for adapting these interventions across individuals. However, inter-subject variability in EEG signals, further compounded by movement-induced artefacts and motor planning differences, hinders the generalisability of BCIs and results in lengthy calibration processes. We propose Individual Tangent Space Alignment (ITSA), a novel pre-alignment strategy incorporating subject-specific recentering, distribution matching, and supervised rotational alignment to enhance cross-subject generalisation. Our hybrid architecture fuses Regularised Common Spatial Patterns (RCSP) with Riemannian geometry in parallel and sequential configurations, improving class separability while maintaining the geometric structure of covariance matrices for robust statistical computation. Using leave-one-subject-out cross-validation, `ITSA' demonstrates significant performance improvements across subjects and conditions. The parallel fusion approach shows the greatest enhancement over its sequential counterpart, with robust performance maintained across varying data conditions and electrode configurations. The code will be made publicly available at the time of publication.",
      "code_url": null
    },
    "2508.08159v1": {
      "title": "Federated Learning for Epileptic Seizure Prediction Across Heterogeneous EEG Datasets",
      "url": "http://arxiv.org/abs/2508.08159v1",
      "authors": "Cem Ata Baykara, Saurav Raj Pandey, Ali Burak \u00dcnal, Harlin Lee, Mete Akg\u00fcn",
      "update_time": "2025-08-11",
      "abstract": "Developing accurate and generalizable epileptic seizure prediction models from electroencephalography (EEG) data across multiple clinical sites is hindered by patient privacy regulations and significant data heterogeneity (non-IID characteristics). Federated Learning (FL) offers a privacy-preserving framework for collaborative training, but standard aggregation methods like Federated Averaging (FedAvg) can be biased by dominant datasets in heterogeneous settings. This paper investigates FL for seizure prediction using a single EEG channel across four diverse public datasets (Siena, CHB-MIT, Helsinki, NCH), representing distinct patient populations (adult, pediatric, neonate) and recording conditions. We implement privacy-preserving global normalization and propose a Random Subset Aggregation strategy, where each client trains on a fixed-size random subset of its data per round, ensuring equal contribution during aggregation. Our results show that locally trained models fail to generalize across sites, and standard weighted FedAvg yields highly skewed performance (e.g., 89.0% accuracy on CHB-MIT but only 50.8% on Helsinki and 50.6% on NCH). In contrast, Random Subset Aggregation significantly improves performance on under-represented clients (accuracy increases to 81.7% on Helsinki and 68.7% on NCH) and achieves a superior macro-average accuracy of 77.1% and pooled accuracy of 80.0% across all sites, demonstrating a more robust and fair global model. This work highlights the potential of balanced FL approaches for building effective and generalizable seizure prediction systems in realistic, heterogeneous multi-hospital environments while respecting data privacy.",
      "code_url": null
    },
    "2508.08124v1": {
      "title": "NeuroDx-LM: A Clinical Large-Scale Model for EEG-based Neurological Disorder Detection",
      "url": "http://arxiv.org/abs/2508.08124v1",
      "authors": "Guanghao Jin, Yuan Liang, Yihan Ma, Jingpei Wu, Guoyang Liu",
      "update_time": "2025-08-11",
      "abstract": "Large-scale models pre-trained on Electroencephalography (EEG) have shown promise in clinical applications such as neurological disorder detection. However, the practical deployment of EEG-based large-scale models faces critical challenges such as limited labeled EEG data and suboptimal performance in clinical scenarios. To address these issues, we propose NeuroDx-LM, a novel large-scale model specifically designed for detecting EEG-based neurological disorders. Our key contributions include (i) a Selective Temporal-Frequency Embedding mechanism that adaptively captures complex temporal and spectral patterns in EEG signals; and (ii) a Progressive Feature-Aware Training strategy that refines feature representation in a two-stage process. In the first stage, our model learns the fundamental discriminative features of EEG activities; in the second stage, the model further extracts more specialized fine-grained features for accurate diagnostic performance. We evaluated NeuroDx-LM on the CHB-MIT and Schizophrenia datasets, achieving state-of-the-art performance in EEG-based seizure and schizophrenia detection, respectively. These results demonstrate the great potential of EEG-based large-scale models to advance clinical applicability. Our code is available at https://github.com/LetItBe12345/NeuroDx-LM.",
      "code_url": null
    },
    "2508.07731v1": {
      "title": "CognitiveArm: Enabling Real-Time EEG-Controlled Prosthetic Arm Using Embodied Machine Learning",
      "url": "http://arxiv.org/abs/2508.07731v1",
      "authors": "Abdul Basit, Maha Nawaz, Saim Rehman, Muhammad Shafique",
      "update_time": "2025-08-11",
      "abstract": "Efficient control of prosthetic limbs via non-invasive brain-computer interfaces (BCIs) requires advanced EEG processing, including pre-filtering, feature extraction, and action prediction, performed in real time on edge AI hardware. Achieving this on resource-constrained devices presents challenges in balancing model complexity, computational efficiency, and latency. We present CognitiveArm, an EEG-driven, brain-controlled prosthetic system implemented on embedded AI hardware, achieving real-time operation without compromising accuracy. The system integrates BrainFlow, an open-source library for EEG data acquisition and streaming, with optimized deep learning (DL) models for precise brain signal classification. Using evolutionary search, we identify Pareto-optimal DL configurations through hyperparameter tuning, optimizer analysis, and window selection, analyzed individually and in ensemble configurations. We apply model compression techniques such as pruning and quantization to optimize models for embedded deployment, balancing efficiency and accuracy. We collected an EEG dataset and designed an annotation pipeline enabling precise labeling of brain signals corresponding to specific intended actions, forming the basis for training our optimized DL models. CognitiveArm also supports voice commands for seamless mode switching, enabling control of the prosthetic arm's 3 degrees of freedom (DoF). Running entirely on embedded hardware, it ensures low latency and real-time responsiveness. A full-scale prototype, interfaced with the OpenBCI UltraCortex Mark IV EEG headset, achieved up to 90% accuracy in classifying three core actions (left, right, idle). Voice integration enables multiplexed, variable movement for everyday tasks (e.g., handshake, cup picking), enhancing real-world performance and demonstrating CognitiveArm's potential for advanced prosthetic control.",
      "code_url": null
    },
    "2508.07677v1": {
      "title": "Improving Continuous Grasp Force Decoding from EEG with Time-Frequency Regressors and Premotor-Parietal Network Integration",
      "url": "http://arxiv.org/abs/2508.07677v1",
      "authors": "Parth G. Dangi, Yogesh Kumar Meena",
      "update_time": "2025-08-11",
      "abstract": "Brain-machine interfaces (BMIs) have significantly advanced neuro-rehabilitation by enhancing motor control. However, accurately decoding continuous grasp force remains a challenge, limiting the effectiveness of BMI applications for fine motor tasks. Current models tend to prioritise algorithmic complexity rather than incorporating neurophysiological insights into force control, which is essential for developing effective neural engineering solutions. To address this, we propose EEGForceMap, an EEG-based methodology that isolates signals from the premotor-parietal region and extracts task-specific components. We construct three distinct time-frequency feature sets, which are validated by comparing them with prior studies, and use them for force prediction with linear, non-linear, and deep learning-based regressors. The performance of these regressors was evaluated on the WAY-EEG-GAL dataset that includes 12 subjects. Our results show that integrating EEGForceMap approach with regressor models yields a 61.7% improvement in subject-specific conditions (R-squared = 0.815) and a 55.7% improvement in subject-independent conditions (R-squared = 0.785) over the state-of-the-art kinematic decoder models. Furthermore, an ablation study confirms that each preprocessing step significantly enhances decoding accuracy. This work contributes to the advancement of responsive BMIs for stroke rehabilitation and assistive robotics by improving EEG-based decoding of dynamic grasp force.",
      "code_url": null
    },
    "2508.07283v1": {
      "title": "Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment",
      "url": "http://arxiv.org/abs/2508.07283v1",
      "authors": "Bujar Raufi",
      "update_time": "2025-08-10",
      "abstract": "This study explores the intersection of electroencephalography (EEG) microstates and Large Language Models (LLMs) to enhance the assessment of cognitive load states. By utilizing EEG microstate features, the research aims to fine-tune LLMs for improved predictions of distinct cognitive states, specifically 'Rest' and 'Load'. The experimental design is delineated in four comprehensive stages: dataset collection and preprocessing, microstate segmentation and EEG backfitting, feature extraction paired with prompt engineering, and meticulous LLM model selection and refinement. Employing a supervised learning paradigm, the LLM is trained to identify cognitive load states based on EEG microstate features integrated into prompts, producing accurate discrimination of cognitive load. A curated dataset, linking EEG features to specified cognitive load conditions, underpins the experimental framework. The results indicate a significant improvement in model performance following the proposed fine-tuning, showcasing the potential of EEG-informed LLMs in cognitive neuroscience and cognitive AI applications. This approach not only contributes to the understanding of brain dynamics but also paves the way for advancements in machine learning techniques applicable to cognitive load and cognitive AI research.",
      "code_url": null
    },
    "2508.07253v1": {
      "title": "PySeizure: A single machine learning classifier framework to detect seizures in diverse datasets",
      "url": "http://arxiv.org/abs/2508.07253v1",
      "authors": "Bartlomiej Chybowski, Shima Abdullateef, Hollan Haule, Alfredo Gonzalez-Sulser, Javier Escudero",
      "update_time": "2025-08-10",
      "abstract": "Reliable seizure detection is critical for diagnosing and managing epilepsy, yet clinical workflows remain dependent on time-consuming manual EEG interpretation. While machine learning has shown promise, existing approaches often rely on dataset-specific optimisations, limiting their real-world applicability and reproducibility. Here, we introduce an innovative, open-source machine-learning framework that enables robust and generalisable seizure detection across varied clinical datasets. We evaluate our approach on two publicly available EEG datasets that differ in patient populations and electrode configurations. To enhance robustness, the framework incorporates an automated pre-processing pipeline to standardise data and a majority voting mechanism, in which multiple models independently assess each second of EEG before reaching a final decision. We train, tune, and evaluate models within each dataset, assessing their cross-dataset transferability. Our models achieve high within-dataset performance (AUC 0.904+/-0.059 for CHB-MIT and 0.864+/-0.060 for TUSZ) and demonstrate strong generalisation across datasets despite differences in EEG setups and populations (AUC 0.615+/-0.039 for models trained on CHB-MIT and tested on TUSZ and 0.762+/-0.175 in the reverse case) without any post-processing. Furthermore, a mild post-processing improved the within-dataset results to 0.913+/-0.064 and 0.867+/-0.058 and cross-dataset results to 0.619+/-0.036 and 0.768+/-0.172. These results underscore the potential of, and essential considerations for, deploying our framework in diverse clinical settings. By making our methodology fully reproducible, we provide a foundation for advancing clinically viable, dataset-agnostic seizure detection systems. This approach has the potential for widespread adoption, complementing rather than replacing expert interpretation, and accelerating clinical integration.",
      "code_url": null
    },
    "2508.07220v1": {
      "title": "Neural Bridge Processes",
      "url": "http://arxiv.org/abs/2508.07220v1",
      "authors": "Jian Xu, Yican Liu, Qibin Zhao, John Paisley, Delu Zeng",
      "update_time": "2025-08-10",
      "abstract": "Learning stochastic functions from partially observed context-target pairs is a fundamental problem in probabilistic modeling. Traditional models like Gaussian Processes (GPs) face scalability issues with large datasets and assume Gaussianity, limiting their applicability. While Neural Processes (NPs) offer more flexibility, they struggle with capturing complex, multi-modal target distributions. Neural Diffusion Processes (NDPs) enhance expressivity through a learned diffusion process but rely solely on conditional signals in the denoising network, resulting in weak input coupling from an unconditional forward process and semantic mismatch at the diffusion endpoint. In this work, we propose Neural Bridge Processes (NBPs), a novel method for modeling stochastic functions where inputs x act as dynamic anchors for the entire diffusion trajectory. By reformulating the forward kernel to explicitly depend on x, NBP enforces a constrained path that strictly terminates at the supervised target. This approach not only provides stronger gradient signals but also guarantees endpoint coherence. We validate NBPs on synthetic data, EEG signal regression and image regression tasks, achieving substantial improvements over baselines. These results underscore the effectiveness of DDPM-style bridge sampling in enhancing both performance and theoretical consistency for structured prediction tasks.",
      "code_url": null
    }
  },
  "BCI": {
    "2508.08763v1": {
      "title": "Quantifying the direct and indirect impact of COVID-19 vaccination: evidence from Victoria, Australia",
      "url": "http://arxiv.org/abs/2508.08763v1",
      "authors": "Lixin Lin, Haydar Demirhan, Peter Eizenberg, James M. Trauer, Lewi Stone",
      "update_time": "2025-08-12",
      "abstract": "Vaccines not only directly protect vaccinated individuals but also contribute to protect the entire population via indirect herd-immunity benefits. However, researchers have long struggled to quantify these indirect effects at the population level, hindering assessment of vaccination program effectiveness. We developed a new method to estimate these effects, thereby markedly improving measures of the number of infections, hospitalizations, and deaths averted by vaccination. Our population-based analysis of 6,440,000 residents of Victoria, Australia reveal strong indirect effects during the Delta outbreak (September-November 2021). By modelling a non-vaccination counterfactual, we conservatively estimate 316,000 infections were averted (95\\% BCI: 232k-406k), as well as 33,500 hospitalizations (95\\% BCI: 22.2k-46.2k), and 4,900 deaths (95\\% BCI: 2.9k-7.3k). These are 4.0, 7.5, and 8.0 times higher, respectively, than observed. Half of the averted infections and around one-quarter of hospitalizations and deaths were attributable to indirect protection. Homogeneous vaccination across LGAs could have reduced outcomes by approximately 25\\%.",
      "code_url": null
    },
    "2508.08681v1": {
      "title": "Multi-dimensional Neural Decoding with Orthogonal Representations for Brain-Computer Interfaces",
      "url": "http://arxiv.org/abs/2508.08681v1",
      "authors": "Kaixi Tian, Shengjia Zhao, Yuhan Zhang, Shan Yu",
      "update_time": "2025-08-12",
      "abstract": "Current brain-computer interfaces primarily decode single motor variables, limiting their ability to support natural, high-bandwidth neural control that requires simultaneous extraction of multiple correlated motor dimensions. We introduce Multi-dimensional Neural Decoding (MND), a task formulation that simultaneously extracts multiple motor variables (direction, position, velocity, acceleration) from single neural population recordings. MND faces two key challenges: cross-task interference when decoding correlated motor dimensions from shared cortical representations, and generalization issues across sessions, subjects, and paradigms. To address these challenges, we propose OrthoSchema, a multi-task framework inspired by cortical orthogonal subspace organization and cognitive schema reuse. OrthoSchema enforces representation orthogonality to eliminate cross-task interference and employs selective feature reuse transfer for few-shot cross-session, subject and paradigm adaptation. Experiments on macaque motor cortex datasets demonstrate that OrthoSchema significantly improves decoding accuracy in cross-session, cross-subject and challenging cross-paradigm generalization tasks, with larger performance improvements when fine-tuning samples are limited. Ablation studies confirm the synergistic effects of all components are crucial, with OrthoSchema effectively modeling cross-task features and capturing session relationships for robust transfer. Our results provide new insights into scalable and robust neural decoding for real-world BCI applications.",
      "code_url": null
    },
    "2508.08216v1": {
      "title": "Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion",
      "url": "http://arxiv.org/abs/2508.08216v1",
      "authors": "Nicole Lai-Tan, Xiao Gu, Marios G. Philiastides, Fani Deligianni",
      "update_time": "2025-08-11",
      "abstract": "Personalised music-based interventions offer a powerful means of supporting motor rehabilitation by dynamically tailoring auditory stimuli to provide external timekeeping cues, modulate affective states, and stabilise gait patterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for adapting these interventions across individuals. However, inter-subject variability in EEG signals, further compounded by movement-induced artefacts and motor planning differences, hinders the generalisability of BCIs and results in lengthy calibration processes. We propose Individual Tangent Space Alignment (ITSA), a novel pre-alignment strategy incorporating subject-specific recentering, distribution matching, and supervised rotational alignment to enhance cross-subject generalisation. Our hybrid architecture fuses Regularised Common Spatial Patterns (RCSP) with Riemannian geometry in parallel and sequential configurations, improving class separability while maintaining the geometric structure of covariance matrices for robust statistical computation. Using leave-one-subject-out cross-validation, `ITSA' demonstrates significant performance improvements across subjects and conditions. The parallel fusion approach shows the greatest enhancement over its sequential counterpart, with robust performance maintained across varying data conditions and electrode configurations. The code will be made publicly available at the time of publication.",
      "code_url": null
    },
    "2508.07731v1": {
      "title": "CognitiveArm: Enabling Real-Time EEG-Controlled Prosthetic Arm Using Embodied Machine Learning",
      "url": "http://arxiv.org/abs/2508.07731v1",
      "authors": "Abdul Basit, Maha Nawaz, Saim Rehman, Muhammad Shafique",
      "update_time": "2025-08-11",
      "abstract": "Efficient control of prosthetic limbs via non-invasive brain-computer interfaces (BCIs) requires advanced EEG processing, including pre-filtering, feature extraction, and action prediction, performed in real time on edge AI hardware. Achieving this on resource-constrained devices presents challenges in balancing model complexity, computational efficiency, and latency. We present CognitiveArm, an EEG-driven, brain-controlled prosthetic system implemented on embedded AI hardware, achieving real-time operation without compromising accuracy. The system integrates BrainFlow, an open-source library for EEG data acquisition and streaming, with optimized deep learning (DL) models for precise brain signal classification. Using evolutionary search, we identify Pareto-optimal DL configurations through hyperparameter tuning, optimizer analysis, and window selection, analyzed individually and in ensemble configurations. We apply model compression techniques such as pruning and quantization to optimize models for embedded deployment, balancing efficiency and accuracy. We collected an EEG dataset and designed an annotation pipeline enabling precise labeling of brain signals corresponding to specific intended actions, forming the basis for training our optimized DL models. CognitiveArm also supports voice commands for seamless mode switching, enabling control of the prosthetic arm's 3 degrees of freedom (DoF). Running entirely on embedded hardware, it ensures low latency and real-time responsiveness. A full-scale prototype, interfaced with the OpenBCI UltraCortex Mark IV EEG headset, achieved up to 90% accuracy in classifying three core actions (left, right, idle). Voice integration enables multiplexed, variable movement for everyday tasks (e.g., handshake, cup picking), enhancing real-world performance and demonstrating CognitiveArm's potential for advanced prosthetic control.",
      "code_url": null
    },
    "2508.04128v1": {
      "title": "Neuro-MoBRE: Exploring Multi-subject Multi-task Intracranial Decoding via Explicit Heterogeneity Resolving",
      "url": "http://arxiv.org/abs/2508.04128v1",
      "authors": "Di Wu, Yifei Jia, Siyuan Li, Shiqi Zhao, Jie Yang, Mohamad Sawan",
      "update_time": "2025-08-06",
      "abstract": "Neurophysiological decoding, fundamental to advancing brain-computer interface (BCI) technologies, has significantly benefited from recent advances in deep learning. However, existing decoding approaches largely remain constrained to single-task scenarios and individual subjects, limiting their broader applicability and generalizability. Efforts towards creating large-scale neurophysiological foundation models have shown promise, but continue to struggle with significant challenges due to pervasive data heterogeneity across subjects and decoding tasks. Simply increasing model parameters and dataset size without explicitly addressing this heterogeneity fails to replicate the scaling successes seen in natural language processing. Here, we introduce the Neural Mixture of Brain Regional Experts (Neuro-MoBRE), a general-purpose decoding framework explicitly designed to manage the ubiquitous data heterogeneity in neurophysiological modeling. Neuro-MoBRE incorporates a brain-regional-temporal embedding mechanism combined with a mixture-of-experts approach, assigning neural signals from distinct brain regions to specialized regional experts on a unified embedding basis, thus explicitly resolving both structural and functional heterogeneity. Additionally, our region-masked autoencoding pre-training strategy further enhances representational consistency among subjects, complemented by a task-disentangled information aggregation method tailored to effectively handle task-specific neural variations. Evaluations conducted on intracranial recordings from 11 subjects across five diverse tasks, including complex language decoding and epileptic seizure diagnosis, demonstrate that Neuro-MoBRE surpasses prior art and exhibits robust generalization for zero-shot decoding on unseen subjects.",
      "code_url": null
    },
    "2508.02528v1": {
      "title": "From Pixels to Pathology: Restoration Diffusion for Diagnostic-Consistent Virtual IHC",
      "url": "http://arxiv.org/abs/2508.02528v1",
      "authors": "Jingsong Liu, Xiaofeng Deng, Han Li, Azar Kazemi, Christian Grashei, Gesa Wilkens, Xin You, Tanja Groll, Nassir Navab, Carolin Mogler, Peter J. Sch\u00fcffler",
      "update_time": "2025-08-04",
      "abstract": "Hematoxylin and eosin (H&E) staining is the clinical standard for assessing tissue morphology, but it lacks molecular-level diagnostic information. In contrast, immunohistochemistry (IHC) provides crucial insights into biomarker expression, such as HER2 status for breast cancer grading, but remains costly and time-consuming, limiting its use in time-sensitive clinical workflows. To address this gap, virtual staining from H&E to IHC has emerged as a promising alternative, yet faces two core challenges: (1) Lack of fair evaluation of synthetic images against misaligned IHC ground truths, and (2) preserving structural integrity and biological variability during translation. To this end, we present an end-to-end framework encompassing both generation and evaluation in this work. We introduce Star-Diff, a structure-aware staining restoration diffusion model that reformulates virtual staining as an image restoration task. By combining residual and noise-based generation pathways, Star-Diff maintains tissue structure while modeling realistic biomarker variability. To evaluate the diagnostic consistency of the generated IHC patches, we propose the Semantic Fidelity Score (SFS), a clinical-grading-task-driven metric that quantifies class-wise semantic degradation based on biomarker classification accuracy. Unlike pixel-level metrics such as SSIM and PSNR, SFS remains robust under spatial misalignment and classifier uncertainty. Experiments on the BCI dataset demonstrate that Star-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity and diagnostic relevance. With rapid inference and strong clinical alignment,it presents a practical solution for applications such as intraoperative virtual IHC synthesis.",
      "code_url": null
    },
    "2508.02063v1": {
      "title": "TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs",
      "url": "http://arxiv.org/abs/2508.02063v1",
      "authors": "Amitava Das, Vinija Jain, Aman Chadha",
      "update_time": "2025-08-04",
      "abstract": "Large Language Models (LLMs) fine-tuned to align with human values often exhibit alignment drift, producing unsafe or policy-violating completions when exposed to adversarial prompts, decoding perturbations, or paraphrased jailbreaks. While prior work has behaviorally characterized alignment failure, little is known about the training-time belief sources underlying these failures. We introduce TraceAlign, a unified framework for tracing unsafe completions back to their root causes in the model's training corpus. Central to our approach is the Belief Conflict Index (BCI), which quantifies semantic inconsistency between generated spans and aligned policies, based on retrieved training documents using suffix-array matching. We propose three complementary interventions: (i) TraceShield, an inference-time safety filter that refuses completions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a contrastive fine-tuning objective penalizing high-BCI continuations during DPO, and (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam expansions predicted to yield high-BCI spans. Together, these defenses reduce alignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB) while preserving utility on standard tasks, with delta less than 0.2 and improved refusal quality. We further derive a theoretical upper bound on drift likelihood via suffix-array span statistics, linking memorization frequency and length to adversarial reactivation risk. TraceAlign thus provides the first scalable, traceable, and grounded toolkit for understanding and mitigating alignment failures at source. To encourage further exploration and development, we open-source our implementation at: https://anonymous.4open.science/r/tracealign-2DA7",
      "code_url": null
    },
    "2508.01510v1": {
      "title": "DIY hybrid SSVEP-P300 LED stimuli for BCI platform using EMOTIV EEG headset",
      "url": "http://arxiv.org/abs/2508.01510v1",
      "authors": "Surej Mouli, Ramaswamy Palaniappan",
      "update_time": "2025-08-02",
      "abstract": "A fully customisable chip-on board (COB) LED design to evoke two brain responses simultaneously (steady state visual evoked potential (SSVEP) and transient evoked potential, P300) is discussed in this paper. Considering different possible modalities in braincomputer interfacing (BCI), SSVEP is widely accepted as it requires a lesser number of electroencephalogram (EEG) electrodes and minimal training time. The aim of this work was to produce a hybrid BCI hardware platform to evoke SSVEP and P300 precisely with reduced fatigue and improved classification performance. The system comprises of four independent radial green visual stimuli controlled individually by a 32-bit microcontroller platform to evoke SSVEP and four red LEDs flashing at random intervals to generate P300 events. The system can also record the P300 event timestamps that can be used in classification, to improve the accuracy and reliability. The hybrid stimulus was tested for realtime classification accuracy by controlling a LEGO robot to move in four directions.",
      "code_url": null
    },
    "2507.21435v1": {
      "title": "MindChat: Enhancing BCI Spelling with Large Language Models in Realistic Scenarios",
      "url": "http://arxiv.org/abs/2507.21435v1",
      "authors": "JIaheng Wang, Yucun Zhong, Chengjie Huang, Lin Yao",
      "update_time": "2025-07-29",
      "abstract": "Brain-computer interface (BCI) spellers can render a new communication channel independent of peripheral nervous system, which are especially valuable for patients with severe motor disabilities. However, current BCI spellers often require users to type intended utterances letter-by-letter while spelling errors grow proportionally due to inaccurate electroencephalogram (EEG) decoding, largely impeding the efficiency and usability of BCIs in real-world communication. In this paper, we present MindChat, a large language model (LLM)-assisted BCI speller to enhance BCI spelling efficiency by reducing users' manual keystrokes. Building upon prompt engineering, we prompt LLMs (GPT-4o) to continuously suggest context-aware word and sentence completions/predictions during spelling. Online copy-spelling experiments encompassing four dialogue scenarios demonstrate that MindChat saves more than 62\\% keystrokes and over 32\\% spelling time compared with traditional BCI spellers. We envision high-speed BCI spellers enhanced by LLMs will potentially lead to truly practical applications.",
      "code_url": null
    },
    "2507.21037v1": {
      "title": "When Brain Foundation Model Meets Cauchy-Schwarz Divergence: A New Framework for Cross-Subject Motor Imagery Decoding",
      "url": "http://arxiv.org/abs/2507.21037v1",
      "authors": "Jinzhou Wu, Baoping Tang, Qikang Li, Yi Wang, Cheng Li, Shujian Yu",
      "update_time": "2025-07-28",
      "abstract": "Decoding motor imagery (MI) electroencephalogram (EEG) signals, a key non-invasive brain-computer interface (BCI) paradigm for controlling external systems, has been significantly advanced by deep learning. However, MI-EEG decoding remains challenging due to substantial inter-subject variability and limited labeled target data, which necessitate costly calibration for new users. Many existing multi-source domain adaptation (MSDA) methods indiscriminately incorporate all available source domains, disregarding the large inter-subject differences in EEG signals, which leads to negative transfer and excessive computational costs. Moreover, while many approaches focus on feature distribution alignment, they often neglect the explicit dependence between features and decision-level outputs, limiting their ability to preserve discriminative structures. To address these gaps, we propose a novel MSDA framework that leverages a pretrained large Brain Foundation Model (BFM) for dynamic and informed source subject selection, ensuring only relevant sources contribute to adaptation. Furthermore, we employ Cauchy-Schwarz (CS) and Conditional CS (CCS) divergences to jointly perform feature-level and decision-level alignment, enhancing domain invariance while maintaining class discriminability. Extensive evaluations on two benchmark MI-EEG datasets demonstrate that our framework outperforms a broad range of state-of-the-art baselines. Additional experiments with a large source pool validate the scalability and efficiency of BFM-guided selection, which significantly reduces training time without sacrificing performance.",
      "code_url": null
    }
  },
  "fMRI": {
    "2508.07106v1": {
      "title": "BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation",
      "url": "http://arxiv.org/abs/2508.07106v1",
      "authors": "Yiran Huang, Amirhossein Nouranizadeh, Christine Ahrends, Mengjia Xu",
      "update_time": "2025-08-09",
      "abstract": "Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widely used to study human brain activity. fMRI signals in areas across the brain transiently synchronise and desynchronise their activity in a highly structured manner, even when an individual is at rest. These functional connectivity dynamics may be related to behaviour and neuropsychiatric disease. To model these dynamics, temporal brain connectivity representations are essential, as they reflect evolving interactions between brain regions and provide insight into transient neural states and network reconfigurations. However, conventional graph neural networks (GNNs) often struggle to capture long-range temporal dependencies in dynamic fMRI data. To address this challenge, we propose BrainATCL, an unsupervised, nonparametric framework for adaptive temporal brain connectivity learning, enabling functional link prediction and age estimation. Our method dynamically adjusts the lookback window for each snapshot based on the rate of newly added edges. Graph sequences are subsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporal representations of dynamic functional connectivity in resting-state fMRI data of 1,000 participants from the Human Connectome Project. To further improve spatial modeling, we incorporate brain structure and function-informed edge attributes, i.e., the left/right hemispheric identity and subnetwork membership of brain regions, enabling the model to capture biologically meaningful topological patterns. We evaluate our BrainATCL on two tasks: functional link prediction and age estimation. The experimental results demonstrate superior performance and strong generalization, including in cross-session prediction scenarios.",
      "code_url": null
    },
    "2508.06118v1": {
      "title": "Ensemble-Based Graph Representation of fMRI Data for Cognitive Brain State Classification",
      "url": "http://arxiv.org/abs/2508.06118v1",
      "authors": "Daniil Vlasenko, Vadim Ushakov, Alexey Zaikin, Denis Zakharov",
      "update_time": "2025-08-08",
      "abstract": "Understanding and classifying human cognitive brain states based on neuroimaging data remains one of the foremost and most challenging problems in neuroscience, owing to the high dimensionality and intrinsic noise of the signals. In this work, we propose an ensemble-based graph representation method of functional magnetic resonance imaging (fMRI) data for the task of binary brain-state classification. Our method builds the graph by leveraging multiple base machine-learning models: each edge weight reflects the difference in posterior probabilities between two cognitive states, yielding values in the range [-1, 1] that encode confidence in a given state. We applied this approach to seven cognitive tasks from the Human Connectome Project (HCP 1200 Subject Release), including working memory, gambling, motor activity, language, social cognition, relational processing, and emotion processing. Using only the mean incident edge weights of the graphs as features, a simple logistic-regression classifier achieved average accuracies from 97.07% to 99.74%. We also compared our ensemble graphs with classical correlation-based graphs in a classification task with a graph neural network (GNN). In all experiments, the highest classification accuracy was obtained with ensemble graphs. These results demonstrate that ensemble graphs convey richer topological information and enhance brain-state discrimination. Our approach preserves edge-level interpretability of the fMRI graph representation, is adaptable to multiclass and regression tasks, and can be extended to other neuroimaging modalities and pathological-state classification.",
      "code_url": null
    },
    "2508.06589v1": {
      "title": "A Federated Learning Framework for Handling Subtype Confounding and Heterogeneity in Large-Scale Neuroimaging Diagnosis",
      "url": "http://arxiv.org/abs/2508.06589v1",
      "authors": "Xinglin Zhao, Yanwen Wang, Xiaobo Liu, Yanrong Hao, Rui Cao, Xin Wen",
      "update_time": "2025-08-08",
      "abstract": "Computer-aided diagnosis (CAD) systems play a crucial role in analyzing neuroimaging data for neurological and psychiatric disorders. However, small-sample studies suffer from low reproducibility, while large-scale datasets introduce confounding heterogeneity due to multiple disease subtypes being labeled under a single category. To address these challenges, we propose a novel federated learning framework tailored for neuroimaging CAD systems. Our approach includes a dynamic navigation module that routes samples to the most suitable local models based on latent subtype representations, and a meta-integration module that combines predictions from heterogeneous local models into a unified diagnostic output. We evaluated our framework using a comprehensive dataset comprising fMRI data from over 1300 MDD patients and 1100 healthy controls across multiple study cohorts. Experimental results demonstrate significant improvements in diagnostic accuracy and robustness compared to traditional methods. Specifically, our framework achieved an average accuracy of 74.06\\% across all tested sites, showcasing its effectiveness in handling subtype heterogeneity and enhancing model generalizability. Ablation studies further confirmed the importance of both the dynamic navigation and meta-integration modules in improving performance. By addressing data heterogeneity and subtype confounding, our framework advances reliable and reproducible neuroimaging CAD systems, offering significant potential for personalized medicine and clinical decision-making in neurology and psychiatry.",
      "code_url": null
    },
    "2508.02480v1": {
      "title": "MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding",
      "url": "http://arxiv.org/abs/2508.02480v1",
      "authors": "Wenwen Zeng, Yonghuang Wu, Yifan Chen, Xuan Xie, Chengqian Zhao, Feiyu Yin, Guoqing Wu, Jinhua Yu",
      "update_time": "2025-08-04",
      "abstract": "Reconstructing dynamic videos from fMRI is important for understanding visual cognition and enabling vivid brain-computer interfaces. However, current methods are critically limited to single-shot clips, failing to address the multi-shot nature of real-world experiences. Multi-shot reconstruction faces fundamental challenges: fMRI signal mixing across shots, the temporal resolution mismatch between fMRI and video obscuring rapid scene changes, and the lack of dedicated multi-shot fMRI-video datasets. To overcome these limitations, we propose a novel divide-and-decode framework for multi-shot fMRI video reconstruction. Our core innovations are: (1) A shot boundary predictor module explicitly decomposing mixed fMRI signals into shot-specific segments. (2) Generative keyframe captioning using LLMs, which decodes robust textual descriptions from each segment, overcoming temporal blur by leveraging high-level semantics. (3) Novel large-scale data synthesis (20k samples) from existing datasets. Experimental results demonstrate our framework outperforms state-of-the-art methods in multi-shot reconstruction fidelity. Ablation studies confirm the critical role of fMRI decomposition and semantic captioning, with decomposition significantly improving decoded caption CLIP similarity by 71.8%. This work establishes a new paradigm for multi-shot fMRI reconstruction, enabling accurate recovery of complex visual narratives through explicit decomposition and semantic prompting.",
      "code_url": null
    },
    "2507.23116v1": {
      "title": "Alpha-Z divergence unveils further distinct phenotypic traits of human brain connectivity fingerprint",
      "url": "http://arxiv.org/abs/2507.23116v1",
      "authors": "Md Kaosar Uddin, Nghi Nguyen, Huajun Huang, Duy Duong-Tran, Jingyi Zheng",
      "update_time": "2025-07-30",
      "abstract": "The accurate identification of individuals from functional connectomes (FCs) is critical for advancing individualized assessments in neuropsychiatric research. Traditional methods, such as Pearson's correlation, have limitations in capturing the complex, non-Euclidean geometry of FC data, leading to suboptimal performance in identification performance. Recent developments have introduced geodesic distance as a more robust metric; however, its performance is highly sensitive to regularization choices, which vary by spatial scale and task condition. To address these challenges, we propose a novel divergence-based distance metric, the Alpha-Z Bures-Wasserstein divergence, which provides a more flexible and geometry-aware framework for FC comparison. Unlike prior methods, our approach does not require meticulous parameter tuning and maintains strong identification performance across multiple task conditions and spatial resolutions. We evaluate our approach against both traditional (e.g., Euclidean, Pearson) and state-of-the-art manifold-based distances (e.g., affine-invariant, log-Euclidean, Bures-Wasserstein), and systematically investigate how varying regularization strengths affect geodesic distance performance on the Human Connectome Project dataset. Our results show that the proposed method significantly improves identification rates over traditional and geodesic distances, particularly when optimized regularization is applied, and especially in high-dimensional settings where matrix rank deficiencies degrade existing metrics. We further validate its generalizability across resting-state and task-based fMRI, using multiple parcellation schemes. These findings suggest that the new divergence provides a more reliable and generalizable framework for functional connectivity analysis, offering enhanced sensitivity in linking FC patterns to cognitive and behavioral outcomes.",
      "code_url": null
    },
    "2507.23057v1": {
      "title": "Neural Energy Landscapes Predict Working Memory Decline After Brain Tumor Resection",
      "url": "http://arxiv.org/abs/2507.23057v1",
      "authors": "Triet M. Tran, Sina Khanmohammadi",
      "update_time": "2025-07-30",
      "abstract": "Surgical resection is the primary treatment option for brain tumor patients, but it carries the risk of postoperative cognitive dysfunction. This study investigates how tumor-induced alterations in presurgical neural dynamics relate to postoperative working memory decline. We analyzed functional magnetic resonance imaging (fMRI) of brain tumor patients before surgery and extracted energy landscapes of high-order brain interactions. We then examined the relation between these energy features and postoperative working memory performance using statistical and machine learning (random forest) models. Patients with lower postoperative working memory scores exhibited fewer but more extreme transitions between local energy minima and maxima, whereas patients with higher scores showed more frequent but less extreme shifts. Furthermore, the presurgical high-order energy features were able to accurately predict postoperative working memory decline with a mean accuracy of 90\\%, F1 score of 87.5\\%, and an AUC of 0.95. Our study suggests that the brain tumor-induced disruptions in high-order neural dynamics before surgery are predictive of postoperative working memory decline. Our findings pave the path for personalized surgical planning and targeted interventions to mitigate cognitive risks associated with brain tumor resection.",
      "code_url": null
    },
    "2507.22378v1": {
      "title": "Whole-brain Transferable Representations from Large-Scale fMRI Data Improve Task-Evoked Brain Activity Decoding",
      "url": "http://arxiv.org/abs/2507.22378v1",
      "authors": "Yueh-Po Peng, Vincent K. M. Cheung, Li Su",
      "update_time": "2025-07-30",
      "abstract": "A fundamental challenge in neuroscience is to decode mental states from brain activity. While functional magnetic resonance imaging (fMRI) offers a non-invasive approach to capture brain-wide neural dynamics with high spatial precision, decoding from fMRI data -- particularly from task-evoked activity -- remains challenging due to its high dimensionality, low signal-to-noise ratio, and limited within-subject data. Here, we leverage recent advances in computer vision and propose STDA-SwiFT, a transformer-based model that learns transferable representations from large-scale fMRI datasets via spatial-temporal divided attention and self-supervised contrastive learning. Using pretrained voxel-wise representations from 995 subjects in the Human Connectome Project (HCP), we show that our model substantially improves downstream decoding performance of task-evoked activity across multiple sensory and cognitive domains, even with minimal data preprocessing. We demonstrate performance gains from larger receptor fields afforded by our memory-efficient attention mechanism, as well as the impact of functional relevance in pretraining data when fine-tuning on small samples. Our work showcases transfer learning as a viable approach to harness large-scale datasets to overcome challenges in decoding brain activity from fMRI data.",
      "code_url": null
    },
    "2507.22263v1": {
      "title": "Deep Learning for Gradient and BCG Artifacts Removal in EEG During Simultaneous fMRI",
      "url": "http://arxiv.org/abs/2507.22263v1",
      "authors": "K. A. Shahriar, E. H. Bhuiyan, Q. Luo, M. E. H. Chowdhury, X. J. Zhou",
      "update_time": "2025-07-29",
      "abstract": "Simultaneous EEG-fMRI recording combines high temporal and spatial resolution for tracking neural activity. However, its usefulness is greatly limited by artifacts from magnetic resonance (MR), especially gradient artifacts (GA) and ballistocardiogram (BCG) artifacts, which interfere with the EEG signal. To address this issue, we used a denoising autoencoder (DAR), a deep learning framework designed to reduce MR-related artifacts in EEG recordings. Using paired data that includes both artifact-contaminated and MR-corrected EEG from the CWL EEG-fMRI dataset, DAR uses a 1D convolutional autoencoder to learn a direct mapping from noisy to clear signal segments. Compared to traditional artifact removal methods like principal component analysis (PCA), independent component analysis (ICA), average artifact subtraction (AAS), and wavelet thresholding, DAR shows better performance. It achieves a root-mean-squared error (RMSE) of 0.0218 $\\pm$ 0.0152, a structural similarity index (SSIM) of 0.8885 $\\pm$ 0.0913, and a signal-to-noise ratio (SNR) gain of 14.63 dB. Statistical analysis with paired t-tests confirms that these improvements are significant (p<0.001; Cohen's d>1.2). A leave-one-subject-out (LOSO) cross-validation protocol shows that the model generalizes well, yielding an average RMSE of 0.0635 $\\pm$ 0.0110 and an SSIM of 0.6658 $\\pm$ 0.0880 across unseen subjects. Additionally, saliency-based visualizations demonstrate that DAR highlights areas with dense artifacts, which makes its decisions easier to interpret. Overall, these results position DAR as a potential and understandable solution for real-time EEG artifact removal in simultaneous EEG-fMRI applications.",
      "code_url": null
    },
    "2508.02703v2": {
      "title": "Measuring Dependencies between Biological Signals with Self-supervision, and its Limitations",
      "url": "http://arxiv.org/abs/2508.02703v2",
      "authors": "Evangelos Sariyanidi, John D. Herrington, Lisa Yankowitz, Pratik Chaudhari, Theodore D. Satterthwaite, Casey J. Zampella, Robert T. Schultz, Russell T. Shinohara, Birkan Tunc",
      "update_time": "2025-08-08",
      "abstract": "Measuring the statistical dependence between observed signals is a primary tool for scientific discovery. However, biological systems often exhibit complex non-linear interactions that currently cannot be captured without a priori knowledge regarding the nature of dependence. We introduce a self-supervised approach, concurrence, which is inspired by the observation that if two signals are dependent, then one should be able to distinguish between temporally aligned vs. misaligned segments extracted from them. Experiments with fMRI, physiological and behavioral signals show that, to our knowledge, concurrence is the first approach that can expose relationships across such a wide spectrum of signals and extract scientifically relevant differences without ad-hoc parameter tuning or reliance on a priori information, providing a potent tool for scientific discoveries across fields. However, dependencies caused by extraneous factors remain an open problem, thus researchers should validate that exposed relationships truly pertain to the question(s) of interest.",
      "code_url": null
    },
    "2507.22229v1": {
      "title": "TRIBE: TRImodal Brain Encoder for whole-brain fMRI response prediction",
      "url": "http://arxiv.org/abs/2507.22229v1",
      "authors": "St\u00e9phane d'Ascoli, J\u00e9r\u00e9my Rapin, Yohann Benchetrit, Hubert Banville, Jean-R\u00e9mi King",
      "update_time": "2025-07-29",
      "abstract": "Historically, neuroscience has progressed by fragmenting into specialized domains, each focusing on isolated modalities, tasks, or brain regions. While fruitful, this approach hinders the development of a unified model of cognition. Here, we introduce TRIBE, the first deep neural network trained to predict brain responses to stimuli across multiple modalities, cortical areas and individuals. By combining the pretrained representations of text, audio and video foundational models and handling their time-evolving nature with a transformer, our model can precisely model the spatial and temporal fMRI responses to videos, achieving the first place in the Algonauts 2025 brain encoding competition with a significant margin over competitors. Ablations show that while unimodal models can reliably predict their corresponding cortical networks (e.g. visual or auditory networks), they are systematically outperformed by our multimodal model in high-level associative cortices. Currently applied to perception and comprehension, our approach paves the way towards building an integrative model of representations in the human brain. Our code is available at https://github.com/facebookresearch/algonauts-2025.",
      "code_url": null
    }
  },
  "MEG": {
    "2507.23525v1": {
      "title": "Wave Turbulence and Cortical Dynamics",
      "url": "http://arxiv.org/abs/2507.23525v1",
      "authors": "Gerald Kaushallye Cooray",
      "update_time": "2025-07-31",
      "abstract": "Cortical activity recorded through EEG and MEG reflects complex dynamics that span multiple temporal and spatial scales. Spectral analyses of these signals consistently reveal power-law behaviour, a hallmark of turbulent systems. In this paper, we derive a kinetic equation for neural field activity based on wave turbulence theory, highlighting how quantities such as energy and pseudo-particle density flow through wave-space (k-space) via direct and inverse cascades. We explore how different forms of nonlinearity, particularly 3-wave and 4-wave interactions, shape spectral features, including harmonic generation, spectral dispersion, and transient dynamics. While the observed power-law decays in empirical data are broadly consistent with turbulent cascades, variations across studies, such as the presence of dual decay rates or harmonic structures, point to a diversity of underlying mechanisms. We argue that although no single model fully explains all spectral observations, key constraints emerge: namely, that cortical dynamics exhibit features consistent with turbulent wave systems involving both single and dual cascades and a mixture of 3- and 4-wave interactions. This turbulence-based framework offers a principled and unifying approach to interpreting large-scale brain activity, including state transitions and seizure dynamics.",
      "code_url": null
    },
    "2507.21961v1": {
      "title": "Following the Committor Flow: A Data-Driven Discovery of Transition Pathways",
      "url": "http://arxiv.org/abs/2507.21961v1",
      "authors": "Cheng Giuseppe Chen, Chenyu Tang, Alberto Meg\u00edas, Radu A. Talmazan, Sergio Contreras Arredondo, Beno\u00eet Roux, Christophe Chipot",
      "update_time": "2025-07-29",
      "abstract": "The discovery of transition pathways to unravel distinct reaction mechanisms and, in general, rare events that occur in molecular systems is still a challenge. Recent advances have focused on analyzing the transition path ensemble using the committor probability, widely regarded as the most informative one-dimensional reaction coordinate. Consistency between transition pathways and the committor function is essential for accurate mechanistic insight. In this work, we propose an iterative framework to infer the committor and, subsequently, to identify the most relevant transition pathways. Starting from an initial guess for the transition path, we generate biased sampling from which we train a neural network to approximate the committor probability. From this learned committor, we extract dominant transition channels as discretized strings lying on isocommittor surfaces. These pathways are then used to enhance sampling and iteratively refine both the committor and the transition paths until convergence. The resulting committor enables accurate estimation of the reaction rate constant. We demonstrate the effectiveness of our approach on benchmark systems, including a two-dimensional model potential, peptide conformational transitions, and a Diels--Alder reaction.",
      "code_url": null
    },
    "2507.17700v1": {
      "title": "From Atoms to Dynamics: Learning the Committor Without Collective Variables",
      "url": "http://arxiv.org/abs/2507.17700v1",
      "authors": "Sergio Contreras Arredondo, Chenyu Tang, Radu A. Talmazan, Alberto Meg\u00edas, Cheng Giuseppe Chen, Christophe Chipot",
      "update_time": "2025-07-23",
      "abstract": "This Brief Communication introduces a graph-neural-network architecture built on geometric vector perceptrons to predict the committor function directly from atomic coordinates, bypassing the need for hand-crafted collective variables (CVs). The method offers atom-level interpretability, pinpointing the key atomic players in complex transitions without relying on prior assumptions. Applied across diverse molecular systems, the method accurately infers the committor function and highlights the importance of each heavy atom in the transition mechanism. It also yields precise estimates of the rate constants for the underlying processes. The proposed approach opens new avenues for understanding and modeling complex dynamics, by enabling CV-free learning and automated identification of physically meaningful reaction coordinates of complex molecular processes.",
      "code_url": null
    },
    "2507.14224v1": {
      "title": "Diffusion-based translation between unpaired spontaneous premature neonatal EEG and fetal MEG",
      "url": "http://arxiv.org/abs/2507.14224v1",
      "authors": "Beno\u00eet Brebion, Alban Gallard, Katrin Sippel, Amer Zaylaa, Hubert Preissl, Sahar Moghimi, Fabrice Wallois, Ya\u00ebl Fr\u00e9gier",
      "update_time": "2025-07-16",
      "abstract": "Background and objective: Brain activity in premature newborns has traditionally been studied using electroencephalography (EEG), leading to substantial advances in our understanding of early neural development. However, since brain development takes root at the fetal stage, a critical window of this process remains largely unknown. The only technique capable of recording neural activity in the intrauterine environment is fetal magnetoencephalography (fMEG), but this approach presents challenges in terms of data quality and scarcity. Using artificial intelligence, the present research aims to transfer the well-established knowledge from EEG studies to fMEG to improve understanding of prenatal brain development, laying the foundations for better detection and treatment of potential pathologies. Methods: We developed an unpaired diffusion translation method based on dual diffusion bridges, which notably includes numerical integration improvements to obtain more qualitative results at a lower computational cost. Models were trained on our unpaired dataset of bursts of spontaneous activity from 30 high-resolution premature newborns EEG recordings and 44 fMEG recordings. Results: We demonstrate that our method achieves significant improvement upon previous results obtained with Generative Adversarial Networks (GANs), by almost 5% on the mean squared error in the time domain, and completely eliminating the mode collapse problem in the frequency domain, thus achieving near-perfect signal fidelity. Conclusion: We set a new state of the art in the EEG-fMEG unpaired translation problem, as our developed tool completely paves the way for early brain activity analysis. Overall, we also believe that our method could be reused for other unpaired signal translation applications.",
      "code_url": null
    },
    "2507.09747v1": {
      "title": "BrainFLORA: Uncovering Brain Concept Representation via Multimodal Neural Embeddings",
      "url": "http://arxiv.org/abs/2507.09747v1",
      "authors": "Dongyang Li, Haoyang Qin, Mingyang Wu, Chen Wei, Quanying Liu",
      "update_time": "2025-07-13",
      "abstract": "Understanding how the brain represents visual information is a fundamental challenge in neuroscience and artificial intelligence. While AI-driven decoding of neural data has provided insights into the human visual system, integrating multimodal neuroimaging signals, such as EEG, MEG, and fMRI, remains a critical hurdle due to their inherent spatiotemporal misalignment. Current approaches often analyze these modalities in isolation, limiting a holistic view of neural representation. In this study, we introduce BrainFLORA, a unified framework for integrating cross-modal neuroimaging data to construct a shared neural representation. Our approach leverages multimodal large language models (MLLMs) augmented with modality-specific adapters and task decoders, achieving state-of-the-art performance in joint-subject visual retrieval task and has the potential to extend multitasking. Combining neuroimaging analysis methods, we further reveal how visual concept representations align across neural modalities and with real world object perception. We demonstrate that the brain's structured visual concept representations exhibit an implicit mapping to physical-world stimuli, bridging neuroscience and machine learning from different modalities of neural imaging. Beyond methodological advancements, BrainFLORA offers novel implications for cognitive neuroscience and brain-computer interfaces (BCIs). Our code is available at https://github.com/ncclab-sustech/BrainFLORA.",
      "code_url": null
    },
    "2507.06610v1": {
      "title": "Resonant leptogenesis in inverse see-saw framework with modular $S_4$ symmetry",
      "url": "http://arxiv.org/abs/2507.06610v1",
      "authors": "Abhishek, V. Suryanarayana Mummidi",
      "update_time": "2025-07-09",
      "abstract": "This work introduces a model for lepton mass generation and flavor mixing, realized through a (2,3) inverse seesaw structure within a modular \\( S_4 \\) symmetry framework. The model employs modular forms to construct the lepton Yukawa couplings, thereby significantly simplifying the model by reducing its complexity. A detailed numerical analysis demonstrates consistency with current neutrino oscillation data, yielding constrained predictions for the mixing angles and CP-violating phases. The Dirac CP phase is sharply localized near \\( \\delta_{\\rm CP} \\sim 359^\\circ \\), and the model predicts an effective Majorana mass \\( |m_{ee}| \\sim \\mathcal{O}(10^{-3}) \\,\\text{eV} \\), Within the scope of upcoming experiments on neutrinoless double beta decay such as nEXO and AMoRE-II. The model also remains consistent with current bounds on charged lepton flavor violating processes from MEG and BaBar. We further explore resonant leptogenesis enabled by quasi-degenerate heavy neutrino states, and show that observed baryon asymmetry of the universe can be succesfully generated within this framework. The combined treatment of low-energy observables and high-scale baryogenesis demonstrates the predictivity and testability of the modular \\( S_4 \\)-based ISS(2,3) framework.",
      "code_url": null
    },
    "2506.20534v1": {
      "title": "Revisiting CHAMPAGNE: Sparse Bayesian Learning as Reweighted Sparse Coding",
      "url": "http://arxiv.org/abs/2506.20534v1",
      "authors": "Dylan Sechet, Matthieu Kowalski, Samy Mokhtari, Bruno Torr\u00e9sani",
      "update_time": "2025-06-25",
      "abstract": "This paper revisits the CHAMPAGNE algorithm within the Sparse Bayesian Learning (SBL) framework and establishes its connection to reweighted sparse coding. We demonstrate that the SBL objective can be reformulated as a reweighted $\\ell_{21}$-minimization problem, providing a more straightforward interpretation of the sparsity mechanism and enabling the design of an efficient iterative algorithm. Additionally, we analyze the behavior of this reformulation in the low signal-to-noise ratio (SNR) regime, showing that it simplifies to a weighted $\\ell_{21}$-regularized least squares problem. Numerical experiments validate the proposed approach, highlighting its improved computational efficiency and ability to produce exact sparse solutions, particularly in simulated MEG source localization tasks.",
      "code_url": null
    },
    "2506.12817v1": {
      "title": "Magnetoencephalography (MEG) Based Non-Invasive Chinese Speech Decoding",
      "url": "http://arxiv.org/abs/2506.12817v1",
      "authors": "Zhihong Jia, Hongbin Wang, Yuanzhong Shen, Feng Hu, Jiayu An, Kai Shu, Dongrui Wu",
      "update_time": "2025-06-15",
      "abstract": "As an emerging paradigm of brain-computer interfaces (BCIs), speech BCI has the potential to directly reflect auditory perception and thoughts, offering a promising communication alternative for patients with aphasia. Chinese is one of the most widely spoken languages in the world, whereas there is very limited research on speech BCIs for Chinese language. This paper reports a text-magnetoencephalography (MEG) dataset for non-invasive Chinese speech BCIs. It also proposes a multi-modality assisted speech decoding (MASD) algorithm to capture both text and acoustic information embedded in brain signals during speech activities. Experiment results demonstrated the effectiveness of both our text-MEG dataset and our proposed MASD algorithm. To our knowledge, this is the first study on modality-assisted decoding for non-invasive speech BCIs.",
      "code_url": null
    },
    "2506.10165v1": {
      "title": "The 2025 PNPL Competition: Speech Detection and Phoneme Classification in the LibriBrain Dataset",
      "url": "http://arxiv.org/abs/2506.10165v1",
      "authors": "Gilad Landau, Miran \u00d6zdogan, Gereon Elvers, Francesco Mantegna, Pratik Somaiya, Dulhan Jayalath, Luisa Kurth, Teyun Kwon, Brendan Shillingford, Greg Farquhar, Minqi Jiang, Karim Jerbi, Hamza Abdelhedi, Yorguin Mantilla Ramos, Caglar Gulcehre, Mark Woolrich, Natalie Voets, Oiwi Parker Jones",
      "update_time": "2025-06-11",
      "abstract": "The advance of speech decoding from non-invasive brain data holds the potential for profound societal impact. Among its most promising applications is the restoration of communication to paralysed individuals affected by speech deficits such as dysarthria, without the need for high-risk surgical interventions. The ultimate aim of the 2025 PNPL competition is to produce the conditions for an \"ImageNet moment\" or breakthrough in non-invasive neural decoding, by harnessing the collective power of the machine learning community.   To facilitate this vision we present the largest within-subject MEG dataset recorded to date (LibriBrain) together with a user-friendly Python library (pnpl) for easy data access and integration with deep learning frameworks. For the competition we define two foundational tasks (i.e. Speech Detection and Phoneme Classification from brain data), complete with standardised data splits and evaluation metrics, illustrative benchmark models, online tutorial code, a community discussion board, and public leaderboard for submissions. To promote accessibility and participation the competition features a Standard track that emphasises algorithmic innovation, as well as an Extended track that is expected to reward larger-scale computing, accelerating progress toward a non-invasive brain-computer interface for speech.",
      "code_url": null
    },
    "2506.08511v1": {
      "title": "The Predictive Brain: Neural Correlates of Word Expectancy Align with Large Language Model Prediction Probabilities",
      "url": "http://arxiv.org/abs/2506.08511v1",
      "authors": "Nikola K\u00f6lbl, Konstantin Tziridis, Andreas Maier, Thomas Kinfe, Ricardo Chavarriaga, Achim Schilling, Patrick Krauss",
      "update_time": "2025-06-10",
      "abstract": "Predictive coding theory suggests that the brain continuously anticipates upcoming words to optimize language processing, but the neural mechanisms remain unclear, particularly in naturalistic speech. Here, we simultaneously recorded EEG and MEG data from 29 participants while they listened to an audio book and assigned predictability scores to nouns using the BERT language model. Our results show that higher predictability is associated with reduced neural responses during word recognition, as reflected in lower N400 amplitudes, and with increased anticipatory activity before word onset. EEG data revealed increased pre-activation in left fronto-temporal regions, while MEG showed a tendency for greater sensorimotor engagement in response to low-predictability words, suggesting a possible motor-related component to linguistic anticipation. These findings provide new evidence that the brain dynamically integrates top-down predictions with bottom-up sensory input to facilitate language comprehension. To our knowledge, this is the first study to demonstrate these effects using naturalistic speech stimuli, bridging computational language models with neurophysiological data. Our findings provide novel insights for cognitive computational neuroscience, advancing the understanding of predictive processing in language and inspiring the development of neuroscience-inspired AI. Future research should explore the role of prediction and sensory precision in shaping neural responses and further refine models of language processing.",
      "code_url": null
    }
  },
  "neuroAI": {
    "2507.06645v1": {
      "title": "Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers",
      "url": "http://arxiv.org/abs/2507.06645v1",
      "authors": "Thomas Klein, Sascha Meyen, Wieland Brendel, Felix A. Wichmann, Kristof Meding",
      "update_time": "2025-07-09",
      "abstract": "Benchmarking models is a key factor for the rapid progress in machine learning (ML) research. Thus, further progress depends on improving benchmarking metrics. A standard metric to measure the behavioral alignment between ML models and human observers is error consistency (EC). EC allows for more fine-grained comparisons of behavior than other metrics such as e.g. accuracy, and has been used in the influential Brain-Score benchmark to rank different DNNs by their behavioral consistency with humans. Previously, EC values have been reported without confidence intervals. However, empirically measured EC values are typically noisy -- thus, without confidence intervals, valid benchmarking conclusions are problematic. Here we improve on standard EC in two ways: First, we show how to obtain confidence intervals for EC using a bootstrapping technique, allowing us to derive significance tests for EC. Second, we propose a new computational model relating the EC between two classifiers to the implicit probability that one of them copies responses from the other. This view of EC allows us to give practical guidance to scientists regarding the number of trials required for sufficiently powerful, conclusive experiments. Finally, we use our methodology to revisit popular NeuroAI-results. We find that while the general trend of behavioral differences between humans and machines holds up to scrutiny, many reported differences between deep vision models are statistically insignificant. Our methodology enables researchers to design adequately powered experiments that can reliably detect behavioral differences between models, providing a foundation for more rigorous benchmarking of behavioral alignment.",
      "code_url": null
    },
    "2507.02103v1": {
      "title": "What Neuroscience Can Teach AI About Learning in Continuously Changing Environments",
      "url": "http://arxiv.org/abs/2507.02103v1",
      "authors": "Daniel Durstewitz, Bruno Averbeck, Georgia Koppe",
      "update_time": "2025-07-02",
      "abstract": "Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.",
      "code_url": null
    },
    "2506.04536v2": {
      "title": "NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models",
      "url": "http://arxiv.org/abs/2506.04536v2",
      "authors": "Luca Ghafourpour, Valentin Duruisseaux, Bahareh Tolooshams, Philip H. Wong, Costas A. Anastassiou, Anima Anandkumar",
      "update_time": "2025-06-12",
      "abstract": "Characterizing the diverse computational properties of human neurons via multimodal electrophysiological, transcriptomic, and morphological data provides the foundation for constructing and validating bio-realistic neuron models that can advance our understanding of fundamental mechanisms underlying brain function. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. To capture variability, ensembles of deterministic models are often used, but are difficult to scale as model generation requires repeating computationally expensive optimization for each neuron. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on data generated from biophysically realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE is the first scaled-up deep learning framework validated on real experimental data, enabling efficient generation of synthetic neurons that exhibit trial-to-trial variability and achieve a $4200\\times$ speedup over numerical solvers. To this end, NOBLE captures fundamental neural properties, opening the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.",
      "code_url": null
    },
    "2505.16080v1": {
      "title": "SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation",
      "url": "http://arxiv.org/abs/2505.16080v1",
      "authors": "Jiayue Liu, Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Kuo Yang, Xu Wang, Yang Wang",
      "update_time": "2025-05-21",
      "abstract": "Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.",
      "code_url": null
    },
    "2503.06286v1": {
      "title": "A 7T fMRI dataset of synthetic images for out-of-distribution modeling of vision",
      "url": "http://arxiv.org/abs/2503.06286v1",
      "authors": "Alessandro T. Gifford, Radoslaw M. Cichy, Thomas Naselaris, Kendrick Kay",
      "update_time": "2025-03-08",
      "abstract": "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD) are boosting NeuroAI research by enabling computational models of the brain with performances beyond what was possible just a decade ago. However, these datasets lack out-of-distribution (OOD) components, which are crucial for the development of more robust models. Here, we address this limitation by releasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the eight NSD subjects for 284 carefully controlled synthetic images. We show that NSD-synthetic's fMRI responses reliably encode stimulus-related information and are OOD with respect to NSD. Furthermore, OOD generalization tests on NSD-synthetic reveal differences between models of the brain that are not detected with NSD - specifically, self-supervised deep neural networks better explain neural responses than their task-supervised counterparts. These results showcase how NSD-synthetic enables OOD generalization tests that facilitate the development of more robust models of visual processing, and the formulation of more accurate theories of human vision.",
      "code_url": null
    },
    "2502.16238v1": {
      "title": "Brain-Model Evaluations Need the NeuroAI Turing Test",
      "url": "http://arxiv.org/abs/2502.16238v1",
      "authors": "Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi",
      "update_time": "2025-02-22",
      "abstract": "What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \\emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.",
      "code_url": null
    },
    "2501.02402v1": {
      "title": "Asynchronous Hebbian/anti-Hebbian networks",
      "url": "http://arxiv.org/abs/2501.02402v1",
      "authors": "Henrique Reis Aguiar, Matthias H. Hennig",
      "update_time": "2025-01-04",
      "abstract": "Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.",
      "code_url": null
    },
    "2411.18526v2": {
      "title": "NeuroAI for AI Safety",
      "url": "http://arxiv.org/abs/2411.18526v2",
      "authors": "Patrick Mineault, Niccol\u00f2 Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador",
      "update_time": "2025-04-03",
      "abstract": "As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.",
      "code_url": null
    },
    "2411.14633v1": {
      "title": "Evaluating Representational Similarity Measures from the Lens of Functional Correspondence",
      "url": "http://arxiv.org/abs/2411.14633v1",
      "authors": "Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla",
      "update_time": "2024-11-21",
      "abstract": "Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research.",
      "code_url": null
    },
    "2409.05771v1": {
      "title": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models",
      "url": "http://arxiv.org/abs/2409.05771v1",
      "authors": "Emily Cheng, Richard J. Antonello",
      "update_time": "2024-09-09",
      "abstract": "Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first \"composition\" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties.",
      "code_url": null
    }
  },
  "medical": {
    "2508.09087v1": {
      "title": "Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision",
      "url": "http://arxiv.org/abs/2508.09087v1",
      "authors": "Ahsan Habib Akash, Greg Murray, Annahita Amireskandari, Joel Palko, Carol Laxson, Binod Bhattarai, Prashnna Gyawali",
      "update_time": "2025-08-12",
      "abstract": "Vision-Language Models (VLMs) have achieved remarkable success on multimodal tasks such as image-text retrieval and zero-shot classification, yet they can exhibit demographic biases even when explicit protected attributes are absent during training. In this work, we focus on automated glaucoma screening from retinal fundus images, a critical application given that glaucoma is a leading cause of irreversible blindness and disproportionately affects underserved populations. Building on a reweighting-based contrastive learning framework, we introduce an attribute-agnostic debiasing method that (i) infers proxy subgroups via unsupervised clustering of image-image embeddings, (ii) computes gradient-similarity weights between the CLIP-style multimodal loss and a SimCLR-style image-pair contrastive loss, and (iii) applies these weights in a joint, top-$k$ weighted objective to upweight underperforming clusters. This label-free approach adaptively targets the hardest examples, thereby reducing subgroup disparities. We evaluate our method on the Harvard FairVLMed glaucoma subset, reporting Equalized Odds Distance (EOD), Equalized Subgroup AUC (ES AUC), and Groupwise AUC to demonstrate equitable performance across inferred demographic subgroups.",
      "code_url": null
    },
    "2508.09014v1": {
      "title": "Uncertainty-aware Cross-training for Semi-supervised Medical Image Segmentation",
      "url": "http://arxiv.org/abs/2508.09014v1",
      "authors": "Kaiwen Huang, Tao Zhou, Huazhu Fu, Yizhe Zhang, Yi Zhou, Xiao-Jun Wu",
      "update_time": "2025-08-12",
      "abstract": "Semi-supervised learning has gained considerable popularity in medical image segmentation tasks due to its capability to reduce reliance on expert-examined annotations. Several mean-teacher (MT) based semi-supervised methods utilize consistency regularization to effectively leverage valuable information from unlabeled data. However, these methods often heavily rely on the student model and overlook the potential impact of cognitive biases within the model. Furthermore, some methods employ co-training using pseudo-labels derived from different inputs, yet generating high-confidence pseudo-labels from perturbed inputs during training remains a significant challenge. In this paper, we propose an Uncertainty-aware Cross-training framework for semi-supervised medical image Segmentation (UC-Seg). Our UC-Seg framework incorporates two distinct subnets to effectively explore and leverage the correlation between them, thereby mitigating cognitive biases within the model. Specifically, we present a Cross-subnet Consistency Preservation (CCP) strategy to enhance feature representation capability and ensure feature consistency across the two subnets. This strategy enables each subnet to correct its own biases and learn shared semantics from both labeled and unlabeled data. Additionally, we propose an Uncertainty-aware Pseudo-label Generation (UPG) component that leverages segmentation results and corresponding uncertainty maps from both subnets to generate high-confidence pseudo-labels. We extensively evaluate the proposed UC-Seg on various medical image segmentation tasks involving different modality images, such as MRI, CT, ultrasound, colonoscopy, and so on. The results demonstrate that our method achieves superior segmentation accuracy and generalization performance compared to other state-of-the-art semi-supervised methods. Our code will be released at https://github.com/taozh2017/UCSeg.",
      "code_url": null
    },
    "2508.08937v1": {
      "title": "Accelerated Volumetric Compression without Hierarchies: A Fourier Feature Based Implicit Neural Representation Approach",
      "url": "http://arxiv.org/abs/2508.08937v1",
      "authors": "Leona \u017d\u016frkov\u00e1, Petr Strako\u0161, Michal Krav\u010denko, Tom\u00e1\u0161 Brzobohat\u00fd, Lubom\u00edr \u0158\u00edha",
      "update_time": "2025-08-12",
      "abstract": "Volumetric data compression is critical in fields like medical imaging, scientific simulation, and entertainment. We introduce a structure-free neural compression method combining Fourierfeature encoding with selective voxel sampling, yielding compact volumetric representations and faster convergence. Our dynamic voxel selection uses morphological dilation to prioritize active regions, reducing redundant computation without any hierarchical metadata. In the experiment, sparse training reduced training time by 63.7 % (from 30 to 11 minutes) with only minor quality loss: PSNR dropped 0.59 dB (from 32.60 to 32.01) and SSIM by 0.008 (from 0.948 to 0.940). The resulting neural representation, stored solely as network weights, achieves a compression rate of 14 and eliminates traditional data-loading overhead. This connects coordinate-based neural representation with efficient volumetric compression, offering a scalable, structure-free solution for practical applications.",
      "code_url": null
    },
    "2508.08803v1": {
      "title": "Steady-State Free Precession NMR in the Presence of Heteronuclear Couplings and Decoupling: More Than Meets the Eye",
      "url": "http://arxiv.org/abs/2508.08803v1",
      "authors": "Sundaresan Jayanthi, Zuzana Osifov\u00e1, Mark Shif, Adonis Lupulescu, Lucio Frydman",
      "update_time": "2025-08-12",
      "abstract": "Fourier Transform (FT) has been a mainstay of analytical 13C and 15N NMR. On the other hand it has been shown that Steady State Free Precession (SSFP) experiments which depart from this scheme can, under certain conditions, endow 13C and 15N small molecule NMR with comparable sensitivity and resolution. SSFP is one of the earliest and most widely used NMR pulse sequences, yet its analyses have focused on isolated spin-half ensembles such as water. The present study demonstrates that significant deviations from such isolated spin-half behavior may occur when SSFP is applied in the presence of spin-spin couplings. Even in the simplest case supporting such couplings, a single 13C J coupled to a 1H, departures from the isolated spin-half behavior arise in the 13C SSFP response, both in the absence and in the presence of 1H spin decoupling. In the former case deviations are produced by the differential relaxation of antiphase two-spin terms generated by the pulse train; in the latter case, magnified interferences may arise between the SSFP pulses and the coherent perturbation arising upon 1H decoupling. Although both phenomena are also known in FT NMR, the spectral distortions that they will originate may be much larger in the SSFP case, particularly if interpulse delays in large flip angle SSFP pulse trains resonate with the coupling perturbations. The origins of these effects are here analyzed for heteronuclear spin-half systems and corroborated with 13C NMR SSFP experiments recorded under different conditions. Additional considerations aimed at magnifying or suppressing these effects, as well as extensions to more complex scenarios, are also briefly discussed.",
      "code_url": null
    },
    "2508.08764v1": {
      "title": "CARES: Collaborative Agentic Reasoning for Error Detection in Surgery",
      "url": "http://arxiv.org/abs/2508.08764v1",
      "authors": "Chang Han Low, Zhu Zhuo, Ziyue Wang, Jialang Xu, Haofeng Liu, Nazir Sirajudeen, Matthew Boal, Philip J. Edwards, Danail Stoyanov, Nader Francis, Jiehui Zhong, Di Gu, Evangelos B. Mazomenos, Yueming Jin",
      "update_time": "2025-08-12",
      "abstract": "Robotic-assisted surgery (RAS) introduces complex challenges that current surgical error detection methods struggle to address effectively due to limited training data and methodological constraints. Therefore, we construct MERP (Multi-class Error in Robotic Prostatectomy), a comprehensive dataset for error detection in robotic prostatectomy with frame-level annotations featuring six clinically aligned error categories. In addition, we propose CARES (Collaborative Agentic Reasoning for Error Detection in Surgery), a novel zero-shot clinically-informed and risk-stratified agentic reasoning architecture for multi-class surgical error detection. CARES implements adaptive generation of medically informed, error-specific Chain-of-Thought (CoT) prompts across multiple expertise levels. The framework employs risk-aware routing to assign error task to expertise-matched reasoning pathways based on complexity and clinical impact. Subsequently, each pathway decomposes surgical error analysis into three specialized agents with temporal, spatial, and procedural analysis. Each agent analyzes using dynamically selected prompts tailored to the assigned expertise level and error type, generating detailed and transparent reasoning traces. By incorporating clinically informed reasoning from established surgical assessment guidelines, CARES enables zero-shot surgical error detection without prior training. Evaluation demonstrates superior performance with 54.3 mF1 on RARP and 52.0 mF1 on MERP datasets, outperforming existing zero-shot approaches by up to 14% while remaining competitive with trained models. Ablation studies demonstrate the effectiveness of our method. The dataset and code will be publicly available.",
      "code_url": null
    },
    "2508.08730v1": {
      "title": "Magical: Medical Lay Language Generation via Semantic Invariance and Layperson-tailored Adaptation",
      "url": "http://arxiv.org/abs/2508.08730v1",
      "authors": "Weibin Liao, Tianlong Wang, Yinghao Zhu, Yasha Wang, Junyi Gao, Liantao Ma",
      "update_time": "2025-08-12",
      "abstract": "Medical Lay Language Generation (MLLG) plays a vital role in improving the accessibility of complex scientific content for broader audiences. Recent literature to MLLG commonly employ parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using paired expert-lay language datasets. However, LoRA struggles with the challenges posed by multi-source heterogeneous MLLG datasets. Specifically, through a series of exploratory experiments, we reveal that standard LoRA fail to meet the requirement for semantic fidelity and diverse lay-style generation in MLLG task. To address these limitations, we propose Magical, an asymmetric LoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical employs a shared matrix $A$ for abstractive summarization, along with multiple isolated matrices $B$ for diverse lay-style generation. To preserve semantic fidelity during the lay language generation process, Magical introduces a Semantic Invariance Constraint to mitigate semantic subspace shifts on matrix $A$. Furthermore, to better adapt to diverse lay-style generation, Magical incorporates the Recommendation-guided Switch, an externally interface to prompt the LLM to switch between different matrices $B$. Experimental results on three real-world lay language generation datasets demonstrate that Magical consistently outperforms prompt-based methods, vanilla LoRA, and its recent variants, while also reducing trainable parameters by 31.66%.",
      "code_url": null
    },
    "2508.08724v1": {
      "title": "Hierarchical Variable Importance with Statistical Control for Medical Data-Based Prediction",
      "url": "http://arxiv.org/abs/2508.08724v1",
      "authors": "Joseph Paillard, Antoine Collas, Denis A. Engemann, Bertrand Thirion",
      "update_time": "2025-08-12",
      "abstract": "Recent advances in machine learning have greatly expanded the repertoire of predictive methods for medical imaging. However, the interpretability of complex models remains a challenge, which limits their utility in medical applications. Recently, model-agnostic methods have been proposed to measure conditional variable importance and accommodate complex non-linear models. However, they often lack power when dealing with highly correlated data, a common problem in medical imaging. We introduce Hierarchical-CPI, a model-agnostic variable importance measure that frames the inference problem as the discovery of groups of variables that are jointly predictive of the outcome. By exploring subgroups along a hierarchical tree, it remains computationally tractable, yet also enjoys explicit family-wise error rate control. Moreover, we address the issue of vanishing conditional importance under high correlation with a tree-based importance allocation mechanism. We benchmarked Hierarchical-CPI against state-of-the-art variable importance methods. Its effectiveness is demonstrated in two neuroimaging datasets: classifying dementia diagnoses from MRI data (ADNI dataset) and analyzing the Berger effect on EEG data (TDBRAIN dataset), identifying biologically plausible variables.",
      "code_url": null
    },
    "2508.08679v1": {
      "title": "MMIF-AMIN: Adaptive Loss-Driven Multi-Scale Invertible Dense Network for Multimodal Medical Image Fusion",
      "url": "http://arxiv.org/abs/2508.08679v1",
      "authors": "Tao Luo, Weihua Xu",
      "update_time": "2025-08-12",
      "abstract": "Multimodal medical image fusion (MMIF) aims to integrate images from different modalities to produce a comprehensive image that enhances medical diagnosis by accurately depicting organ structures, tissue textures, and metabolic information. Capturing both the unique and complementary information across multiple modalities simultaneously is a key research challenge in MMIF. To address this challenge, this paper proposes a novel image fusion method, MMIF-AMIN, which features a new architecture that can effectively extract these unique and complementary features. Specifically, an Invertible Dense Network (IDN) is employed for lossless feature extraction from individual modalities. To extract complementary information between modalities, a Multi-scale Complementary Feature Extraction Module (MCFEM) is designed, which incorporates a hybrid attention mechanism, convolutional layers of varying sizes, and Transformers. An adaptive loss function is introduced to guide model learning, addressing the limitations of traditional manually-designed loss functions and enhancing the depth of data mining. Extensive experiments demonstrate that MMIF-AMIN outperforms nine state-of-the-art MMIF methods, delivering superior results in both quantitative and qualitative analyses. Ablation experiments confirm the effectiveness of each component of the proposed method. Additionally, extending MMIF-AMIN to other image fusion tasks also achieves promising performance.",
      "code_url": null
    },
    "2508.08660v1": {
      "title": "Unified and Semantically Grounded Domain Adaptation for Medical Image Segmentation",
      "url": "http://arxiv.org/abs/2508.08660v1",
      "authors": "Xin Wang, Yin Guo, Jiamin Xia, Kaiyu Zhang, Niranjan Balu, Mahmud Mossa-Basha, Linda Shapiro, Chun Yuan",
      "update_time": "2025-08-12",
      "abstract": "Most prior unsupervised domain adaptation approaches for medical image segmentation are narrowly tailored to either the source-accessible setting, where adaptation is guided by source-target alignment, or the source-free setting, which typically resorts to implicit supervision mechanisms such as pseudo-labeling and model distillation. This substantial divergence in methodological designs between the two settings reveals an inherent flaw: the lack of an explicit, structured construction of anatomical knowledge that naturally generalizes across domains and settings. To bridge this longstanding divide, we introduce a unified, semantically grounded framework that supports both source-accessible and source-free adaptation. Fundamentally distinct from all prior works, our framework's adaptability emerges naturally as a direct consequence of the model architecture, without the need for any handcrafted adaptation strategies. Specifically, our model learns a domain-agnostic probabilistic manifold as a global space of anatomical regularities, mirroring how humans establish visual understanding. Thus, the structural content in each image can be interpreted as a canonical anatomy retrieved from the manifold and a spatial transformation capturing individual-specific geometry. This disentangled, interpretable formulation enables semantically meaningful prediction with intrinsic adaptability. Extensive experiments on challenging cardiac and abdominal datasets show that our framework achieves state-of-the-art results in both settings, with source-free performance closely approaching its source-accessible counterpart, a level of consistency rarely observed in prior works. Beyond quantitative improvement, we demonstrate strong interpretability of the proposed framework via manifold traversal for smooth shape manipulation.",
      "code_url": null
    },
    "2508.08602v1": {
      "title": "Biomedical Signal Processing: EEG and ECG Classification with Discrete Wavelet Transforms, Energy Distribution, and Convolutional Neural Networks",
      "url": "http://arxiv.org/abs/2508.08602v1",
      "authors": "Justin London",
      "update_time": "2025-08-12",
      "abstract": "Biomedical signal processing extract meaningful information from physiological signals like electrocardiograms (ECGs), electroencephalograms (EEGs), and electromyograms (EMGs) to diagnose, monitor, and treat medical conditions and diseases such as seizures, cardiomyopathy, and neuromuscular disorders, respectively. Traditional manual physician analysis of electrical recordings is prone to human error as subtle anomolies may not be detected. Recently, advanced deep learning has significantly improved the accuracy of biomedical signal analysis. A multi-modal deep learning model is proposed that utilizes discrete wavelet transforms for signal pre-processing to reduce noise. A multi-modal image fusion and multimodal feature fusion framework is utilized that converts numeric biomedical signals into 2D and 3D images for image processing using Gramian angular fields, recurrency plots, and Markov transition fields. In this paper, deep learning models are applied to ECG, EEG, and human activity signals using actual medical datasets, brain, and heart recordings. The results demonstrate that using a multi-modal approach using wavelet transforms improves the accuracy of disease and disorder classification.",
      "code_url": null
    }
  }
}