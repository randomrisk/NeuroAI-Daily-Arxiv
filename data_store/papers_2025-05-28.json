{
  "Brain": {
    "2505.20029v1": {
      "title": "Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)",
      "url": "http://arxiv.org/abs/2505.20029v1",
      "authors": "Subba Reddy Oota, Akshett Jindal, Ishani Mondal, Khushbu Pahwa, Satya Sai Srinath Namburi, Manish Shrivastava, Maneesh Singh, Bapi S. Raju, Manish Gupta",
      "update_time": "2025-05-26",
      "abstract": "Transformer-based language models, though not explicitly trained to mimic brain recordings, have demonstrated surprising alignment with brain activity. Progress in these models-through increased size, instruction-tuning, and multimodality-has led to better representational alignment with neural data. Recently, a new class of instruction-tuned multimodal LLMs (MLLMs) have emerged, showing remarkable zero-shot capabilities in open-ended multimodal vision tasks. However, it is unknown whether MLLMs, when prompted with natural instructions, lead to better brain alignment and effectively capture instruction-specific representations. To address this, we first investigate brain alignment, i.e., measuring the degree of predictivity of neural visual activity using text output response embeddings from MLLMs as participants engage in watching natural scenes. Experiments with 10 different instructions show that MLLMs exhibit significantly better brain alignment than vision-only models and perform comparably to non-instruction-tuned multimodal models like CLIP. We also find that while these MLLMs are effective at generating high-quality responses suitable to the task-specific instructions, not all instructions are relevant for brain alignment. Further, by varying instructions, we make the MLLMs encode instruction-specific visual concepts related to the input image. This analysis shows that MLLMs effectively capture count-related and recognition-related concepts, demonstrating strong alignment with brain activity. Notably, the majority of the explained variance of the brain encoding models is shared between MLLM embeddings of image captioning and other instructions. These results suggest that enhancing MLLMs' ability to capture task-specific information could lead to better differentiation between various types of instructions, and thereby improving their precision in predicting brain responses."
    },
    "2505.20027v1": {
      "title": "Multi-modal brain encoding models for multi-modal stimuli",
      "url": "http://arxiv.org/abs/2505.20027v1",
      "authors": "Subba Reddy Oota, Khushbu Pahwa, Mounika Marreddy, Maneesh Singh, Manish Gupta, Bapi S. Raju",
      "update_time": "2025-05-26",
      "abstract": "Despite participants engaging in unimodal stimuli, such as watching images or silent videos, recent work has demonstrated that multi-modal Transformer models can predict visual brain activity impressively well, even with incongruent modality representations. This raises the question of how accurately these multi-modal models can predict brain activity when participants are engaged in multi-modal stimuli. As these models grow increasingly popular, their use in studying neural activity provides insights into how our brains respond to such multi-modal naturalistic stimuli, i.e., where it separates and integrates information across modalities through a hierarchy of early sensory regions to higher cognition. We investigate this question by using multiple unimodal and two types of multi-modal models-cross-modal and jointly pretrained-to determine which type of model is more relevant to fMRI brain activity when participants are engaged in watching movies. We observe that both types of multi-modal models show improved alignment in several language and visual regions. This study also helps in identifying which brain regions process unimodal versus multi-modal information. We further investigate the contribution of each modality to multi-modal alignment by carefully removing unimodal features one by one from multi-modal representations, and find that there is additional information beyond the unimodal embeddings that is processed in the visual and language regions. Based on this investigation, we find that while for cross-modal models, their brain alignment is partially attributed to the video modality; for jointly pretrained models, it is partially attributed to both the video and audio modalities. This serves as a strong motivation for the neuroscience community to investigate the interpretability of these models for deepening our understanding of multi-modal information processing in brain."
    },
    "2505.19954v1": {
      "title": "An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning",
      "url": "http://arxiv.org/abs/2505.19954v1",
      "authors": "Andrew Zamai, Nathanael Fijalkow, Boris Mansencal, Laurent Simon, Eloi Navet, Pierrick Coupe",
      "update_time": "2025-05-26",
      "abstract": "The differential diagnosis of neurodegenerative dementias is a challenging clinical task, mainly because of the overlap in symptom presentation and the similarity of patterns observed in structural neuroimaging. To improve diagnostic efficiency and accuracy, deep learning-based methods such as Convolutional Neural Networks and Vision Transformers have been proposed for the automatic classification of brain MRIs. However, despite their strong predictive performance, these models find limited clinical utility due to their opaque decision making. In this work, we propose a framework that integrates two core components to enhance diagnostic transparency. First, we introduce a modular pipeline for converting 3D T1-weighted brain MRIs into textual radiology reports. Second, we explore the potential of modern Large Language Models (LLMs) to assist clinicians in the differential diagnosis between Frontotemporal dementia subtypes, Alzheimer's disease, and normal aging based on the generated reports. To bridge the gap between predictive accuracy and explainability, we employ reinforcement learning to incentivize diagnostic reasoning in LLMs. Without requiring supervised reasoning traces or distillation from larger models, our approach enables the emergence of structured diagnostic rationales grounded in neuroimaging findings. Unlike post-hoc explainability methods that retrospectively justify model decisions, our framework generates diagnostic rationales as part of the inference process-producing causally grounded explanations that inform and guide the model's decision-making process. In doing so, our framework matches the diagnostic performance of existing deep learning methods while offering rationales that support its diagnostic conclusions."
    },
    "2505.19879v1": {
      "title": "The Study of Human Preference Based on Integrated Analysis of N1 and LPP Components",
      "url": "http://arxiv.org/abs/2505.19879v1",
      "authors": "Siyuan Li, Xiangze Meng, Yijian Yang, Yiwen Xu, Yunfei Wang, Chenghu Qiu, Hanyi Jiang, Pin Wu, Shegnbo Chen, Xiao Wei, Hao Wang, Lan Ni, Huiran Zhang",
      "update_time": "2025-05-26",
      "abstract": "Human preference research is a significant domain in psychology and psychophysiology, with broad applications in psychiatric evaluation and daily life quality enhancement. This study explores the neural mechanisms of human preference judgments through the analysis of event-related potentials (ERPs), specifically focusing on the early N1 component and the late positive potential (LPP). Using a mixed-image dataset covering items such as hats, fruits, snacks, scarves, drinks, and pets, we elicited a range of emotional responses from participants while recording their brain activity via EEG. Our work innovatively combines the N1 and LPP components to reveal distinct patterns across different preference levels. The N1 component, particularly in frontal regions, showed increased amplitude for preferred items, indicating heightened early visual attention. Similarly, the LPP component exhibited larger amplitudes for both preferred and non-preferred items, reflecting deeper emotional engagement and cognitive evaluation. In addition, we introduced a relationship model that integrates these ERP components to assess the intensity and direction of preferences, providing a novel method for interpreting EEG data in the context of emotional responses. These findings offer valuable insights into the cognitive and emotional processes underlying human preferences and present new possibilities for brain-computer interface applications, personalized marketing, and product design."
    },
    "2505.19652v1": {
      "title": "SACM: SEEG-Audio Contrastive Matching for Chinese Speech Decoding",
      "url": "http://arxiv.org/abs/2505.19652v1",
      "authors": "Hongbin Wang, Zhihong Jia, Yuanzhong Shen, Ziwei Wang, Siyang Li, Kai Shu, Feng Hu, Dongrui Wu",
      "update_time": "2025-05-26",
      "abstract": "Speech disorders such as dysarthria and anarthria can severely impair the patient's ability to communicate verbally. Speech decoding brain-computer interfaces (BCIs) offer a potential alternative by directly translating speech intentions into spoken words, serving as speech neuroprostheses. This paper reports an experimental protocol for Mandarin Chinese speech decoding BCIs, along with the corresponding decoding algorithms. Stereo-electroencephalography (SEEG) and synchronized audio data were collected from eight drug-resistant epilepsy patients as they conducted a word-level reading task. The proposed SEEG and Audio Contrastive Matching (SACM), a contrastive learning-based framework, achieved decoding accuracies significantly exceeding chance levels in both speech detection and speech decoding tasks. Electrode-wise analysis revealed that a single sensorimotor cortex electrode achieved performance comparable to that of the full electrode array. These findings provide valuable insights for developing more accurate online speech decoding BCIs.",
      "code_url": "https://github.com/WangHongbinary/SACM"
    },
    "2505.19626v1": {
      "title": "Decoding Speaker-Normalized Pitch from EEG for Mandarin Perception",
      "url": "http://arxiv.org/abs/2505.19626v1",
      "authors": "Jiaxin Chen, Yiming Wang, Ziyu Zhang, Jiayang Han, Yin-Long Liu, Rui Feng, Xiuyuan Liang, Zhen-Hua Ling, Jiahong Yuan",
      "update_time": "2025-05-26",
      "abstract": "The same speech content produced by different speakers exhibits significant differences in pitch contour, yet listeners' semantic perception remains unaffected. This phenomenon may stem from the brain's perception of pitch contours being independent of individual speakers' pitch ranges. In this work, we recorded electroencephalogram (EEG) while participants listened to Mandarin monosyllables with varying tones, phonemes, and speakers. The CE-ViViT model is proposed to decode raw or speaker-normalized pitch contours directly from EEG. Experimental results demonstrate that the proposed model can decode pitch contours with modest errors, achieving performance comparable to state-of-the-art EEG regression methods. Moreover, speaker-normalized pitch contours were decoded more accurately, supporting the neural encoding of relative pitch."
    },
    "2505.19548v1": {
      "title": "How Syntax Specialization Emerges in Language Models",
      "url": "http://arxiv.org/abs/2505.19548v1",
      "authors": "Xufeng Duan, Zhaoqian Yao, Yunhao Zhang, Shaonan Wang, Zhenguang G. Cai",
      "update_time": "2025-05-26",
      "abstract": "Large language models (LLMs) have been found to develop surprising internal specializations: Individual neurons, attention heads, and circuits become selectively sensitive to syntactic structure, reflecting patterns observed in the human brain. While this specialization is well-documented, how it emerges during training and what influences its development remains largely unknown.   In this work, we tap into the black box of specialization by tracking its formation over time. By quantifying internal syntactic consistency across minimal pairs from various syntactic phenomena, we identify a clear developmental trajectory: Syntactic sensitivity emerges gradually, concentrates in specific layers, and exhibits a 'critical period' of rapid internal specialization. This process is consistent across architectures and initialization parameters (e.g., random seeds), and is influenced by model scale and training data. We therefore reveal not only where syntax arises in LLMs but also how some models internalize it during training. To support future research, we will release the code, models, and training checkpoints upon acceptance."
    },
    "2505.19487v1": {
      "title": "SpikeStereoNet: A Brain-Inspired Framework for Stereo Depth Estimation from Spike Streams",
      "url": "http://arxiv.org/abs/2505.19487v1",
      "authors": "Zhuoheng Gao, Yihao Li, Jiyao Zhang, Rui Zhao, Tong Wu, Hao Tang, Zhaofei Yu, Hao Dong, Guozhang Chen, Tiejun Huang",
      "update_time": "2025-05-26",
      "abstract": "Conventional frame-based cameras often struggle with stereo depth estimation in rapidly changing scenes. In contrast, bio-inspired spike cameras emit asynchronous events at microsecond-level resolution, providing an alternative sensing modality. However, existing methods lack specialized stereo algorithms and benchmarks tailored to the spike data. To address this gap, we propose SpikeStereoNet, a brain-inspired framework and the first to estimate stereo depth directly from raw spike streams. The model fuses raw spike streams from two viewpoints and iteratively refines depth estimation through a recurrent spiking neural network (RSNN) update module. To benchmark our approach, we introduce a large-scale synthetic spike stream dataset and a real-world stereo spike dataset with dense depth annotations. SpikeStereoNet outperforms existing methods on both datasets by leveraging spike streams' ability to capture subtle edges and intensity shifts in challenging regions such as textureless surfaces and extreme lighting conditions. Furthermore, our framework exhibits strong data efficiency, maintaining high accuracy even with substantially reduced training data. The source code and datasets will be publicly available."
    },
    "2505.19296v1": {
      "title": "Exploring Self-Organized Criticality through Memory-Driven Random Walks on Complex Networks",
      "url": "http://arxiv.org/abs/2505.19296v1",
      "authors": "Mohammad Jafari",
      "update_time": "2025-05-25",
      "abstract": "We present a stochastic model of memory-driven random walks on complex networks, where a single walker explores the network through a combination of local random walks and memory-based resetting. The walker either moves to a neighboring node or resets to a previously visited node with a probability proportional to the visitation history.Each node in the network accumulates stress with every visit, and once the accumulated stress exceeds a threshold, the node topples, redistributing the stress to its neighbors. This redistribution can trigger cascading avalanches across the network. We demonstrate that this simple memory-driven mechanism, combined with local stress dynamics, leads to the emergence of self-organized criticality (SOC) across various network topologies, including Small-World and BA networks.The resulting avalanche size distributions follow power-law statistics, with exponents that are consistent with those found in classical SOC systems. Our model requires no fine-tuning of parameters to achieve criticality and offers a novel insight into how memory, network structure, and local stress interactions can give rise to emergent critical phenomena. These findings have implications for understanding cascading failures in infrastructure, neural avalanches in brain dynamics, and information spread in complex networks."
    },
    "2505.19157v1": {
      "title": "Efficient and robust solvers for a cell-by-cell dual-poroelasticity problem",
      "url": "http://arxiv.org/abs/2505.19157v1",
      "authors": "Marius Causemann, Miroslav Kuchta",
      "update_time": "2025-05-25",
      "abstract": "This paper presents a scalable and robust solver for a cell-by-cell dual-poroelasticity model, describing the mechanical interactions between brains cells embedded in extracellular space. Explicitly representing the complex cellular shapes, the proposed approach models both intracellular and extracellular spaces as distinct poroelastic media, separated by a permeable cell membrane which allows hydrostatic and osmotic pressure-driven fluid exchange. The solver, which employs a three-field (displacement, total pressure, and fluid pressure) formulation, leverages the framework of norm-equivalent preconditioning and appropriately fitted norms to ensure robustness across all material parameters of the model. Scalability for large and complex geometries is achieved through efficient Algebraic Multigrid (AMG) approximations of the preconditioners' individual blocks. Furthermore, we accommodate diverse boundary conditions, including full Dirichlet boundary conditions for displacement, which we handle efficiently using the Sherman-Morrison-Woodbury formula. Numerical experiments demonstrate the preconditioners' robustness and performance across various parameters relevant to realistic scenarios, and a large scale example of cellular swelling on a dense reconstruction of the mouse visual cortex highlights the method's potential for investigating complex physiological processes like cellular volume regulation in detailed biological structures."
    }
  },
  "EEG": {
    "2505.19879v1": {
      "title": "The Study of Human Preference Based on Integrated Analysis of N1 and LPP Components",
      "url": "http://arxiv.org/abs/2505.19879v1",
      "authors": "Siyuan Li, Xiangze Meng, Yijian Yang, Yiwen Xu, Yunfei Wang, Chenghu Qiu, Hanyi Jiang, Pin Wu, Shegnbo Chen, Xiao Wei, Hao Wang, Lan Ni, Huiran Zhang",
      "update_time": "2025-05-26",
      "abstract": "Human preference research is a significant domain in psychology and psychophysiology, with broad applications in psychiatric evaluation and daily life quality enhancement. This study explores the neural mechanisms of human preference judgments through the analysis of event-related potentials (ERPs), specifically focusing on the early N1 component and the late positive potential (LPP). Using a mixed-image dataset covering items such as hats, fruits, snacks, scarves, drinks, and pets, we elicited a range of emotional responses from participants while recording their brain activity via EEG. Our work innovatively combines the N1 and LPP components to reveal distinct patterns across different preference levels. The N1 component, particularly in frontal regions, showed increased amplitude for preferred items, indicating heightened early visual attention. Similarly, the LPP component exhibited larger amplitudes for both preferred and non-preferred items, reflecting deeper emotional engagement and cognitive evaluation. In addition, we introduced a relationship model that integrates these ERP components to assess the intensity and direction of preferences, providing a novel method for interpreting EEG data in the context of emotional responses. These findings offer valuable insights into the cognitive and emotional processes underlying human preferences and present new possibilities for brain-computer interface applications, personalized marketing, and product design."
    },
    "2505.19626v1": {
      "title": "Decoding Speaker-Normalized Pitch from EEG for Mandarin Perception",
      "url": "http://arxiv.org/abs/2505.19626v1",
      "authors": "Jiaxin Chen, Yiming Wang, Ziyu Zhang, Jiayang Han, Yin-Long Liu, Rui Feng, Xiuyuan Liang, Zhen-Hua Ling, Jiahong Yuan",
      "update_time": "2025-05-26",
      "abstract": "The same speech content produced by different speakers exhibits significant differences in pitch contour, yet listeners' semantic perception remains unaffected. This phenomenon may stem from the brain's perception of pitch contours being independent of individual speakers' pitch ranges. In this work, we recorded electroencephalogram (EEG) while participants listened to Mandarin monosyllables with varying tones, phonemes, and speakers. The CE-ViViT model is proposed to decode raw or speaker-normalized pitch contours directly from EEG. Experimental results demonstrate that the proposed model can decode pitch contours with modest errors, achieving performance comparable to state-of-the-art EEG regression methods. Moreover, speaker-normalized pitch contours were decoded more accurately, supporting the neural encoding of relative pitch."
    },
    "2505.19009v1": {
      "title": "Capturing Aperiodic Temporal Dynamics of EEG Signals through Stochastic Fluctuation Modeling",
      "url": "http://arxiv.org/abs/2505.19009v1",
      "authors": "Yuhao Sun, Zhiyuan Ma, Xinke Shen, Jinhao Li, Guan Wang, Sen Song",
      "update_time": "2025-05-25",
      "abstract": "Electrophysiological brain signals, such as electroencephalography (EEG), exhibit both periodic and aperiodic components, with the latter often modeled as 1/f noise and considered critical to cognitive and neurological processes. Although various theoretical frameworks have been proposed to account for aperiodic activity, its scale-invariant and long-range temporal dependency remain insufficiently explained. Drawing on neural fluctuation theory, we propose a novel framework that parameterizes intrinsic stochastic neural fluctuations to account for aperiodic dynamics. Within this framework, we introduce two key parameters-self-similarity and scale factor-to characterize these fluctuations. Our findings reveal that EEG fluctuations exhibit self-similar and non-stable statistical properties, challenging the assumptions of conventional stochastic models in neural dynamical modeling. Furthermore, the proposed parameters enable the reconstruction of EEG-like signals that faithfully replicate the aperiodic spectrum, including the characteristic 1/f spectral profile, and long range dependency. By linking structured neural fluctuations to empirically observed aperiodic EEG activity, this work offers deeper mechanistic insights into brain dynamics, resulting in a more robust biomarker candidate than the traditional 1/f slope, and provides a computational methodology for generating biologically plausible neurophysiological signals."
    },
    "2505.18117v1": {
      "title": "Multi-Modal Spectral Parametrization Method (MMSPM) for analyzing EEG activity with distinct scaling regimes",
      "url": "http://arxiv.org/abs/2505.18117v1",
      "authors": "Frigyes Samuel Racz, John Milton, Juan Luis Cabrera, G\u00e1bor Csukly, Jos\u00e9 del R. Mill\u00e1n",
      "update_time": "2025-05-23",
      "abstract": "Aperiodic neural activity has been the subject of intense research interest lately as it could reflect on the cortical excitation/inhibition ratio, which is suspected to be affected in numerous clinical conditions. This phenomenon is characterized via the aperiodic scaling exponent $\\beta$, equal to the spectral slope following log-log transformation of power spectra. Despite recent progress, however, most current methods do not take into consideration the plausible multimodal nature in the power spectra of neurophysiological recordings - i.e., $\\beta$ might be different in low- ($\\beta_{lo}$) and high-frequency ($\\beta_{hi}$) regimes -, especially in case of $|\\beta_{lo}|>|\\beta_{hi}|$. Here we propose an algorithm, the multi-modal spectral parametrization method (MMSPM) that aims to account for this issue. MMSPM estimates $\\beta_{lo}$ and $\\beta_{hi}$ separately using a constrained, piece-wise regression technique, and also assesses if they are significantly different or instead the spectrum is indeed unimodal and can be characterized simply with broadband $\\beta$. Here we present the MMSPM algorithm and evaluate its performance in silico on simulated power spectra. Then, we use MMSPM on resting-state electroencephalography (EEG) data collected from 19 young, healthy volunteers, as well as on a separate dataset of EEG recordings from 30 schizophrenia patients and 31 healthy controls, and demonstrate that broadband (0.1-100 Hz and 0.5-45 Hz) EEG spectra can indeed present a bimodality pattern with significantly steeper low-range ($<\\sim2$ Hz) and flatter high-range scaling regimes (i.e., $|\\beta_{lo}|>|\\beta_{hi}|$).   Clinical relevance: The MMSPM method characterizes aperiodic neural activity in distinct scaling regimes, which can be relevant in numerous pathological conditions such as dementia or schizophrenia."
    },
    "2505.17972v1": {
      "title": "MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from Long EEG Recordings",
      "url": "http://arxiv.org/abs/2505.17972v1",
      "authors": "Kazi Mahmudul Hassan, Xuyang Zhao, Hidenori Sugano, Toshihisa Tanaka",
      "update_time": "2025-05-23",
      "abstract": "Feature engineering for generalized seizure detection models remains a significant challenge. Recently proposed models show variable performance depending on the training data and remain ineffective at accurately distinguishing artifacts from seizure data. In this study, we propose a novel end-to-end model, ''Multiresolutional EEGWaveNet (MR-EEGWaveNet),'' which efficiently distinguishes seizure events from background electroencephalogram (EEG) and artifacts/noise by capturing both temporal dependencies across different time frames and spatial relationships between channels. The model has three modules: convolution, feature extraction, and predictor. The convolution module extracts features through depth-wise and spatio-temporal convolution. The feature extraction module individually reduces the feature dimension extracted from EEG segments and their sub-segments. Subsequently, the extracted features are concatenated into a single vector for classification using a fully connected classifier called the predictor module. In addition, an anomaly score-based post-classification processing technique was introduced to reduce the false-positive rates of the model. Experimental results were reported and analyzed using different parameter settings and datasets (Siena (public) and Juntendo (private)). The proposed MR-EEGWaveNet significantly outperformed the conventional non-multiresolution approach, improving the F1 scores from 0.177 to 0.336 on Siena and 0.327 to 0.488 on Juntendo, with precision gains of 15.9% and 20.62%, respectively."
    },
    "2505.16724v1": {
      "title": "Advancing Brainwave Modeling with a Codebook-Based Foundation Model",
      "url": "http://arxiv.org/abs/2505.16724v1",
      "authors": "Konstantinos Barmpas, Na Lee, Yannis Panagakis, Dimitrios A. Adamos, Nikolaos Laskaris, Stefanos Zafeiriou",
      "update_time": "2025-05-22",
      "abstract": "Recent advances in large-scale pre-trained Electroencephalogram (EEG) models have shown great promise, driving progress in Brain-Computer Interfaces (BCIs) and healthcare applications. However, despite their success, many existing pre-trained models have struggled to fully capture the rich information content of neural oscillations, a limitation that fundamentally constrains their performance and generalizability across diverse BCI tasks. This limitation is frequently rooted in suboptimal architectural design choices which constrain their representational capacity. In this work, we introduce LaBraM++, an enhanced Large Brainwave Foundation Model (LBM) that incorporates principled improvements grounded in robust signal processing foundations. LaBraM++ demonstrates substantial gains across a variety of tasks, consistently outperforming its originally-based architecture and achieving competitive results when compared to other open-source LBMs. Its superior performance and training efficiency highlight its potential as a strong foundation for future advancements in LBMs."
    },
    "2505.17142v1": {
      "title": "MetaSTH-Sleep: Towards Effective Few-Shot Sleep Stage Classification with Spatial-Temporal Hypergraph Enhanced Meta-Learning",
      "url": "http://arxiv.org/abs/2505.17142v1",
      "authors": "Jingyu Li, Tiehua Zhang, Jinze Wang, Yi Zhang, Yuhuan Li, Yifan Zhao, Zhishu Shen, Jiannan Liu",
      "update_time": "2025-05-22",
      "abstract": "Accurate classification of sleep stages based on bio-signals is fundamental for automatic sleep stage annotation. Traditionally, this task relies on experienced clinicians to manually annotate data, a process that is both time-consuming and labor-intensive. In recent years, deep learning methods have shown promise in automating this task. However, three major challenges remain: (1) deep learning models typically require large-scale labeled datasets, making them less effective in real-world settings where annotated data is limited; (2) significant inter-individual variability in bio-signals often results in inconsistent model performance when applied to new subjects, limiting generalization; and (3) existing approaches often overlook the high-order relationships among bio-signals, failing to simultaneously capture signal heterogeneity and spatial-temporal dependencies. To address these issues, we propose MetaSTH-Sleep, a few-shot sleep stage classification framework based on spatial-temporal hypergraph enhanced meta-learning. Our approach enables rapid adaptation to new subjects using only a few labeled samples, while the hypergraph structure effectively models complex spatial interconnections and temporal dynamics simultaneously in EEG signals. Experimental results demonstrate that MetaSTH-Sleep achieves substantial performance improvements across diverse subjects, offering valuable insights to support clinicians in sleep stage annotation."
    },
    "2505.15747v2": {
      "title": "Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs",
      "url": "http://arxiv.org/abs/2505.15747v2",
      "authors": "Kanan Kiguchi, Yunhao Tu, Katsuhiro Ajito, Fady Alnajjar, Kazuyuki Murase",
      "update_time": "2025-05-22",
      "abstract": "We propose a novel framework for integrating fragmented multi-modal data in Alzheimer's disease (AD) research using large language models (LLMs) and knowledge graphs. While traditional multimodal analysis requires matched patient IDs across datasets, our approach demonstrates population-level integration of MRI, gene expression, biomarkers, EEG, and clinical indicators from independent cohorts. Statistical analysis identified significant features in each modality, which were connected as nodes in a knowledge graph. LLMs then analyzed the graph to extract potential correlations and generate hypotheses in natural language. This approach revealed several novel relationships, including a potential pathway linking metabolic risk factors to tau protein abnormalities via neuroinflammation (r>0.6, p<0.001), and unexpected correlations between frontal EEG channels and specific gene expression profiles (r=0.42-0.58, p<0.01). Cross-validation with independent datasets confirmed the robustness of major findings, with consistent effect sizes across cohorts (variance <15%). The reproducibility of these findings was further supported by expert review (Cohen's k=0.82) and computational validation. Our framework enables cross modal integration at a conceptual level without requiring patient ID matching, offering new possibilities for understanding AD pathology through fragmented data reuse and generating testable hypotheses for future research."
    },
    "2505.15364v1": {
      "title": "MHANet: Multi-scale Hybrid Attention Network for Auditory Attention Detection",
      "url": "http://arxiv.org/abs/2505.15364v1",
      "authors": "Lu Li, Cunhang Fan, Hongyu Zhang, Jingjing Zhang, Xiaoke Yang, Jian Zhou, Zhao Lv",
      "update_time": "2025-05-21",
      "abstract": "Auditory attention detection (AAD) aims to detect the target speaker in a multi-talker environment from brain signals, such as electroencephalography (EEG), which has made great progress. However, most AAD methods solely utilize attention mechanisms sequentially and overlook valuable multi-scale contextual information within EEG signals, limiting their ability to capture long-short range spatiotemporal dependencies simultaneously. To address these issues, this paper proposes a multi-scale hybrid attention network (MHANet) for AAD, which consists of the multi-scale hybrid attention (MHA) module and the spatiotemporal convolution (STC) module. Specifically, MHA combines channel attention and multi-scale temporal and global attention mechanisms. This effectively extracts multi-scale temporal patterns within EEG signals and captures long-short range spatiotemporal dependencies simultaneously. To further improve the performance of AAD, STC utilizes temporal and spatial convolutions to aggregate expressive spatiotemporal representations. Experimental results show that the proposed MHANet achieves state-of-the-art performance with fewer trainable parameters across three datasets, 3 times lower than that of the most advanced model. Code is available at: https://github.com/fchest/MHANet.",
      "code_url": "https://github.com/fchest/mhanet"
    },
    "2505.15203v1": {
      "title": "EEG-Based Inter-Patient Epileptic Seizure Detection Combining Domain Adversarial Training with CNN-BiLSTM Network",
      "url": "http://arxiv.org/abs/2505.15203v1",
      "authors": "Rina Tazaki, Tomoyuki Akiyama, Akira Furui",
      "update_time": "2025-05-21",
      "abstract": "Automated epileptic seizure detection from electroencephalogram (EEG) remains challenging due to significant individual differences in EEG patterns across patients. While existing studies achieve high accuracy with patient-specific approaches, they face difficulties in generalizing to new patients. To address this, we propose a detection framework combining domain adversarial training with a convolutional neural network (CNN) and a bidirectional long short-term memory (BiLSTM). First, the CNN extracts local patient-invariant features through domain adversarial training, which optimizes seizure detection accuracy while minimizing patient-specific characteristics. Then, the BiLSTM captures temporal dependencies in the extracted features to model seizure evolution patterns. Evaluation using EEG recordings from 20 patients with focal epilepsy demonstrated superior performance over non-adversarial methods, achieving high detection accuracy across different patients. The integration of adversarial training with temporal modeling enables robust cross-patient seizure detection."
    }
  },
  "BCI": {
    "2505.19652v1": {
      "title": "SACM: SEEG-Audio Contrastive Matching for Chinese Speech Decoding",
      "url": "http://arxiv.org/abs/2505.19652v1",
      "authors": "Hongbin Wang, Zhihong Jia, Yuanzhong Shen, Ziwei Wang, Siyang Li, Kai Shu, Feng Hu, Dongrui Wu",
      "update_time": "2025-05-26",
      "abstract": "Speech disorders such as dysarthria and anarthria can severely impair the patient's ability to communicate verbally. Speech decoding brain-computer interfaces (BCIs) offer a potential alternative by directly translating speech intentions into spoken words, serving as speech neuroprostheses. This paper reports an experimental protocol for Mandarin Chinese speech decoding BCIs, along with the corresponding decoding algorithms. Stereo-electroencephalography (SEEG) and synchronized audio data were collected from eight drug-resistant epilepsy patients as they conducted a word-level reading task. The proposed SEEG and Audio Contrastive Matching (SACM), a contrastive learning-based framework, achieved decoding accuracies significantly exceeding chance levels in both speech detection and speech decoding tasks. Electrode-wise analysis revealed that a single sensorimotor cortex electrode achieved performance comparable to that of the full electrode array. These findings provide valuable insights for developing more accurate online speech decoding BCIs.",
      "code_url": "https://github.com/WangHongbinary/SACM"
    },
    "2505.16724v1": {
      "title": "Advancing Brainwave Modeling with a Codebook-Based Foundation Model",
      "url": "http://arxiv.org/abs/2505.16724v1",
      "authors": "Konstantinos Barmpas, Na Lee, Yannis Panagakis, Dimitrios A. Adamos, Nikolaos Laskaris, Stefanos Zafeiriou",
      "update_time": "2025-05-22",
      "abstract": "Recent advances in large-scale pre-trained Electroencephalogram (EEG) models have shown great promise, driving progress in Brain-Computer Interfaces (BCIs) and healthcare applications. However, despite their success, many existing pre-trained models have struggled to fully capture the rich information content of neural oscillations, a limitation that fundamentally constrains their performance and generalizability across diverse BCI tasks. This limitation is frequently rooted in suboptimal architectural design choices which constrain their representational capacity. In this work, we introduce LaBraM++, an enhanced Large Brainwave Foundation Model (LBM) that incorporates principled improvements grounded in robust signal processing foundations. LaBraM++ demonstrates substantial gains across a variety of tasks, consistently outperforming its originally-based architecture and achieving competitive results when compared to other open-source LBMs. Its superior performance and training efficiency highlight its potential as a strong foundation for future advancements in LBMs."
    },
    "2505.14192v1": {
      "title": "QSVM-QNN: Quantum Support Vector Machine Based Quantum Neural Network Learning Algorithm for Brain-Computer Interfacing Systems",
      "url": "http://arxiv.org/abs/2505.14192v1",
      "authors": "Bikash K. Behera, Saif Al-Kuwari, Ahmed Farouk",
      "update_time": "2025-05-20",
      "abstract": "A brain-computer interface (BCI) system enables direct communication between the brain and external devices, offering significant potential for assistive technologies and advanced human-computer interaction. Despite progress, BCI systems face persistent challenges, including signal variability, classification inefficiency, and difficulty adapting to individual users in real time. In this study, we propose a novel hybrid quantum learning model, termed QSVM-QNN, which integrates a Quantum Support Vector Machine (QSVM) with a Quantum Neural Network (QNN), to improve classification accuracy and robustness in EEG-based BCI tasks. Unlike existing models, QSVM-QNN combines the decision boundary capabilities of QSVM with the expressive learning power of QNN, leading to superior generalization performance. The proposed model is evaluated on two benchmark EEG datasets, achieving high accuracies of 0.990 and 0.950, outperforming both classical and standalone quantum models. To demonstrate real-world viability, we further validated the robustness of QNN, QSVM, and QSVM-QNN against six realistic quantum noise models, including bit flip and phase damping. These experiments reveal that QSVM-QNN maintains stable performance under noisy conditions, establishing its applicability for deployment in practical, noisy quantum environments. Beyond BCI, the proposed hybrid quantum architecture is generalizable to other biomedical and time-series classification tasks, offering a scalable and noise-resilient solution for next-generation neurotechnological systems."
    },
    "2505.13446v1": {
      "title": "Unlocking Non-Invasive Brain-to-Text",
      "url": "http://arxiv.org/abs/2505.13446v1",
      "authors": "Dulhan Jayalath, Gilad Landau, Oiwi Parker Jones",
      "update_time": "2025-05-19",
      "abstract": "Despite major advances in surgical brain-to-text (B2T), i.e. transcribing speech from invasive brain recordings, non-invasive alternatives have yet to surpass even chance on standard metrics. This remains a barrier to building a non-invasive brain-computer interface (BCI) capable of restoring communication in paralysed individuals without surgery. Here, we present the first non-invasive B2T result that significantly exceeds these critical baselines, raising BLEU by $1.4\\mathrm{-}2.6\\times$ over prior work. This result is driven by three contributions: (1) we extend recent word-classification models with LLM-based rescoring, transforming single-word predictors into closed-vocabulary B2T systems; (2) we introduce a predictive in-filling approach to handle out-of-vocabulary (OOV) words, substantially expanding the effective vocabulary; and (3) we demonstrate, for the first time, how to scale non-invasive B2T models across datasets, unlocking deep learning at scale and improving accuracy by $2.1\\mathrm{-}2.3\\times$. Through these contributions, we offer new insights into the roles of data quality and vocabulary size. Together, our results remove a major obstacle to realising practical non-invasive B2T systems."
    },
    "2505.13021v1": {
      "title": "The role of data partitioning on the performance of EEG-based deep learning models in supervised cross-subject analysis: a preliminary study",
      "url": "http://arxiv.org/abs/2505.13021v1",
      "authors": "Federico Del Pup, Andrea Zanola, Louis Fabrice Tshimanga, Alessandra Bertoldo, Livio Finos, Manfredo Atzori",
      "update_time": "2025-05-19",
      "abstract": "Deep learning is significantly advancing the analysis of electroencephalography (EEG) data by effectively discovering highly nonlinear patterns within the signals. Data partitioning and cross-validation are crucial for assessing model performance and ensuring study comparability, as they can produce varied results and data leakage due to specific signal properties (e.g., biometric). Such variability leads to incomparable studies and, increasingly, overestimated performance claims, which are detrimental to the field. Nevertheless, no comprehensive guidelines for proper data partitioning and cross-validation exist in the domain, nor is there a quantitative evaluation of their impact on model accuracy, reliability, and generalizability. To assist researchers in identifying optimal experimental strategies, this paper thoroughly investigates the role of data partitioning and cross-validation in evaluating EEG deep learning models. Five cross-validation settings are compared across three supervised cross-subject classification tasks (BCI, Parkinson's, and Alzheimer's disease detection) and four established architectures of increasing complexity (ShallowConvNet, EEGNet, DeepConvNet, and Temporal-based ResNet). The comparison of over 100,000 trained models underscores, first, the importance of using subject-based cross-validation strategies for evaluating EEG deep learning models, except when within-subject analyses are acceptable (e.g., BCI). Second, it highlights the greater reliability of nested approaches (N-LNSO) compared to non-nested counterparts, which are prone to data leakage and favor larger models overfitting to validation data. In conclusion, this work provides EEG deep learning researchers with an analysis of data partitioning and cross-validation and offers guidelines to avoid data leakage, currently undermining the domain with potentially overestimated performance claims.",
      "code_url": "https://github.com/medmaxlab/eegpartition"
    },
    "2505.11139v1": {
      "title": "Covariance Density Neural Networks",
      "url": "http://arxiv.org/abs/2505.11139v1",
      "authors": "Om Roy, Yashar Moshfeghi, Keith Smith",
      "update_time": "2025-05-16",
      "abstract": "Graph neural networks have re-defined how we model and predict on network data but there lacks a consensus on choosing the correct underlying graph structure on which to model signals. CoVariance Neural Networks (VNN) address this issue by using the sample covariance matrix as a Graph Shift Operator (GSO). Here, we improve on the performance of VNNs by constructing a Density Matrix where we consider the sample Covariance matrix as a quasi-Hamiltonian of the system in the space of random variables. Crucially, using this density matrix as the GSO allows components of the data to be extracted at different scales, allowing enhanced discriminability and performance. We show that this approach allows explicit control of the stability-discriminability trade-off of the network, provides enhanced robustness to noise compared to VNNs, and outperforms them in useful real-life applications where the underlying covariance matrix is informative. In particular, we show that our model can achieve strong performance in subject-independent Brain Computer Interface EEG motor imagery classification, outperforming EEGnet while being faster. This shows how covariance density neural networks provide a basis for the notoriously difficult task of transferability of BCIs when evaluated on unseen individuals."
    },
    "2505.10786v1": {
      "title": "Bridging BCI and Communications: A MIMO Framework for EEG-to-ECoG Wireless Channel Modeling",
      "url": "http://arxiv.org/abs/2505.10786v1",
      "authors": "Jiaheng Wang, Zhenyu Wang, Tianheng Xu, Yuan Si, Ang Li, Ting Zhou, Xi Zhao, Honglin Hu",
      "update_time": "2025-05-16",
      "abstract": "As a method to connect human brain and external devices, Brain-computer interfaces (BCIs) are receiving extensive research attention. Recently, the integration of communication theory with BCI has emerged as a popular trend, offering potential to enhance system performance and shape next-generation communications.   A key challenge in this field is modeling the brain wireless communication channel between intracranial electrocorticography (ECoG) emitting neurons and extracranial electroencephalography (EEG) receiving electrodes. However, the complex physiology of brain challenges the application of traditional channel modeling methods, leaving relevant research in its infancy. To address this gap, we propose a frequency-division multiple-input multiple-output (MIMO) estimation framework leveraging simultaneous macaque EEG and ECoG recordings, while employing neurophysiology-informed regularization to suppress noise interference. This approach reveals profound similarities between neural signal propagation and multi-antenna communication systems. Experimental results show improved estimation accuracy over conventional methods while highlighting a trade-off between frequency resolution and temporal stability determined by signal duration. This work establish a conceptual bridge between neural interfacing and communication theory, accelerating synergistic developments in both fields."
    },
    "2505.10536v1": {
      "title": "Real-World fNIRS-Based Brain-Computer Interfaces: Benchmarking Deep Learning and Classical Models in Interactive Gaming",
      "url": "http://arxiv.org/abs/2505.10536v1",
      "authors": "Mohammad Ghalavand, Javad Hatami, Seyed Kamaledin Setarehdan, Hananeh Ghalavand",
      "update_time": "2025-05-15",
      "abstract": "Brain-Computer Interfaces enable direct communication between the brain and external systems, with functional Near-Infrared Spectroscopy emerging as a portable and non-invasive method for capturing cerebral hemodynamics. This study investigates the classification of rest and task states during a realistic, interactive tennis simulation using fNIRS signals and a range of machine learning approaches. We benchmarked traditional classifiers based on engineered features, Long Short-Term Memory networks on raw time-series data, and Convolutional Neural Networks applied to Gramian Angular Field-transformed images. Ensemble models like Extra Trees and Gradient Boosting achieved accuracies above 97 percent, while the ResNet-based CNN reached 95.0 percent accuracy with a near-perfect AUC of 99.2 percent, outperforming both LSTM and EfficientNet architectures. A novel data augmentation strategy was employed to equalize trial durations while preserving physiological integrity. Feature importance analyses revealed that both oxygenated and deoxygenated hemoglobin signals, particularly slope and RMS metrics, were key contributors to classification performance. These findings demonstrate the strong potential of fNIRS-based BCIs for deployment in dynamic, real-world environments and underscore the advantages of deep learning models in decoding complex neural signals."
    },
    "2505.07592v1": {
      "title": "Decoding Chess Puzzle Play and Standard Cognitive Tasks for BCI: A Low-Cost EEG Study",
      "url": "http://arxiv.org/abs/2505.07592v1",
      "authors": "Matthew Russell, Samuel Youkeles, William Xia, Kenny Zheng, Aman Shah, Robert J. K. Jacob",
      "update_time": "2025-05-12",
      "abstract": "While consumer-grade electroencephalography (EEG) devices show promise for Brain-Computer Interface (BCI) applications, their efficacy in detecting subtle cognitive states remains understudied. Using a combination of established cognitive paradigms (N-Back, Stroop, and Mental Rotation) and a novel ecological task (Chess puzzles), we demonstrate successful distinctions of workload levels within some tasks, as well as differentiation between task types using the MUSE 2 device. With machine learning we further show reliable predictive power to differentiate between workload levels in the N-Back task, while also achieving effective cross-task classification. These findings demonstrate that consumer-grade EEG devices can effectively detect and differentiate various forms of cognitive workload, and that they can be leveraged with some success towards real-time classification distinguishing workload in some tasks, as well as in differentiating between nuanced cognitive states, supporting their potential use in adaptive BCI applications. Research code and data are further provided for future researchers."
    },
    "2505.06793v1": {
      "title": "HistDiST: Histopathological Diffusion-based Stain Transfer",
      "url": "http://arxiv.org/abs/2505.06793v1",
      "authors": "Erik Gro\u00dfkopf, Valay Bundele, Mehran Hossienzadeh, Hendrik P. A. Lensch",
      "update_time": "2025-05-11",
      "abstract": "Hematoxylin and Eosin (H&E) staining is the cornerstone of histopathology but lacks molecular specificity. While Immunohistochemistry (IHC) provides molecular insights, it is costly and complex, motivating H&E-to-IHC translation as a cost-effective alternative. Existing translation methods are mainly GAN-based, often struggling with training instability and limited structural fidelity, while diffusion-based approaches remain underexplored. We propose HistDiST, a Latent Diffusion Model (LDM) based framework for high-fidelity H&E-to-IHC translation. HistDiST introduces a dual-conditioning strategy, utilizing Phikon-extracted morphological embeddings alongside VAE-encoded H&E representations to ensure pathology-relevant context and structural consistency. To overcome brightness biases, we incorporate a rescaled noise schedule, v-prediction, and trailing timesteps, enforcing a zero-SNR condition at the final timestep. During inference, DDIM inversion preserves the morphological structure, while an eta-cosine noise schedule introduces controlled stochasticity, balancing structural consistency and molecular fidelity. Moreover, we propose Molecular Retrieval Accuracy (MRA), a novel pathology-aware metric leveraging GigaPath embeddings to assess molecular relevance. Extensive evaluations on MIST and BCI datasets demonstrate that HistDiST significantly outperforms existing methods, achieving a 28% improvement in MRA on the H&E-to-Ki67 translation task, highlighting its effectiveness in capturing true IHC semantics."
    }
  },
  "fMRI": {
    "2505.20027v1": {
      "title": "Multi-modal brain encoding models for multi-modal stimuli",
      "url": "http://arxiv.org/abs/2505.20027v1",
      "authors": "Subba Reddy Oota, Khushbu Pahwa, Mounika Marreddy, Maneesh Singh, Manish Gupta, Bapi S. Raju",
      "update_time": "2025-05-26",
      "abstract": "Despite participants engaging in unimodal stimuli, such as watching images or silent videos, recent work has demonstrated that multi-modal Transformer models can predict visual brain activity impressively well, even with incongruent modality representations. This raises the question of how accurately these multi-modal models can predict brain activity when participants are engaged in multi-modal stimuli. As these models grow increasingly popular, their use in studying neural activity provides insights into how our brains respond to such multi-modal naturalistic stimuli, i.e., where it separates and integrates information across modalities through a hierarchy of early sensory regions to higher cognition. We investigate this question by using multiple unimodal and two types of multi-modal models-cross-modal and jointly pretrained-to determine which type of model is more relevant to fMRI brain activity when participants are engaged in watching movies. We observe that both types of multi-modal models show improved alignment in several language and visual regions. This study also helps in identifying which brain regions process unimodal versus multi-modal information. We further investigate the contribution of each modality to multi-modal alignment by carefully removing unimodal features one by one from multi-modal representations, and find that there is additional information beyond the unimodal embeddings that is processed in the visual and language regions. Based on this investigation, we find that while for cross-modal models, their brain alignment is partially attributed to the video modality; for jointly pretrained models, it is partially attributed to both the video and audio modalities. This serves as a strong motivation for the neuroscience community to investigate the interpretability of these models for deepening our understanding of multi-modal information processing in brain."
    },
    "2505.15946v2": {
      "title": "MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding",
      "url": "http://arxiv.org/abs/2505.15946v2",
      "authors": "Yuxiang Wei, Yanteng Zhang, Xi Xiao, Tianyang Wang, Xiao Wang, Vince D. Calhoun",
      "update_time": "2025-05-26",
      "abstract": "Decoding visual experiences from fMRI offers a powerful avenue to understand human perception and develop advanced brain-computer interfaces. However, current progress often prioritizes maximizing reconstruction fidelity while overlooking interpretability, an essential aspect for deriving neuroscientific insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework designed for high-fidelity, adaptable, and interpretable visual reconstruction. MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture where distinct experts process fMRI signals from functionally related voxel groups, mimicking specialized brain networks. The experts are first trained to encode fMRI into the frozen CLIP space. A finetuned diffusion model then synthesizes images, guided by expert outputs through a novel dual-stage routing mechanism that dynamically weighs expert contributions across the diffusion process. MoRE-Brain offers three main advancements: First, it introduces a novel Mixture-of-Experts architecture grounded in brain network principles for neuro-decoding. Second, it achieves efficient cross-subject generalization by sharing core expert networks while adapting only subject-specific routers. Third, it provides enhanced mechanistic insight, as the explicit routing reveals precisely how different modeled brain regions shape the semantic and spatial attributes of the reconstructed image. Extensive experiments validate MoRE-Brain's high reconstruction fidelity, with bottleneck analyses further demonstrating its effective utilization of fMRI signals, distinguishing genuine neural decoding from over-reliance on generative priors. Consequently, MoRE-Brain marks a substantial advance towards more generalizable and interpretable fMRI-based visual decoding. Code will be publicly available soon: https://github.com/yuxiangwei0808/MoRE-Brain."
    },
    "2505.15813v1": {
      "title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex",
      "url": "http://arxiv.org/abs/2505.15813v1",
      "authors": "Muquan Yu, Mu Nan, Hossein Adeli, Jacob S. Prince, John A. Pyles, Leila Wehbe, Margaret M. Henderson, Michael J. Tarr, Andrew F. Luo",
      "update_time": "2025-05-21",
      "abstract": "Understanding functional representations within higher visual cortex is a fundamental question in computational neuroscience. While artificial neural networks pretrained on large-scale datasets exhibit striking representational alignment with human neural responses, learning image-computable models of visual cortex relies on individual-level, large-scale fMRI datasets. The necessity for expensive, time-intensive, and often impractical data acquisition limits the generalizability of encoders to new subjects and stimuli. BraInCoRL uses in-context learning to predict voxelwise neural responses from few-shot examples without any additional finetuning for novel subjects and stimuli. We leverage a transformer architecture that can flexibly condition on a variable number of in-context image stimuli, learning an inductive bias over multiple subjects. During training, we explicitly optimize the model for in-context learning. By jointly conditioning on image features and voxel activations, our model learns to directly generate better performing voxelwise models of higher visual cortex. We demonstrate that BraInCoRL consistently outperforms existing voxelwise encoder designs in a low-data regime when evaluated on entirely novel images, while also exhibiting strong test-time scaling behavior. The model also generalizes to an entirely new visual fMRI dataset, which uses different subjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates better interpretability of neural signals in higher visual cortex by attending to semantically relevant stimuli. Finally, we show that our framework enables interpretable mappings from natural language queries to voxel selectivity."
    },
    "2505.14556v1": {
      "title": "Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI",
      "url": "http://arxiv.org/abs/2505.14556v1",
      "authors": "Marl\u00e8ne Careil, Yohann Benchetrit, Jean-R\u00e9mi King",
      "update_time": "2025-05-20",
      "abstract": "Brain-to-image decoding has been recently propelled by the progress in generative AI models and the availability of large ultra-high field functional Magnetic Resonance Imaging (fMRI). However, current approaches depend on complicated multi-stage pipelines and preprocessing steps that typically collapse the temporal dimension of brain recordings, thereby limiting time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural Activity Diffusion for Image Reconstruction), a new single-stage diffusion model designed for reconstructing images from dynamically evolving fMRI recordings. Our approach offers three main contributions. First, Dynadiff simplifies training as compared to existing approaches. Second, our model outperforms state-of-the-art models on time-resolved fMRI signals, especially on high-level semantic image reconstruction metrics, while remaining competitive on preprocessed fMRI data that collapse time. Third, this approach allows a precise characterization of the evolution of image representations in brain activity. Overall, this work lays the foundation for time-resolved brain-to-image decoding.",
      "code_url": "https://github.com/facebookresearch/dynadiff"
    },
    "2505.18193v1": {
      "title": "Riemannian Flow Matching for Brain Connectivity Matrices via Pullback Geometry",
      "url": "http://arxiv.org/abs/2505.18193v1",
      "authors": "Antoine Collas, Ce Ju, Nicolas Salvy, Bertrand Thirion",
      "update_time": "2025-05-20",
      "abstract": "Generating realistic brain connectivity matrices is key to analyzing population heterogeneity in brain organization, understanding disease, and augmenting data in challenging classification problems. Functional connectivity matrices lie in constrained spaces--such as the set of symmetric positive definite or correlation matrices--that can be modeled as Riemannian manifolds. However, using Riemannian tools typically requires redefining core operations (geodesics, norms, integration), making generative modeling computationally inefficient. In this work, we propose DiffeoCFM, an approach that enables conditional flow matching (CFM) on matrix manifolds by exploiting pullback metrics induced by global diffeomorphisms on Euclidean spaces. We show that Riemannian CFM with such metrics is equivalent to applying standard CFM after data transformation. This equivalence allows efficient vector field learning, and fast sampling with standard ODE solvers. We instantiate DiffeoCFM with two different settings: the matrix logarithm for covariance matrices and the normalized Cholesky decomposition for correlation matrices. We evaluate DiffeoCFM on three large-scale fMRI datasets with more than 4600 scans from 2800 subjects (ADNI, ABIDE, OASIS-3) and two EEG motor imagery datasets with over 30000 trials from 26 subjects (BNCI2014-002 and BNCI2015-001). It enables fast training and achieves state-of-the-art performance, all while preserving manifold constraints."
    },
    "2505.12653v1": {
      "title": "High-dimensional structure underlying individual differences in naturalistic visual experience",
      "url": "http://arxiv.org/abs/2505.12653v1",
      "authors": "Chihye Han, Michael F. Bonner",
      "update_time": "2025-05-19",
      "abstract": "How do different brains create unique visual experiences from identical sensory input? While neural representations vary across individuals, the fundamental architecture underlying these differences remains poorly understood. Here, we reveal that individual visual experience emerges from a high-dimensional neural geometry across the visual cortical hierarchy. Using spectral decomposition of fMRI responses during naturalistic movie viewing, we find that idiosyncratic neural patterns persist across multiple orders of magnitude of latent dimensions. Remarkably, each dimensional range encodes qualitatively distinct aspects of individual processing, and this multidimensional neural geometry predicts subsequent behavioral differences in memory recall. These fine-grained patterns of inter-individual variability cannot be reduced to those detected by conventional intersubject correlation measures. Our findings demonstrate that subjective visual experience arises from information integrated across an expansive multidimensional manifold. This geometric framework offers a powerful new lens for understanding how diverse brains construct unique perceptual worlds from shared experiences.",
      "code_url": "https://github.com/kelseyhan-jhu/idiosyncratic-neural-geometry"
    },
    "2505.12552v1": {
      "title": "FreqSelect: Frequency-Aware fMRI-to-Image Reconstruction",
      "url": "http://arxiv.org/abs/2505.12552v1",
      "authors": "Junliang Ye, Lei Wang, Md Zakir Hossain",
      "update_time": "2025-05-18",
      "abstract": "Reconstructing natural images from functional magnetic resonance imaging (fMRI) data remains a core challenge in natural decoding due to the mismatch between the richness of visual stimuli and the noisy, low resolution nature of fMRI signals. While recent two-stage models, combining deep variational autoencoders (VAEs) with diffusion models, have advanced this task, they treat all spatial-frequency components of the input equally. This uniform treatment forces the model to extract meaning features and suppress irrelevant noise simultaneously, limiting its effectiveness. We introduce FreqSelect, a lightweight, adaptive module that selectively filters spatial-frequency bands before encoding. By dynamically emphasizing frequencies that are most predictive of brain activity and suppressing those that are uninformative, FreqSelect acts as a content-aware gate between image features and natural data. It integrates seamlessly into standard very deep VAE-diffusion pipelines and requires no additional supervision. Evaluated on the Natural Scenes dataset, FreqSelect consistently improves reconstruction quality across both low- and high-level metrics. Beyond performance gains, the learned frequency-selection patterns offer interpretable insights into how different visual frequencies are represented in the brain. Our method generalizes across subjects and scenes, and holds promise for extension to other neuroimaging modalities, offering a principled approach to enhancing both decoding accuracy and neuroscientific interpretability."
    },
    "2505.12196v1": {
      "title": "Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled",
      "url": "http://arxiv.org/abs/2505.12196v1",
      "authors": "Yi-Chien Lin, Hongao Zhu, William Schuler",
      "update_time": "2025-05-18",
      "abstract": "The impressive linguistic abilities of large language models (LLMs) have recommended them as models of human sentence processing, with some conjecturing a positive 'quality-power' relationship (Wilcox et al., 2023), in which language models' (LMs') fit to psychometric data continues to improve as their ability to predict words in context increases. This is important because it suggests that elements of LLM architecture, such as veridical attention to context and a unique objective of predicting upcoming words, reflect the architecture of the human sentence processing faculty, and that any inadequacies in predicting human reading time and brain imaging data may be attributed to insufficient model complexity, which recedes as larger models become available. Recent studies (Oh and Schuler, 2023) have shown this scaling inverts after a point, as LMs become excessively large and accurate, when word prediction probability (as information-theoretic surprisal) is used as a predictor. Other studies propose the use of entire vectors from differently sized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting doubt on the value of surprisal as a predictor, but do not control for the larger number of predictors in vectors from larger LMs. This study evaluates LLM scaling using entire LLM vectors, while controlling for the larger number of predictors in vectors from larger LLMs. Results show that inverse scaling obtains, suggesting that inadequacies in predicting human reading time and brain imaging data may be due to substantial misalignment between LLMs and human sentence processing, which worsens as larger models are used."
    },
    "2505.12068v1": {
      "title": "Learning High-Order Relationships with Hypergraph Attention-based Spatio-Temporal Aggregation for Brain Disease Analysis",
      "url": "http://arxiv.org/abs/2505.12068v1",
      "authors": "Wenqi Hu, Xuerui Su, Guanliang Li, Yidi Pan, Aijing Lin",
      "update_time": "2025-05-17",
      "abstract": "Traditional functional connectivity based on functional magnetic resonance imaging (fMRI) can only capture pairwise interactions between brain regions. Hypergraphs, which reveal high-order relationships among multiple brain regions, have been widely used for disease analysis. However, existing methods often rely on predefined hypergraph structures, limiting their ability to model complex patterns. Moreover, temporal information, an essential component of brain high-order relationships, is frequently overlooked. To address these limitations, we propose a novel framework that jointly learns informative and sparse high-order brain structures along with their temporal dynamics. Inspired by the information bottleneck principle, we introduce an objective that maximizes information and minimizes redundancy, aiming to retain disease-relevant high-order features while suppressing irrelevant signals. Our model comprises a multi-hyperedge binary mask module for hypergraph structure learning, a hypergraph self-attention aggregation module that captures spatial features through adaptive attention across nodes and hyperedges, and a spatio-temporal low-dimensional network for extracting discriminative spatio-temporal representations for disease classification. Experiments on benchmark fMRI datasets demonstrate that our method outperforms the state-of-the-art approaches and successfully identifies meaningful high-order brain interactions. These findings provide new insights into brain network modeling and the study of neuropsychiatric disorders."
    },
    "2505.11806v1": {
      "title": "Robust outlier detection for heterogeneous distributions applicable to censoring in functional MRI",
      "url": "http://arxiv.org/abs/2505.11806v1",
      "authors": "Saranjeet Singh Saluja, Fatma Parlak, Damon Pham, Amanda Mejia",
      "update_time": "2025-05-17",
      "abstract": "Functional magnetic resonance imaging (fMRI) data are prone to intense \"burst\" noise artifacts due to head movements and other sources. Such volumes can be considered as high-dimensional outliers that can be identified using statistical outlier detection techniques, which allows for controlling the false positive rate. Previous work has used dimension reduction and multivariate outlier detection techniques, including the use of robust minimum covariance determinant (MCD) distances. Under Gaussianity, the distribution of these robust distances can be approximated, and an upper quantile of that distribution can be used to identify outlying volumes. Unfortunately, the Gaussian assumption is unrealistic for fMRI data in this context. One way to address this is to transform the data to Normality. A limitation of existing robust methods for this purpose, such as robust Box-Cox and Yeo-Johnson transformations, is that they can deal with skew but not heavy or light tails. Here, we develop a novel robust method for transformation to central Normality based on the highly flexible sinh-arcsinh (SHASH) family of distributions. To avoid the influence of outliers, it is crucial to initialize the outlier labels with a high degree of sensitivity. For this purpose, we consider a commonplace robust z-score approach, and a modified isolation forest (iForest) approach, a popular technique for anomaly detection in machine learning. Through extensive simulation studies, we find that our proposed SHASH transformation initialized using iForest clearly outperforms benchmark methods in a variety of settings, including skewed and heavy tailed distributions, and light to heavy outlier contamination. We also apply the proposed techniques to several example datasets and find this combination to have consistently strong performance."
    }
  },
  "MEG": {
    "2505.15355v1": {
      "title": "Decoding Phone Pairs from MEG Signals Across Speech Modalities",
      "url": "http://arxiv.org/abs/2505.15355v1",
      "authors": "Xabier de Zuazo, Eva Navas, Ibon Saratxaga, Mathieu Bourguignon, Nicola Molinaro",
      "update_time": "2025-05-21",
      "abstract": "Understanding the neural mechanisms underlying speech production is essential for both advancing cognitive neuroscience theory and developing practical communication technologies. In this study, we investigated magnetoencephalography signals to decode phones from brain activity during speech production and perception (passive listening and voice playback) tasks. Using a dataset comprising 17 participants, we performed pairwise phone classification, extending our analysis to 15 phonetic pairs. Multiple machine learning approaches, including regularized linear models and neural network architectures, were compared to determine their effectiveness in decoding phonetic information. Our results demonstrate significantly higher decoding accuracy during speech production (76.6%) compared to passive listening and playback modalities (~51%), emphasizing the richer neural information available during overt speech. Among the models, the Elastic Net classifier consistently outperformed more complex neural networks, highlighting the effectiveness of traditional regularization techniques when applied to limited and high-dimensional MEG datasets. Besides, analysis of specific brain frequency bands revealed that low-frequency oscillations, particularly Delta (0.2-3 Hz) and Theta (4-7 Hz), contributed the most substantially to decoding accuracy, suggesting that these bands encode critical speech production-related neural processes. Despite using advanced denoising methods, it remains unclear whether decoding solely reflects neural activity or if residual muscular or movement artifacts also contributed, indicating the need for further methodological refinement. Overall, our findings underline the critical importance of examining overt speech production paradigms, which, despite their complexity, offer opportunities to improve brain-computer interfaces to help individuals with severe speech impairments.",
      "code_url": "https://github.com/hitz-zentroa/meg-phone-decoding"
    },
    "2505.18185v1": {
      "title": "BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals",
      "url": "http://arxiv.org/abs/2505.18185v1",
      "authors": "Qinfan Xiao, Ziyun Cui, Chi Zhang, Siqi Chen, Wen Wu, Andrew Thwaites, Alexandra Woolgar, Bowen Zhou, Chao Zhang",
      "update_time": "2025-05-18",
      "abstract": "Electroencephalography (EEG) and magnetoencephalography (MEG) measure neural activity non-invasively by capturing electromagnetic fields generated by dendritic currents. Although rooted in the same biophysics, EEG and MEG exhibit distinct signal patterns, further complicated by variations in sensor configurations across modalities and recording devices. Existing approaches typically rely on separate, modality- and dataset-specific models, which limits the performance and cross-domain scalability. This paper proposes BrainOmni, the first brain foundation model that generalises across heterogeneous EEG and MEG recordings. To unify diverse data sources, we introduce BrainTokenizer,the first tokenizer that quantises spatiotemporal brain activity into discrete representations. Central to BrainTokenizer is a novel Sensor Encoder that encodes sensor properties such as spatial layout, orientation, and type, enabling compatibility across devices and modalities. Building upon the discrete representations, BrainOmni learns unified semantic embeddings of brain signals by self-supervised pretraining. To the best of our knowledge, it is the first foundation model to support both EEG and MEG signals, as well as the first to incorporate large-scale MEG pretraining. A total of 1,997 hours of EEG and 656 hours of MEG data are curated and standardised from publicly available sources for pretraining. Experiments show that BrainOmni outperforms both existing foundation models and state-of-the-art task-specific models on a range of downstream tasks. It also demonstrates strong generalisation to unseen EEG and MEG devices. Further analysis reveals that joint EEG-MEG (EMEG) training yields consistent improvements across both modalities. Code and model checkpoints will be released upon acceptance."
    },
    "2505.04764v1": {
      "title": "Charged Lepton Flavor Violating Experiments with Muons",
      "url": "http://arxiv.org/abs/2505.04764v1",
      "authors": "Dylan Palo",
      "update_time": "2025-05-07",
      "abstract": "We report on the status of charged lepton flavor violating (CLFV) experiments with muons. We focus on the three \"golden channels\": $\\mu^{+} \\rightarrow e^{+} \\gamma$, $\\mu^{+} \\rightarrow e^{+} e^{-} e^{+}$ and $\\mu^{-} N \\rightarrow e^{-} N$. The collection of upcoming experiments aim for sensitivity improvements up to $10^{4}$ with respect to previous searches. The MEG II experiment, searching for $\\mu^{+} \\rightarrow e^{+} \\gamma$, is currently in its 4th year of physics data-taking with a published result from its first year of data. The Mu3e experiment is an upcoming experiment searching for $\\mu^{+} \\rightarrow e^{+} e^{-} e^{+}$ with plans of physics data-taking as soon as 2025. The Mu2e and COMET experiments are upcoming searches for $\\mu^{-} N \\rightarrow e^{-} N$ with the goal of physics data-taking starting in 2027 and 2026 respectively. This proceeding summarizes the signal signature, expected background, resolutions, and timelines for the mentioned searches."
    },
    "2504.15711v1": {
      "title": "New limit on the \u03bc+->e+\u03b3decay with the MEG II experiment",
      "url": "http://arxiv.org/abs/2504.15711v1",
      "authors": "K. Afanaciev, A. M. Baldini, S. Ban, H. Benmansour, G. Boca, P. W. Cattaneo, G. Cavoto, F. Cei, M. Chiappini, A. Corvaglia, G. Dal Maso, A. De Bari, M. De Gerone, L. Ferrari Barusso, M. Francesconi, L. Galli, G. Gallucci, F. Gatti, L. Gerritzen, F. Grancagnolo, E. G. Grandoni, M. Grassi, D. N. Grigoriev, M. Hildebrandt, F. Ignatov, F. Ikeda, T. Iwamoto, S. Karpov, P. -R. Kettle, N. Khomutov, A. Kolesnikov, N. Kravchuk, V. Krylov, N. Kuchinskiy, F. Leonetti, W. Li, V. Malyshev, A. Matsushita, S. Mihara, W. Moltzon, Toshinori Mori, D. Nicolo', H. Nishiguchi, A. Ochi, H. Ogawa, W. Ootani, A. Oya, D. Palo, M. Panareo, A. Papa, D. Pasciuto, A. Popov, F. Renga, S. Ritt, M. Rossella, A. Rozhdestvensky, S. Scarpellini, G. Signorelli, H. Suzuki, M. Takahashi, Y. Uchiyama, R. Umakoshi, A. Venturini, B. Vitali, C. Voena, K. Yamamoto, R. Yokota, T. Yonemoto",
      "update_time": "2025-04-22",
      "abstract": "This letter reports the result of the search for the decay \\mu+->e+\\gamma undertaken at the Paul Scherrer Institut in Switzerland with the MEG II experiment using the data collected in the 2021- 2022 physics runs. The sensitivity of this search is 2.2x10-13, a factor of 2.4 better than that of the full MEG dataset and obtained in a data taking period of about one fourth that of MEG, thanks to the superior performances of the new detector. The result is consistent with the expected background, yielding an upper limit on the branching ratio of B(\\mu+->e+\\gamma)<1.5 x 10-13 (90 % C.L.). Additional improvements are expected with the data collected during the years 2023-2024. The data-taking will continue in the coming years."
    },
    "2504.11332v1": {
      "title": "Constraints from muon $g-2$ on a gauged non-universal $U(1)_{X}$ model with inverse see-saw neutrinos",
      "url": "http://arxiv.org/abs/2504.11332v1",
      "authors": "J. S. Alvarado, R. Martinez, Cristian Sierra",
      "update_time": "2025-04-15",
      "abstract": "We study the effects on a non-universal $U(1)_{X}$ extension of the Standard Model given the alternative value obtained by the Budapest-Marseille-Wuppertal (BMW) group for the anomalous magnetic moment of the muon $g-2$. The model explains the fermion mass hierarchy through the non-universality of the extra gauge symmetry and by an additional $\\mathbb{Z}_{2}$ discrete symmetry, where the heaviest fermions acquire their masses from two different scales determined by two Higgs doublets and one singlet, whereas the lightest fermions obtain their masses from radiative corrections. From cancellation of chiral anomalies, the model also includes heavy extra fermions, both charged and neutral. The latter are right-handed neutrinos that acquire masses via an inverse see-saw mechanism, reproducing the observed squared mass differences for the active neutrinos. Using the latest lattice calculation of the leading hadronic vacuum polarization (HVP) contribution to the muon $g-2$, we compute the dominant one-loop diagrams mediated by the $W$ and charged Higgs bosons, both with a heavy Majorana neutrino in the loop, setting bounds for masses of the new particles. We also provide predictions for observables that can probe our model in the future such as charged lepton flavor violating searches at Belle II like $\\tau\\to \\mu\\gamma$, $\\tau\\to e\\gamma$ and at MEG II for $\\mu\\to e\\gamma$."
    },
    "2503.16892v1": {
      "title": "Multifractal analysis based on the weak scaling exponent and applications to MEG recordings in neuroscience",
      "url": "http://arxiv.org/abs/2503.16892v1",
      "authors": "Patrice Abry, Phipippe Ciuciu, Merlin Dumeur, St\u00e9phane Jaffard, Guillaume Sa\u00ebs",
      "update_time": "2025-03-21",
      "abstract": "We develop the mathematical properties of a multifractal analysis of data based on the weak scaling exponent. The advantage of this analysis is that it does not require any a priori global regularity assumption on the analyzed signal, in contrast with the previously used H{\\\"o}lder or p-exponents. As an illustration, we show that this technique allows one to perform a multifractal analysis of MEG signals, which records electromagnetic brain activity, that was not theoretically valid using the formerly introduced methods based on H{\\\"o}lder or p-exponents."
    },
    "2503.10965v2": {
      "title": "Auditing language models for hidden objectives",
      "url": "http://arxiv.org/abs/2503.10965v2",
      "authors": "Samuel Marks, Johannes Treutlein, Trenton Bricken, Jack Lindsey, Jonathan Marcus, Siddharth Mishra-Sharma, Daniel Ziegler, Emmanuel Ameisen, Joshua Batson, Tim Belonax, Samuel R. Bowman, Shan Carter, Brian Chen, Hoagy Cunningham, Carson Denison, Florian Dietz, Satvik Golechha, Akbir Khan, Jan Kirchner, Jan Leike, Austin Meek, Kei Nishimura-Gasparian, Euan Ong, Christopher Olah, Adam Pearce, Fabien Roger, Jeanne Salle, Andy Shih, Meg Tong, Drake Thomas, Kelley Rivoire, Adam Jermyn, Monte MacDiarmid, Tom Henighan, Evan Hubinger",
      "update_time": "2025-03-28",
      "abstract": "We study the feasibility of conducting alignment audits: investigations into whether models have undesired objectives. As a testbed, we train a language model with a hidden objective. Our training pipeline first teaches the model about exploitable errors in RLHF reward models (RMs), then trains the model to exploit some of these errors. We verify via out-of-distribution evaluations that the model generalizes to exhibit whatever behaviors it believes RMs rate highly, including ones not reinforced during training. We leverage this model to study alignment audits in two ways. First, we conduct a blind auditing game where four teams, unaware of the model's hidden objective or training, investigate it for concerning behaviors and their causes. Three teams successfully uncovered the model's hidden objective using techniques including interpretability with sparse autoencoders (SAEs), behavioral attacks, and training data analysis. Second, we conduct an unblinded follow-up study of eight techniques for auditing the model, analyzing their strengths and limitations. Overall, our work provides a concrete example of using alignment audits to discover a model's hidden objective and proposes a methodology for practicing and validating progress in alignment auditing."
    },
    "2503.06086v1": {
      "title": "Characterizing optimal monitoring edge-geodetic sets for some structured graph classes",
      "url": "http://arxiv.org/abs/2503.06086v1",
      "authors": "Florent Foucaud, Arti Pandey, Kaustav Paul",
      "update_time": "2025-03-08",
      "abstract": "Given a graph $G=(V,E)$, a set $S\\subseteq V$ is said to be a monitoring edge-geodetic set if the deletion of any edge in the graph results in a change in the distance between at least one pair of vertices in $S$. The minimum size of such a set in $G$ is called the monitoring edge-geodetic number of $G$ and is denoted by $meg(G)$.   In this work, we compute the monitoring edge-geodetic number efficiently for the following graph classes: distance-hereditary graphs, $P_4$-sparse graphs, bipartite permutation graphs, and strongly chordal graphs. The algorithms follow from structural characterizations of the optimal monitoring edge-geodetic sets for these graph classes in terms of \\emph{mandatory vertices} (those that need to be in every solution). This extends previous results from the literature for cographs, interval graphs and block graphs."
    },
    "2503.05211v1": {
      "title": "Language-specific Tonal Features Drive Speaker-Listener Neural Synchronization",
      "url": "http://arxiv.org/abs/2503.05211v1",
      "authors": "Chen Hong, Xiangbin Teng, Yu Li, Shen-Mou Hsu, Feng-Ming Tsao, Patrick C. M. Wong, Gangyi Feng",
      "update_time": "2025-03-07",
      "abstract": "Verbal communication transmits information across diverse linguistic levels, with neural synchronization (NS) between speakers and listeners emerging as a putative mechanism underlying successful exchange. However, the specific speech features driving this synchronization and how language-specific versus universal characteristics facilitate information transfer remain poorly understood. We developed a novel content-based interbrain encoding model to disentangle the contributions of acoustic and linguistic features to speaker-listener NS during Mandarin storytelling and listening, as measured via magnetoencephalography (MEG). Results revealed robust NS throughout frontotemporal-parietal networks with systematic time lags between speech production and perception. Crucially, suprasegmental lexical tone features (tone categories, pitch height, and pitch contour), essential for lexical meaning in Mandarin, contributed more significantly to NS than either acoustic elements or universal segmental units (consonants and vowels). These tonal features generated distinctive spatiotemporal NS patterns, creating language-specific neural \"communication channels\" that facilitated efficient representation sharing between interlocutors. Furthermore, the strength and patterns of NS driven by these language-specific features predicted communication success. These findings demonstrate the neural mechanisms underlying shared representations during verbal exchange and highlight how language-specific features can shape neural coupling to optimize information transfer during human communication."
    },
    "2502.16797v1": {
      "title": "Forecasting Rare Language Model Behaviors",
      "url": "http://arxiv.org/abs/2502.16797v1",
      "authors": "Erik Jones, Meg Tong, Jesse Mu, Mohammed Mahfoud, Jan Leike, Roger Grosse, Jared Kaplan, William Fithian, Ethan Perez, Mrinank Sharma",
      "update_time": "2025-02-24",
      "abstract": "Standard language model evaluations can fail to capture risks that emerge only at deployment scale. For example, a model may produce safe responses during a small-scale beta test, yet reveal dangerous information when processing billions of requests at deployment. To remedy this, we introduce a method to forecast potential risks across orders of magnitude more queries than we test during evaluation. We make forecasts by studying each query's elicitation probability -- the probability the query produces a target behavior -- and demonstrate that the largest observed elicitation probabilities predictably scale with the number of queries. We find that our forecasts can predict the emergence of diverse undesirable behaviors -- such as assisting users with dangerous chemical synthesis or taking power-seeking actions -- across up to three orders of magnitude of query volume. Our work enables model developers to proactively anticipate and patch rare failures before they manifest during large-scale deployments."
    }
  },
  "neuroAI": {
    "2505.16080v1": {
      "title": "SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation",
      "url": "http://arxiv.org/abs/2505.16080v1",
      "authors": "Jiayue Liu, Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Kuo Yang, Xu Wang, Yang Wang",
      "update_time": "2025-05-21",
      "abstract": "Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation."
    },
    "2503.06286v1": {
      "title": "A 7T fMRI dataset of synthetic images for out-of-distribution modeling of vision",
      "url": "http://arxiv.org/abs/2503.06286v1",
      "authors": "Alessandro T. Gifford, Radoslaw M. Cichy, Thomas Naselaris, Kendrick Kay",
      "update_time": "2025-03-08",
      "abstract": "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD) are boosting NeuroAI research by enabling computational models of the brain with performances beyond what was possible just a decade ago. However, these datasets lack out-of-distribution (OOD) components, which are crucial for the development of more robust models. Here, we address this limitation by releasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the eight NSD subjects for 284 carefully controlled synthetic images. We show that NSD-synthetic's fMRI responses reliably encode stimulus-related information and are OOD with respect to NSD. Furthermore, OOD generalization tests on NSD-synthetic reveal differences between models of the brain that are not detected with NSD - specifically, self-supervised deep neural networks better explain neural responses than their task-supervised counterparts. These results showcase how NSD-synthetic enables OOD generalization tests that facilitate the development of more robust models of visual processing, and the formulation of more accurate theories of human vision."
    },
    "2502.16238v1": {
      "title": "Brain-Model Evaluations Need the NeuroAI Turing Test",
      "url": "http://arxiv.org/abs/2502.16238v1",
      "authors": "Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi",
      "update_time": "2025-02-22",
      "abstract": "What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \\emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development."
    },
    "2501.02402v1": {
      "title": "Asynchronous Hebbian/anti-Hebbian networks",
      "url": "http://arxiv.org/abs/2501.02402v1",
      "authors": "Henrique Reis Aguiar, Matthias H. Hennig",
      "update_time": "2025-01-04",
      "abstract": "Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.",
      "code_url": "https://github.com/henri-edinb/async_learning"
    },
    "2411.18526v2": {
      "title": "NeuroAI for AI Safety",
      "url": "http://arxiv.org/abs/2411.18526v2",
      "authors": "Patrick Mineault, Niccol\u00f2 Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador",
      "update_time": "2025-04-03",
      "abstract": "As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety."
    },
    "2411.14633v1": {
      "title": "Evaluating Representational Similarity Measures from the Lens of Functional Correspondence",
      "url": "http://arxiv.org/abs/2411.14633v1",
      "authors": "Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla",
      "update_time": "2024-11-21",
      "abstract": "Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research."
    },
    "2409.05771v1": {
      "title": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models",
      "url": "http://arxiv.org/abs/2409.05771v1",
      "authors": "Emily Cheng, Richard J. Antonello",
      "update_time": "2024-09-09",
      "abstract": "Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first \"composition\" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties."
    },
    "2407.04117v2": {
      "title": "Predictive Coding Networks and Inference Learning: Tutorial and Survey",
      "url": "http://arxiv.org/abs/2407.04117v2",
      "authors": "Bj\u00f6rn van Zwol, Ro Jefferson, Egon L. van den Broek",
      "update_time": "2024-07-22",
      "abstract": "Recent years have witnessed a growing call for renewed emphasis on neuroscience-inspired approaches in artificial intelligence research, under the banner of NeuroAI. A prime example of this is predictive coding networks (PCNs), based on the neuroscientific framework of predictive coding. This framework views the brain as a hierarchical Bayesian inference model that minimizes prediction errors through feedback connections. Unlike traditional neural networks trained with backpropagation (BP), PCNs utilize inference learning (IL), a more biologically plausible algorithm that explains patterns of neural activity that BP cannot. Historically, IL has been more computationally intensive, but recent advancements have demonstrated that it can achieve higher efficiency than BP with sufficient parallelization. Furthermore, PCNs can be mathematically considered a superset of traditional feedforward neural networks (FNNs), significantly extending the range of trainable architectures. As inherently probabilistic (graphical) latent variable models, PCNs provide a versatile framework for both supervised learning and unsupervised (generative) modeling that goes beyond traditional artificial neural networks. This work provides a comprehensive review and detailed formal specification of PCNs, particularly situating them within the context of modern ML methods. Additionally, we introduce a Python library (PRECO) for practical implementation. This positions PC as a promising framework for future ML innovations."
    },
    "2306.10168v3": {
      "title": "Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis",
      "url": "http://arxiv.org/abs/2306.10168v3",
      "authors": "Mitchell Ostrow, Adam Eisen, Leo Kozachkov, Ila Fiete",
      "update_time": "2023-10-29",
      "abstract": "How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.",
      "code_url": "https://github.com/mitchellostrow/dsa"
    },
    "2305.11275v2": {
      "title": "Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture",
      "url": "http://arxiv.org/abs/2305.11275v2",
      "authors": "Galen Pogoncheff, Jacob Granley, Michael Beyeler",
      "update_time": "2023-05-25",
      "abstract": "Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence."
    }
  },
  "medical": {
    "2505.20093v1": {
      "title": "Technical recommendation on multiplex MR elastography for tomographic mapping of abdominal stiffness with a focus on the pancreas and pancreatic ductal adenocarcinoma",
      "url": "http://arxiv.org/abs/2505.20093v1",
      "authors": "Jakob Schattenfroh, Salma Almutawakel, Jan Bieling, Johannes Castelein, Melanie Estrella, Philippe Garteiser, Viktor Hartung, Karl H. Hillebrandt, Adrian T. Huber, Laura K\u00f6rner, Thomas Kr\u00f6ncke, Thomas Malinka, Hans-Jonas Meyer, Tom Meyer, Uwe Pelzer, Felix Pfister, Igor M. Sauer, Anna Speth, Bernard E. Van Beers, Carsten Warmuth, Nienke P. M. Wassenaar, Yanglei Wu, Rolf Reiter, Ingolf Sack",
      "update_time": "2025-05-26",
      "abstract": "Objectives: MR elastography (MRE) offers valuable mechanical tissue characterization, however, in deep abdominal organs like the pancreas conventional single-driver, single-frequency approaches often fail. This study evaluates whether multiplex MRE using multiple drivers and vibration frequencies can overcome these limitations.   Methods: This study used single-shot spin-echo MRE in 18 healthy volunteers targeting the liver, pancreas, kidneys, and spleen. Each healthy volunteer underwent 16 MRE examinations with different sets of four vibration frequencies (30-60 Hz) and four driver combinations, and an additional null experiment without vibrations. Further, a cohort of 14 patients with pancreatic ductal adenocarcinoma (PDAC) were retrospectively assessed. The quality of shear-wave fields and stiffness maps was assessed by displacement amplitudes and image sharpness.   Results: In healthy volunteers, abdominal coverage with displacement amplitudes above the pre-determined noise level of 4 {\\mu}m varied between MRE configurations: 24.2% ([0.0%-56.2%], single-driver, 60 Hz), 66.9% ([24.8%-97.7%], single-driver, 30-60 Hz), 70.2% ([0.0%-92.5%], multi-driver, 60 Hz) and 99.9% ([89.4%-100%], multi-driver, 30-60 Hz). In the pancreas, >60% coverage was achieved in all subjects using four drivers and multiple frequencies. This was achieved in only 2/18 subjects using single-driver/single-frequency MRE. Patients with PDAC had 99.1% [91.4%-100%] coverage in the pancreas and 96.3% [63.1%-100%] abdominal coverage (multi-driver, 30-60 Hz).   Conclusion: MRE with four drivers and multiple vibration frequencies between 30-60 Hz enables tomographic mapping of tissue stiffness across the entire abdomen, including the pancreas. Multiplex MRE offers a promising approach for generating detailed images of abdominal stiffness, potentially enhancing clinical diagnostics for abdominal and pancreatic diseases."
    },
    "2505.20020v1": {
      "title": "Ontology- and LLM-based Data Harmonization for Federated Learning in Healthcare",
      "url": "http://arxiv.org/abs/2505.20020v1",
      "authors": "Natallia Kokash, Lei Wang, Thomas H. Gillespie, Adam Belloum, Paola Grosso, Sara Quinney, Lang Li, Bernard de Bono",
      "update_time": "2025-05-26",
      "abstract": "The rise of electronic health records (EHRs) has unlocked new opportunities for medical research, but privacy regulations and data heterogeneity remain key barriers to large-scale machine learning. Federated learning (FL) enables collaborative modeling without sharing raw data, yet faces challenges in harmonizing diverse clinical datasets. This paper presents a two-step data alignment strategy integrating ontologies and large language models (LLMs) to support secure, privacy-preserving FL in healthcare, demonstrating its effectiveness in a real-world project involving semantic mapping of EHR data."
    },
    "2505.19858v1": {
      "title": "A Unified Solution to Video Fusion: From Multi-Frame Learning to Benchmarking",
      "url": "http://arxiv.org/abs/2505.19858v1",
      "authors": "Zixiang Zhao, Haowen Bai, Bingxin Ke, Yukun Cui, Lilun Deng, Yulun Zhang, Kai Zhang, Konrad Schindler",
      "update_time": "2025-05-26",
      "abstract": "The real world is dynamic, yet most image fusion methods process static frames independently, ignoring temporal correlations in videos and leading to flickering and temporal inconsistency. To address this, we propose Unified Video Fusion (UniVF), a novel framework for temporally coherent video fusion that leverages multi-frame learning and optical flow-based feature warping for informative, temporally coherent video fusion. To support its development, we also introduce Video Fusion Benchmark (VF-Bench), the first comprehensive benchmark covering four video fusion tasks: multi-exposure, multi-focus, infrared-visible, and medical fusion. VF-Bench provides high-quality, well-aligned video pairs obtained through synthetic data generation and rigorous curation from existing datasets, with a unified evaluation protocol that jointly assesses the spatial quality and temporal consistency of video fusion. Extensive experiments show that UniVF achieves state-of-the-art results across all tasks on VF-Bench. Project page: https://vfbench.github.io."
    },
    "2505.19846v1": {
      "title": "Zero-Shot Pseudo Labels Generation Using SAM and CLIP for Semi-Supervised Semantic Segmentation",
      "url": "http://arxiv.org/abs/2505.19846v1",
      "authors": "Nagito Saito, Shintaro Ito, Koichi Ito, Takafumi Aoki",
      "update_time": "2025-05-26",
      "abstract": "Semantic segmentation is a fundamental task in medical image analysis and autonomous driving and has a problem with the high cost of annotating the labels required in training. To address this problem, semantic segmentation methods based on semi-supervised learning with a small number of labeled data have been proposed. For example, one approach is to train a semantic segmentation model using images with annotated labels and pseudo labels. In this approach, the accuracy of the semantic segmentation model depends on the quality of the pseudo labels, and the quality of the pseudo labels depends on the performance of the model to be trained and the amount of data with annotated labels. In this paper, we generate pseudo labels using zero-shot annotation with the Segment Anything Model (SAM) and Contrastive Language-Image Pretraining (CLIP), improve the accuracy of the pseudo labels using the Unified Dual-Stream Perturbations Approach (UniMatch), and use them as enhanced labels to train a semantic segmentation model. The effectiveness of the proposed method is demonstrated through the experiments using the public datasets: PASCAL and MS COCO."
    },
    "2505.19811v1": {
      "title": "Physically Plausible Vectorial Metrics for Polarization Information Analysis",
      "url": "http://arxiv.org/abs/2505.19811v1",
      "authors": "Runchen Zhang, Xuke Qiu, Yifei Ma, Zimo Zhao, An Aloysius Wang, Jinge Guo, Ji Qin, Steve J. Elston, Stephen M. Morris, Chao He",
      "update_time": "2025-05-26",
      "abstract": "The Mueller Matrix Polar Decomposition method decomposes a Mueller matrix into a diattenuator, a retarder, and a depolarizer. Among these elements, the retarder, which plays a key role in medical and material characterization, is modelled as a circular retarder followed by a linear retarder when using this approach. However, this model may not accurately reflect the actual structure of the retarder in certain cases, as many practical retarders do not have a layered structure or consist of multiple (unknown) layers. Misinterpretation, therefore, may occur when the actual structure differs from the model. Here we circumvent this limitation by proposing to use a physically plausible parameter set that includes the axis orientation angle $\\phi$, the degree of ellipticity $\\chi$, and the elliptical retardance $\\rho$. By working with this set of parameters, an overall characterization of a retarder is provided, encompassing its full optical response without making any assumptions about the structure of the material. In this study, experiments were carried out on liquid crystalline samples to validate the feasibility of our approach, demonstrating that the physically plausible parameter set adopted provides a useful tool for a broader range of applications in both biomedical imaging and optical material analysis."
    },
    "2505.19798v1": {
      "title": "Challenges and perspectives in using multimodal imaging techniques to advance the understanding of fish intestinal microvilli",
      "url": "http://arxiv.org/abs/2505.19798v1",
      "authors": "Ankit Butola, Luis E. Villegas-Hern\u00e1ndez, Dhivya B. Thiyagarajan, Bart\u0142omiej Zapotoczny, Roy A. Dalmo, Balpreet Singh Ahluwalia",
      "update_time": "2025-05-26",
      "abstract": "The primary function of intestinal microvilli is to increase the surface area of the intestinal lining to maximize nutrient absorption. Despite its importance to the fish health, the small size and dense footprint of microvilli hinders its investigation and necessitates the need of advanced microscopy methods for its visualization. Characterization of the microvilli using super-resolution microscopy provides insights into their structural organization, spatial distribution, and surface properties. Here, we present a comprehensive investigation of different optical, electron and force microscopy methods for analysis of fish microvilli. The super-resolution optical microscopy methods used are 3D structured illumination microscopy (SIM), stimulated emission depletion microscopy (STED), and fluorescence fluctuation microscopy. We also visualized the intestinal microvilli in fish using diffraction-limited optical microscopy methods including confocal and total internal reflection fluorescence microscopy. Additionally, label-free microscopy methods, such as quantitative phase microscopy (QPM) and bright-field imaging, were also employed. To obtain ultra-high resolution, we used scanning electron microscopy (SEM), transmission electron microscopy (TEM) and atomic force microscopy (AFM). We demonstrate a systematic comparison of these microscopy techniques in resolving and quantifying microvilli features, ranging from 1-2 {\\mu}m structural morphology to 10-100 nm surface details. Our results highlight the advantages, limitations, and complementary nature of each method in capturing microvilli characteristics across different scales. These techniques are used for super-resolution and high-resolution imaging of Atlantic salmon microvilli gastrointestinal tract."
    },
    "2505.19779v1": {
      "title": "Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models",
      "url": "http://arxiv.org/abs/2505.19779v1",
      "authors": "Mobina Mansoori, Sajjad Shahabodini, Farnoush Bayatmakou, Jamshid Abouei, Konstantinos N. Plataniotis, Arash Mohammadi",
      "update_time": "2025-05-26",
      "abstract": "Using massive datasets, foundation models are large-scale, pre-trained models that perform a wide range of tasks. These models have shown consistently improved results with the introduction of new methods. It is crucial to analyze how these trends impact the medical field and determine whether these advancements can drive meaningful change. This study investigates the application of recent state-of-the-art foundation models, DINOv2, MAE, VMamba, CoCa, SAM2, and AIMv2, for medical image classification. We explore their effectiveness on datasets including CBIS-DDSM for mammography, ISIC2019 for skin lesions, APTOS2019 for diabetic retinopathy, and CHEXPERT for chest radiographs. By fine-tuning these models and evaluating their configurations, we aim to understand the potential of these advancements in medical image classification. The results indicate that these advanced models significantly enhance classification outcomes, demonstrating robust performance despite limited labeled data. Based on our results, AIMv2, DINOv2, and SAM2 models outperformed others, demonstrating that progress in natural domain training has positively impacted the medical domain and improved classification outcomes. Our code is publicly available at: https://github.com/sajjad-sh33/Medical-Transfer-Learning."
    },
    "2505.19659v1": {
      "title": "LangDAug: Langevin Data Augmentation for Multi-Source Domain Generalization in Medical Image Segmentation",
      "url": "http://arxiv.org/abs/2505.19659v1",
      "authors": "Piyush Tiwary, Kinjawl Bhattacharyya, Prathosh A. P",
      "update_time": "2025-05-26",
      "abstract": "Medical image segmentation models often struggle to generalize across different domains due to various reasons. Domain Generalization (DG) methods overcome this either through representation learning or data augmentation (DAug). While representation learning methods seek domain-invariant features, they often rely on ad-hoc techniques and lack formal guarantees. DAug methods, which enrich model representations through synthetic samples, have shown comparable or superior performance to representation learning approaches. We propose LangDAug, a novel $\\textbf{Lang}$evin $\\textbf{D}$ata $\\textbf{Aug}$mentation for multi-source domain generalization in 2D medical image segmentation. LangDAug leverages Energy-Based Models (EBMs) trained via contrastive divergence to traverse between source domains, generating intermediate samples through Langevin dynamics. Theoretical analysis shows that LangDAug induces a regularization effect, and for GLMs, it upper-bounds the Rademacher complexity by the intrinsic dimensionality of the data manifold. Through extensive experiments on Fundus segmentation and 2D MRI prostate segmentation benchmarks, we show that LangDAug outperforms state-of-the-art domain generalization methods and effectively complements existing domain-randomization approaches. The codebase for our method is available at https://github.com/backpropagator/LangDAug."
    },
    "2505.19630v1": {
      "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue",
      "url": "http://arxiv.org/abs/2505.19630v1",
      "authors": "Yichun Feng, Jiawei Wang, Lu Zhou, Yixue Li",
      "update_time": "2025-05-26",
      "abstract": "Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Existing systems rely on a one-way information transmission mode where patients must fully describe their symptoms in a single round, leading to nonspecific diagnostic recommendations when complaints are vague. Traditional multi-turn dialogue methods based on supervised learning are constrained by static data-driven paradigms, lacking generalizability and struggling to intelligently extract key clinical information. To address these limitations, we propose DoctorAgent-RL, a reinforcement learning (RL)-based multi-agent collaborative framework that models medical consultations as a dynamic decision-making process under uncertainty. The doctor agent continuously optimizes its questioning strategy within the RL framework through multi-turn interactions with the patient agent, dynamically adjusting its information-gathering path based on comprehensive rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop interaction strategies aligned with clinical reasoning logic, rather than superficially imitating patterns in existing dialogue data. Notably, we constructed MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating patient interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing models in both multi-turn reasoning capability and final diagnostic performance, demonstrating practical value in assisting clinical consultations. https://github.com/JarvisUSTC/DoctorAgent-RL"
    },
    "2505.19603v1": {
      "title": "Rep3D: Re-parameterize Large 3D Kernels with Low-Rank Receptive Modeling for Medical Imaging",
      "url": "http://arxiv.org/abs/2505.19603v1",
      "authors": "Ho Hin Lee, Quan Liu, Shunxing Bao, Yuankai Huo, Bennett A. Landman",
      "update_time": "2025-05-26",
      "abstract": "In contrast to vision transformers, which model long-range dependencies through global self-attention, large kernel convolutions provide a more efficient and scalable alternative, particularly in high-resolution 3D volumetric settings. However, naively increasing kernel size often leads to optimization instability and degradation in performance. Motivated by the spatial bias observed in effective receptive fields (ERFs), we hypothesize that different kernel elements converge at variable rates during training. To support this, we derive a theoretical connection between element-wise gradients and first-order optimization, showing that structurally re-parameterized convolution blocks inherently induce spatially varying learning rates. Building on this insight, we introduce Rep3D, a 3D convolutional framework that incorporates a learnable spatial prior into large kernel training. A lightweight two-stage modulation network generates a receptive-biased scaling mask, adaptively re-weighting kernel updates and enabling local-to-global convergence behavior. Rep3D adopts a plain encoder design with large depthwise convolutions, avoiding the architectural complexity of multi-branch compositions. We evaluate Rep3D on five challenging 3D segmentation benchmarks and demonstrate consistent improvements over state-of-the-art baselines, including transformer-based and fixed-prior re-parameterization methods. By unifying spatial inductive bias with optimization-aware learning, Rep3D offers an interpretable, and scalable solution for 3D medical image analysis. The source code is publicly available at https://github.com/leeh43/Rep3D."
    }
  }
}