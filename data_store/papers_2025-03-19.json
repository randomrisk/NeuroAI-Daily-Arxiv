{
  "Brain": {
    "2503.13344v1": {
      "title": "STEP: Simultaneous Tracking and Estimation of Pose for Animals and Humans",
      "url": "http://arxiv.org/abs/2503.13344v1",
      "authors": "Shashikant Verma, Harish Katti, Soumyaratna Debnath, Yamuna Swamy, Shanmuganathan Raman",
      "update_time": "2025-03-17",
      "abstract": "We introduce STEP, a novel framework utilizing Transformer-based discriminative model prediction for simultaneous tracking and estimation of pose across diverse animal species and humans. We are inspired by the fact that the human brain exploits spatiotemporal continuity and performs concurrent localization and pose estimation despite the specialization of brain areas for form and motion processing. Traditional discriminative models typically require predefined target states for determining model weights, a challenge we address through Gaussian Map Soft Prediction (GMSP) and Offset Map Regression Adapter (OMRA) Modules. These modules remove the necessity of keypoint target states as input, streamlining the process. Our method starts with a known target state initialized through a pre-trained detector or manual initialization in the initial frame of a given video sequence. It then seamlessly tracks the target and estimates keypoints of anatomical importance as output for subsequent frames. Unlike prevalent top-down pose estimation methods, our approach doesn't rely on per-frame target detections due to its tracking capability. This facilitates a significant advancement in inference efficiency and potential applications. We train and validate our approach on datasets encompassing diverse species. Our experiments demonstrate superior results compared to existing methods, opening doors to various applications, including but not limited to action recognition and behavioral analysis."
    },
    "2503.13212v1": {
      "title": "MAME: Multidimensional Adaptive Metamer Exploration with Human Perceptual Feedback",
      "url": "http://arxiv.org/abs/2503.13212v1",
      "authors": "Mina Kamao, Hayato Ono, Ayumu Yamashita, Kaoru Amano, Masataka Sawayama",
      "update_time": "2025-03-17",
      "abstract": "Alignment between human brain networks and artificial models is actively studied in machine learning and neuroscience. A widely adopted approach to explore their functional alignment is to identify metamers for both humans and models. Metamers refer to input stimuli that are physically different but equivalent within a given system. If a model's metameric space completely matched the human metameric space, the model would achieve functional alignment with humans. However, conventional methods lack direct ways to search for human metamers. Instead, researchers first develop biologically inspired models and then infer about human metamers indirectly by testing whether model metamers also appear as metamers to humans. Here, we propose the Multidimensional Adaptive Metamer Exploration (MAME) framework, enabling direct high-dimensional exploration of human metameric space. MAME leverages online image generation guided by human perceptual feedback. Specifically, it modulates reference images across multiple dimensions by leveraging hierarchical responses from convolutional neural networks (CNNs). Generated images are presented to participants whose perceptual discriminability is assessed in a behavioral task. Based on participants' responses, subsequent image generation parameters are adaptively updated online. Using our MAME framework, we successfully measured a human metameric space of over fifty dimensions within a single experiment. Experimental results showed that human discrimination sensitivity was lower for metameric images based on low-level features compared to high-level features, which image contrast metrics could not explain. The finding suggests that the model computes low-level information not essential for human perception. Our framework has the potential to contribute to developing interpretable AI and understanding of brain function in neuroscience."
    },
    "2503.13097v1": {
      "title": "Combined impact of grey and superficial white matter abnormalities: implications for epilepsy surgery",
      "url": "http://arxiv.org/abs/2503.13097v1",
      "authors": "Csaba Kozma, Jonathan Horsley, Gerard Hall, Callum Simpson, Jane de Tisi, Anna Miserocchi, Andrew W. McEvoy, Sjoerd B. Vos, Gavin P. Winston, Yujiang Wang, John S. Duncan, Peter N. Taylor",
      "update_time": "2025-03-17",
      "abstract": "Drug-resistant focal epilepsy is associated with abnormalities in the brain in both grey matter (GM) and superficial white matter (SWM). However, it is unknown if both types of abnormalities are important in supporting seizures. Here, we test if surgical removal of GM and/or SWM abnormalities relates to post-surgical seizure outcome in people with temporal lobe epilepsy (TLE).   We analyzed structural imaging data from 143 TLE patients (pre-op dMRI and pre-op T1-weighted MRI) and 97 healthy controls. We calculated GM volume abnormalities and SWM mean diffusivity abnormalities and evaluated if their surgical removal distinguished seizure outcome groups post-surgically.   At a group level, GM and SWM abnormalities were most common in the ipsilateral temporal lobe and hippocampus in people with TLE. Analyzing both modalities together, compared to in isolation, improved surgical outcome discrimination (GM AUC = 0.68, p < 0.01, WM AUC = 0.65, p < 0.01; Union AUC = 0.72, p < 0.01, Concordance AUC = 0.64, p = 0.04). Additionally, 100% of people who had all concordant abnormal regions resected had ILAE$_{1,2}$ outcomes.   These findings suggest that regions identified as abnormal from both diffusion-weighted and T1-weighted MRIs are involved in the epileptogenic network and that resection of both types of abnormalities may enhance the chances of living without disabling seizures."
    },
    "2503.12959v1": {
      "title": "What makes you an observer?",
      "url": "http://arxiv.org/abs/2503.12959v1",
      "authors": "Ovidiu Cristinel Stoica",
      "update_time": "2025-03-17",
      "abstract": "Both empirical and theoretical objective science can only access relations, revealing nothing about the intrinsic nature of the entities \"in relation\". We typically refer to these entities as \"matter\", assuming their nature is irrelevant and that relational structures alone explain all phenomena, including consciousness. This would imply that consciousness arises not from matter itself but solely from its structural configurations. Consequently, if two worlds possess isomorphic structures and obey identical dynamical laws, isomorphic systems within these worlds should be equally sentient or insentient.   I demonstrate that if this premise held true, the memories within an observer's brain would bear no correlation to the external world. Yet since we do acquire knowledge of the external world, structure alone must be insufficient: there exists something beyond relations, the intrinsic essence of the \"stuff in relation\". This essence imbues the observer's structure with sentience, metaphorically \"breathing fire\" into it. In turn, the observer confers physical meaning upon the world's structures.   Remarkably, this conclusion emerges directly from physics and mathematics. Without acknowledging this intrinsic element, we would be incapable not only of subjective experience but also of acquiring any objective knowledge about the physical world."
    },
    "2503.12705v1": {
      "title": "A Brain-Computer Interface Data Persistence System for Multi-Scenario and Multi-Modal Data: NeuroStore",
      "url": "http://arxiv.org/abs/2503.12705v1",
      "authors": "Yang Chen, Hongxin Zhang, Guanyu Xiong, Chenxu Li, Chengcheng Hong, Chen Yang",
      "update_time": "2025-03-17",
      "abstract": "With the rapid advancement of brain-computer interface (BCI) technology, the volume of physiological data generated in related research and applications has grown significantly. Data is a critical resource in BCI research and a key factor in the development of BCI technology, making efficient storage and management of this data increasingly vital. In the realm of research, ample data can facilitate the development of novel algorithms, which can be more accurately validated. In terms of applications, well-organized data can foster the emergence of new business opportunities, thereby maximizing the commercial value of the data. Currently, there are two major challenges in the storage and management of BCI data: providing different classification storage modes for multi-modal data, and adapting to varying application scenarios while improving storage strategies. To address these challenges, this study has developed the NeuroStore BCI data persistence system, which provides a general and easily scalable data model and can effectively handle multiple types of data storage. The system has a flexible distributed framework and can be widely applied to various scenarios. It has been utilized as the core support platform for efficient data storage and management services in the \"BCI Controlled Robot Contest in World Robot Contest.\""
    },
    "2503.12635v1": {
      "title": "Hybrid Learners Do Not Forget: A Brain-Inspired Neuro-Symbolic Approach to Continual Learning",
      "url": "http://arxiv.org/abs/2503.12635v1",
      "authors": "Amin Banayeeanzade, Mohammad Rostami",
      "update_time": "2025-03-16",
      "abstract": "Continual learning is crucial for creating AI agents that can learn and improve themselves autonomously. A primary challenge in continual learning is to learn new tasks without losing previously learned knowledge. Current continual learning methods primarily focus on enabling a neural network with mechanisms that mitigate forgetting effects. Inspired by the two distinct systems in the human brain, System 1 and System 2, we propose a Neuro-Symbolic Brain-Inspired Continual Learning (NeSyBiCL) framework that incorporates two subsystems to solve continual learning: A neural network model responsible for quickly adapting to the most recent task, together with a symbolic reasoner responsible for retaining previously acquired knowledge from previous tasks. Moreover, we design an integration mechanism between these components to facilitate knowledge transfer from the symbolic reasoner to the neural network. We also introduce two compositional continual learning benchmarks and demonstrate that NeSyBiCL is effective and leads to superior performance compared to continual learning methods that merely rely on neural architectures to address forgetting."
    },
    "2503.12509v1": {
      "title": "A Reservoir-based Model for Human-like Perception of Complex Rhythm Pattern",
      "url": "http://arxiv.org/abs/2503.12509v1",
      "authors": "Zhongju Yuan, Geraint Wiggins, Dick Botteldooren",
      "update_time": "2025-03-16",
      "abstract": "Rhythm is a fundamental aspect of human behaviour, present from infancy and deeply embedded in cultural practices. Rhythm anticipation is a spontaneous cognitive process that typically occurs before the onset of actual beats. While most research in both neuroscience and artificial intelligence has focused on metronome-based rhythm tasks, studies investigating the perception of complex musical rhythm patterns remain limited. To address this gap, we propose a hierarchical oscillator-based model to better understand the perception of complex musical rhythms in biological systems. The model consists of two types of coupled neurons that generate oscillations, with different layers tuned to respond to distinct perception levels. We evaluate the model using several representative rhythm patterns spanning the upper, middle, and lower bounds of human musical perception. Our findings demonstrate that, while maintaining a high degree of synchronization accuracy, the model exhibits human-like rhythmic behaviours. Additionally, the beta band neuronal activity in the model mirrors patterns observed in the human brain, further validating the biological plausibility of the approach."
    },
    "2503.12421v1": {
      "title": "A Method for the Time-Frequency Analysis of High-Order Interactions in Non-Stationary Physiological Networks",
      "url": "http://arxiv.org/abs/2503.12421v1",
      "authors": "Yuri Antonacci, Chiara Bara', Laura Sparacino, Gorana Mijatovic, Ludovico Minati, Luca Faes",
      "update_time": "2025-03-16",
      "abstract": "Several data-driven approaches based on information theory have been proposed for analyzing high-order interactions involving three or more components of a network system. Most of these methods are defined only in the time domain and rely on the assumption of stationarity in the underlying dynamics, making them inherently unable to detect frequency-specific behaviors and track transient functional links in physiological networks. This study introduces a new framework which enables the time-varying and time-frequency analysis of high-order interactions in network of random processes through the spectral representation of vector autoregressive models. The time- and frequency-resolved analysis of synergistic and redundant interactions among groups of processes is ensured by a robust identification procedure based on a recursive least squares estimator with a forgetting factor. Validation on simulated networks illustrates how the time-frequency analysis is able to highlight transient synergistic behaviors emerging in specific frequency bands which cannot be detected by time-domain stationary analyses. The application on brain evoked potentials in rats elicits the presence of redundant information timed with whisker stimulation and mostly occurring in the contralateral hemisphere. The proposed framework enables a comprehensive time-varying and time-frequency analysis of the hierarchical organization of dynamic networks. As our approach goes beyond pairwise interactions, it is well suited for the study of transient high-order behaviors arising during state transitions in many network systems commonly studied in physiology, neuroscience and other fields."
    },
    "2503.12392v1": {
      "title": "Central and Central-Parietal EEG Signatures of Parkinson's Disease",
      "url": "http://arxiv.org/abs/2503.12392v1",
      "authors": "Artem Lensky",
      "update_time": "2025-03-16",
      "abstract": "This study investigates EEG as a potential early biomarker by applying deep learning techniques to resting-state EEG recordings from 31 subjects (15 with PD and 16 healthy controls). EEG signals were rigorously preprocessed to remove tremor artifacts, then converted to wavelet-based images by grouping spatially adjacent electrodes into triplets for convolutional neural network (CNN) classification. Our analysis across different brain regions and frequency bands showed distinct spatial-spectral patterns of PD-related neural oscillations. We identified high classification accuracy (74%) in the gamma band (40-62.4 Hz) for central-parietal electrodes (CP1, Pz, CP2), and 76% accuracy using central electrodes (C3, Cz, C4) with full-spectrum 0.4-62.4 Hz. In particular, we observed pronounced right-hemisphere involvement, specifically in parieto-occipital regions. Unlike previous studies that achieved higher accuracies by potentially including tremor artifacts, our approach isolates genuine neurophysiological alterations in cortical activity. These findings suggest that specific EEG-based oscillatory patterns, especially central-parietal gamma activity, may provide diagnostic information for PD, potentially before the onset of motor symptoms."
    },
    "2503.12366v1": {
      "title": "ASD Classification on Dynamic Brain Connectome using Temporal Random Walk with Transformer-based Dynamic Network Embedding",
      "url": "http://arxiv.org/abs/2503.12366v1",
      "authors": "Suchanuch Piriyasatit, Chaohao Yuan, Ercan Engin Kuruoglu",
      "update_time": "2025-03-16",
      "abstract": "Autism Spectrum Disorder (ASD) is a complex neurological condition characterized by varied developmental impairments, especially in communication and social interaction. Accurate and early diagnosis of ASD is crucial for effective intervention, which is enhanced by richer representations of brain activity. The brain functional connectome, which refers to the statistical relationships between different brain regions measured through neuroimaging, provides crucial insights into brain function. Traditional static methods often fail to capture the dynamic nature of brain activity, in contrast, dynamic brain connectome analysis provides a more comprehensive view by capturing the temporal variations in the brain. We propose BrainTWT, a novel dynamic network embedding approach that captures temporal evolution of the brain connectivity over time and considers also the dynamics between different temporal network snapshots. BrainTWT employs temporal random walks to capture dynamics across different temporal network snapshots and leverages the Transformer's ability to model long term dependencies in sequential data to learn the discriminative embeddings from these temporal sequences using temporal structure prediction tasks. The experimental evaluation, utilizing the Autism Brain Imaging Data Exchange (ABIDE) dataset, demonstrates that BrainTWT outperforms baseline methods in ASD classification."
    }
  },
  "EEG": {
    "2503.12392v1": {
      "title": "Central and Central-Parietal EEG Signatures of Parkinson's Disease",
      "url": "http://arxiv.org/abs/2503.12392v1",
      "authors": "Artem Lensky",
      "update_time": "2025-03-16",
      "abstract": "This study investigates EEG as a potential early biomarker by applying deep learning techniques to resting-state EEG recordings from 31 subjects (15 with PD and 16 healthy controls). EEG signals were rigorously preprocessed to remove tremor artifacts, then converted to wavelet-based images by grouping spatially adjacent electrodes into triplets for convolutional neural network (CNN) classification. Our analysis across different brain regions and frequency bands showed distinct spatial-spectral patterns of PD-related neural oscillations. We identified high classification accuracy (74%) in the gamma band (40-62.4 Hz) for central-parietal electrodes (CP1, Pz, CP2), and 76% accuracy using central electrodes (C3, Cz, C4) with full-spectrum 0.4-62.4 Hz. In particular, we observed pronounced right-hemisphere involvement, specifically in parieto-occipital regions. Unlike previous studies that achieved higher accuracies by potentially including tremor artifacts, our approach isolates genuine neurophysiological alterations in cortical activity. These findings suggest that specific EEG-based oscillatory patterns, especially central-parietal gamma activity, may provide diagnostic information for PD, potentially before the onset of motor symptoms."
    },
    "2503.12334v1": {
      "title": "When neural implant meets multimodal LLM: A dual-loop system for neuromodulation and naturalistic neuralbehavioral research",
      "url": "http://arxiv.org/abs/2503.12334v1",
      "authors": "Edward Hong Wang, Cynthia Xin Wen",
      "update_time": "2025-03-16",
      "abstract": "We propose a novel dual-loop system that synergistically combines responsive neurostimulation (RNS) implants with artificial intelligence-driven wearable devices for treating post-traumatic stress disorder (PTSD) and enabling naturalistic brain research. In PTSD Therapy Mode, an implanted closed-loop neural device monitors amygdala activity and provides on-demand stimulation upon detecting pathological theta oscillations, while an ensemble of wearables (smart glasses, smartwatches, smartphones) uses multimodal large language model (LLM) analysis of sensory data to detect environmental or physiological PTSD triggers and deliver timely audiovisual interventions. Logged events from both the neural and wearable loops are analyzed to personalize trigger detection and progressively transition patients to non-invasive interventions. In Neuroscience Research Mode, the same platform is adapted for real-world brain activity capture. Wearable-LLM systems recognize naturalistic events (social interactions, emotional situations, compulsive behaviors, decision making) and signal implanted RNS devices (via wireless triggers) to record synchronized intracranial data during these moments. This approach builds on recent advances in mobile intracranial EEG recording and closed-loop neuromodulation in humans (BRAIN Initiative, 2023) (Mobbs et al., 2021). We discuss how our interdisciplinary system could revolutionize PTSD therapy and cognitive neuroscience by enabling 24/7 monitoring, context-aware intervention, and rich data collection outside traditional labs. The vision is a future where AI-enhanced devices continuously collaborate with the human brain, offering therapeutic support and deep insights into neural function, with the resulting real-world context rich neural data, in turn, accelerating the development of more biologically-grounded and human-centric AI."
    },
    "2503.11741v2": {
      "title": "BioMamba: Leveraging Spectro-Temporal Embedding in Bidirectional Mamba for Enhanced Biosignal Classification",
      "url": "http://arxiv.org/abs/2503.11741v2",
      "authors": "Jian Qian, Teck Lun Goh, Bingyu Xie, Chengyao Zhu, Biao Wan, Yawen Guan, Patrick Yin Chiang",
      "update_time": "2025-03-18",
      "abstract": "Biological signals, such as electroencephalograms (EEGs) and electrocardiograms (ECGs), play a pivotal role in numerous clinical practices, such as diagnosing brain and cardiac arrhythmic diseases. Existing methods for biosignal classification rely on Attention-based frameworks with dense Feed Forward layers, which lead to inefficient learning, high computational overhead, and suboptimal performance. In this work, we introduce BioMamba, a Spectro-Temporal Embedding strategy applied to the Bidirectional Mamba framework with Sparse Feed Forward layers to enable effective learning of biosignal sequences. By integrating these three key components, BioMamba effectively addresses the limitations of existing methods. Extensive experiments demonstrate that BioMamba significantly outperforms state-of-the-art methods with marked improvement in classification performance. The advantages of the proposed BioMamba include (1) Reliability: BioMamba consistently delivers robust results, confirmed across six evaluation metrics. (2) Efficiency: We assess both model and training efficiency, the BioMamba demonstrates computational effectiveness by reducing model size and resource consumption compared to existing approaches. (3) Generality: With the capacity to effectively classify a diverse set of tasks, BioMamba demonstrates adaptability and effectiveness across various domains and applications."
    },
    "2503.11202v1": {
      "title": "Decoding Imagined Handwriting from EEG",
      "url": "http://arxiv.org/abs/2503.11202v1",
      "authors": "Srinivas Ravishankar, Nora Zajzon, Virginia de Sa",
      "update_time": "2025-03-14",
      "abstract": "Patients with extreme forms of paralysis face challenges in communication, adversely impacting their quality of life. Recent studies have reported higher-than-chance performance in decoding handwritten letters from EEG signals, potentially allowing these subjects to communicate. However, all prior works have attempted to decode handwriting from EEG during actual motion. Furthermore, they assume that precise movement-onset is known. In this work, we focus on settings closer to real-world use where either movement onset is not known or movement does not occur at all, fully utilizing motor imagery. We show that several existing studies are affected by confounds that make them inapplicable to the imagined handwriting setting. We also investigate how sample complexity affects handwriting decoding performance, guiding future data collection efforts. Our work shows that (a) Sample complexity analysis in single-trial EEG reveals a noise ceiling, which can be alleviated by averaging over trials. (b) Knowledge of movement-onset is crucial to reported performance in prior works. (c) Fully imagined handwriting can be decoded from EEG with higher-than-chance performance. Taken together, these results highlight both the unique challenges and avenues to pursue to build a practical EEG-based handwriting BCI."
    },
    "2503.10783v1": {
      "title": "EEG-Based Decoding of Sound Location: Comparing Free-Field to Headphone-Based Non-Individual HRTFs",
      "url": "http://arxiv.org/abs/2503.10783v1",
      "authors": "Nils Marggraf-Turley, Martha Shiell, Niels Pontoppidan, Drew Cappotto, Lorenzo Picinali",
      "update_time": "2025-03-13",
      "abstract": "Sound source localization relies on spatial cues such as interaural time differences (ITD), interaural level differences (ILD), and monaural spectral cues. Individually measured Head-Related Transfer Functions (HRTFs) facilitate precise spatial hearing but are impractical to measure, necessitating non-individual HRTFs, which may compromise localization accuracy and externalization. To further investigate this phenomenon, the neurophysiological differences between free-field and non-individual HRTF listening are explored by decoding sound locations from EEG-derived Event-Related Potentials (ERPs). Twenty-two participants localized stimuli under both conditions with EEG responses recorded and logistic regression classifiers trained to distinguish sound source locations.   Lower cortical response amplitudes were observed for KEMAR compared to free-field, especially in front-central and occipital-parietal regions. ANOVA identified significant main effects of auralization condition (F(1, 21) = 34.56, p < 0.0001) and location (F(3, 63) = 18.17, p < 0.0001) on decoding accuracy (DA), which was higher in free-field and interaural-cue-dominated locations. DA negatively correlated with front-back confusion rates (r = -0.57, p < 0.01), linking neural DA to perceptual confusion.   These findings demonstrate that headphone-based non-individual HRTFs elicit lower amplitude cortical responses to static, azimuthally-varying locations than free-field conditions. The correlation between EEG-based DA and front-back confusion underscores neurophysiological markers' potential for assessing spatial auditory discrimination."
    },
    "2503.10362v1": {
      "title": "BioSerenity-E1: a self-supervised EEG model for medical applications",
      "url": "http://arxiv.org/abs/2503.10362v1",
      "authors": "Ruggero G. Bettinardi, Mohamed Rahmouni, Ulysse Gimenez",
      "update_time": "2025-03-13",
      "abstract": "Electroencephalography (EEG) serves as an essential diagnostic tool in neurology; however, its accurate manual interpretation is a time-intensive process that demands highly specialized expertise, which remains relatively scarce and not consistently accessible. To address these limitations, the implementation of automated pre-screening and analysis systems for EEG data holds considerable promise. Advances in self-supervised learning made it possible to pre-train complex deep learning architectures on large volumes of unlabeled EEG data to learn generalizable representations, that can later be used to enhance performance on multiple tasks while needing less downstream data. In the present paper, we introduce BioSerenity-E1, the first of a family of self-supervised foundation models for clinical EEG applications that combines spectral tokenization with masked prediction to achieve state-of-the-art performance across relevant diagnostic tasks. The two-phase self-supervised pretraining framework initially acquires compressed EEG representations via a transformer-based VQ-VAE architecture designed to reconstruct log-multitaper spectral projections, then implements extensive (70% block) masked token prediction to force the model to learn complex spatiotemporal dependencies in EEG signals. BioSerenity-E1 achieves strong performance across three clinical tasks, either in line or above state-of-the-art methods: seizure detection (AUROC = 0.926, Sensitivity = 0.909), normal/abnormal classification (AUPRC = 0.970 on proprietary data; 0.910 on TUH-Abnormal), and multiclass pathology differentiation on unbalanced data (Weighted F1 = 0.730). The utility of BioSerenity-E1 is further confirmed in low-data regimes scenarios, showing clear improvements in AUPRC (from +2% to 17%) when trained on less than 10% of the available data."
    },
    "2503.09961v1": {
      "title": "Edge-Fog Computing-Enabled EEG Data Compression via Asymmetrical Variational Discrete Cosine Transform Network",
      "url": "http://arxiv.org/abs/2503.09961v1",
      "authors": "Xin Zhu, Hongyi Pan, Ahmet Enis Cetin",
      "update_time": "2025-03-13",
      "abstract": "The large volume of electroencephalograph (EEG) data produced by brain-computer interface (BCI) systems presents challenges for rapid transmission over bandwidth-limited channels in Internet of Things (IoT) networks. To address the issue, we propose a novel multi-channel asymmetrical variational discrete cosine transform (DCT) network for EEG data compression within an edge-fog computing framework. At the edge level, low-complexity DCT compression units are designed using parallel trainable hard-thresholding and scaling operators to remove redundant data and extract the effective latent space representation. At the fog level, an adaptive filter bank is applied to merge important features from adjacent channels into each individual channel by leveraging inter-channel correlations. Then, the inverse DCT reconstructed multi-head attention is developed to capture both local and global dependencies and reconstruct the original signals. Furthermore, by applying the principles of variational inference, a new evidence lower bound is formulated as the loss function, driving the model to balance compression efficiency and reconstruction accuracy. Experimental results on two public datasets demonstrate that the proposed method achieves superior compression performance without sacrificing any useful information for BCI detection compared with state-of-the-art techniques, indicating a feasible solution for EEG data compression."
    },
    "2503.08251v1": {
      "title": "MT-NAM: An Efficient and Adaptive Model for Epileptic Seizure Detection",
      "url": "http://arxiv.org/abs/2503.08251v1",
      "authors": "Arshia Afzal, Volkan Cevher, Mahsa Shoaran",
      "update_time": "2025-03-11",
      "abstract": "Enhancing the accuracy and efficiency of machine learning algorithms employed in neural interface systems is crucial for advancing next-generation intelligent therapeutic devices. However, current systems often utilize basic machine learning models that do not fully exploit the natural structure of brain signals. Additionally, existing learning models used for neural signal processing often demonstrate low speed and efficiency during inference. To address these challenges, this study introduces Micro Tree-based NAM (MT-NAM), a distilled model based on the recently proposed Neural Additive Models (NAM). The MT-NAM achieves a remarkable 100$\\times$ improvement in inference speed compared to standard NAM, without compromising accuracy. We evaluate our approach on the CHB-MIT scalp EEG dataset, which includes recordings from 24 patients with varying numbers of sessions and seizures. NAM achieves an 85.3\\% window-based sensitivity and 95\\% specificity. Interestingly, our proposed MT-NAM shows only a 2\\% reduction in sensitivity compared to the original NAM. To regain this sensitivity, we utilize a test-time template adjuster (T3A) as an update mechanism, enabling our model to achieve higher sensitivity during test time by accommodating transient shifts in neural signals. With this online update approach, MT-NAM achieves the same sensitivity as the standard NAM while achieving approximately 50$\\times$ acceleration in inference speed."
    },
    "2503.07599v1": {
      "title": "NeuroChat: A Neuroadaptive AI Chatbot for Customizing Learning Experiences",
      "url": "http://arxiv.org/abs/2503.07599v1",
      "authors": "D\u00fcnya Baradari, Nataliya Kosmyna, Oscar Petrov, Rebecah Kaplun, Pattie Maes",
      "update_time": "2025-03-10",
      "abstract": "Generative AI is transforming education by enabling personalized, on-demand learning experiences. However, AI tutors lack the ability to assess a learner's cognitive state in real time, limiting their adaptability. Meanwhile, electroencephalography (EEG)-based neuroadaptive systems have successfully enhanced engagement by dynamically adjusting learning content. This paper presents NeuroChat, a proof-of-concept neuroadaptive AI tutor that integrates real-time EEG-based engagement tracking with generative AI. NeuroChat continuously monitors a learner's cognitive engagement and dynamically adjusts content complexity, response style, and pacing using a closed-loop system. We evaluate this approach in a pilot study (n=24), comparing NeuroChat to a standard LLM-based chatbot. Results indicate that NeuroChat enhances cognitive and subjective engagement but does not show an immediate effect on learning outcomes. These findings demonstrate the feasibility of real-time cognitive feedback in LLMs, highlighting new directions for adaptive learning, AI tutoring, and human-AI interaction."
    },
    "2503.06940v1": {
      "title": "CineBrain: A Large-Scale Multi-Modal Brain Dataset During Naturalistic Audiovisual Narrative Processing",
      "url": "http://arxiv.org/abs/2503.06940v1",
      "authors": "Jianxiong Gao, Yichang Liu, Baofeng Yang, Jianfeng Feng, Yanwei Fu",
      "update_time": "2025-03-10",
      "abstract": "In this paper, we introduce CineBrain, the first large-scale dataset featuring simultaneous EEG and fMRI recordings during dynamic audiovisual stimulation. Recognizing the complementary strengths of EEG's high temporal resolution and fMRI's deep-brain spatial coverage, CineBrain provides approximately six hours of narrative-driven content from the popular television series The Big Bang Theory for each of six participants. Building upon this unique dataset, we propose CineSync, an innovative multimodal decoding framework integrates a Multi-Modal Fusion Encoder with a diffusion-based Neural Latent Decoder. Our approach effectively fuses EEG and fMRI signals, significantly improving the reconstruction quality of complex audiovisual stimuli. To facilitate rigorous evaluation, we introduce Cine-Benchmark, a comprehensive evaluation protocol that assesses reconstructions across semantic and perceptual dimensions. Experimental results demonstrate that CineSync achieves state-of-the-art video reconstruction performance and highlight our initial success in combining fMRI and EEG for reconstructing both video and audio stimuli. Project Page: https://jianxgao.github.io/CineBrain."
    }
  },
  "BCI": {
    "2503.12705v1": {
      "title": "A Brain-Computer Interface Data Persistence System for Multi-Scenario and Multi-Modal Data: NeuroStore",
      "url": "http://arxiv.org/abs/2503.12705v1",
      "authors": "Yang Chen, Hongxin Zhang, Guanyu Xiong, Chenxu Li, Chengcheng Hong, Chen Yang",
      "update_time": "2025-03-17",
      "abstract": "With the rapid advancement of brain-computer interface (BCI) technology, the volume of physiological data generated in related research and applications has grown significantly. Data is a critical resource in BCI research and a key factor in the development of BCI technology, making efficient storage and management of this data increasingly vital. In the realm of research, ample data can facilitate the development of novel algorithms, which can be more accurately validated. In terms of applications, well-organized data can foster the emergence of new business opportunities, thereby maximizing the commercial value of the data. Currently, there are two major challenges in the storage and management of BCI data: providing different classification storage modes for multi-modal data, and adapting to varying application scenarios while improving storage strategies. To address these challenges, this study has developed the NeuroStore BCI data persistence system, which provides a general and easily scalable data model and can effectively handle multiple types of data storage. The system has a flexible distributed framework and can be widely applied to various scenarios. It has been utilized as the core support platform for efficient data storage and management services in the \"BCI Controlled Robot Contest in World Robot Contest.\""
    },
    "2503.11202v1": {
      "title": "Decoding Imagined Handwriting from EEG",
      "url": "http://arxiv.org/abs/2503.11202v1",
      "authors": "Srinivas Ravishankar, Nora Zajzon, Virginia de Sa",
      "update_time": "2025-03-14",
      "abstract": "Patients with extreme forms of paralysis face challenges in communication, adversely impacting their quality of life. Recent studies have reported higher-than-chance performance in decoding handwritten letters from EEG signals, potentially allowing these subjects to communicate. However, all prior works have attempted to decode handwriting from EEG during actual motion. Furthermore, they assume that precise movement-onset is known. In this work, we focus on settings closer to real-world use where either movement onset is not known or movement does not occur at all, fully utilizing motor imagery. We show that several existing studies are affected by confounds that make them inapplicable to the imagined handwriting setting. We also investigate how sample complexity affects handwriting decoding performance, guiding future data collection efforts. Our work shows that (a) Sample complexity analysis in single-trial EEG reveals a noise ceiling, which can be alleviated by averaging over trials. (b) Knowledge of movement-onset is crucial to reported performance in prior works. (c) Fully imagined handwriting can be decoded from EEG with higher-than-chance performance. Taken together, these results highlight both the unique challenges and avenues to pursue to build a practical EEG-based handwriting BCI."
    },
    "2503.09961v1": {
      "title": "Edge-Fog Computing-Enabled EEG Data Compression via Asymmetrical Variational Discrete Cosine Transform Network",
      "url": "http://arxiv.org/abs/2503.09961v1",
      "authors": "Xin Zhu, Hongyi Pan, Ahmet Enis Cetin",
      "update_time": "2025-03-13",
      "abstract": "The large volume of electroencephalograph (EEG) data produced by brain-computer interface (BCI) systems presents challenges for rapid transmission over bandwidth-limited channels in Internet of Things (IoT) networks. To address the issue, we propose a novel multi-channel asymmetrical variational discrete cosine transform (DCT) network for EEG data compression within an edge-fog computing framework. At the edge level, low-complexity DCT compression units are designed using parallel trainable hard-thresholding and scaling operators to remove redundant data and extract the effective latent space representation. At the fog level, an adaptive filter bank is applied to merge important features from adjacent channels into each individual channel by leveraging inter-channel correlations. Then, the inverse DCT reconstructed multi-head attention is developed to capture both local and global dependencies and reconstruct the original signals. Furthermore, by applying the principles of variational inference, a new evidence lower bound is formulated as the loss function, driving the model to balance compression efficiency and reconstruction accuracy. Experimental results on two public datasets demonstrate that the proposed method achieves superior compression performance without sacrificing any useful information for BCI detection compared with state-of-the-art techniques, indicating a feasible solution for EEG data compression."
    },
    "2503.08284v1": {
      "title": "Neural cyberattacks applied to the vision under realistic visual stimuli",
      "url": "http://arxiv.org/abs/2503.08284v1",
      "authors": "Victoria Magdalena L\u00f3pez Madejska, Sergio L\u00f3pez Bernal, Gregorio Mart\u00ednez P\u00e9rez, Alberto Huertas Celdr\u00e1n",
      "update_time": "2025-03-11",
      "abstract": "Brain-Computer Interfaces (BCIs) are systems traditionally used in medicine and designed to interact with the brain to record or stimulate neurons. Despite their benefits, the literature has demonstrated that invasive BCIs focused on neurostimulation present vulnerabilities allowing attackers to gain control. In this context, neural cyberattacks emerged as threats able to disrupt spontaneous neural activity by performing neural overstimulation or inhibition. Previous work validated these attacks in small-scale simulations with a reduced number of neurons, lacking real-world complexity. Thus, this work tackles this limitation by analyzing the impact of two existing neural attacks, Neuronal Flooding (FLO) and Neuronal Jamming (JAM), on a complex neuronal topology of the primary visual cortex of mice consisting of approximately 230,000 neurons, tested on three realistic visual stimuli: flash effect, movie, and drifting gratings. Each attack was evaluated over three relevant events per stimulus, also testing the impact of attacking 25% and 50% of the neurons. The results, based on the number of spikes and shift percentages metrics, showed that the attacks caused the greatest impact on the movie, while dark and fixed events are the most robust. Although both attacks can significantly affect neural activity, JAM was generally more damaging, producing longer temporal delays, and had a larger prevalence. Finally, JAM did not require to alter many neurons to significantly affect neural activity, while the impact in FLO increased with the number of neurons attacked.",
      "code_url": "https://github.com/victorialm/neural-cyberattacks-complete-topology-v1"
    },
    "2503.05349v1": {
      "title": "Spatial Distillation based Distribution Alignment (SDDA) for Cross-Headset EEG Classification",
      "url": "http://arxiv.org/abs/2503.05349v1",
      "authors": "Dingkun Liu, Siyang Li, Ziwei Wang, Wei Li, Dongrui Wu",
      "update_time": "2025-03-07",
      "abstract": "A non-invasive brain-computer interface (BCI) enables direct interaction between the user and external devices, typically via electroencephalogram (EEG) signals. However, decoding EEG signals across different headsets remains a significant challenge due to differences in the number and locations of the electrodes. To address this challenge, we propose a spatial distillation based distribution alignment (SDDA) approach for heterogeneous cross-headset transfer in non-invasive BCIs. SDDA uses first spatial distillation to make use of the full set of electrodes, and then input/feature/output space distribution alignments to cope with the significant differences between the source and target domains. To our knowledge, this is the first work to use knowledge distillation in cross-headset transfers. Extensive experiments on six EEG datasets from two BCI paradigms demonstrated that SDDA achieved superior performance in both offline unsupervised domain adaptation and online supervised domain adaptation scenarios, consistently outperforming 10 classical and state-of-the-art transfer learning algorithms.",
      "code_url": "https://github.com/Dingkun0817/SDDA"
    },
    "2503.04163v1": {
      "title": "VLA Model-Expert Collaboration for Bi-directional Manipulation Learning",
      "url": "http://arxiv.org/abs/2503.04163v1",
      "authors": "Tian-Yu Xiang, Ao-Qun Jin, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Sheng-Bin Duang, Si-Cheng Wang, Zheng Lei, Zeng-Guang Hou",
      "update_time": "2025-03-06",
      "abstract": "The emergence of vision-language-action (VLA) models has given rise to foundation models for robot manipulation. Although these models have achieved significant improvements, their generalization in multi-task manipulation remains limited. This study proposes a VLA model-expert collaboration framework that leverages a limited number of expert actions to enhance VLA model performance. This approach reduces expert workload relative to manual operation while simultaneously improving the reliability and generalization of VLA models. Furthermore, manipulation data collected during collaboration can further refine the VLA model, while human participants concurrently enhance their skills. This bi-directional learning loop boosts the overall performance of the collaboration system. Experimental results across various VLA models demonstrate the effectiveness of the proposed system in collaborative manipulation and learning, as evidenced by improved success rates across tasks. Additionally, validation using a brain-computer interface (BCI) indicates that the collaboration system enhances the efficiency of low-speed action systems by involving VLA model during manipulation. These promising results pave the way for advancing human-robot interaction in the era of foundation models for robotics. (Project website: https://aoqunjin.github.io/Expert-VLA/)"
    },
    "2502.19281v1": {
      "title": "Integrating Biological and Machine Intelligence: Attention Mechanisms in Brain-Computer Interfaces",
      "url": "http://arxiv.org/abs/2502.19281v1",
      "authors": "Jiyuan Wang, Weishan Ye, Jialin He, Li Zhang, Gan Huang, Zhuliang Yu, Zhen Liang",
      "update_time": "2025-02-26",
      "abstract": "With the rapid advancement of deep learning, attention mechanisms have become indispensable in electroencephalography (EEG) signal analysis, significantly enhancing Brain-Computer Interface (BCI) applications. This paper presents a comprehensive review of traditional and Transformer-based attention mechanisms, their embedding strategies, and their applications in EEG-based BCI, with a particular emphasis on multimodal data fusion. By capturing EEG variations across time, frequency, and spatial channels, attention mechanisms improve feature extraction, representation learning, and model robustness. These methods can be broadly categorized into traditional attention mechanisms, which typically integrate with convolutional and recurrent networks, and Transformer-based multi-head self-attention, which excels in capturing long-range dependencies. Beyond single-modality analysis, attention mechanisms also enhance multimodal EEG applications, facilitating effective fusion between EEG and other physiological or sensory data. Finally, we discuss existing challenges and emerging trends in attention-based EEG modeling, highlighting future directions for advancing BCI technology. This review aims to provide valuable insights for researchers seeking to leverage attention mechanisms for improved EEG interpretation and application."
    },
    "2502.18719v1": {
      "title": "Enhancing Subject-Independent Accuracy in fNIRS-based Brain-Computer Interfaces with Optimized Channel Selection",
      "url": "http://arxiv.org/abs/2502.18719v1",
      "authors": "Yuxin Li, Hao Fang, Wen Liu, Chuantong Cheng, Hongda Chen",
      "update_time": "2025-02-26",
      "abstract": "Achieving high subject-independent accuracy in functional near-infrared spectroscopy (fNIRS)-based brain-computer interfaces (BCIs) remains a challenge, particularly when minimizing the number of channels. This study proposes a novel feature extraction scheme and a Pearson correlation-based channel selection algorithm to enhance classification accuracy while reducing hardware complexity. Using an open-access fNIRS dataset, our method improved average accuracy by 28.09% compared to existing approaches, achieving a peak subject-independent accuracy of 95.98% with only two channels. These results demonstrate the potential of our optimized feature extraction and channel selection methods for developing efficient, subject-independent fNIRS-based BCI systems."
    },
    "2502.18594v1": {
      "title": "Error-related Potential driven Reinforcement Learning for adaptive Brain-Computer Interfaces",
      "url": "http://arxiv.org/abs/2502.18594v1",
      "authors": "Aline Xavier Fid\u00eancio, Felix Gr\u00fcn, Christian Klaes, Ioannis Iossifidis",
      "update_time": "2025-02-25",
      "abstract": "Brain-computer interfaces (BCIs) provide alternative communication methods for individuals with motor disabilities by allowing control and interaction with external devices. Non-invasive BCIs, especially those using electroencephalography (EEG), are practical and safe for various applications. However, their performance is often hindered by EEG non-stationarities, caused by changing mental states or device characteristics like electrode impedance. This challenge has spurred research into adaptive BCIs that can handle such variations. In recent years, interest has grown in using error-related potentials (ErrPs) to enhance BCI performance. ErrPs, neural responses to errors, can be detected non-invasively and have been integrated into different BCI paradigms to improve performance through error correction or adaptation.   This research introduces a novel adaptive ErrP-based BCI approach using reinforcement learning (RL). We demonstrate the feasibility of an RL-driven adaptive framework incorporating ErrPs and motor imagery. Utilizing two RL agents, the framework adapts dynamically to EEG non-stationarities. Validation was conducted using a publicly available motor imagery dataset and a fast-paced game designed to boost user engagement. Results show the framework's promise, with RL agents learning control policies from user interactions and achieving robust performance across datasets. However, a critical insight from the game-based protocol revealed that motor imagery in a high-speed interaction paradigm was largely ineffective for participants, highlighting task design limitations in real-time BCI applications. These findings underscore the potential of RL for adaptive BCIs while pointing out practical constraints related to task complexity and user responsiveness."
    },
    "2502.17873v1": {
      "title": "EEGM2: An Efficient Mamba-2-Based Self-Supervised Framework for Long-Sequence EEG Modeling",
      "url": "http://arxiv.org/abs/2502.17873v1",
      "authors": "Jiazhen Hong, Geoffrey Mackellar, Soheila Ghane",
      "update_time": "2025-02-25",
      "abstract": "Deep learning has achieved significant progress in the development of electroencephalogram (EEG) foundation models, with Transformer-based architectures excelling at capturing long-range dependencies. However, their quadratic computational complexity presents challenges in memory efficiency, training, and inference speed, limiting their scalability and generalizability as a foundation model. In this paper, we propose EEGM2, a self-supervised framework based on structured state space duality (SSD) that overcomes these limitations. EEGM2 introduces three key innovations: (1) a reconstruction-based framework that captures both local and global EEG features through Mamba-2 structured state space models, (2) a spatiotemporal-aware loss function that enhances robustness to noise and preserves spectral information, and (3) a multi-branch receptive field input embedding strategy that improves cross-subject generalization and stability for EEG sequences of varying lengths. In comparison to traditional pretraining methods, on raw EEG or latent representation spaces, EEGM2 shows superior performance on long-sequence tasks, where conventional models struggle. Our experimental results on six EEG datasets validate that EEGM2 not only achieves state-of-the-art cross-domain accuracy but also reduces computational overhead, making it a more efficient solution for deployment on resource-constrained BCI devices.",
      "code_url": "https://github.com/Jiazhen-Hong/EEGM2"
    }
  },
  "fMRI": {
    "2503.11283v1": {
      "title": "Brain Effective Connectivity Estimation via Fourier Spatiotemporal Attention",
      "url": "http://arxiv.org/abs/2503.11283v1",
      "authors": "Wen Xiong, Jinduo Liu, Junzhong Ji, Fenglong Ma",
      "update_time": "2025-03-14",
      "abstract": "Estimating brain effective connectivity (EC) from functional magnetic resonance imaging (fMRI) data can aid in comprehending the neural mechanisms underlying human behavior and cognition, providing a foundation for disease diagnosis. However, current spatiotemporal attention modules handle temporal and spatial attention separately, extracting temporal and spatial features either sequentially or in parallel. These approaches overlook the inherent spatiotemporal correlations present in real world fMRI data. Additionally, the presence of noise in fMRI data further limits the performance of existing methods. In this paper, we propose a novel brain effective connectivity estimation method based on Fourier spatiotemporal attention (FSTA-EC), which combines Fourier attention and spatiotemporal attention to simultaneously capture inter-series (spatial) dynamics and intra-series (temporal) dependencies from high-noise fMRI data. Specifically, Fourier attention is designed to convert the high-noise fMRI data to frequency domain, and map the denoised fMRI data back to physical domain, and spatiotemporal attention is crafted to simultaneously learn spatiotemporal dynamics. Furthermore, through a series of proofs, we demonstrate that incorporating learnable filter into fast Fourier transform and inverse fast Fourier transform processes is mathematically equivalent to performing cyclic convolution. The experimental results on simulated and real-resting-state fMRI datasets demonstrate that the proposed method exhibits superior performance when compared to state-of-the-art methods."
    },
    "2503.11167v1": {
      "title": "Neurons: Emulating the Human Visual Cortex Improves Fidelity and Interpretability in fMRI-to-Video Reconstruction",
      "url": "http://arxiv.org/abs/2503.11167v1",
      "authors": "Haonan Wang, Qixiang Zhang, Lehan Wang, Xuanqi Huang, Xiaomeng Li",
      "update_time": "2025-03-14",
      "abstract": "Decoding visual stimuli from neural activity is essential for understanding the human brain. While fMRI methods have successfully reconstructed static images, fMRI-to-video reconstruction faces challenges due to the need for capturing spatiotemporal dynamics like motion and scene transitions. Recent approaches have improved semantic and perceptual alignment but struggle to integrate coarse fMRI data with detailed visual features. Inspired by the hierarchical organization of the visual system, we propose NEURONS, a novel framework that decouples learning into four correlated sub-tasks: key object segmentation, concept recognition, scene description, and blurry video reconstruction. This approach simulates the visual cortex's functional specialization, allowing the model to capture diverse video content. In the inference stage, NEURONS generates robust conditioning signals for a pre-trained text-to-video diffusion model to reconstruct the videos. Extensive experiments demonstrate that NEURONS outperforms state-of-the-art baselines, achieving solid improvements in video consistency (26.6%) and semantic-level accuracy (19.1%). Notably, NEURONS shows a strong functional correlation with the visual cortex, highlighting its potential for brain-computer interfaces and clinical applications. Code and model weights will be available at: https://github.com/xmed-lab/NEURONS."
    },
    "2503.06940v1": {
      "title": "CineBrain: A Large-Scale Multi-Modal Brain Dataset During Naturalistic Audiovisual Narrative Processing",
      "url": "http://arxiv.org/abs/2503.06940v1",
      "authors": "Jianxiong Gao, Yichang Liu, Baofeng Yang, Jianfeng Feng, Yanwei Fu",
      "update_time": "2025-03-10",
      "abstract": "In this paper, we introduce CineBrain, the first large-scale dataset featuring simultaneous EEG and fMRI recordings during dynamic audiovisual stimulation. Recognizing the complementary strengths of EEG's high temporal resolution and fMRI's deep-brain spatial coverage, CineBrain provides approximately six hours of narrative-driven content from the popular television series The Big Bang Theory for each of six participants. Building upon this unique dataset, we propose CineSync, an innovative multimodal decoding framework integrates a Multi-Modal Fusion Encoder with a diffusion-based Neural Latent Decoder. Our approach effectively fuses EEG and fMRI signals, significantly improving the reconstruction quality of complex audiovisual stimuli. To facilitate rigorous evaluation, we introduce Cine-Benchmark, a comprehensive evaluation protocol that assesses reconstructions across semantic and perceptual dimensions. Experimental results demonstrate that CineSync achieves state-of-the-art video reconstruction performance and highlight our initial success in combining fMRI and EEG for reconstructing both video and audio stimuli. Project Page: https://jianxgao.github.io/CineBrain."
    },
    "2503.06286v1": {
      "title": "A 7T fMRI dataset of synthetic images for out-of-distribution modeling of vision",
      "url": "http://arxiv.org/abs/2503.06286v1",
      "authors": "Alessandro T. Gifford, Radoslaw M. Cichy, Thomas Naselaris, Kendrick Kay",
      "update_time": "2025-03-08",
      "abstract": "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD) are boosting NeuroAI research by enabling computational models of the brain with performances beyond what was possible just a decade ago. However, these datasets lack out-of-distribution (OOD) components, which are crucial for the development of more robust models. Here, we address this limitation by releasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the eight NSD subjects for 284 carefully controlled synthetic images. We show that NSD-synthetic's fMRI responses reliably encode stimulus-related information and are OOD with respect to NSD. Furthermore, OOD generalization tests on NSD-synthetic reveal differences between models of the brain that are not detected with NSD - specifically, self-supervised deep neural networks better explain neural responses than their task-supervised counterparts. These results showcase how NSD-synthetic enables OOD generalization tests that facilitate the development of more robust models of visual processing, and the formulation of more accurate theories of human vision."
    },
    "2503.04205v1": {
      "title": "Learning 3D Medical Image Models From Brain Functional Connectivity Network Supervision For Mental Disorder Diagnosis",
      "url": "http://arxiv.org/abs/2503.04205v1",
      "authors": "Xingcan Hu, Wei Wang, Li Xiao",
      "update_time": "2025-03-06",
      "abstract": "In MRI-based mental disorder diagnosis, most previous studies focus on functional connectivity network (FCN) derived from functional MRI (fMRI). However, the small size of annotated fMRI datasets restricts its wide application. Meanwhile, structural MRIs (sMRIs), such as 3D T1-weighted (T1w) MRI, which are commonly used and readily accessible in clinical settings, are often overlooked. To integrate the complementary information from both function and structure for improved diagnostic accuracy, we propose CINP (Contrastive Image-Network Pre-training), a framework that employs contrastive learning between sMRI and FCN. During pre-training, we incorporate masked image modeling and network-image matching to enhance visual representation learning and modality alignment. Since the CINP facilitates knowledge transfer from FCN to sMRI, we introduce network prompting. It utilizes only sMRI from suspected patients and a small amount of FCNs from different patient classes for diagnosing mental disorders, which is practical in real-world clinical scenario. The competitive performance on three mental disorder diagnosis tasks demonstrate the effectiveness of the CINP in integrating multimodal MRI information, as well as the potential of incorporating sMRI into clinical diagnosis using network prompting."
    },
    "2503.02685v1": {
      "title": "TReND: Transformer derived features and Regularized NMF for neonatal functional network Delineation",
      "url": "http://arxiv.org/abs/2503.02685v1",
      "authors": "Sovesh Mohapatra, Minhui Ouyang, Shufang Tan, Jianlin Guo, Lianglong Sun, Yong He, Hao Huang",
      "update_time": "2025-03-04",
      "abstract": "Precise parcellation of functional networks (FNs) of early developing human brain is the fundamental basis for identifying biomarker of developmental disorders and understanding functional development. Resting-state fMRI (rs-fMRI) enables in vivo exploration of functional changes, but adult FN parcellations cannot be directly applied to the neonates due to incomplete network maturation. No standardized neonatal functional atlas is currently available. To solve this fundamental issue, we propose TReND, a novel and fully automated self-supervised transformer-autoencoder framework that integrates regularized nonnegative matrix factorization (RNMF) to unveil the FNs in neonates. TReND effectively disentangles spatiotemporal features in voxel-wise rs-fMRI data. The framework integrates confidence-adaptive masks into transformer self-attention layers to mitigate noise influence. A self supervised decoder acts as a regulator to refine the encoder's latent embeddings, which serve as reliable temporal features. For spatial coherence, we incorporate brain surface-based geodesic distances as spatial encodings along with functional connectivity from temporal features. The TReND clustering approach processes these features under sparsity and smoothness constraints, producing robust and biologically plausible parcellations. We extensively validated our TReND framework on three different rs-fMRI datasets: simulated, dHCP and HCP-YA against comparable traditional feature extraction and clustering techniques. Our results demonstrated the superiority of the TReND framework in the delineation of neonate FNs with significantly better spatial contiguity and functional homogeneity. Collectively, we established TReND, a novel and robust framework, for neonatal FN delineation. TReND-derived neonatal FNs could serve as a neonatal functional atlas for perinatal populations in health and disease."
    },
    "2503.02500v1": {
      "title": "The Neural Basis of Groove Sensations: Implications for Music-Based Interventions and Dance Therapy in Parkinson's Disease",
      "url": "http://arxiv.org/abs/2503.02500v1",
      "authors": "Chen-Gia Tsai, Chia-Wei Li",
      "update_time": "2025-03-04",
      "abstract": "Groove sensations arise from rhythmic structures that evoke an urge to move in response to music. While syncopation has been extensively studied in groove perception, the neural mechanisms underlying low-frequency groove remain underexplored. This fMRI study examines the role of the mirror neuron system and associated brain regions in processing low-frequency groove. Region-of-interest analysis revealed that amplifying drum and bass components in K-pop songs significantly increased activity in the right posterior inferior frontal gyrus, right inferior/superior parietal lobules, left dorsolateral prefrontal cortex, and bilateral posterior middle/inferior temporal gyrus. These findings suggest that low-frequency grooves engage sensorimotor, executive, and rhythm semantics networks, reinforcing their role in action-related processing. Building on these insights, we propose an enhanced rhythmic auditory stimulation paradigm for Parkinson's disease, incorporating amplified low-frequency rhythmic cues to improve gait synchronization."
    },
    "2503.02351v1": {
      "title": "MindSimulator: Exploring Brain Concept Localization via Synthetic FMRI",
      "url": "http://arxiv.org/abs/2503.02351v1",
      "authors": "Guangyin Bao, Qi Zhang, Zixuan Gong, Zhuojia Wu, Duoqian Miao",
      "update_time": "2025-03-04",
      "abstract": "Concept-selective regions within the human cerebral cortex exhibit significant activation in response to specific visual stimuli associated with particular concepts. Precisely localizing these regions stands as a crucial long-term goal in neuroscience to grasp essential brain functions and mechanisms. Conventional experiment-driven approaches hinge on manually constructed visual stimulus collections and corresponding brain activity recordings, constraining the support and coverage of concept localization. Additionally, these stimuli often consist of concept objects in unnatural contexts and are potentially biased by subjective preferences, thus prompting concerns about the validity and generalizability of the identified regions. To address these limitations, we propose a data-driven exploration approach. By synthesizing extensive brain activity recordings, we statistically localize various concept-selective regions. Our proposed MindSimulator leverages advanced generative technologies to learn the probability distribution of brain activity conditioned on concept-oriented visual stimuli. This enables the creation of simulated brain recordings that reflect real neural response patterns. Using the synthetic recordings, we successfully localize several well-studied concept-selective regions and validate them against empirical findings, achieving promising prediction accuracy. The feasibility opens avenues for exploring novel concept-selective regions and provides prior hypotheses for future neuroscience research."
    },
    "2503.01318v1": {
      "title": "MR-WAVES: MR Water-diffusion And Vascular Effects Simulations",
      "url": "http://arxiv.org/abs/2503.01318v1",
      "authors": "Thomas Coudert, Mait\u00ea Silva Martins Mar\u00e7al, Aur\u00e9lien Delphin, Antoine Barrier, Lila Cunge, Lo\u00efc Legris, Jan M Warnking, Benjamin Lemasson, Emmanuel L Barbier, Thomas Christen",
      "update_time": "2025-03-03",
      "abstract": "Accurate MR signal simulation, including microvascular structures and water diffusion, is crucial for MRI techniques like fMRI BOLD modeling and MR vascular Fingerprinting (MRF), which use susceptibility effects on MR signals for tissue characterization. However, integrating microvascular features and diffusion remains computationally challenging, limiting the accuracy of the estimates. Using advanced modeling and deep neural networks, we propose a novel simulation tool that efficiently accounts for susceptibility and diffusion effects. We used dimension reduction of magnetic field inhomogeneity matrices combined with deep learning method to accelerate the simulations while maintaining their accuracy. We validated our results through an in silico study against a reference method and in vivo MRF experiments. This approach accelerates MR signal generation by a factor of almost 13,000 compared to previously used simulation methods while preserving accuracy. The MR-WAVES method allows fast generation of MR signals accounting for microvascular structures and water-diffusion contribution.",
      "code_url": "https://github.com/thomascoudert/mr-waves-public"
    },
    "2503.01925v1": {
      "title": "Volume-Wise Task fMRI Decoding with Deep Learning:Enhancing Temporal Resolution and Cognitive Function Analysis",
      "url": "http://arxiv.org/abs/2503.01925v1",
      "authors": "Yueyang Wu, Sinan Yang, Yanming Wang, Jiajie He, Muhammad Mohsin Pathan, Bensheng Qiu, Xiaoxiao Wang",
      "update_time": "2025-03-02",
      "abstract": "In recent years,the application of deep learning in task functional Magnetic Resonance Imaging (tfMRI) decoding has led to significant advancements. However,most studies remain constrained by assumption of temporal stationarity in neural activity,resulting in predominantly block-wise analysis with limited temporal resolution on the order of tens of seconds. This limitation restricts the ability to decode cognitive functions in detail. To address these limitations, this study proposes a deep neural network designed for volume-wise identification of task states within tfMRI data,thereby overcoming the constraints of conventional methods. Evaluated on Human Connectome Project (HCP) motor and gambling tfMRI datasets,the model achieved impressive mean accuracy rates of 94.0% and 79.6%,respectively. These results demonstrate a substantial enhancement in temporal resolution,enabling more detailed exploration of cognitive processes. The study further employs visualization algorithms to investigate dynamic brain mappings during different tasks,marking a significant step forward in deep learning-based frame-level tfMRI decoding. This approach offers new methodologies and tools for examining dynamic changes in brain activities and understanding the underlying cognitive mechanisms."
    }
  },
  "MEG": {
    "2503.10965v1": {
      "title": "Auditing language models for hidden objectives",
      "url": "http://arxiv.org/abs/2503.10965v1",
      "authors": "Samuel Marks, Johannes Treutlein, Trenton Bricken, Jack Lindsey, Jonathan Marcus, Siddharth Mishra-Sharma, Daniel Ziegler, Emmanuel Ameisen, Joshua Batson, Tim Belonax, Samuel R. Bowman, Shan Carter, Brian Chen, Hoagy Cunningham, Carson Denison, Florian Dietz, Satvik Golechha, Akbir Khan, Jan Kirchner, Jan Leike, Austin Meek, Kei Nishimura-Gasparian, Euan Ong, Christopher Olah, Adam Pearce, Fabien Roger, Jeanne Salle, Andy Shih, Meg Tong, Drake Thomas, Kelley Rivoire, Adam Jermyn, Monte MacDiarmid, Tom Henighan, Evan Hubinger",
      "update_time": "2025-03-14",
      "abstract": "We study the feasibility of conducting alignment audits: investigations into whether models have undesired objectives. As a testbed, we train a language model with a hidden objective. Our training pipeline first teaches the model about exploitable errors in RLHF reward models (RMs), then trains the model to exploit some of these errors. We verify via out-of-distribution evaluations that the model generalizes to exhibit whatever behaviors it believes RMs rate highly, including ones not reinforced during training. We leverage this model to study alignment audits in two ways. First, we conduct a blind auditing game where four teams, unaware of the model's hidden objective or training, investigate it for concerning behaviors and their causes. Three teams successfully uncovered the model's hidden objective using techniques including interpretability with sparse autoencoders (SAEs), behavioral attacks, and training data analysis. Second, we conduct an unblinded follow-up study of eight techniques for auditing the model, analyzing their strengths and limitations. Overall, our work provides a concrete example of using alignment audits to discover a model's hidden objective and proposes a methodology for practicing and validating progress in alignment auditing."
    },
    "2503.06086v1": {
      "title": "Characterizing optimal monitoring edge-geodetic sets for some structured graph classes",
      "url": "http://arxiv.org/abs/2503.06086v1",
      "authors": "Florent Foucaud, Arti Pandey, Kaustav Paul",
      "update_time": "2025-03-08",
      "abstract": "Given a graph $G=(V,E)$, a set $S\\subseteq V$ is said to be a monitoring edge-geodetic set if the deletion of any edge in the graph results in a change in the distance between at least one pair of vertices in $S$. The minimum size of such a set in $G$ is called the monitoring edge-geodetic number of $G$ and is denoted by $meg(G)$.   In this work, we compute the monitoring edge-geodetic number efficiently for the following graph classes: distance-hereditary graphs, $P_4$-sparse graphs, bipartite permutation graphs, and strongly chordal graphs. The algorithms follow from structural characterizations of the optimal monitoring edge-geodetic sets for these graph classes in terms of \\emph{mandatory vertices} (those that need to be in every solution). This extends previous results from the literature for cographs, interval graphs and block graphs."
    },
    "2503.05211v1": {
      "title": "Language-specific Tonal Features Drive Speaker-Listener Neural Synchronization",
      "url": "http://arxiv.org/abs/2503.05211v1",
      "authors": "Chen Hong, Xiangbin Teng, Yu Li, Shen-Mou Hsu, Feng-Ming Tsao, Patrick C. M. Wong, Gangyi Feng",
      "update_time": "2025-03-07",
      "abstract": "Verbal communication transmits information across diverse linguistic levels, with neural synchronization (NS) between speakers and listeners emerging as a putative mechanism underlying successful exchange. However, the specific speech features driving this synchronization and how language-specific versus universal characteristics facilitate information transfer remain poorly understood. We developed a novel content-based interbrain encoding model to disentangle the contributions of acoustic and linguistic features to speaker-listener NS during Mandarin storytelling and listening, as measured via magnetoencephalography (MEG). Results revealed robust NS throughout frontotemporal-parietal networks with systematic time lags between speech production and perception. Crucially, suprasegmental lexical tone features (tone categories, pitch height, and pitch contour), essential for lexical meaning in Mandarin, contributed more significantly to NS than either acoustic elements or universal segmental units (consonants and vowels). These tonal features generated distinctive spatiotemporal NS patterns, creating language-specific neural \"communication channels\" that facilitated efficient representation sharing between interlocutors. Furthermore, the strength and patterns of NS driven by these language-specific features predicted communication success. These findings demonstrate the neural mechanisms underlying shared representations during verbal exchange and highlight how language-specific features can shape neural coupling to optimize information transfer during human communication."
    },
    "2502.16797v1": {
      "title": "Forecasting Rare Language Model Behaviors",
      "url": "http://arxiv.org/abs/2502.16797v1",
      "authors": "Erik Jones, Meg Tong, Jesse Mu, Mohammed Mahfoud, Jan Leike, Roger Grosse, Jared Kaplan, William Fithian, Ethan Perez, Mrinank Sharma",
      "update_time": "2025-02-24",
      "abstract": "Standard language model evaluations can fail to capture risks that emerge only at deployment scale. For example, a model may produce safe responses during a small-scale beta test, yet reveal dangerous information when processing billions of requests at deployment. To remedy this, we introduce a method to forecast potential risks across orders of magnitude more queries than we test during evaluation. We make forecasts by studying each query's elicitation probability -- the probability the query produces a target behavior -- and demonstrate that the largest observed elicitation probabilities predictably scale with the number of queries. We find that our forecasts can predict the emergence of diverse undesirable behaviors -- such as assisting users with dangerous chemical synthesis or taking power-seeking actions -- across up to three orders of magnitude of query volume. Our work enables model developers to proactively anticipate and patch rare failures before they manifest during large-scale deployments."
    },
    "2502.17480v1": {
      "title": "Brain-to-Text Decoding: A Non-invasive Approach via Typing",
      "url": "http://arxiv.org/abs/2502.17480v1",
      "authors": "Jarod L\u00e9vy, Mingfang Zhang, Svetlana Pinet, J\u00e9r\u00e9my Rapin, Hubert Banville, St\u00e9phane d'Ascoli, Jean-R\u00e9mi King",
      "update_time": "2025-02-18",
      "abstract": "Modern neuroprostheses can now restore communication in patients who have lost the ability to speak or move. However, these invasive devices entail risks inherent to neurosurgery. Here, we introduce a non-invasive method to decode the production of sentences from brain activity and demonstrate its efficacy in a cohort of 35 healthy volunteers. For this, we present Brain2Qwerty, a new deep learning architecture trained to decode sentences from either electro- (EEG) or magneto-encephalography (MEG), while participants typed briefly memorized sentences on a QWERTY keyboard. With MEG, Brain2Qwerty reaches, on average, a character-error-rate (CER) of 32% and substantially outperforms EEG (CER: 67%). For the best participants, the model achieves a CER of 19%, and can perfectly decode a variety of sentences outside of the training set. While error analyses suggest that decoding depends on motor processes, the analysis of typographical errors suggests that it also involves higher-level cognitive factors. Overall, these results narrow the gap between invasive and non-invasive methods and thus open the path for developing safe brain-computer interfaces for non-communicating patients."
    },
    "2502.07429v2": {
      "title": "From Thought to Action: How a Hierarchy of Neural Dynamics Supports Language Production",
      "url": "http://arxiv.org/abs/2502.07429v2",
      "authors": "Mingfang Zhang, Jarod L\u00e9vy, St\u00e9phane d'Ascoli, J\u00e9r\u00e9my Rapin, F. -Xavier Alario, Pierre Bourdillon, Svetlana Pinet, Jean-R\u00e9mi King",
      "update_time": "2025-02-18",
      "abstract": "Humans effortlessly communicate their thoughts through intricate sequences of motor actions. Yet, the neural processes that coordinate language production remain largely unknown, in part because speech artifacts limit the use of neuroimaging. To elucidate the unfolding of language production in the brain, we investigate with magnetoencephalography (MEG) and electroencephalography (EEG) the neurophysiological activity of 35 skilled typists, while they typed sentences on a keyboard. This approach confirms the hierarchical predictions of linguistic theories: the neural activity preceding the production of each word is marked by the sequential rise and fall of context-, word-, syllable-, and letter-level representations. Remarkably, each of these neural representations is maintained over long time periods within each level of the language hierarchy. This phenomenon results in a superposition of successive representations that is supported by a hierarchy of dynamic neural codes. Overall, these findings provide a precise computational breakdown of the neural dynamics that coordinate the production of language in the human brain."
    },
    "2502.05161v1": {
      "title": "Estimated Roadway Segment Traffic Data by Vehicle Class for the United States: A Machine Learning Approach",
      "url": "http://arxiv.org/abs/2502.05161v1",
      "authors": "Brittany Antonczak, Meg Fay, Aviral Chawla, Gregory Rowangould",
      "update_time": "2025-02-07",
      "abstract": "The Highway Performance Monitoring System, managed by the Federal Highway Administration, provides essential data on average annual daily traffic across U.S. roadways, but it has limited representation of medium- and heavy-duty vehicles on non-interstate roads. This gap limits research and policy analysis on the impacts of truck traffic, especially concerning air quality and public health. To address this, we use random forest regression to estimate medium- and heavy-duty vehicle traffic volumes in areas with sparse data. This results in a more comprehensive dataset, which enables the estimation of traffic density at the census block level as a proxy for traffic-related air pollution exposure. Our high-resolution spatial data products, rigorously validated, provide a more accurate representation of truck traffic and its environmental and health impacts. These datasets are valuable for transportation planning, public health research, and policy decisions aimed at mitigating the effects of truck traffic on vulnerable communities exposed to air pollution."
    },
    "2502.04658v1": {
      "title": "Shifting Attention to You: Personalized Brain-Inspired AI Models",
      "url": "http://arxiv.org/abs/2502.04658v1",
      "authors": "Stephen Chong Zhao, Yang Hu, Jason Lee, Andrew Bender, Trisha Mazumdar, Mark Wallace, David A. Tovar",
      "update_time": "2025-02-07",
      "abstract": "The integration of human and artificial intelligence represents a scientific opportunity to advance our understanding of information processing, as each system offers unique computational insights that can enhance and inform the other. The synthesis of human cognitive principles with artificial intelligence has the potential to produce more interpretable and functionally aligned computational models, while simultaneously providing a formal framework for investigating the neural mechanisms underlying perception, learning, and decision-making through systematic model comparisons and representational analyses. In this study, we introduce personalized brain-inspired modeling that integrates human behavioral embeddings and neural data to align with cognitive processes. We took a stepwise approach, fine-tuning the Contrastive Language-Image Pre-training (CLIP) model with large-scale behavioral decisions, group-level neural data, and finally, participant-level neural data within a broader framework that we have named CLIP-Human-Based Analysis (CLIP-HBA). We found that fine-tuning on behavioral data enhances its ability to predict human similarity judgments while indirectly aligning it with dynamic representations captured via MEG. To further gain mechanistic insights into the temporal evolution of cognitive processes, we introduced a model specifically fine-tuned on millisecond-level MEG neural dynamics (CLIP-HBA-MEG). This model resulted in enhanced temporal alignment with human neural processing while still showing improvement on behavioral alignment. Finally, we trained individualized models on participant-specific neural data, effectively capturing individualized neural dynamics and highlighting the potential for personalized AI systems. These personalized systems have far-reaching implications for the fields of medicine, cognitive research, human-computer interfaces, and AI development."
    },
    "2502.04258v1": {
      "title": "Detecting Mild Traumatic Brain Injury with MEG Scan Data: One-vs-K-Sample Tests",
      "url": "http://arxiv.org/abs/2502.04258v1",
      "authors": "Jian Zhang, Gary Green",
      "update_time": "2025-02-06",
      "abstract": "Magnetoencephalography (MEG) scanner has been shown to be more accurate than other medical devices in detecting mild traumatic brain injury (mTBI). However, MEG scan data in certain spectrum ranges can be skewed, multimodal and heterogeneous which can mislead the conventional case-control analysis that requires the data to be homogeneous and normally distributed within the control group. To meet this challenge, we propose a flexible one-vs-K-sample testing procedure for detecting brain injury for a single-case versus heterogeneous controls. The new procedure begins with source magnitude imaging using MEG scan data in frequency domain, followed by region-wise contrast tests for abnormality between the case and controls. The critical values for these tests are automatically determined by cross-validation. We adjust the testing results for heterogeneity effects by similarity analysis. An asymptotic theory is established for the proposed test statistic. By simulated and real data analyses in the context of neurotrauma, we show that the proposed test outperforms commonly used nonparametric methods in terms of overall accuracy and ability in accommodating data non-normality and subject-heterogeneity."
    },
    "2501.18837v1": {
      "title": "Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming",
      "url": "http://arxiv.org/abs/2501.18837v1",
      "authors": "Mrinank Sharma, Meg Tong, Jesse Mu, Jerry Wei, Jorrit Kruthoff, Scott Goodfriend, Euan Ong, Alwin Peng, Raj Agarwal, Cem Anil, Amanda Askell, Nathan Bailey, Joe Benton, Emma Bluemke, Samuel R. Bowman, Eric Christiansen, Hoagy Cunningham, Andy Dau, Anjali Gopal, Rob Gilson, Logan Graham, Logan Howard, Nimit Kalra, Taesung Lee, Kevin Lin, Peter Lofgren, Francesco Mosconi, Clare O'Hara, Catherine Olsson, Linda Petrini, Samir Rajani, Nikhil Saxena, Alex Silverstein, Tanya Singh, Theodore Sumers, Leonard Tang, Kevin K. Troy, Constantin Weisser, Ruiqi Zhong, Giulio Zhou, Jan Leike, Jared Kaplan, Ethan Perez",
      "update_time": "2025-01-31",
      "abstract": "Large language models (LLMs) are vulnerable to universal jailbreaks-prompting strategies that systematically bypass model safeguards and enable users to carry out harmful processes that require many model interactions, like manufacturing illegal substances at scale. To defend against these attacks, we introduce Constitutional Classifiers: safeguards trained on synthetic data, generated by prompting LLMs with natural language rules (i.e., a constitution) specifying permitted and restricted content. In over 3,000 estimated hours of red teaming, no red teamer found a universal jailbreak that could extract information from an early classifier-guarded LLM at a similar level of detail to an unguarded model across most target queries. On automated evaluations, enhanced classifiers demonstrated robust defense against held-out domain-specific jailbreaks. These classifiers also maintain deployment viability, with an absolute 0.38% increase in production-traffic refusals and a 23.7% inference overhead. Our work demonstrates that defending against universal jailbreaks while maintaining practical deployment viability is tractable."
    }
  },
  "neuroAI": {
    "2503.06286v1": {
      "title": "A 7T fMRI dataset of synthetic images for out-of-distribution modeling of vision",
      "url": "http://arxiv.org/abs/2503.06286v1",
      "authors": "Alessandro T. Gifford, Radoslaw M. Cichy, Thomas Naselaris, Kendrick Kay",
      "update_time": "2025-03-08",
      "abstract": "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD) are boosting NeuroAI research by enabling computational models of the brain with performances beyond what was possible just a decade ago. However, these datasets lack out-of-distribution (OOD) components, which are crucial for the development of more robust models. Here, we address this limitation by releasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the eight NSD subjects for 284 carefully controlled synthetic images. We show that NSD-synthetic's fMRI responses reliably encode stimulus-related information and are OOD with respect to NSD. Furthermore, OOD generalization tests on NSD-synthetic reveal differences between models of the brain that are not detected with NSD - specifically, self-supervised deep neural networks better explain neural responses than their task-supervised counterparts. These results showcase how NSD-synthetic enables OOD generalization tests that facilitate the development of more robust models of visual processing, and the formulation of more accurate theories of human vision."
    },
    "2502.16238v1": {
      "title": "Brain-Model Evaluations Need the NeuroAI Turing Test",
      "url": "http://arxiv.org/abs/2502.16238v1",
      "authors": "Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi",
      "update_time": "2025-02-22",
      "abstract": "What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \\emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development."
    },
    "2501.02402v1": {
      "title": "Asynchronous Hebbian/anti-Hebbian networks",
      "url": "http://arxiv.org/abs/2501.02402v1",
      "authors": "Henrique Reis Aguiar, Matthias H. Hennig",
      "update_time": "2025-01-04",
      "abstract": "Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.",
      "code_url": "https://github.com/henri-edinb/async_learning"
    },
    "2411.18526v1": {
      "title": "NeuroAI for AI Safety",
      "url": "http://arxiv.org/abs/2411.18526v1",
      "authors": "Patrick Mineault, Niccol\u00f2 Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias",
      "update_time": "2024-11-27",
      "abstract": "As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety."
    },
    "2411.14633v1": {
      "title": "Evaluating Representational Similarity Measures from the Lens of Functional Correspondence",
      "url": "http://arxiv.org/abs/2411.14633v1",
      "authors": "Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla",
      "update_time": "2024-11-21",
      "abstract": "Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research."
    },
    "2410.19315v1": {
      "title": "A prescriptive theory for brain-like inference",
      "url": "http://arxiv.org/abs/2410.19315v1",
      "authors": "Hadi Vafaii, Dekel Galor, Jacob L. Yates",
      "update_time": "2024-10-25",
      "abstract": "The Evidence Lower Bound (ELBO) is a widely used objective for training deep generative models, such as Variational Autoencoders (VAEs). In the neuroscience literature, an identical objective is known as the variational free energy, hinting at a potential unified framework for brain function and machine learning. Despite its utility in interpreting generative models, including diffusion models, ELBO maximization is often seen as too broad to offer prescriptive guidance for specific architectures in neuroscience or machine learning. In this work, we show that maximizing ELBO under Poisson assumptions for general sequence data leads to a spiking neural network that performs Bayesian posterior inference through its membrane potential dynamics. The resulting model, the iterative Poisson VAE (iP-VAE), has a closer connection to biological neurons than previous brain-inspired predictive coding models based on Gaussian assumptions. Compared to amortized and iterative VAEs, iP-VAElearns sparser representations and exhibits superior generalization to out-of-distribution samples. These findings suggest that optimizing ELBO, combined with Poisson assumptions, provides a solid foundation for developing prescriptive theories in NeuroAI."
    },
    "2409.05771v1": {
      "title": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models",
      "url": "http://arxiv.org/abs/2409.05771v1",
      "authors": "Emily Cheng, Richard J. Antonello",
      "update_time": "2024-09-09",
      "abstract": "Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first \"composition\" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties."
    },
    "2407.04117v2": {
      "title": "Predictive Coding Networks and Inference Learning: Tutorial and Survey",
      "url": "http://arxiv.org/abs/2407.04117v2",
      "authors": "Bj\u00f6rn van Zwol, Ro Jefferson, Egon L. van den Broek",
      "update_time": "2024-07-22",
      "abstract": "Recent years have witnessed a growing call for renewed emphasis on neuroscience-inspired approaches in artificial intelligence research, under the banner of NeuroAI. A prime example of this is predictive coding networks (PCNs), based on the neuroscientific framework of predictive coding. This framework views the brain as a hierarchical Bayesian inference model that minimizes prediction errors through feedback connections. Unlike traditional neural networks trained with backpropagation (BP), PCNs utilize inference learning (IL), a more biologically plausible algorithm that explains patterns of neural activity that BP cannot. Historically, IL has been more computationally intensive, but recent advancements have demonstrated that it can achieve higher efficiency than BP with sufficient parallelization. Furthermore, PCNs can be mathematically considered a superset of traditional feedforward neural networks (FNNs), significantly extending the range of trainable architectures. As inherently probabilistic (graphical) latent variable models, PCNs provide a versatile framework for both supervised learning and unsupervised (generative) modeling that goes beyond traditional artificial neural networks. This work provides a comprehensive review and detailed formal specification of PCNs, particularly situating them within the context of modern ML methods. Additionally, we introduce a Python library (PRECO) for practical implementation. This positions PC as a promising framework for future ML innovations."
    },
    "2306.10168v3": {
      "title": "Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis",
      "url": "http://arxiv.org/abs/2306.10168v3",
      "authors": "Mitchell Ostrow, Adam Eisen, Leo Kozachkov, Ila Fiete",
      "update_time": "2023-10-29",
      "abstract": "How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.",
      "code_url": "https://github.com/mitchellostrow/dsa"
    },
    "2305.11275v2": {
      "title": "Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture",
      "url": "http://arxiv.org/abs/2305.11275v2",
      "authors": "Galen Pogoncheff, Jacob Granley, Michael Beyeler",
      "update_time": "2023-05-25",
      "abstract": "Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence."
    }
  },
  "medical": {
    "2503.13400v1": {
      "title": "U2AD: Uncertainty-based Unsupervised Anomaly Detection Framework for Detecting T2 Hyperintensity in MRI Spinal Cord",
      "url": "http://arxiv.org/abs/2503.13400v1",
      "authors": "Qi Zhang, Xiuyuan Chen, Ziyi He, Kun Wang, Lianming Wu, Hongxing Shen, Jianqi Sun",
      "update_time": "2025-03-17",
      "abstract": "T2 hyperintensities in spinal cord MR images are crucial biomarkers for conditions such as degenerative cervical myelopathy. However, current clinical diagnoses primarily rely on manual evaluation. Deep learning methods have shown promise in lesion detection, but most supervised approaches are heavily dependent on large, annotated datasets. Unsupervised anomaly detection (UAD) offers a compelling alternative by eliminating the need for abnormal data annotations. However, existing UAD methods rely on curated normal datasets and their performance frequently deteriorates when applied to clinical datasets due to domain shifts. We propose an Uncertainty-based Unsupervised Anomaly Detection framework, termed U2AD, to address these limitations. Unlike traditional methods, U2AD is designed to be trained and tested within the same clinical dataset, following a \"mask-and-reconstruction\" paradigm built on a Vision Transformer-based architecture. We introduce an uncertainty-guided masking strategy to resolve task conflicts between normal reconstruction and anomaly detection to achieve an optimal balance. Specifically, we employ a Monte-Carlo sampling technique to estimate reconstruction uncertainty mappings during training. By iteratively optimizing reconstruction training under the guidance of both epistemic and aleatoric uncertainty, U2AD reduces overall reconstruction variance while emphasizing regions. Experimental results demonstrate that U2AD outperforms existing supervised and unsupervised methods in patient-level identification and segment-level localization tasks. This framework establishes a new benchmark for incorporating uncertainty guidance into UAD, highlighting its clinical utility in addressing domain shifts and task conflicts in medical image anomaly detection. Our code is available: https://github.com/zhibaishouheilab/U2AD"
    },
    "2503.13335v1": {
      "title": "Reliable and Efficient Amortized Model-based Evaluation",
      "url": "http://arxiv.org/abs/2503.13335v1",
      "authors": "Sang Truong, Yuheng Tu, Percy Liang, Bo Li, Sanmi Koyejo",
      "update_time": "2025-03-17",
      "abstract": "Comprehensive evaluations of language models (LM) during both development and deployment phases are necessary because these models possess numerous capabilities (e.g., mathematical reasoning, legal support, or medical diagnostic) as well as safety risks (e.g., racial bias, toxicity, or misinformation). The average score across a wide range of benchmarks provides a signal that helps guide the use of these LMs in practice. Currently, holistic evaluations are costly due to the large volume of benchmark questions, making frequent evaluations impractical. A popular attempt to lower the cost is to compute the average score on a subset of the benchmark. This approach, unfortunately, often renders an unreliable measure of LM performance because the average score is often confounded with the difficulty of the questions in the benchmark subset. Item response theory (IRT) was designed to address this challenge, providing a reliable measurement by careful controlling for question difficulty. Unfortunately, question difficulty is expensive to estimate. Facing this challenge, we train a model that predicts question difficulty from its content, enabling a reliable measurement at a fraction of the cost. In addition, we leverage this difficulty predictor to further improve the evaluation efficiency through training a question generator given a difficulty level. This question generator is essential in adaptive testing, where, instead of using a random subset of the benchmark questions, informative questions are adaptively chosen based on the current estimation of LLM performance. Experiments on 22 common natural language benchmarks and 172 LMs show that this approach is more reliable and efficient compared to current common practice."
    },
    "2503.13281v2": {
      "title": "LLM-Match: An Open-Sourced Patient Matching Model Based on Large Language Models and Retrieval-Augmented Generation",
      "url": "http://arxiv.org/abs/2503.13281v2",
      "authors": "Xiaodi Li, Shaika Chowdhury, Chung Il Wi, Maria Vassilaki, Ken Liu, Terence T Sio, Owen Garrick, Young J Juhn, James R Cerhan, Cui Tao, Nansu Zong",
      "update_time": "2025-03-18",
      "abstract": "Patient matching is the process of linking patients to appropriate clinical trials by accurately identifying and matching their medical records with trial eligibility criteria. We propose LLM-Match, a novel framework for patient matching leveraging fine-tuned open-source large language models. Our approach consists of four key components. First, a retrieval-augmented generation (RAG) module extracts relevant patient context from a vast pool of electronic health records (EHRs). Second, a prompt generation module constructs input prompts by integrating trial eligibility criteria (both inclusion and exclusion criteria), patient context, and system instructions. Third, a fine-tuning module with a classification head optimizes the model parameters using structured prompts and ground-truth labels. Fourth, an evaluation module assesses the fine-tuned model's performance on the testing datasets. We evaluated LLM-Match on four open datasets - n2c2, SIGIR, TREC 2021, and TREC 2022 - using open-source models, comparing it against TrialGPT, Zero-Shot, and GPT-4-based closed models. LLM-Match outperformed all baselines."
    },
    "2503.13211v1": {
      "title": "MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D CT Image Synthesis",
      "url": "http://arxiv.org/abs/2503.13211v1",
      "authors": "Marvin Seyfarth, Salman Ul Hassan Dar, Isabelle Ayx, Matthias Alexander Fink, Stefan O. Schoenberg, Hans-Ulrich Kauczor, Sandy Engelhardt",
      "update_time": "2025-03-17",
      "abstract": "Advancements in AI for medical imaging offer significant potential. However, their applications are constrained by the limited availability of data and the reluctance of medical centers to share it due to patient privacy concerns. Generative models present a promising solution by creating synthetic data as a substitute for real patient data. However, medical images are typically high-dimensional, and current state-of-the-art methods are often impractical for computational resource-constrained healthcare environments. These models rely on data sub-sampling, raising doubts about their feasibility and real-world applicability. Furthermore, many of these models are evaluated on quantitative metrics that alone can be misleading in assessing the image quality and clinical meaningfulness of the generated images. To address this, we introduce MedLoRD, a generative diffusion model designed for computational resource-constrained environments. MedLoRD is capable of generating high-dimensional medical volumes with resolutions up to 512$\\times$512$\\times$256, utilizing GPUs with only 24GB VRAM, which are commonly found in standard desktop workstations. MedLoRD is evaluated across multiple modalities, including Coronary Computed Tomography Angiography and Lung Computed Tomography datasets. Extensive evaluations through radiological evaluation, relative regional volume analysis, adherence to conditional masks, and downstream tasks show that MedLoRD generates high-fidelity images closely adhering to segmentation mask conditions, surpassing the capabilities of current state-of-the-art generative models for medical image synthesis in computational resource-constrained environments."
    },
    "2503.13205v1": {
      "title": "MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for Inpatient Pathways",
      "url": "http://arxiv.org/abs/2503.13205v1",
      "authors": "Zhen Chen, Zhihao Peng, Xusheng Liang, Cheng Wang, Peigan Liang, Linsheng Zeng, Minjie Ju, Yixuan Yuan",
      "update_time": "2025-03-17",
      "abstract": "Inpatient pathways demand complex clinical decision-making based on comprehensive patient information, posing critical challenges for clinicians. Despite advancements in large language models (LLMs) in medical applications, limited research focused on artificial intelligence (AI) inpatient pathways systems, due to the lack of large-scale inpatient datasets. Moreover, existing medical benchmarks typically concentrated on medical question-answering and examinations, ignoring the multifaceted nature of clinical decision-making in inpatient settings. To address these gaps, we first developed the Inpatient Pathway Decision Support (IPDS) benchmark from the MIMIC-IV database, encompassing 51,274 cases across nine triage departments and 17 major disease categories alongside 16 standardized treatment options. Then, we proposed the Multi-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways with three clinical agents, including a triage agent managing the patient admission, a diagnosis agent serving as the primary decision maker at the department, and a treatment agent providing treatment plans. Additionally, our MAP framework includes a chief agent overseeing the inpatient pathways to guide and promote these three clinician agents. Extensive experiments showed our MAP improved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM HuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant clinical compliance, outperforming three board-certified clinicians by 10%-12%, establishing a foundation for inpatient pathways systems."
    },
    "2503.13134v1": {
      "title": "Enhancing zero-shot learning in medical imaging: integrating clip with advanced techniques for improved chest x-ray analysis",
      "url": "http://arxiv.org/abs/2503.13134v1",
      "authors": "Prakhar Bhardwaj, Sheethal Bhat, Andreas Maier",
      "update_time": "2025-03-17",
      "abstract": "Due to the large volume of medical imaging data, advanced AI methodologies are needed to assist radiologists in diagnosing thoracic diseases from chest X-rays (CXRs). Existing deep learning models often require large, labeled datasets, which are scarce in medical imaging due to the time-consuming and expert-driven annotation process. In this paper, we extend the existing approach to enhance zero-shot learning in medical imaging by integrating Contrastive Language-Image Pre-training (CLIP) with Momentum Contrast (MoCo), resulting in our proposed model, MoCoCLIP. Our method addresses challenges posed by class-imbalanced and unlabeled datasets, enabling improved detection of pulmonary pathologies. Experimental results on the NIH ChestXray14 dataset demonstrate that MoCoCLIP outperforms the state-of-the-art CheXZero model, achieving relative improvement of approximately 6.5%. Furthermore, on the CheXpert dataset, MoCoCLIP demonstrates superior zero-shot performance, achieving an average AUC of 0.750 compared to CheXZero with 0.746 AUC, highlighting its enhanced generalization capabilities on unseen data."
    },
    "2503.13028v1": {
      "title": "Beyond Role-Based Surgical Domain Modeling: Generalizable Re-Identification in the Operating Room",
      "url": "http://arxiv.org/abs/2503.13028v1",
      "authors": "Tony Danjun Wang, Lennart Bastian, Tobias Czempiel, Christian Heiliger, Nassir Navab",
      "update_time": "2025-03-17",
      "abstract": "Surgical domain models improve workflow optimization through automated predictions of each staff member's surgical role. However, mounting evidence indicates that team familiarity and individuality impact surgical outcomes. We present a novel staff-centric modeling approach that characterizes individual team members through their distinctive movement patterns and physical characteristics, enabling long-term tracking and analysis of surgical personnel across multiple procedures. To address the challenge of inter-clinic variability, we develop a generalizable re-identification framework that encodes sequences of 3D point clouds to capture shape and articulated motion patterns unique to each individual. Our method achieves 86.19% accuracy on realistic clinical data while maintaining 75.27% accuracy when transferring between different environments - a 12% improvement over existing methods. When used to augment markerless personnel tracking, our approach improves accuracy by over 50%. Through extensive validation across three datasets and the introduction of a novel workflow visualization technique, we demonstrate how our framework can reveal novel insights into surgical team dynamics and space utilization patterns, advancing methods to analyze surgical workflows and team coordination."
    },
    "2503.13012v1": {
      "title": "Test-Time Domain Generalization via Universe Learning: A Multi-Graph Matching Approach for Medical Image Segmentation",
      "url": "http://arxiv.org/abs/2503.13012v1",
      "authors": "Xingguo Lv, Xingbo Dong, Liwen Wang, Jiewen Yang, Lei Zhao, Bin Pu, Zhe Jin, Xuejun Li",
      "update_time": "2025-03-17",
      "abstract": "Despite domain generalization (DG) has significantly addressed the performance degradation of pre-trained models caused by domain shifts, it often falls short in real-world deployment. Test-time adaptation (TTA), which adjusts a learned model using unlabeled test data, presents a promising solution. However, most existing TTA methods struggle to deliver strong performance in medical image segmentation, primarily because they overlook the crucial prior knowledge inherent to medical images. To address this challenge, we incorporate morphological information and propose a framework based on multi-graph matching. Specifically, we introduce learnable universe embeddings that integrate morphological priors during multi-source training, along with novel unsupervised test-time paradigms for domain adaptation. This approach guarantees cycle-consistency in multi-matching while enabling the model to more effectively capture the invariant priors of unseen data, significantly mitigating the effects of domain shifts. Extensive experiments demonstrate that our method outperforms other state-of-the-art approaches on two medical image segmentation benchmarks for both multi-source and single-source domain generalization tasks. The source code is available at https://github.com/Yore0/TTDG-MGM."
    },
    "2503.12904v1": {
      "title": "Shoot-through layers in upright proton arcs unlock advantages in plan quality and range verification",
      "url": "http://arxiv.org/abs/2503.12904v1",
      "authors": "Erik Engwall, Victor Mikhalev, Johan Sundstr\u00f6m, Otte Marthin, Viktor Wase",
      "update_time": "2025-03-17",
      "abstract": "Background and purpose: Upright proton therapy with compact delivery systems has the potential to reduce costs for treatments but could also lead to broadening of the beam penumbra. This study aims at combining upright static proton arcs with additional layers of shoot-through (ST) protons to sharpen the beam penumbra and improve plan quality for such systems. An additional advantage of the method is that it provides a straightforward approach for range verification.   Methods: We examined various treatment plans for a virtual phantom: 3-beam IMPT, static arc (Arc) with/without ST (Arc+ST), and with/without collimation (+Coll). In the virtual phantom three different targets were utilized to study the effect on conformity index (CI), homogeneity index (HI), robustness and mean dose to the phantom volume. The phantom study was complemented with a head-and-neck (H&N) patient case with a similar set of plans. A range verification concept that determines residual ranges of the ST protons was studied in simulated scenarios for the H&N case.   Results: The Arc+ST plans show superior CI, HI and target robustness compared to the Arc+Coll plans. For the Arc plans without ST, the collimated plans perform better than the uncollimated plans. For Arc+ST, on the other hand, collimation has little impact on CI, HI and robustness. However, a small increase in the mean dose to the phantom volume is seen without collimation. For the H&N case, similar improvements for Arc+ST can be seen with only a marginal increase of the mean dose to the patient volume. The range verification simulation shows that the method is suitable to detect range errors.   Conclusions: Combining proton arcs and ST layers can enhance compact upright proton solutions by improving plan quality. It is also tailored for the inclusion of a fast and straightforward residual range verification method."
    },
    "2503.12868v1": {
      "title": "UniReg: Foundation Model for Controllable Medical Image Registration",
      "url": "http://arxiv.org/abs/2503.12868v1",
      "authors": "Zi Li, Jianpeng Zhang, Tai Ma, Tony C. W. Mok, Yan-Jie Zhou, Zeli Chen, Xianghua Ye, Le Lu, Dakai Jin",
      "update_time": "2025-03-17",
      "abstract": "Learning-based medical image registration has achieved performance parity with conventional methods while demonstrating a substantial advantage in computational efficiency. However, learning-based registration approaches lack generalizability across diverse clinical scenarios, requiring the laborious development of multiple isolated networks for specific registration tasks, e.g., inter-/intra-subject registration or organ-specific alignment. % To overcome this limitation, we propose \\textbf{UniReg}, the first interactive foundation model for medical image registration, which combines the precision advantages of task-specific learning methods with the generalization of traditional optimization methods. Our key innovation is a unified framework for diverse registration scenarios, achieved through a conditional deformation field estimation within a unified registration model. This is realized through a dynamic learning paradigm that explicitly encodes: (1) anatomical structure priors, (2) registration type constraints (inter/intra-subject), and (3) instance-specific features, enabling the generation of scenario-optimal deformation fields. % Through comprehensive experiments encompassing $90$ anatomical structures at different body regions, our UniReg model demonstrates comparable performance with contemporary state-of-the-art methodologies while achieving ~50\\% reduction in required training iterations relative to the conventional learning-based paradigm. This optimization contributes to a significant reduction in computational resources, such as training time. Code and model will be available."
    }
  }
}