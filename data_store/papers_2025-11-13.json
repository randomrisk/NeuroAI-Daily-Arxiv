{
  "Brain": {
    "astro-ph/0412590v2": {
      "title": "Precise measurement of CMB polarisation from Dome-C: the BRAIN and CLOVER experiments",
      "url": "http://arxiv.org/abs/astro-ph/0412590v2",
      "authors": "M. Piat, C. Rosset, the BRAIN, CLOVER Collaboration",
      "update_time": "2005-09-17",
      "abstract": "The characterisation of CMB polarisation is one of the next challenge in observationnal cosmology. This is especially true for the so-called B-modes that are at least 3 order of magnitude lower than CMB temperature fluctuations. A precise measurement of the angular power spectrum of these B-modes will give important constraints on inflation parameters. In this talk, I will describe two complementary experiments, BRAIN and CLOVER, dedicated to CMB polarisation measurement. These experiments are proposed to be installed in Dome-C, Antarctica, to take advantage of the extreme dryness of the atmosphere and to allow long integration time.",
      "code_url": null
    },
    "2011.11052v1": {
      "title": "Efficient embedding network for 3D brain tumor segmentation",
      "url": "http://arxiv.org/abs/2011.11052v1",
      "authors": "Hicham Messaoudi, Ahror Belaid, Mohamed Lamine Allaoui, Ahcene Zetout, Mohand Said Allili, Souhil Tliba, Douraied Ben Salem, Pierre-Henri Conze",
      "update_time": "2020-11-24",
      "abstract": "3D medical image processing with deep learning greatly suffers from a lack of data. Thus, studies carried out in this field are limited compared to works related to 2D natural image analysis, where very large datasets exist. As a result, powerful and efficient 2D convolutional neural networks have been developed and trained. In this paper, we investigate a way to transfer the performance of a two-dimensional classiffication network for the purpose of three-dimensional semantic segmentation of brain tumors. We propose an asymmetric U-Net network by incorporating the EfficientNet model as part of the encoding branch. As the input data is in 3D, the first layers of the encoder are devoted to the reduction of the third dimension in order to fit the input of the EfficientNet network. Experimental results on validation and test data from the BraTS 2020 challenge demonstrate that the proposed method achieve promising performance.",
      "code_url": null
    },
    "2202.08261v2": {
      "title": "Evaluation and Analysis of Different Aggregation and Hyperparameter Selection Methods for Federated Brain Tumor Segmentation",
      "url": "http://arxiv.org/abs/2202.08261v2",
      "authors": "Ece Isik-Polat, Gorkem Polat, Altan Kocyigit, Alptekin Temizel",
      "update_time": "2022-09-07",
      "abstract": "Availability of large, diverse, and multi-national datasets is crucial for the development of effective and clinically applicable AI systems in the medical imaging domain. However, forming a global model by bringing these datasets together at a central location, comes along with various data privacy and ownership problems. To alleviate these problems, several recent studies focus on the federated learning paradigm, a distributed learning approach for decentralized data. Federated learning leverages all the available data without any need for sharing collaborators' data with each other or collecting them on a central server. Studies show that federated learning can provide competitive performance with conventional central training, while having a good generalization capability. In this work, we have investigated several federated learning approaches on the brain tumor segmentation problem. We explore different strategies for faster convergence and better performance which can also work on strong Non-IID cases.",
      "code_url": null
    },
    "2301.07030v1": {
      "title": "Computational Pathology for Brain Disorders",
      "url": "http://arxiv.org/abs/2301.07030v1",
      "authors": "Gabriel Jimenez, Daniel Racoceanu",
      "update_time": "2023-01-18",
      "abstract": "Non-invasive brain imaging techniques allow understanding the behavior and macro changes in the brain to determine the progress of a disease. However, computational pathology provides a deeper understanding of brain disorders at cellular level, able to consolidate a diagnosis and make the bridge between the medical image and the omics analysis. In traditional histopathology, histology slides are visually inspected, under the microscope, by trained pathologists. This process is time-consuming and labor-intensive; therefore, the emergence of Computational Pathology has triggered great hope to ease this tedious task and make it more robust. This chapter focuses on understanding the state-of-the-art machine learning techniques used to analyze whole slide images within the context of brain disorders. We present a selective set of remarkable machine learning algorithms providing discriminative approaches and quality results on brain disorders. These methodologies are applied to different tasks, such as monitoring mechanisms contributing to disease progression and patient survival rates, analyzing morphological phenotypes for classification and quantitative assessment of disease, improving clinical care, diagnosing tumor specimens, and intraoperative interpretation. Thanks to the recent progress in machine learning algorithms for high-content image processing, computational pathology marks the rise of a new generation of medical discoveries and clinical protocols, including in brain disorders.",
      "code_url": null
    },
    "1204.3928v1": {
      "title": "Approximate invariance of metabolic energy per synapse during development in mammalian brains",
      "url": "http://arxiv.org/abs/1204.3928v1",
      "authors": "Jan Karbowski",
      "update_time": "2012-04-19",
      "abstract": "During mammalian development the cerebral metabolic rate correlates qualitatively with synaptogenesis, and both often exhibit bimodal temporal profiles. Despite these non-monotonic dependencies, it is found based on empirical data for different mammals that regional metabolic rate per synapse is approximately conserved from birth to adulthood for a given species (with a slight deviation from this constancy for human visual and temporal cortices during adolescence). A typical synapse uses about $(7\\pm 2)\\cdot 10^{3}$ glucose molecules per second in primate cerebral cortex, and about 5 times of that amount in cat and rat visual cortices. A theoretical model for brain metabolic expenditure is used to estimate synaptic signaling and neural spiking activity during development. It is found that synaptic efficacy is generally inversely correlated with average firing rate, and additionally, synapses consume a bulk of metabolic energy, roughly $50-90 %$ during most of the developmental process (except human temporal cortex $ < 50%$). Overall, these results suggest a tight regulation of brain electrical and chemical activities during the formation and consolidation of neural connections. This presumably reflects strong energetic constraints on brain development.",
      "code_url": null
    },
    "1602.00933v1": {
      "title": "Disentangling Brain Graphs: A Note on the Conflation of Network and Connectivity Analyses",
      "url": "http://arxiv.org/abs/1602.00933v1",
      "authors": "Sean L. Simpson, Paul J. Laurienti",
      "update_time": "2016-02-03",
      "abstract": "Understanding the human brain remains the Holy Grail in biomedical science, and arguably in all of the sciences. Our brains represent the most complex systems in the world (and some contend the universe) comprising nearly one hundred billion neurons with septillions of possible connections between them. The structure of these connections engenders an efficient hierarchical system capable of consciousness, as well as complex thoughts, feelings, and behaviors. Brain connectivity and network analyses have exploded over the last decade due to their potential in helping us understand both normal and abnormal brain function. Functional connectivity (FC) analysis examines functional associations between time series pairs in specified brain voxels or regions. Brain network analysis serves as a distinct subfield of connectivity analysis in which associations are quantified for all time series pairs to create an interconnected representation of the brain (a brain network), which allows studying its systemic properties. While connectivity analyses underlie network analyses, the subtle distinction between the two research areas has generally been overlooked in the literature, with them often being referred to synonymously. However, developing more useful analytic methods and allowing for more precise biological interpretations requires distinguishing these two complementary domains.",
      "code_url": null
    },
    "2109.01854v2": {
      "title": "Predicting isocitrate dehydrogenase mutation status in glioma using structural brain networks and graph neural networks",
      "url": "http://arxiv.org/abs/2109.01854v2",
      "authors": "Yiran Wei, Yonghao Li, Xi Chen, Carola-Bibiane Sch\u00f6nlieb, Chao Li, Stephen J. Price",
      "update_time": "2021-09-13",
      "abstract": "Glioma is a common malignant brain tumor with distinct survival among patients. The isocitrate dehydrogenase (IDH) gene mutation provides critical diagnostic and prognostic value for glioma. It is of crucial significance to non-invasively predict IDH mutation based on pre-treatment MRI. Machine learning/deep learning models show reasonable performance in predicting IDH mutation using MRI. However, most models neglect the systematic brain alterations caused by tumor invasion, where widespread infiltration along white matter tracts is a hallmark of glioma. Structural brain network provides an effective tool to characterize brain organisation, which could be captured by the graph neural networks (GNN) to more accurately predict IDH mutation.   Here we propose a method to predict IDH mutation using GNN, based on the structural brain network of patients. Specifically, we firstly construct a network template of healthy subjects, consisting of atlases of edges (white matter tracts) and nodes (cortical/subcortical brain regions) to provide regions of interest (ROIs). Next, we employ autoencoders to extract the latent multi-modal MRI features from the ROIs of edges and nodes in patients, to train a GNN architecture for predicting IDH mutation. The results show that the proposed method outperforms the baseline models using the 3D-CNN and 3D-DenseNet. In addition, model interpretation suggests its ability to identify the tracts infiltrated by tumor, corresponding to clinical prior knowledge. In conclusion, integrating brain networks with GNN offers a new avenue to study brain lesions using computational neuroscience and computer vision approaches.",
      "code_url": null
    },
    "0807.0337v1": {
      "title": "Unveiling the mystery of visual information processing in human brain",
      "url": "http://arxiv.org/abs/0807.0337v1",
      "authors": "Emanuel Diamant",
      "update_time": "2008-07-07",
      "abstract": "It is generally accepted that human vision is an extremely powerful information processing system that facilitates our interaction with the surrounding world. However, despite extended and extensive research efforts, which encompass many exploration fields, the underlying fundamentals and operational principles of visual information processing in human brain remain unknown. We still are unable to figure out where and how along the path from eyes to the cortex the sensory input perceived by the retina is converted into a meaningful object representation, which can be consciously manipulated by the brain. Studying the vast literature considering the various aspects of brain information processing, I was surprised to learn that the respected scholarly discussion is totally indifferent to the basic keynote question: \"What is information?\" in general or \"What is visual information?\" in particular. In the old days, it was assumed that any scientific research approach has first to define its basic departure points. Why was it overlooked in brain information processing research remains a conundrum. In this paper, I am trying to find a remedy for this bizarre situation. I propose an uncommon definition of \"information\", which can be derived from Kolmogorov's Complexity Theory and Chaitin's notion of Algorithmic Information. Embracing this new definition leads to an inevitable revision of traditional dogmas that shape the state of the art of brain information processing research. I hope this revision would better serve the challenging goal of human visual information processing modeling.",
      "code_url": null
    },
    "2002.04272v2": {
      "title": "Randomized Multiresolution Scanning in Focal and Fast E/MEG Sensing of Brain Activity with a Variable Depth",
      "url": "http://arxiv.org/abs/2002.04272v2",
      "authors": "Atena Rezaei, Alexandra Koulouri, Sampsa Pursiainen",
      "update_time": "2020-02-24",
      "abstract": "We focus on electromagnetoencephalography imaging of the neural activity and, in particular, finding a robust estimate for the primary current distribution via the hierarchical Bayesian model (HBM). Our aim is to develop a reasonably fast maximum a posteriori (MAP) estimation technique which would be applicable for both superficial and deep areas without specific a priori knowledge of the number or location of the activity. To enable source distinguishability for any depth, we introduce a randomized multiresolution scanning (RAMUS) approach in which the MAP estimate of the brain activity is varied during the reconstruction process. RAMUS aims to provide a robust and accurate imaging outcome for the whole brain, while maintaining the computational cost on an appropriate level. The inverse gamma (IG) distribution is applied as the primary hyperprior in order to achieve an optimal performance for the deep part of the brain. In this proof-of-the-concept study, we consider the detection of simultaneous thalamic and somatosensory activity via numerically simulated data modeling the 14-20 ms post-stimulus somatosensory evoked potential and field response to electrical wrist stimulation. Both a spherical and realistic model are utilized to analyze the source reconstruction discrepancies. In the numerically examined case, RAMUS was observed to enhance the visibility of deep components and also marginalizing the random effects of the discretization and optimization without a remarkable computation cost. A robust and accurate MAP estimate for the primary current density was obtained in both superficial and deep parts of the brain.",
      "code_url": null
    },
    "2411.01896v1": {
      "title": "MBDRes-U-Net: Multi-Scale Lightweight Brain Tumor Segmentation Network",
      "url": "http://arxiv.org/abs/2411.01896v1",
      "authors": "Longfeng Shen, Yanqi Hou, Jiacong Chen, Liangjin Diao, Yaxi Duan",
      "update_time": "2024-11-05",
      "abstract": "Accurate segmentation of brain tumors plays a key role in the diagnosis and treatment of brain tumor diseases. It serves as a critical technology for quantifying tumors and extracting their features. With the increasing application of deep learning methods, the computational burden has become progressively heavier. To achieve a lightweight model with good segmentation performance, this study proposes the MBDRes-U-Net model using the three-dimensional (3D) U-Net codec framework, which integrates multibranch residual blocks and fused attention into the model. The computational burden of the model is reduced by the branch strategy, which effectively uses the rich local features in multimodal images and enhances the segmentation performance of subtumor regions. Additionally, during encoding, an adaptive weighted expansion convolution layer is introduced into the multi-branch residual block, which enriches the feature expression and improves the segmentation accuracy of the model. Experiments on the Brain Tumor Segmentation (BraTS) Challenge 2018 and 2019 datasets show that the architecture could maintain a high precision of brain tumor segmentation while considerably reducing the calculation overhead.Our code is released at https://github.com/Huaibei-normal-university-cv-laboratory/mbdresunet",
      "code_url": null
    }
  },
  "EEG": {
    "2509.02568v1": {
      "title": "EEG-MSAF: An Interpretable Microstate Framework uncovers Default-Mode Decoherence in Early Neurodegeneration",
      "url": "http://arxiv.org/abs/2509.02568v1",
      "authors": "Mohammad Mehedi Hasan, Pedro G. Lind, Hernando Ombao, Anis Yazidi, Rabindra Khadka",
      "update_time": "2025-09-04",
      "abstract": "Dementia (DEM) is a growing global health challenge, underscoring the need for early and accurate diagnosis. Electroencephalography (EEG) provides a non-invasive window into brain activity, but conventional methods struggle to capture its transient complexity. We present the \\textbf{EEG Microstate Analysis Framework (EEG-MSAF)}, an end-to-end pipeline that leverages EEG microstates discrete, quasi-stable topographies to identify DEM-related biomarkers and distinguish DEM, mild cognitive impairment (MCI), and normal cognition (NC). EEG-MSAF comprises three stages: (1) automated microstate feature extraction, (2) classification with machine learning (ML), and (3) feature ranking using Shapley Additive Explanations (SHAP) to highlight key biomarkers. We evaluate on two EEG datasets: the public Chung-Ang University EEG (CAUEEG) dataset and a clinical cohort from Thessaloniki Hospital. Our framework demonstrates strong performance and generalizability. On CAUEEG, EEG-MSAF-SVM achieves \\textbf{89\\% $\\pm$ 0.01 accuracy}, surpassing the deep learning baseline CEEDNET by \\textbf{19.3\\%}. On the Thessaloniki dataset, it reaches \\textbf{95\\% $\\pm$ 0.01 accuracy}, comparable to EEGConvNeXt. SHAP analysis identifies mean correlation and occurrence as the most informative metrics: disruption of microstate C (salience/attention network) dominates DEM prediction, while microstate F, a novel default-mode pattern, emerges as a key early biomarker for both MCI and DEM. By combining accuracy, generalizability, and interpretability, EEG-MSAF advances EEG-based dementia diagnosis and sheds light on brain dynamics across the cognitive spectrum.",
      "code_url": null
    },
    "2403.09707v1": {
      "title": "Understanding data analysis aspects of TMS-EEG in clinical study: a mini review and a case study with open dataset",
      "url": "http://arxiv.org/abs/2403.09707v1",
      "authors": "Hua Cheng",
      "update_time": "2024-03-18",
      "abstract": "Concurrency of transcranial magnetic stimulation with electroencephalography (TMS-EEG) technique is a powerful and challenging methodology for basic research and clinical applications. Aspects considered in experiments for effective TMS-EEG recordings and analysis, including artifact management, data analysis and interpretation and protocols. mini review offers an extensive insight of TMS-EEG methodology in experimental and computational procedures. Case study aims to leverage an openly available, high-quality EEG dataset to delve into the alterations in cortical activity. By applying Intermittent theta-burst stimulation (iTBS) and continuous theta-burst stimulation (cTBS) to the left dorsolateral prefrontal cortex (DLPFC) in healthy individuals, we observe changes in oscillatory patterns within the EEG data. The dataset includes meticulously extracted resting-state EEG recordings, TMS-evoked potential data, and MRI scans. To process these data, we utilized Brainstorm, an open-source Matlab application, which facilitated noise reduction through independent component analysis and signal-space projection techniques. It allowed us to identify, visualize, and analyze TMS-evoked potentials (TEPs) and TMS-induced oscillations (TIOs). In addition, the study presents detailed plots of resting-state EEG power, local mean field power (LMFP), TMS-related spectral perturbation (TSRP), and inter-trial phase clustering (ITPC). Paired t-tests and cluster-based permutation tests have been performed for statistical analysis. The wealth and quality of this dataset make it ideal for examining the neuromodulatory impact of TBS on the prefrontal cortex. Brainstorm's extensive feature set greatly supports the exploration of such neurological data. Future research directions could concentrate on conducting source localization analyses and comparative group studies.",
      "code_url": null
    },
    "1903.10154v1": {
      "title": "An Ensemble Learning Based Classification of Individual Finger Movement from EEG",
      "url": "http://arxiv.org/abs/1903.10154v1",
      "authors": "Sutanu Bera, Rinku Roy, Debdeep Sikdar, Manjunatha Mahadevappa",
      "update_time": "2019-03-26",
      "abstract": "Brain computer interface based assistive technology are currently promoted for motor rehabilitation of the neuromuscular ailed individuals. Recent studies indicate a high potential of utilising electroencephalography (EEG) to extract motor related intentions. Limbic movement intentions are already exhaustively studied by the researchers with high accuracy rate. But, capturing movement of fingers from EEG is still in nascent stage. In this study, we have proposed an ensemble learning based approach for EEG in distinguishing between movements of different fingers, namely, thumb, index, and middle. Six healthy subjects participated in this study. Common spatial patterns (CSP) were extracted as features to classify with the extra tree or extremely randomized tree binary classifier. The average classification accuracy of decoding a finger from rest condition was found to be $74\\%$, wheres in discriminating of movement of pair of fingers average accuracy was $60\\%$. Furthermore, error correcting output coding (ECOC) was added to the binary classifier to use it in multiclass classification. The proposed algorithm achieved a maximum kappa value of 0.36 among the subjects.",
      "code_url": null
    },
    "2201.08780v2": {
      "title": "Real-Time Seizure Detection using EEG: A Comprehensive Comparison of Recent Approaches under a Realistic Setting",
      "url": "http://arxiv.org/abs/2201.08780v2",
      "authors": "Kwanhyung Lee, Hyewon Jeong, Seyun Kim, Donghwa Yang, Hoon-Chul Kang, Edward Choi",
      "update_time": "2022-03-22",
      "abstract": "Electroencephalogram (EEG) is an important diagnostic test that physicians use to record brain activity and detect seizures by monitoring the signals. There have been several attempts to detect seizures and abnormalities in EEG signals with modern deep learning models to reduce the clinical burden. However, they cannot be fairly compared against each other as they were tested in distinct experimental settings. Also, some of them are not trained in real-time seizure detection tasks, making it hard for on-device applications. Therefore in this work, for the first time, we extensively compare multiple state-of-the-art models and signal feature extractors in a real-time seizure detection framework suitable for real-world application, using various evaluation metrics including a new one we propose to evaluate more practical aspects of seizure detection models. Our code is available at https://github.com/AITRICS/EEG_real_time_seizure_detection.",
      "code_url": null
    },
    "2004.12321v5": {
      "title": "Federated Transfer Learning for EEG Signal Classification",
      "url": "http://arxiv.org/abs/2004.12321v5",
      "authors": "Ce Ju, Dashan Gao, Ravikiran Mane, Ben Tan, Yang Liu, Cuntai Guan",
      "update_time": "2021-01-26",
      "abstract": "The success of deep learning (DL) methods in the Brain-Computer Interfaces (BCI) field for classification of electroencephalographic (EEG) recordings has been restricted by the lack of large datasets. Privacy concerns associated with EEG signals limit the possibility of constructing a large EEG-BCI dataset by the conglomeration of multiple small ones for jointly training machine learning models. Hence, in this paper, we propose a novel privacy-preserving DL architecture named federated transfer learning (FTL) for EEG classification that is based on the federated learning framework. Working with the single-trial covariance matrix, the proposed architecture extracts common discriminative information from multi-subject EEG data with the help of domain adaptation techniques. We evaluate the performance of the proposed architecture on the PhysioNet dataset for 2-class motor imagery classification. While avoiding the actual data sharing, our FTL approach achieves 2% higher classification accuracy in a subject-adaptive analysis. Also, in the absence of multi-subject data, our architecture provides 6% better accuracy compared to other state-of-the-art DL architectures.",
      "code_url": null
    },
    "2412.19725v2": {
      "title": "EEG-Reptile: An Automatized Reptile-Based Meta-Learning Library for BCIs",
      "url": "http://arxiv.org/abs/2412.19725v2",
      "authors": "Daniil A. Berdyshev, Artem M. Grachev, Sergei L. Shishkin, Bogdan L. Kozyrskiy",
      "update_time": "2025-01-22",
      "abstract": "Meta-learning, i.e., \"learning to learn\", is a promising approach to enable efficient BCI classifier training with limited amounts of data. It can effectively use collections of in some way similar classification tasks, with rapid adaptation to new tasks where only minimal data are available. However, applying meta-learning to existing classifiers and BCI tasks requires significant effort. To address this issue, we propose EEG-Reptile, an automated library that leverages meta-learning to improve classification accuracy of neural networks in BCIs and other EEG-based applications. It utilizes the Reptile meta-learning algorithm to adapt neural network classifiers of EEG data to the inter-subject domain, allowing for more efficient fine-tuning for a new subject on a small amount of data. The proposed library incorporates an automated hyperparameter tuning module, a data management pipeline, and an implementation of the Reptile meta-learning algorithm. EEG-Reptile automation level allows using it without deep understanding of meta-learning. We demonstrate the effectiveness of EEG-Reptile on two benchmark datasets (BCI IV 2a, Lee2019 MI) and three neural network architectures (EEGNet, FBCNet, EEG-Inception). Our library achieved improvement in both zero-shot and few-shot learning scenarios compared to traditional transfer learning approaches.",
      "code_url": null
    },
    "1403.6384v1": {
      "title": "Human brain distinctiveness based on EEG spectral coherence connectivity",
      "url": "http://arxiv.org/abs/1403.6384v1",
      "authors": "Daria La Rocca, Patrizio Campisi, Balazs Vegso, Peter Cserti, Gyorgy Kozmann, Fabio Babiloni, Fabrizio De Vico Fallani",
      "update_time": "2014-09-10",
      "abstract": "The use of EEG biometrics, for the purpose of automatic people recognition, has received increasing attention in the recent years. Most of current analysis rely on the extraction of features characterizing the activity of single brain regions, like power-spectrum estimates, thus neglecting possible temporal dependencies between the generated EEG signals. However, important physiological information can be extracted from the way different brain regions are functionally coupled. In this study, we propose a novel approach that fuses spectral coherencebased connectivity between different brain regions as a possibly viable biometric feature. The proposed approach is tested on a large dataset of subjects (N=108) during eyes-closed (EC) and eyes-open (EO) resting state conditions. The obtained recognition performances show that using brain connectivity leads to higher distinctiveness with respect to power-spectrum measurements, in both the experimental conditions. Notably, a 100% recognition accuracy is obtained in EC and EO when integrating functional connectivity between regions in the frontal lobe, while a lower 97.41% is obtained in EC (96.26% in EO) when fusing power spectrum information from centro-parietal regions. Taken together, these results suggest that functional connectivity patterns represent effective features for improving EEG-based biometric systems.",
      "code_url": null
    },
    "hep-ph/0408298v1": {
      "title": "Non-factorizable contributions to $\\bar{B^0_d} \\to D_s^{(*)} \\bar{D_s^{(*)}}$",
      "url": "http://arxiv.org/abs/hep-ph/0408298v1",
      "authors": "J. O. Eeg, S. Fajfer, A. Hiorth, A. Prapotnik",
      "update_time": "2009-11-10",
      "abstract": "It is pointed out that decays of the type $B \\to D \\bar{D}$ have no factorizable contributions, unless at least one of the charmed mesons in the final state is a vector meson. The dominant contributions to the decay amplitudes arise from chiral loop contributions and tree level amplitudes generated by soft gluon emissions forming a gluon condensate. We predict that the branching ratios for the processes $\\bar B^0 \\to D_s^+ D_s^-$,   $\\bar B^0 \\to D_s^{+*} D_s^- $ and $\\bar B^0 \\to D_s^+ D_s^{-*}$ are all of order $(3- 4) \\times 10^{-4}$, while $\\bar B^0 \\to D_s^{+*} D_s^{-*}$ has a branching ratio 5 to 10 times bigger. We emphasize that the branching ratios are sensitive to $1/m_c$ corrections.",
      "code_url": null
    },
    "2412.14522v2": {
      "title": "CwA-T: A Channelwise AutoEncoder with Transformer for EEG Abnormality Detection",
      "url": "http://arxiv.org/abs/2412.14522v2",
      "authors": "Youshen Zhao, Keiji Iramina",
      "update_time": "2024-12-25",
      "abstract": "Electroencephalogram (EEG) signals are critical for detecting abnormal brain activity, but their high dimensionality and complexity pose significant challenges for effective analysis. In this paper, we propose CwA-T, a novel framework that combines a channelwise CNN-based autoencoder with a single-head transformer classifier for efficient EEG abnormality detection. The channelwise autoencoder compresses raw EEG signals while preserving channel independence, reducing computational costs and retaining biologically meaningful features. The compressed representations are then fed into the transformer-based classifier, which efficiently models long-term dependencies to distinguish between normal and abnormal signals. Evaluated on the TUH Abnormal EEG Corpus, the proposed model achieves 85.0% accuracy, 76.2% sensitivity, and 91.2% specificity at the per-case level, outperforming baseline models such as EEGNet, Deep4Conv, and FusionCNN. Furthermore, CwA-T requires only 202M FLOPs and 2.9M parameters, making it significantly more efficient than transformer-based alternatives. The framework retains interpretability through its channelwise design, demonstrating great potential for future applications in neuroscience research and clinical practice. The source code is available at https://github.com/YossiZhao/CAE-T.",
      "code_url": null
    },
    "2502.17462v1": {
      "title": "The Case for Cleaner Biosignals: High-fidelity Neural Compressor Enables Transfer from Cleaner iEEG to Noisier EEG",
      "url": "http://arxiv.org/abs/2502.17462v1",
      "authors": "Francesco Stefano Carzaniga, Gary Tom Hoppeler, Michael Hersche, Kaspar Anton Schindler, Abbas Rahimi",
      "update_time": "2025-02-26",
      "abstract": "All data modalities are not created equal, even when the signal they measure comes from the same source. In the case of the brain, two of the most important data modalities are the scalp electroencephalogram (EEG), and the intracranial electroencephalogram (iEEG). They are used by human experts, supported by deep learning (DL) models, to accomplish a variety of tasks, such as seizure detection and motor imagery classification. Although the differences between EEG and iEEG are well understood by human experts, the performance of DL models across these two modalities remains under-explored. To help characterize the importance of clean data on the performance of DL models, we propose BrainCodec, a high-fidelity EEG and iEEG neural compressor. We find that training BrainCodec on iEEG and then transferring to EEG yields higher reconstruction quality than training on EEG directly. In addition, we also find that training BrainCodec on both EEG and iEEG improves fidelity when reconstructing EEG. Our work indicates that data sources with higher SNR, such as iEEG, provide better performance across the board also in the medical time-series domain. BrainCodec also achieves up to a 64x compression on iEEG and EEG without a notable decrease in quality. BrainCodec markedly surpasses current state-of-the-art compression models both in final compression ratio and in reconstruction fidelity. We also evaluate the fidelity of the compressed signals objectively on a seizure detection and a motor imagery task performed by standard DL models. Here, we find that BrainCodec achieves a reconstruction fidelity high enough to ensure no performance degradation on the downstream tasks. Finally, we collect the subjective assessment of an expert neurologist, that confirms the high reconstruction quality of BrainCodec in a realistic scenario. The code is available at https://github.com/IBM/eeg-ieeg-brain-compressor.",
      "code_url": null
    }
  },
  "BCI": {
    "1906.02894v1": {
      "title": "Early Prediction of Epilepsy Seizures VLSI BCI System",
      "url": "http://arxiv.org/abs/1906.02894v1",
      "authors": "Zaghloul Saad Zaghloul, Magdy Bayoumi",
      "update_time": "2019-06-10",
      "abstract": "Controlling the surrounding world and predicting future events has always seemed like a dream, but that could become a reality using a Brain-Computer/Machine Interface (BCI/BMI). Epilepsy is a group of neurological diseases characterized by epileptic seizures. It affects millions of people worldwide, with 80 percent of cases occurring in developing countries. This can result in accidents and sudden, unexpected death. Seizures can happen undetectably in newborns, comatose, or motor-impaired patients, especially due to the fact that many medical personnel is not qualified for EEG signal analysis. Therefore, a portable automated detection and monitoring solution is in high demand. Thus, in this study, a system of a wireless wearable adaptive for early prediction of epilepsy seizures is proposed, works via minimally invasive wireless technology paired with an external control device (e.g., a doctors smartphone), with a higher than standard accuracy 71 percent and prediction time (14.56 sec). This novel architecture has not only opened new opportunities for daily usable BCI implementations, but they can also save a life by helping to prevent a seizure fatal consequences",
      "code_url": null
    },
    "1810.03428v1": {
      "title": "Zero-calibration cVEP BCI using word prediction: a proof of concept",
      "url": "http://arxiv.org/abs/1810.03428v1",
      "authors": "Federica Turi, Nathalie Gayraud, Maureen Clerc",
      "update_time": "2018-10-09",
      "abstract": "Brain Computer Interfaces (BCIs) based on visual evoked potentials (VEP) allow for spelling from a keyboard of flashing characters. Among VEP BCIs, code-modulated visual evoked potentials (c-VEPs) are designed for high-speed communication . In c-VEPs, all characters flash simultaneously. In particular, each character flashes according to a predefined 63-bit binary sequence (m-sequence), circular-shifted by a different time lag. For a given character, the m-sequence evokes a VEP in the electroencephalogram (EEG) of the subject, which can be used as a template. This template is obtained during a calibration phase at the beginning of each session. Then, the system outputs the desired character after a predefined number of repetitions by estimating its time lag with respect to the template. Our work avoids the calibration phase, by extracting from the VEP relative lags between successive characters, and predicting the full word using a dictionary.",
      "code_url": null
    },
    "1805.07064v2": {
      "title": "Evaluation of a congruent auditory feedback for Motor Imagery BCI",
      "url": "http://arxiv.org/abs/1805.07064v2",
      "authors": "Emmanuel Christophe, J\u00e9r\u00e9my Frey, Richard Kronland-Martinet, Jean-Arthur Micoulaud-Franchi, Jelena Mladenovi\u0107, Ga\u00eblle Mougin, Jean Vion-Dury, Solvi Ystad, Mitsuko Aramaki",
      "update_time": "2018-05-23",
      "abstract": "Designing a feedback that helps participants to achieve higher performances is an important concern in brain-computer interface (BCI) research. In a pilot study, we demonstrate how a congruent auditory feedback could improve classification in a electroencephalography (EEG) motor imagery BCI. This is a promising result for creating alternate feedback modality.",
      "code_url": null
    },
    "1905.05587v1": {
      "title": "Would Motor-Imagery based BCI user training benefit from more women experimenters?",
      "url": "http://arxiv.org/abs/1905.05587v1",
      "authors": "Aline Roc, L\u00e9a Pillette, B. N'Kaoua, Fabien Lotte",
      "update_time": "2019-05-15",
      "abstract": "Mental Imagery based Brain-Computer Interfaces (MI-BCI) are a mean to control digital technologies by performing MI tasks alone. Throughout MI-BCI use, human supervision (e.g., experimenter or caregiver) plays a central role. While providing emotional and social feedback, people present BCIs to users and ensure smooth users' progress with BCI use. Though, very little is known about the influence experimenters might have on the results obtained. Such influence is to be expected as social and emotional feedback were shown to influence MI-BCI performances. Furthermore, literature from different fields showed an experimenter effect, and specifically of their gender, on experimental outcome. We assessed the impact of the interaction between experi-menter and participant gender on MI-BCI performances and progress throughout a session. Our results revealed an interaction between participants gender, experimenter gender and progress over runs. It seems to suggest that women experimenters may positively influence partici-pants' progress compared to men experimenters.",
      "code_url": null
    },
    "1707.07935v1": {
      "title": "A generic framework for adaptive EEG-based BCI training and operation",
      "url": "http://arxiv.org/abs/1707.07935v1",
      "authors": "Jelena Mladenovi\u0107, J\u00e9r\u00e9mie Mattout, Fabien Lotte",
      "update_time": "2017-07-26",
      "abstract": "There are numerous possibilities and motivations for an adaptive BCI, which may not be easy to clarify and organize for a newcomer to the field. To our knowledge, there has not been any work done in classifying the literature on adaptive BCI in a comprehensive and structured way. We propose a conceptual framework, a taxonomy of adaptive BCI methods which encompasses most important approaches to fit them in such a way that a reader can clearly visualize which elements are being adapted and for what reason. In the interest of having a clear review of existing adaptive BCIs, this framework considers adaptation approaches for both the user and the machine, i.e., using instructional design observations as well as the usual machine learning techniques. This framework not only provides a coherent review of such extensive literature but also enables the reader to perceive gaps and flaws in the current BCI systems, which would hopefully bring novel solutions for an overall improvement.",
      "code_url": null
    },
    "1805.09109v1": {
      "title": "Active Inference for Adaptive BCI: application to the P300 Speller",
      "url": "http://arxiv.org/abs/1805.09109v1",
      "authors": "Jelena Mladenovi\u0107, J\u00e9r\u00e9my Frey, Emmanuel Maby, Mateus Joffily, Fabien Lotte, Jeremie Mattout",
      "update_time": "2018-05-24",
      "abstract": "Adaptive Brain-Computer interfaces (BCIs) have shown to improve performance, however a general and flexible framework to implement adaptive features is still lacking. We appeal to a generic Bayesian approach, called Active Inference (AI), to infer user's intentions or states and act in a way that optimizes performance. In realistic P300-speller simulations, AI outperforms traditional algorithms with an increase in bit rate between 18% and 59%, while offering a possibility of unifying various adaptive implementations within one generic framework.",
      "code_url": null
    },
    "1111.1842v1": {
      "title": "Freeze the BCI until the user is ready: a pilot study of a BCI inhibitor",
      "url": "http://arxiv.org/abs/1111.1842v1",
      "authors": "Laurent George, Laurent Bonnet, Anatole L\u00e9cuyer",
      "update_time": "2011-11-09",
      "abstract": "In this paper we introduce the concept of Brain-Computer Interface (BCI) inhibitor, which is meant to standby the BCI until the user is ready, in order to improve the overall performance and usability of the system. BCI inhibitor can be defined as a system that monitors user's state and inhibits BCI interaction until specific requirements (e.g. brain activity pattern, user attention level) are met. In this pilot study, a hybrid BCI is designed and composed of a classic synchronous BCI system based on motor imagery and a BCI inhibitor. The BCI inhibitor initiates the control period of the BCI when requirements in terms of brain activity are reached (i.e. stability in the beta band). Preliminary results with four participants suggest that BCI inhibitor system can improve BCI performance.",
      "code_url": null
    },
    "2508.09242v1": {
      "title": "Cross-BCI, A Cross-BCI-Paradigm Classifica-tion Model Towards Universal BCI Applications",
      "url": "http://arxiv.org/abs/2508.09242v1",
      "authors": "Gaojie Zhou, Junhua Li",
      "update_time": "2025-08-14",
      "abstract": "Classification models used in brain-computer interface (BCI) are usually designed for a single BCI paradigm. This requires the redevelopment of the model when applying it to a new BCI paradigm, resulting in repeated costs and effort. Moreover, less complex deep learning models are desired for practical usage, as well as for deployment on portable devices. In or-der to fill the above gaps, we, in this study, proposed a light-weight and unified decoding model for cross-BCI-paradigm classification. The proposed model starts with a tempo-spatial convolution. It is followed by a multi-scale local feature selec-tion module, aiming to extract local features shared across BCI paradigms and generate weighted features. Finally, a mul-ti-dimensional global feature extraction module is designed, in which multi-dimensional global features are extracted from the weighted features and fused with the weighted features to form high-level feature representations associated with BCI para-digms. The results, evaluated on a mixture of three classical BCI paradigms (i.e., MI, SSVEP, and P300), demon-strate that the proposed model achieves 88.39%, 82.36%, 80.01%, and 0.8092 for accuracy, macro-precision, mac-ro-recall, and macro-F1-score, respectively, significantly out-performing the compared models. This study pro-vides a feasible solution for cross-BCI-paradigm classifica-tion. It lays a technological foundation for de-veloping a new generation of unified decoding systems, paving the way for low-cost and universal practical applications.",
      "code_url": null
    },
    "1501.01144v2": {
      "title": "Airborne Ultrasonic Tactile Display BCI",
      "url": "http://arxiv.org/abs/1501.01144v2",
      "authors": "Katsuhiko Hamada, Hiromu Mori, Hiroyuki Shinoda, Tomasz M. Rutkowski",
      "update_time": "2016-01-05",
      "abstract": "This chapter presents results of our project, which studied whether contactless and airborne ultrasonic tactile display (AUTD) stimuli delivered to a user's palms could serve as a platform for a brain computer interface (BCI) paradigm. We used six palm positions to evoke combined somatosensory brain responses to implement a novel contactless tactile BCI. This achievement was awarded the top prize in the Annual BCI Research Award 2014 competition. This chapter also presents a comparison with a classical attached vibrotactile transducer-based BCI paradigm. Experiment results from subjects performing online experiments validate the novel BCI paradigm.",
      "code_url": null
    },
    "1803.04808v1": {
      "title": "Semi-BCI Algebras",
      "url": "http://arxiv.org/abs/1803.04808v1",
      "authors": "Regivan H. N. Santiago, Benjamin Bedregal, Jo\u00e3o Marcos, Carlos Caleiro, Jocivania Pinheiro",
      "update_time": "2018-03-14",
      "abstract": "The notion of semi-BCI algebras is introduced and some of its properties are investigated. This algebra is another generalization for BCI-algebras. It arises from the \"intervalization\" of BCI algebras. Semi-BCI have a similar structure to Pseudo-BCI algebras however they are not the same. In this paper we also provide an investigation on the similarity between these classes of algebras by showing how they relate to the process of intervalization.",
      "code_url": null
    }
  },
  "fMRI": {
    "2412.07783v3": {
      "title": "Swin fMRI Transformer Predicts Early Neurodevelopmental Outcomes from Neonatal fMRI",
      "url": "http://arxiv.org/abs/2412.07783v3",
      "authors": "Patrick Styll, Dowon Kim, Jiook Cha",
      "update_time": "2025-01-31",
      "abstract": "Brain development in the first few months of human life is a critical phase characterized by rapid structural growth and functional organization. Accurately predicting developmental outcomes during this time is crucial for identifying delays and enabling timely interventions. This study introduces the SwiFT (Swin 4D fMRI Transformer) model, designed to predict Bayley-III composite scores using neonatal fMRI from the Developing Human Connectome Project (dHCP). To enhance predictive accuracy, we apply dimensionality reduction via group independent component analysis (ICA) and pretrain SwiFT on large adult fMRI datasets to address the challenges of limited neonatal data. Our analysis shows that SwiFT significantly outperforms baseline models in predicting cognitive, motor, and language outcomes, leveraging both single-label and multi-label prediction strategies. The model's attention-based architecture processes spatiotemporal data end-to-end, delivering superior predictive performance. Additionally, we use Integrated Gradients with Smoothgrad sQuare (IG-SQ) to interpret predictions, identifying neural spatial representations linked to early cognitive and behavioral development. These findings underscore the potential of Transformer models to advance neurodevelopmental research and clinical practice.",
      "code_url": null
    },
    "2403.06361v2": {
      "title": "See Through Their Minds: Learning Transferable Neural Representation from Cross-Subject fMRI",
      "url": "http://arxiv.org/abs/2403.06361v2",
      "authors": "Yulong Liu, Yongqiang Ma, Guibo Zhu, Haodong Jing, Nanning Zheng",
      "update_time": "2024-06-14",
      "abstract": "Deciphering visual content from functional Magnetic Resonance Imaging (fMRI) helps illuminate the human vision system. However, the scarcity of fMRI data and noise hamper brain decoding model performance. Previous approaches primarily employ subject-specific models, sensitive to training sample size. In this paper, we explore a straightforward but overlooked solution to address data scarcity. We propose shallow subject-specific adapters to map cross-subject fMRI data into unified representations. Subsequently, a shared deeper decoding model decodes cross-subject features into the target feature space. During training, we leverage both visual and textual supervision for multi-modal brain decoding. Our model integrates a high-level perception decoding pipeline and a pixel-wise reconstruction pipeline guided by high-level perceptions, simulating bottom-up and top-down processes in neuroscience. Empirical experiments demonstrate robust neural representation learning across subjects for both pipelines. Moreover, merging high-level and low-level information improves both low-level and high-level reconstruction metrics. Additionally, we successfully transfer learned general knowledge to new subjects by training new adapters with limited training data. Compared to previous state-of-the-art methods, notably pre-training-based methods (Mind-Vis and fMRI-PTE), our approach achieves comparable or superior results across diverse tasks, showing promise as an alternative method for cross-subject fMRI data pre-training. Our code and pre-trained weights will be publicly released at https://github.com/YulongBonjour/See_Through_Their_Minds.",
      "code_url": null
    },
    "1802.01334v3": {
      "title": "Information Assisted Dictionary Learning for fMRI data analysis",
      "url": "http://arxiv.org/abs/1802.01334v3",
      "authors": "Manuel Morante, Yannis Kopsinis, Sergios Theodoridis, Athanassios Protopapas",
      "update_time": "2019-08-20",
      "abstract": "In this paper, the task-related fMRI problem is treated in its matrix factorization formulation, focused on the Dictionary Learning (DL) approach. The new method allows the incorporation of a priori knowledge associated both with the experimental design as well as with available brain Atlases. Moreover, the proposed method can efficiently cope with uncertainties related to the HRF modeling. In addition, the proposed method bypasses one of the major drawbacks that are associated with DL methods; that is, the selection of the sparsity-related regularization parameters. In our formulation, an alternative sparsity promoting constraint is employed, that bears a direct relation to the number of voxels in the spatial maps. Hence, the related parameters can be tuned using information that is available from brain atlases. The proposed method is evaluated against several other popular techniques, including GLM. The obtained performance gains are reported via a novel realistic synthetic fMRI dataset as well as real data that are related to a challenging experimental design.",
      "code_url": null
    },
    "2410.18110v1": {
      "title": "Learning Image Derived PDE-Phenotypes from fMRI Data",
      "url": "http://arxiv.org/abs/2410.18110v1",
      "authors": "Ion Bica, Ryan Trang, Rui Hu, Wanhua Su, Zhichun Zhai, Qingrun Zhang",
      "update_time": "2024-10-25",
      "abstract": "Partial Differential Equations (PDEs) model various physical phenomena, such as electromagnetic fields and fluid mechanics. Methods like Sparse Identification of Nonlinear Dynamics (SINDy) and PDE-Net 2.0 have been developed to identify and model PDEs based on data using sparse optimization and deep neural networks, respectively. While PDE models are less commonly applied to fMRI data, they hold the potential for uncovering hidden connections and essential components in brain activity. Using the ADHD200 dataset, we applied Canonical Independent Component Analysis (CanICA) and Uniform Manifold Approximation (UMAP) for dimensionality reduction of fMRI data. We then used Sparse Ridge Regression to identify PDEs from the reduced data, achieving high accuracy in classifying attention deficit hyperactivity disorder (ADHD). The study demonstrates a novel approach to extracting meaningful features from fMRI data for neurological disorder analysis to understand the role of oxygen transport (delivery $\\&$ consumption) in the brain during neural activity relevant for studying intracranial pathologies.",
      "code_url": null
    },
    "2305.18274v2": {
      "title": "Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors",
      "url": "http://arxiv.org/abs/2305.18274v2",
      "authors": "Paul S. Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Ethan Cohen, Aidan J. Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth A. Norman, Tanishq Mathew Abraham",
      "update_time": "2023-10-10",
      "abstract": "We present MindEye, a novel fMRI-to-image approach to retrieve and reconstruct viewed images from brain activity. Our model comprises two parallel submodules that are specialized for retrieval (using contrastive learning) and reconstruction (using a diffusion prior). MindEye can map fMRI brain activity to any high dimensional multimodal latent space, like CLIP image space, enabling image reconstruction using generative models that accept embeddings from this latent space. We comprehensively compare our approach with other existing methods, using both qualitative side-by-side comparisons and quantitative evaluations, and show that MindEye achieves state-of-the-art performance in both reconstruction and retrieval tasks. In particular, MindEye can retrieve the exact original image even among highly similar candidates indicating that its brain embeddings retain fine-grained image-specific information. This allows us to accurately retrieve images even from large-scale databases like LAION-5B. We demonstrate through ablations that MindEye's performance improvements over previous methods result from specialized submodules for retrieval and reconstruction, improved training techniques, and training models with orders of magnitude more parameters. Furthermore, we show that MindEye can better preserve low-level image features in the reconstructions by using img2img, with outputs from a separate autoencoder. All code is available on GitHub.",
      "code_url": null
    },
    "2311.00342v1": {
      "title": "fMRI-PTE: A Large-scale fMRI Pretrained Transformer Encoder for Multi-Subject Brain Activity Decoding",
      "url": "http://arxiv.org/abs/2311.00342v1",
      "authors": "Xuelin Qian, Yun Wang, Jingyang Huo, Jianfeng Feng, Yanwei Fu",
      "update_time": "2023-11-02",
      "abstract": "The exploration of brain activity and its decoding from fMRI data has been a longstanding pursuit, driven by its potential applications in brain-computer interfaces, medical diagnostics, and virtual reality. Previous approaches have primarily focused on individual subject analysis, highlighting the need for a more universal and adaptable framework, which is the core motivation behind our work. In this work, we propose fMRI-PTE, an innovative auto-encoder approach for fMRI pre-training, with a focus on addressing the challenges of varying fMRI data dimensions due to individual brain differences. Our approach involves transforming fMRI signals into unified 2D representations, ensuring consistency in dimensions and preserving distinct brain activity patterns. We introduce a novel learning strategy tailored for pre-training 2D fMRI images, enhancing the quality of reconstruction. fMRI-PTE's adaptability with image generators enables the generation of well-represented fMRI features, facilitating various downstream tasks, including within-subject and cross-subject brain activity decoding. Our contributions encompass introducing fMRI-PTE, innovative data transformation, efficient training, a novel learning strategy, and the universal applicability of our approach. Extensive experiments validate and support our claims, offering a promising foundation for further research in this domain.",
      "code_url": null
    },
    "1404.2917v2": {
      "title": "How to test cognitive theory with fMRI",
      "url": "http://arxiv.org/abs/1404.2917v2",
      "authors": "Christopher H. Chatham, David Badre",
      "update_time": "2015-07-08",
      "abstract": "The objective of this chapter is to provide a guide to using functional magnetic resonance imaging (fMRI) to inform cognitive theory. This is, of course, a daunting task, as the premise itself - that fMRI data can inform cognitive theory - is still actively debated. Below, we touch on this debate as a means of framing our guide. In particular, we argue that cognitive theories can be constrained by neuroscientific data, including that offered by fMRI, but to do so requires embellishing the cognitive theory so that it can make predictions for neuroscience; much the same as how testing a cognitive theory using behavior requires embellishing that theory to make experimentally realizable behavioral predictions (i.e., the process of generating operational definitions). Moreover, recent years have seen the development of several new approaches that allow fMRI to better test neurally-embellished models. Along with a review of several ways of testing neurally-embellished cognitive theory using fMRI, we also consider the inferential challenges that can accompany these approaches. Readers of this chapter should gain an understanding of both of the potential power and the challenges associated with fMRI as a cognitive neuroscience methodology.",
      "code_url": null
    },
    "1201.4481v2": {
      "title": "EEG-assisted retrospective motion correction for fMRI: E-REMCOR",
      "url": "http://arxiv.org/abs/1201.4481v2",
      "authors": "Vadim Zotev, Han Yuan, Raquel Phillips, Jerzy Bodurka",
      "update_time": "2012-08-31",
      "abstract": "We propose a method for retrospective motion correction of fMRI data in simultaneous EEG-fMRI that employs the EEG array as a sensitive motion detector. EEG motion artifacts are used to generate motion regressors describing rotational head movements with millisecond temporal resolution. These regressors are utilized for slice-specific motion correction of unprocessed fMRI data. Performance of the method is demonstrated by correction of fMRI data from five patients with major depressive disorder, who exhibited head movements by 1-3 mm during a resting EEG-fMRI run. The fMRI datasets, corrected using eight to ten EEG-based motion regressors, show significant improvements in temporal SNR (TSNR) of fMRI time series, particularly in the frontal brain regions and near the surface of the brain. The TSNR improvements are as high as 50% for large brain areas in single-subject analysis and as high as 25% when the results are averaged across the subjects. Simultaneous application of the EEG-based motion correction and physiological noise correction by means of RETROICOR leads to average TSNR enhancements as high as 35% for large brain regions. These TSNR improvements are largely preserved after the subsequent fMRI volume registration and regression of fMRI motion parameters. The proposed EEG-assisted method of retrospective fMRI motion correction (referred to as E-REMCOR) can be used to improve quality of fMRI data with severe motion artifacts and to reduce spurious correlations between the EEG and fMRI data caused by head movements. It does not require any specialized equipment beyond the standard EEG-fMRI instrumentation and can be applied retrospectively to any existing EEG-fMRI data set.",
      "code_url": null
    },
    "2012.06972v1": {
      "title": "fMRI-Kernel Regression: A Kernel-based Method for Pointwise Statistical Analysis of rs-fMRI for Population Studies",
      "url": "http://arxiv.org/abs/2012.06972v1",
      "authors": "Anand A. Joshi, Soyoung Choi, Haleh Akrami, Richard M. Leahy",
      "update_time": "2020-12-15",
      "abstract": "Due to the spontaneous nature of resting-state fMRI (rs-fMRI) signals, cross-subject comparison and therefore, group studies of rs-fMRI are challenging. Most existing group comparison methods use features extracted from the fMRI time series, such as connectivity features, independent component analysis (ICA), and functional connectivity density (FCD) methods. However, in group studies, especially in the case of spectrum disorders, distances to a single atlas or a representative subject do not fully reflect the differences between subjects that may lie on a multi-dimensional spectrum. Moreover, there may not exist an individual subject or even an average atlas in such cases that is representative of all subjects. Here we describe an approach that measures pairwise distances between the synchronized rs-fMRI signals of pairs of subjects instead of to a single reference point. We also present a method for fMRI data comparison that leverages this generated pairwise feature to establish a radial basis function kernel matrix. This kernel matrix is used in turn to perform kernel regression of rs-fMRI to a clinical variable such as a cognitive or neurophysiological performance score of interest. This method opens a new pointwise analysis paradigm for fMRI data. We demonstrate the application of this method by performing a pointwise analysis on the cortical surface using rs-fMRI data to identify cortical regions associated with variability in ADHD index. While pointwise analysis methods are common in anatomical studies such as cortical thickness analysis and voxel- and tensor-based morphometry and its variants, such a method is lacking for rs-fMRI and could improve the utility of rs-fMRI for group studies. The method presented in this paper is aimed at filling this gap.",
      "code_url": null
    },
    "2509.05873v1": {
      "title": "Assessment of 7T task fMRI value over 3T task fMRI",
      "url": "http://arxiv.org/abs/2509.05873v1",
      "authors": "Dalton H Bermudez",
      "update_time": "2025-09-09",
      "abstract": "Ultra-high field 7T fMRI offers notable advantages over 3T fMRI, including higher signal-to-noise and contrast-to-noise ratios, enabling finer spatial and temporal resolution. This study explores the differences in activation maps from Human Connectome Project datasets between 7T and 3T field strengths, focusing on visual identified using the Glasser atlas. Functional tasks for each scanner were designed to include visual stimuli, with data processed uniformly to ensure comparability. Results showed significantly higher beta coefficients for common regions of activation, such as V3A and V3B, in 7T compared to 3T datasets. This suggests that 7T fMRI data more accurately reflect the idealized time course of task-related conditions, likely due to improved sensitivity to blood oxygenation level-dependent (BOLD) signals. However, variations in experimental design and acquisition parameters between scanners complicate the direct comparison of beta coefficients.",
      "code_url": null
    }
  },
  "MEG": {
    "1605.05081v3": {
      "title": "Search for the Lepton Flavour Violating Decay $\u03bc^{+} \\to e^+ \u03b3$ with the Full Dataset of the MEG Experiment",
      "url": "http://arxiv.org/abs/1605.05081v3",
      "authors": "The MEG Collaboration",
      "update_time": "2016-08-01",
      "abstract": "The final results of the search for the lepton flavour violating decay $\u03bc^{+} \\rightarrow {\\rm e^{+}} \u03b3$ based on the full dataset collected by the MEG experiment at the Paul Scherrer Institut in the period 2009--2013 and totalling $7.5\\times 10^{14}$ stopped muons on target are presented. No significant excess of events is observed in the dataset with respect to the expected background and a new upper limit on the branching ratio of this decay of $BR( \u03bc^{+} \\rightarrow {\\rm e^{+}} \u03b3) < 4.2 \\times 10^{-13}$ (90\\%\\ confidence level) is established, which represents the most stringent limit on the existence of this decay to date.",
      "code_url": null
    },
    "2310.11902v3": {
      "title": "Operation and performance of MEG II detector",
      "url": "http://arxiv.org/abs/2310.11902v3",
      "authors": "MEG II Collaboration, K. Afanaciev, A. M. Baldini, S. Ban, V. Baranov, H. Benmansour, M. Biasotti, G. Boca, P. W. Cattaneo, G. Cavoto, F. Cei, M. Chiappini, G. Chiarello, A. Corvaglia, F. Cuna, G. Dal Maso, A. De Bari, M. De Gerone, L. Ferrari Barusso, M. Francesconi, L. Galli, G. Gallucci, F. Gatti, L. Gerritzen, F. Grancagnolo, E. G. Grandoni, M. Grassi, D. N. Grigoriev, M. Hildebrandt, K. Ieki, F. Ignatov, F. Ikeda, T. Iwamoto, S. Karpov, P. -R. Kettle, N. Khomutov, S. Kobayashi, A. Kolesnikov, N. Kravchuk, V. Krylov, N. Kuchinskiy, W. Kyle, T. Libeiro, V. Malyshev, A. Matsushita, M. Meucci, S. Mihara, W. Molzon, Toshinori Mori, F. Morsani, M. Nakao, D. Nicol\u00f2, H. Nishiguchi, A. Ochi, S. Ogawa, R. Onda, W. Ootani, A. Oya, D. Palo, M. Panareo, A. Papa, V. Pettinacci, A. Popov, F. Raffaelli, F. Renga, S. Ritt, M. Rossella, A. Rozhdestvensky, P. Schwendimann, K. Shimada, G. Signorelli, A. Stoykov, M. Takahashi, G. F. Tassielli, K. Toyoda, Y. Uchiyama, M. Usami, A. Venturini, B. Vitali, C. Voena, K. Yamamoto, K. Yanai, T. Yonemoto, K. Yoshida, Yu. V. Yudin",
      "update_time": "2024-01-09",
      "abstract": "The MEG II experiment, located at the Paul Scherrer Institut (PSI) in Switzerland, is the successor to the MEG experiment, which completed data taking in 2013. MEG II started fully operational data taking in 2021, with the goal of improving the sensitivity of the mu+ -> e+ gamma decay down to 6e-14 almost an order of magnitude better than the current limit. In this paper, we describe the operation and performance of the experiment and give a new estimate of its sensitivity versus data acquisition time.",
      "code_url": null
    },
    "1312.3217v3": {
      "title": "Measurement of the radiative decay of polarized muons in the MEG experiment",
      "url": "http://arxiv.org/abs/1312.3217v3",
      "authors": "MEG Collaboration, A. M. Baldini, Y. Bao, E. Baracchini, C. Bemporad, F. Berg, M. Biasotti, G. Boca, P. W. Cattaneo, G. Cavoto, F. Cei, G. Chiarello, C. Chiri, A. de Bari, M. De Gerone, A. D'Onofrio, S. Dussoni, Y. Fujii, L. Galli, F. Gatti, F. Grancagnolo, M. Grassi, A. Graziosi, D. N. Grigoriev, T. Haruyama, M. Hildebrandt, Z. Hodge, K. Ieki, F. Ignatov, T. Iwamoto, D. Kaneko, Tae Im Kang, P. -R. Kettle, B. I. Khazin, N. Khomutov, A. Korenchenko, N. Kravchuk, G. M. A. Lim, S. Mihara, W. Molzon, Toshinori Mori, A. Mtchedlishvili, S. Nakaura, D. Nicol\u00f2, H. Nishiguchi, M. Nishimura, S. Ogawa, W. Ootani, M. Panareo, A. Papa, A. Pepino, G. Piredda, G. Pizzigoni, A. Popov, F. Renga, E. Ripiccini, S. Ritt, M. Rossella, G. Rutar, R. Sawada, F. Sergiampietri, G. Signorelli, G. F. Tassielli, F. Tenchini, Y. Uchiyama, M. Venturini, C. Voena, A. Yamamoto, K. Yoshida, Z. You, Yu. V. Yudin",
      "update_time": "2016-03-08",
      "abstract": "We studied the radiative muon decay $\u03bc^+ \\to e^+\u03bd\\bar\u03bd\u03b3$ by using for the first time an almost fully polarized muon source. We identified a large sample (~13000) of these decays in a total sample of 1.8x10^14 positive muon decays collected in the MEG experiment in the years 2009--2010 and measured the branching ratio B($\u03bc^+ \\to e^+\u03bd\\bar\u03bd\u03b3$) = (6.03+-0.14(stat.)+-0.53(sys.))x10^-8 for E_e > 45 MeV and E_\u03b3 > 40 MeV, consistent with the Standard Model prediction. The precise measurement of this decay mode provides a basic tool for the timing calibration, a normalization channel, and a strong quality check of the complete MEG experiment in the search for $\u03bc^+ \\to e^+\u03b3$ process.",
      "code_url": null
    },
    "2107.10767v2": {
      "title": "The Search for $\u03bc^+\\to e^+ \u03b3$ with 10$^{-14}$ Sensitivity: the Upgrade of the MEG Experiment",
      "url": "http://arxiv.org/abs/2107.10767v2",
      "authors": "The MEG II Collaboration, Alessandro M. Baldini, Vladimir Baranov, Michele Biasotti, Gianluigi Boca, Paolo W. Cattaneo, Gianluca Cavoto, Fabrizio Cei, Marco Chiappini, Gianluigi Chiarello, Alessandro Corvaglia, Federica Cuna, Giovanni dal Maso, Antonio de Bari, Matteo De Gerone, Marco Francesconi, Luca Galli, Giovanni Gallucci, Flavio Gatti, Francesco Grancagnolo, Marco Grassi, Dmitry N. Grigoriev, Malte Hildebrandt, Kei Ieki, Fedor Ignatov, Toshiyuki Iwamoto, Peter-Raymond Kettle, Nikolay Khomutov, Satoru Kobayashi, Alexander Kolesnikov, Nikolay Kravchuk, Victor Krylov, Nikolay Kuchinskiy, William Kyle, Terence Libeiro, Vladimir Malyshev, Manuel Meucci, Satoshi Mihara, William Molzon, Toshinori Mori, Alexander Mtchedlishvili, Mitsutaka Nakao, Donato Nicol\u00f2, Hajime Nishiguchi, Shinji Ogawa, Rina Onda, Wataru Ootani, Atsushi Oya, Dylan Palo, Marco Panareo, Angela Papa, Valerio Pettinacci, Alexander Popov, Francesco Renga, Stefan Ritt, Massimo Rossella, Aleksander Rozhdestvensky, Patrick Schwendimann, Kohei Shimada, Giovanni Signorelli, Alexey Stoykov, Giovanni F. Tassielli, Kazuki Toyoda, Yusuke Uchiyama, Masashi Usami, Cecilia Voena, Kosuke Yanai, Kensuke Yamamoto, Taku Yonemoto, Yury V. Yudin",
      "update_time": "2021-09-02",
      "abstract": "The MEG experiment took data at the Paul Scherrer Institute in the years 2009--2013 to test the violation of the lepton flavour conservation law, which originates from an accidental symmetry that the Standard Model of elementary particle physics has, and published the most stringent limit on the charged lepton flavour violating decay $\u03bc^+ \\rightarrow {\\rm e}^+ \u03b3$: BR($\u03bc^+ \\rightarrow {\\rm e}^+ \u03b3$) $<4.2 \\times 10^{-13}$ at 90% confidence level. The MEG detector has been upgraded in order to reach a sensitivity of $6\\times10^{-14}$. The basic principle of MEG II is to achieve the highest possible sensitivity using the full muon beam intensity at the Paul Scherrer Institute ($7\\times10^{7}$ muons/s) with an upgraded detector. The main improvements are better rate capability of all sub-detectors and improved resolutions while keeping the same detector concept. In this paper, we present the current status of the preparation, integration and commissioning of the MEG II detector in the recent engineering runs.",
      "code_url": null
    },
    "0908.2594v2": {
      "title": "A limit for the mu -> e gamma decay from the MEG experiment",
      "url": "http://arxiv.org/abs/0908.2594v2",
      "authors": "MEG collaboration, J. Adam, X. Bai, A. Baldini, E. Baracchini, A. Barchiesi, C. Bemporad, G. Boca, P. W. Cattaneo, G. Cavoto, G. Cecchet, F. Cei, C. Cerri, A. De Bari, M. De Gerone, T. Doke, S. Dussoni, J. Egger, L. Galli, G. Gallucci, F. Gatti, B. Golden, M. Grassi, D. N. Grigoriev, T. Haruyama, M. Hildebrandt, Y. Hisamatsu, F. Ignatov, T. Iwamoto, D. Kaneko, P. -R. Kettle, B. I. Khazin, O. Kiselev, A. Korenchenko, N. Kravchuk, A. Maki, S. Mihara, W. Molzon, T. Mori, D. Mzavia, H. Natori, R. Nard\u00f2, D. Nicol\u00f2, H. Nishiguchi, Y. Nishimura, W. Ootani, M. Panareo, A. Papa, R. Pazzi, G. Piredda, A. Popov, F. Renga, S. Ritt, M. Rossella, R. Sawada, M. Schneebeli, F. Sergiampietri, G. Signorelli, S. Suzuki, C. Topchyan, V. Tumakov, Y. Uchiyama, R. Valle, C. Voena, F. Xiao, S. Yamada, A. Yamamoto, S. Yamashita, Yu. V. Yudin, D. Zanello",
      "update_time": "2010-04-21",
      "abstract": "A search for the decay mu -> e gamma, performed at PSI and based on data from the initial three months of operation of the MEG experiment, yields an upper limit on the branching ratio of BR(mu -> e gamma) < 2.8 x 10**-11 (90% C.L.). This corresponds to the measurement of positrons and photons from ~ 10**14 stopped mu-decays by means of a superconducting positron spectrometer and a 900 litre liquid xenon photon detector.",
      "code_url": null
    },
    "2310.12614v3": {
      "title": "A search for $\u03bc^+\\to e^+\u03b3$ with the first dataset of the MEG II experiment",
      "url": "http://arxiv.org/abs/2310.12614v3",
      "authors": "MEG II collaboration, K. Afanaciev, A. M. Baldini, S. Ban, V. Baranov, H. Benmansour, M. Biasotti, G. Boca, P. W. Cattaneo, G. Cavoto, F. Cei, M. Chiappini, G. Chiarello, A. Corvaglia, F. Cuna, G. Dal Maso, A. De Bari, M. De Gerone, L. Ferrari Barusso, M. Francesconi, L. Galli, G. Gallucci, F. Gatti, L. Gerritzen, F. Grancagnolo, E. G. Grandoni, M. Grassi, D. N. Grigoriev, M. Hildebrandt, K. Ieki, F. Ignatov, F. Ikeda, T. Iwamoto, S. Karpov, P. -R. Kettle, N. Khomutov, S. Kobayashi, A. Kolesnikov, N. Kravchuk, V. Krylov, N. Kuchinskiy, W. Kyle, T. Libeiro, V. Malyshev, A. Matsushita, M. Meucci, S. Mihara, W. Molzon, Toshinori Mori, M. Nakao, D. Nicol\u00f2, H. Nishiguchi, A. Ochi, S. Ogawa, R. Onda, W. Ootani, A. Oya, D. Palo, M. Panareo, A. Papa, V. Pettinacci, A. Popov, F. Renga, S. Ritt, M. Rossella, A. Rozhdestvensky, P. Schwendimann, K. Shimada, G. Signorelli, M. Takahashi, G. F. Tassielli, K. Toyoda, Y. Uchiyama, M. Usami, A. Venturini, B. Vitali, C. Voena, K. Yamamoto, K. Yanai, T. Yonemoto, K. Yoshida, Yu. V. Yudin",
      "update_time": "2024-01-09",
      "abstract": "The MEG II experiment, based at the Paul Scherrer Institut in Switzerland, reports the result of a search for the decay $\u03bc^+\\to e^+\u03b3$ from data taken in the first physics run in 2021. No excess of events over the expected background is observed, yielding an upper limit on the branching ratio of B($\u03bc^+\\to e^+\u03b3$) < $7.5 \\times 10^{-13}$ (90% C.L.). The combination of this result and the limit obtained by MEG gives B($\u03bc^+\\to e^+\u03b3$) < $3.1 \\times 10^{-13}$ (90% C.L.), which is the most stringent limit to date. A ten-fold larger sample of data is being collected during the years 2022-2023, and data-taking will continue in the coming years.",
      "code_url": null
    },
    "1805.10981v2": {
      "title": "Adaptive neural network classifier for decoding MEG signals",
      "url": "http://arxiv.org/abs/1805.10981v2",
      "authors": "Ivan Zubarev, Rasmus Zetter, Hanna-Leena Halme, Lauri Parkkonen",
      "update_time": "2019-02-12",
      "abstract": "Convolutional Neural Networks (CNN) outperform traditional classification methods in many domains. Recently these methods have gained attention in neuroscience and particularly in brain-computer interface (BCI) community. Here, we introduce a CNN optimized for classification of brain states from magnetoencephalographic (MEG) measurements. Our CNN design is based on a generative model of the electromagnetic (EEG and MEG) brain signals and is readily interpretable in neurophysiological terms. We show here that the proposed network is able to decode event-related responses as well as modulations of oscillatory brain activity and that it outperforms more complex neural networks and traditional classifiers used in the field. Importantly, the model is robust to inter-individual differences and can successfully generalize to new subjects in offline and online classification.",
      "code_url": null
    },
    "2404.09256v1": {
      "title": "Foundational GPT Model for MEG",
      "url": "http://arxiv.org/abs/2404.09256v1",
      "authors": "Richard Csaky, Mats W. J. van Es, Oiwi Parker Jones, Mark Woolrich",
      "update_time": "2024-04-16",
      "abstract": "Deep learning techniques can be used to first training unsupervised models on large amounts of unlabelled data, before fine-tuning the models on specific tasks. This approach has seen massive success for various kinds of data, e.g. images, language, audio, and holds the promise of improving performance in various downstream tasks (e.g. encoding or decoding brain data). However, there has been limited progress taking this approach for modelling brain signals, such as Magneto-/electroencephalography (M/EEG). Here we propose two classes of deep learning foundational models that can be trained using forecasting of unlabelled MEG. First, we consider a modified Wavenet; and second, we consider a modified Transformer-based (GPT2) model. The modified GPT2 includes a novel application of tokenisation and embedding methods, allowing a model developed initially for the discrete domain of language to be applied to continuous multichannel time series data. We also extend the forecasting framework to include condition labels as inputs, enabling better modelling (encoding) of task data. We compare the performance of these deep learning models with standard linear autoregressive (AR) modelling on MEG data. This shows that GPT2-based models provide better modelling capabilities than Wavenet and linear AR models, by better reproducing the temporal, spatial and spectral characteristics of real data and evoked activity in task data. We show how the GPT2 model scales well to multiple subjects, while adapting its model to each subject through subject embedding. Finally, we show how such a model can be useful in downstream decoding tasks through data simulation. All code is available on GitHub (https://github.com/ricsinaruto/MEG-transfer-decoding).",
      "code_url": null
    },
    "1501.05068v1": {
      "title": "Difficulties applying recent blind source separation techniques to EEG and MEG",
      "url": "http://arxiv.org/abs/1501.05068v1",
      "authors": "Kevin H. Knuth",
      "update_time": "2015-01-22",
      "abstract": "High temporal resolution measurements of human brain activity can be performed by recording the electric potentials on the scalp surface (electroencephalography, EEG), or by recording the magnetic fields near the surface of the head (magnetoencephalography, MEG). The analysis of the data is problematic due to the fact that multiple neural generators may be simultaneously active and the potentials and magnetic fields from these sources are superimposed on the detectors. It is highly desirable to un-mix the data into signals representing the behaviors of the original individual generators. This general problem is called blind source separation and several recent techniques utilizing maximum entropy, minimum mutual information, and maximum likelihood estimation have been applied. These techniques have had much success in separating signals such as natural sounds or speech, but appear to be ineffective when applied to EEG or MEG signals. Many of these techniques implicitly assume that the source distributions have a large kurtosis, whereas an analysis of EEG/MEG signals reveals that the distributions are multimodal. This suggests that more effective separation techniques could be designed for EEG and MEG signals.",
      "code_url": null
    },
    "2411.07994v1": {
      "title": "Search for the X17 particle in $^{7}\\mathrm{Li}(\\mathrm{p},\\mathrm{e}^+ \\mathrm{e}^{-}) ^{8}\\mathrm{Be}$ processes with the MEG II detector",
      "url": "http://arxiv.org/abs/2411.07994v1",
      "authors": "The MEG II collaboration, K. Afanaciev, A. M. Baldini, S. Ban, H. Benmansour, G. Boca, P. W. Cattaneo, G. Cavoto, F. Cei, M. Chiappini, A. Corvaglia, G. Dal Maso, A. De Bari, M. De Gerone, L. Ferrari Barusso, M. Francesconi, L. Galli, G. Gallucci, F. Gatti, L. Gerritzen, F. Grancagnolo, E. G. Grandoni, M. Grassi, D. N. Grigoriev, M. Hildebrandt, F. Ignatov, F. Ikeda, T. Iwamoto, S. Karpov, P. -R. Kettle, N. Khomutov, A. Kolesnikov, N. Kravchuk, V. Krylov, N. Kuchinskiy, F. Leonetti, W. Li, V. Malyshev, A. Matsushita, M. Meucci, S. Mihara, W. Molzon, T. Mori, D. Nicol\u00f2, H. Nishiguchi, A. Ochi, W. Ootani, A. Oya, D. Palo, M. Panareo, A. Papa, V. Pettinacci, A. Popov, F. Renga, S. Ritt, M. Rossella, A. Rozhdestvensky. S. Scarpellini, P. Schwendimann, G. Signorelli, M. Takahashi, Y. Uchiyama, A. Venturini, B. Vitali, C. Voena, K. Yamamoto, R. Yokota, T. Yonemoto",
      "update_time": "2024-11-13",
      "abstract": "The observation of a resonance structure in the opening angle of the electron-positron pairs in the $^{7}$Li(p,\\ee) $^{8}$Be reaction was claimed and interpreted as the production and subsequent decay of a hypothetical particle (X17). Similar excesses, consistent with this particle, were later observed in processes involving $^{4}$He and $^{12}$C nuclei with the same experimental technique. The MEG II apparatus at PSI, designed to search for the $\u03bc^+ \\rightarrow \\mathrm{e}^+ \u03b3$ decay, can be exploited to investigate the existence of this particle and study its nature. Protons from a Cockroft-Walton accelerator, with an energy up to 1.1 MeV, were delivered on a dedicated Li-based target. The $\u03b3$ and the e$^{+}$e$^{-}$ pair emerging from the $^8\\mathrm{Be}^*$ transitions were studied with calorimeters and a spectrometer, featuring a broader angular acceptance than previous experiments. We present in this paper the analysis of a four-week data-taking in 2023 with a beam energy of 1080 keV, resulting in the excitation of two different resonances with Q-value \\SI{17.6}{\\mega\\electronvolt} and \\SI{18.1}{\\mega\\electronvolt}. No significant signal was found, and limits at \\SI{90}{\\percent} C.L. on the branching ratios (relative to the $\u03b3$ emission) of the two resonances to X17 were set, $R_{17.6} < 1.8 \\times 10^{-6} $ and $R_{18.1} < 1.2 \\times 10^{-5} $.",
      "code_url": null
    }
  },
  "neuroAI": {
    "2509.23896v2": {
      "title": "A Computational Perspective on NeuroAI and Synthetic Biological Intelligence",
      "url": "http://arxiv.org/abs/2509.23896v2",
      "authors": "Dhruvik Patel, Md Sayed Tanveer, Jesus Gonzalez-Ferrer, Alon Loeffler, Brett J. Kagan, Mohammed A. Mostajo-Radji, Ge Wang",
      "update_time": "2025-10-10",
      "abstract": "NeuroAI is an emerging field at the intersection of neuroscience and artificial intelligence, where insights from brain function guide the design of intelligent systems. A central area within this field is synthetic biological intelligence (SBI), which combines the adaptive learning properties of biological neural networks with engineered hardware and software. SBI systems provide a platform for modeling neural computation, developing biohybrid architectures, and enabling new forms of embodied intelligence. In this review, we organize the NeuroAI landscape into three interacting domains: hardware, software, and wetware. We outline computational frameworks that integrate biological and non-biological systems and highlight recent advances in organoid intelligence, neuromorphic computing, and neuro-symbolic learning. These developments collectively point toward a new class of systems that compute through interactions between living neural tissue and digital algorithms.",
      "code_url": null
    },
    "2212.04401v1": {
      "title": "A Rubric for Human-like Agents and NeuroAI",
      "url": "http://arxiv.org/abs/2212.04401v1",
      "authors": "Ida Momennejad",
      "update_time": "2022-12-09",
      "abstract": "Researchers across cognitive, neuro-, and computer sciences increasingly reference human-like artificial intelligence and neuroAI. However, the scope and use of the terms are often inconsistent. Contributed research ranges widely from mimicking behaviour, to testing machine learning methods as neurally plausible hypotheses at the cellular or functional levels, or solving engineering problems. However, it cannot be assumed nor expected that progress on one of these three goals will automatically translate to progress in others. Here a simple rubric is proposed to clarify the scope of individual contributions, grounded in their commitments to human-like behaviour, neural plausibility, or benchmark/engineering goals. This is clarified using examples of weak and strong neuroAI and human-like agents, and discussing the generative, corroborate, and corrective ways in which the three dimensions interact with one another. The author maintains that future progress in artificial intelligence will need strong interactions across the disciplines, with iterative feedback loops and meticulous validity tests, leading to both known and yet-unknown advances that may span decades to come.",
      "code_url": null
    },
    "2502.16238v1": {
      "title": "Brain-Model Evaluations Need the NeuroAI Turing Test",
      "url": "http://arxiv.org/abs/2502.16238v1",
      "authors": "Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi",
      "update_time": "2025-02-27",
      "abstract": "What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \\emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.",
      "code_url": null
    },
    "2301.09245v2": {
      "title": "Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks",
      "url": "http://arxiv.org/abs/2301.09245v2",
      "authors": "Feng-Lei Fan, Yingxin Li, Hanchuan Peng, Tieyong Zeng, Fei Wang",
      "update_time": "2023-03-14",
      "abstract": "Throughout history, the development of artificial intelligence, particularly artificial neural networks, has been open to and constantly inspired by the increasingly deepened understanding of the brain, such as the inspiration of neocognitron, which is the pioneering work of convolutional neural networks. Per the motives of the emerging field: NeuroAI, a great amount of neuroscience knowledge can help catalyze the next generation of AI by endowing a network with more powerful capabilities. As we know, the human brain has numerous morphologically and functionally different neurons, while artificial neural networks are almost exclusively built on a single neuron type. In the human brain, neuronal diversity is an enabling factor for all kinds of biological intelligent behaviors. Since an artificial network is a miniature of the human brain, introducing neuronal diversity should be valuable in terms of addressing those essential problems of artificial networks such as efficiency, interpretability, and memory. In this Primer, we first discuss the preliminaries of biological neuronal diversity and the characteristics of information transmission and processing in a biological neuron. Then, we review studies of designing new neurons for artificial networks. Next, we discuss what gains can neuronal diversity bring into artificial networks and exemplary applications in several important fields. Lastly, we discuss the challenges and future directions of neuronal diversity to explore the potential of NeuroAI.",
      "code_url": null
    },
    "2210.08340v3": {
      "title": "Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolution",
      "url": "http://arxiv.org/abs/2210.08340v3",
      "authors": "Anthony Zador, Sean Escola, Blake Richards, Bence \u00d6lveczky, Yoshua Bengio, Kwabena Boahen, Matthew Botvinick, Dmitri Chklovskii, Anne Churchland, Claudia Clopath, James DiCarlo, Surya Ganguli, Jeff Hawkins, Konrad Koerding, Alexei Koulakov, Yann LeCun, Timothy Lillicrap, Adam Marblestone, Bruno Olshausen, Alexandre Pouget, Cristina Savin, Terrence Sejnowski, Eero Simoncelli, Sara Solla, David Sussillo, Andreas S. Tolias, Doris Tsao",
      "update_time": "2023-02-24",
      "abstract": "Neuroscience has long been an essential driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI. A core component of this is the embodied Turing test, which challenges AI animal models to interact with the sensorimotor world at skill levels akin to their living counterparts. The embodied Turing test shifts the focus from those capabilities like game playing and language that are especially well-developed or uniquely human to those capabilities, inherited from over 500 million years of evolution, that are shared with all animals. Building models that can pass the embodied Turing test will provide a roadmap for the next generation of AI.",
      "code_url": null
    },
    "2411.18526v2": {
      "title": "NeuroAI for AI Safety",
      "url": "http://arxiv.org/abs/2411.18526v2",
      "authors": "Patrick Mineault, Niccol\u00f2 Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador",
      "update_time": "2025-04-04",
      "abstract": "As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.",
      "code_url": null
    },
    "2011.07464v2": {
      "title": "Predictive Coding, Variational Autoencoders, and Biological Connections",
      "url": "http://arxiv.org/abs/2011.07464v2",
      "authors": "Joseph Marino",
      "update_time": "2021-10-26",
      "abstract": "This paper reviews predictive coding, from theoretical neuroscience, and variational autoencoders, from machine learning, identifying the common origin and mathematical framework underlying both areas. As each area is prominent within its respective field, more firmly connecting these areas could prove useful in the dialogue between neuroscience and machine learning. After reviewing each area, we discuss two possible correspondences implied by this perspective: cortical pyramidal dendrites as analogous to (non-linear) deep networks and lateral inhibition as analogous to normalizing flows. These connections may provide new directions for further investigations in each field.",
      "code_url": null
    },
    "2507.02103v1": {
      "title": "What Neuroscience Can Teach AI About Learning in Continuously Changing Environments",
      "url": "http://arxiv.org/abs/2507.02103v1",
      "authors": "Daniel Durstewitz, Bruno Averbeck, Georgia Koppe",
      "update_time": "2025-07-04",
      "abstract": "Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.",
      "code_url": null
    },
    "2305.11275v2": {
      "title": "Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture",
      "url": "http://arxiv.org/abs/2305.11275v2",
      "authors": "Galen Pogoncheff, Jacob Granley, Michael Beyeler",
      "update_time": "2023-05-29",
      "abstract": "Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence.",
      "code_url": null
    },
    "2505.16080v1": {
      "title": "SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation",
      "url": "http://arxiv.org/abs/2505.16080v1",
      "authors": "Jiayue Liu, Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Kuo Yang, Xu Wang, Yang Wang",
      "update_time": "2025-05-23",
      "abstract": "Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.",
      "code_url": null
    }
  },
  "medical": {
    "2304.10517v3": {
      "title": "Segment Anything Model for Medical Image Analysis: an Experimental Study",
      "url": "http://arxiv.org/abs/2304.10517v3",
      "authors": "Maciej A. Mazurowski, Haoyu Dong, Hanxue Gu, Jichen Yang, Nicholas Konz, Yixin Zhang",
      "update_time": "2023-08-09",
      "abstract": "Training segmentation models for medical images continues to be challenging due to the limited availability of data annotations. Segment Anything Model (SAM) is a foundation model that is intended to segment user-defined objects of interest in an interactive manner. While the performance on natural images is impressive, medical image domains pose their own set of challenges. Here, we perform an extensive evaluation of SAM's ability to segment medical images on a collection of 19 medical imaging datasets from various modalities and anatomies. We report the following findings: (1) SAM's performance based on single prompts highly varies depending on the dataset and the task, from IoU=0.1135 for spine MRI to IoU=0.8650 for hip X-ray. (2) Segmentation performance appears to be better for well-circumscribed objects with prompts with less ambiguity and poorer in various other scenarios such as the segmentation of brain tumors. (3) SAM performs notably better with box prompts than with point prompts. (4) SAM outperforms similar methods RITM, SimpleClick, and FocalClick in almost all single-point prompt settings. (5) When multiple-point prompts are provided iteratively, SAM's performance generally improves only slightly while other methods' performance improves to the level that surpasses SAM's point-based performance. We also provide several illustrations for SAM's performance on all tested datasets, iterative segmentation, and SAM's behavior given prompt ambiguity. We conclude that SAM shows impressive zero-shot segmentation performance for certain medical imaging datasets, but moderate to poor performance for others. SAM has the potential to make a significant impact in automated medical image segmentation in medical imaging, but appropriate care needs to be applied when using it.",
      "code_url": null
    },
    "2311.02115v2": {
      "title": "Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging",
      "url": "http://arxiv.org/abs/2311.02115v2",
      "authors": "Emma A. M. Stanley, Raissa Souza, Anthony Winder, Vedant Gulve, Kimberly Amador, Matthias Wilms, Nils D. Forkert",
      "update_time": "2024-07-02",
      "abstract": "Artificial intelligence (AI) models trained using medical images for clinical tasks often exhibit bias in the form of disparities in performance between subgroups. Since not all sources of biases in real-world medical imaging data are easily identifiable, it is challenging to comprehensively assess how those biases are encoded in models, and how capable bias mitigation methods are at ameliorating performance disparities. In this article, we introduce a novel analysis framework for systematically and objectively investigating the impact of biases in medical images on AI models. We developed and tested this framework for conducting controlled in silico trials to assess bias in medical imaging AI using a tool for generating synthetic magnetic resonance images with known disease effects and sources of bias. The feasibility is showcased by using three counterfactual bias scenarios to measure the impact of simulated bias effects on a convolutional neural network (CNN) classifier and the efficacy of three bias mitigation strategies. The analysis revealed that the simulated biases resulted in expected subgroup performance disparities when the CNN was trained on the synthetic datasets. Moreover, reweighing was identified as the most successful bias mitigation strategy for this setup, and we demonstrated how explainable AI methods can aid in investigating the manifestation of bias in the model using this framework. Developing fair AI models is a considerable challenge given that many and often unknown sources of biases can be present in medical imaging datasets. In this work, we present a novel methodology to objectively study the impact of biases and mitigation strategies on deep learning pipelines, which can support the development of clinical AI that is robust and responsible.",
      "code_url": null
    },
    "2111.10480v6": {
      "title": "TransMorph: Transformer for unsupervised medical image registration",
      "url": "http://arxiv.org/abs/2111.10480v6",
      "authors": "Junyu Chen, Eric C. Frey, Yufan He, William P. Segars, Ye Li, Yong Du",
      "update_time": "2022-10-18",
      "abstract": "In the last decade, convolutional neural networks (ConvNets) have been a major focus of research in medical image analysis. However, the performances of ConvNets may be limited by a lack of explicit consideration of the long-range spatial relationships in an image. Recently Vision Transformer architectures have been proposed to address the shortcomings of ConvNets and have produced state-of-the-art performances in many medical imaging applications. Transformers may be a strong candidate for image registration because their substantially larger receptive field enables a more precise comprehension of the spatial correspondence between moving and fixed images. Here, we present TransMorph, a hybrid Transformer-ConvNet model for volumetric medical image registration. This paper also presents diffeomorphic and Bayesian variants of TransMorph: the diffeomorphic variants ensure the topology-preserving deformations, and the Bayesian variant produces a well-calibrated registration uncertainty estimate. We extensively validated the proposed models using 3D medical images from three applications: inter-patient and atlas-to-patient brain MRI registration and phantom-to-CT registration. The proposed models are evaluated in comparison to a variety of existing registration methods and Transformer architectures. Qualitative and quantitative results demonstrate that the proposed Transformer-based model leads to a substantial performance improvement over the baseline methods, confirming the effectiveness of Transformers for medical image registration.",
      "code_url": null
    },
    "2212.08228v2": {
      "title": "SADM: Sequence-Aware Diffusion Model for Longitudinal Medical Image Generation",
      "url": "http://arxiv.org/abs/2212.08228v2",
      "authors": "Jee Seok Yoon, Chenghao Zhang, Heung-Il Suk, Jia Guo, Xiaoxiao Li",
      "update_time": "2023-06-19",
      "abstract": "Human organs constantly undergo anatomical changes due to a complex mix of short-term (e.g., heartbeat) and long-term (e.g., aging) factors. Evidently, prior knowledge of these factors will be beneficial when modeling their future state, i.e., via image generation. However, most of the medical image generation tasks only rely on the input from a single image, thus ignoring the sequential dependency even when longitudinal data is available. Sequence-aware deep generative models, where model input is a sequence of ordered and timestamped images, are still underexplored in the medical imaging domain that is featured by several unique challenges: 1) Sequences with various lengths; 2) Missing data or frame, and 3) High dimensionality. To this end, we propose a sequence-aware diffusion model (SADM) for the generation of longitudinal medical images. Recently, diffusion models have shown promising results in high-fidelity image generation. Our method extends this new technique by introducing a sequence-aware transformer as the conditional module in a diffusion model. The novel design enables learning longitudinal dependency even with missing data during training and allows autoregressive generation of a sequence of images during inference. Our extensive experiments on 3D longitudinal medical images demonstrate the effectiveness of SADM compared with baselines and alternative methods. The code is available at https://github.com/ubc-tea/SADM-Longitudinal-Medical-Image-Generation.",
      "code_url": null
    },
    "2310.15371v1": {
      "title": "Vicinal Feature Statistics Augmentation for Federated 3D Medical Volume Segmentation",
      "url": "http://arxiv.org/abs/2310.15371v1",
      "authors": "Yongsong Huang, Wanqing Xie, Mingzhen Li, Mingmei Cheng, Jinzhou Wu, Weixiao Wang, Jane You, Xiaofeng Liu",
      "update_time": "2023-10-25",
      "abstract": "Federated learning (FL) enables multiple client medical institutes collaboratively train a deep learning (DL) model with privacy protection. However, the performance of FL can be constrained by the limited availability of labeled data in small institutes and the heterogeneous (i.e., non-i.i.d.) data distribution across institutes. Though data augmentation has been a proven technique to boost the generalization capabilities of conventional centralized DL as a \"free lunch\", its application in FL is largely underexplored. Notably, constrained by costly labeling, 3D medical segmentation generally relies on data augmentation. In this work, we aim to develop a vicinal feature-level data augmentation (VFDA) scheme to efficiently alleviate the local feature shift and facilitate collaborative training for privacy-aware FL segmentation. We take both the inner- and inter-institute divergence into consideration, without the need for cross-institute transfer of raw data or their mixup. Specifically, we exploit the batch-wise feature statistics (e.g., mean and standard deviation) in each institute to abstractly represent the discrepancy of data, and model each feature statistic probabilistically via a Gaussian prototype, with the mean corresponding to the original statistic and the variance quantifying the augmentation scope. From the vicinal risk minimization perspective, novel feature statistics can be drawn from the Gaussian distribution to fulfill augmentation. The variance is explicitly derived by the data bias in each individual institute and the underlying feature statistics characterized by all participating institutes. The added-on VFDA consistently yielded marked improvements over six advanced FL methods on both 3D brain tumor and cardiac segmentation.",
      "code_url": null
    },
    "1803.08691v1": {
      "title": "Deep learning and its application to medical image segmentation",
      "url": "http://arxiv.org/abs/1803.08691v1",
      "authors": "Holger R. Roth, Chen Shen, Hirohisa Oda, Masahiro Oda, Yuichiro Hayashi, Kazunari Misawa, Kensaku Mori",
      "update_time": "2018-04-10",
      "abstract": "One of the most common tasks in medical imaging is semantic segmentation. Achieving this segmentation automatically has been an active area of research, but the task has been proven very challenging due to the large variation of anatomy across different patients. However, recent advances in deep learning have made it possible to significantly improve the performance of image recognition and semantic segmentation methods in the field of computer vision. Due to the data driven approaches of hierarchical feature learning in deep learning frameworks, these advances can be translated to medical images without much difficulty. Several variations of deep convolutional neural networks have been successfully applied to medical images. Especially fully convolutional architectures have been proven efficient for segmentation of 3D medical images. In this article, we describe how to build a 3D fully convolutional network (FCN) that can process 3D images in order to produce automatic semantic segmentations. The model is trained and evaluated on a clinical computed tomography (CT) dataset and shows state-of-the-art performance in multi-organ segmentation.",
      "code_url": null
    },
    "2407.03548v1": {
      "title": "HiDiff: Hybrid Diffusion Framework for Medical Image Segmentation",
      "url": "http://arxiv.org/abs/2407.03548v1",
      "authors": "Tao Chen, Chenhui Wang, Zhihao Chen, Yiming Lei, Hongming Shan",
      "update_time": "2025-09-01",
      "abstract": "Medical image segmentation has been significantly advanced with the rapid development of deep learning (DL) techniques. Existing DL-based segmentation models are typically discriminative; i.e., they aim to learn a mapping from the input image to segmentation masks. However, these discriminative methods neglect the underlying data distribution and intrinsic class characteristics, suffering from unstable feature space. In this work, we propose to complement discriminative segmentation methods with the knowledge of underlying data distribution from generative models. To that end, we propose a novel hybrid diffusion framework for medical image segmentation, termed HiDiff, which can synergize the strengths of existing discriminative segmentation models and new generative diffusion models. HiDiff comprises two key components: discriminative segmentor and diffusion refiner. First, we utilize any conventional trained segmentation models as discriminative segmentor, which can provide a segmentation mask prior for diffusion refiner. Second, we propose a novel binary Bernoulli diffusion model (BBDM) as the diffusion refiner, which can effectively, efficiently, and interactively refine the segmentation mask by modeling the underlying data distribution. Third, we train the segmentor and BBDM in an alternate-collaborative manner to mutually boost each other. Extensive experimental results on abdomen organ, brain tumor, polyps, and retinal vessels segmentation datasets, covering four widely-used modalities, demonstrate the superior performance of HiDiff over existing medical segmentation algorithms, including the state-of-the-art transformer- and diffusion-based ones. In addition, HiDiff excels at segmenting small objects and generalizing to new datasets. Source codes are made available at https://github.com/takimailto/HiDiff.",
      "code_url": null
    },
    "1808.05205v1": {
      "title": "Building medical image classifiers with very limited data using segmentation networks",
      "url": "http://arxiv.org/abs/1808.05205v1",
      "authors": "Ken C. L. Wong, Tanveer Syeda-Mahmood, Mehdi Moradi",
      "update_time": "2018-08-16",
      "abstract": "Deep learning has shown promising results in medical image analysis, however, the lack of very large annotated datasets confines its full potential. Although transfer learning with ImageNet pre-trained classification models can alleviate the problem, constrained image sizes and model complexities can lead to unnecessary increase in computational cost and decrease in performance. As many common morphological features are usually shared by different classification tasks of an organ, it is greatly beneficial if we can extract such features to improve classification with limited samples. Therefore, inspired by the idea of curriculum learning, we propose a strategy for building medical image classifiers using features from segmentation networks. By using a segmentation network pre-trained on similar data as the classification task, the machine can first learn the simpler shape and structural concepts before tackling the actual classification problem which usually involves more complicated concepts. Using our proposed framework on a 3D three-class brain tumor type classification problem, we achieved 82% accuracy on 191 testing samples with 91 training samples. When applying to a 2D nine-class cardiac semantic level classification problem, we achieved 86% accuracy on 263 testing samples with 108 training samples. Comparisons with ImageNet pre-trained classifiers and classifiers trained from scratch are presented.",
      "code_url": null
    },
    "2109.02722v2": {
      "title": "Automatic Landmarks Correspondence Detection in Medical Images with an Application to Deformable Image Registration",
      "url": "http://arxiv.org/abs/2109.02722v2",
      "authors": "Monika Grewal, Jan Wiersma, Henrike Westerveld, Peter A. N. Bosman, Tanja Alderliesten",
      "update_time": "2023-02-28",
      "abstract": "Purpose: Deformable Image Registration (DIR) can benefit from additional guidance using corresponding landmarks in the images. However, the benefits thereof are largely understudied, especially due to the lack of automatic landmark detection methods for three-dimensional (3D) medical images.   Approach: We present a Deep Convolutional Neural Network (DCNN), called DCNN-Match, that learns to predict landmark correspondences in 3D images in a self-supervised manner. We trained DCNN-Match on pairs of Computed Tomography (CT) scans containing simulated deformations. We explored five variants of DCNN-Match that use different loss functions and assessed their effect on the spatial density of predicted landmarks and the associated matching errors. We also tested DCNN-Match variants in combination with the open-source registration software Elastix to assess the impact of predicted landmarks in providing additional guidance to DIR.   Results: We tested our approach on lower-abdominal CT scans from cervical cancer patients: 121 pairs containing simulated deformations and 11 pairs demonstrating clinical deformations. The results showed significant improvement in DIR performance when landmark correspondences predicted by DCNN-Match were used in the case of simulated (p = $0e^0$) as well as clinical deformations (p = 0.030). We also observed that the spatial density of the automatic landmarks with respect to the underlying deformation affect the extent of improvement in DIR. Finally, DCNN-Match was found to generalize to Magnetic Resonance Imaging (MRI) scans without requiring retraining, indicating easy applicability to other datasets.   Conclusions: DCNN-Match learns to predict landmark correspondences in 3D medical images in a self-supervised manner, which can improve DIR performance.",
      "code_url": null
    },
    "2006.16806v1": {
      "title": "Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation",
      "url": "http://arxiv.org/abs/2006.16806v1",
      "authors": "Yingda Xia, Dong Yang, Zhiding Yu, Fengze Liu, Jinzheng Cai, Lequan Yu, Zhuotun Zhu, Daguang Xu, Alan Yuille, Holger Roth",
      "update_time": "2020-07-01",
      "abstract": "Although having achieved great success in medical image segmentation, deep learning-based approaches usually require large amounts of well-annotated data, which can be extremely expensive in the field of medical image analysis. Unlabeled data, on the other hand, is much easier to acquire. Semi-supervised learning and unsupervised domain adaptation both take the advantage of unlabeled data, and they are closely related to each other. In this paper, we propose uncertainty-aware multi-view co-training (UMCT), a unified framework that addresses these two tasks for volumetric medical image segmentation. Our framework is capable of efficiently utilizing unlabeled data for better performance. We firstly rotate and permute the 3D volumes into multiple views and train a 3D deep network on each view. We then apply co-training by enforcing multi-view consistency on unlabeled data, where an uncertainty estimation of each view is utilized to achieve accurate labeling. Experiments on the NIH pancreas segmentation dataset and a multi-organ segmentation dataset show state-of-the-art performance of the proposed framework on semi-supervised medical image segmentation. Under unsupervised domain adaptation settings, we validate the effectiveness of this work by adapting our multi-organ segmentation model to two pathological organs from the Medical Segmentation Decathlon Datasets. Additionally, we show that our UMCT-DA model can even effectively handle the challenging situation where labeled source data is inaccessible, demonstrating strong potentials for real-world applications.",
      "code_url": null
    }
  }
}