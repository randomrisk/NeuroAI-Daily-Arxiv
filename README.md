<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2026-02-10</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-08</td>
<td style='padding: 8px;'>Back to Physics: Operator-Guided Generative Paths for SMS MRI Reconstruction</td>
<td style='padding: 6px;'>Zhibo Chen, Yu Guan, Yajuan Huang, Chaoqi Chen, XiangJi, Qiuyun Fan, Dong Liang, Qiegen Liu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07820v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Simultaneous multi-slice (SMS) imaging with in-plane undersampling enables highly accelerated MRI but yields a strongly coupled inverse problem with deterministic inter-slice interference and missing k-space data. Most diffusion-based reconstructions are formulated around Gaussian-noise corruption and rely on additional consistency steps to incorporate SMS physics, which can be mismatched to the operator-governed degradations in SMS acquisition. We propose an operator-guided framework that models the degradation trajectory using known acquisition operators and inverts this process via deterministic updates. Within this framework, we introduce an operator-conditional dual-stream interaction network (OCDI-Net) that explicitly disentangles target-slice content from inter-slice interference and predicts structured degradations for operator-aligned inversion, and we instantiate reconstruction as a two-stage chained inference procedure that performs SMS slice separation followed by in-plane completion. Experiments on fastMRI brain data and prospectively acquired in vivo diffusion MRI data demonstrate improved fidelity and reduced slice leakage over conventional and learning-based SMS reconstructions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>Alteration of the Brains Microbiome and Neuroinflammation Associated with Ventricular Catheters</td>
<td style='padding: 6px;'>Zihan Zhu, Dipankar Biswas, Michael Meggyesy, Di Cao, Gwendolyn Williams, Richard Um, Farzad Maroufi, Ryan P. Lee, Jun Hua, Liangliang Zhang, Jeffrey Capadona, Horst V. Recum, Mark G. Luciano</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07648v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Background and Objectives: Proximal catheter obstruction is the leading cause of ventriculoperitoneal shunt failure, yet the biological triggers of peri-catheter inflammation and tissue ingrowth remain poorly defined. Evidence of bacterial ribosomal RNA in human brain tissue suggests that low-biomass microbial exposure may influence the inflammatory microenvironment surrounding implants. This study examined if microbial signal is detectable in unaltered brain tissue and if catheter implantation produces microbial shifts relevant to shunt dysfunction. Methods: Twenty-nine female mice were assigned to unaltered control (UC), trauma control (TC), plain silicone catheter (PSC), or antibiotic-impregnated catheter (AIC) groups. Brain and cecum tissues were harvested at postoperative days 7 and 28 for 16S rRNA sequencing. Microbial composition and predicted functional pathways were analyzed. A separate cohort underwent longitudinal MRI to assess edema, glial scar formation, and macrophage-associated susceptibility signal. Results: Low-level microbial signal was detected in unaltered brain tissue. Catheter implantation induced material-dependent shifts in brain-associated microbial composition. PSC was associated with enrichment of pro-inflammatory taxa, whereas AIC favored immune-regulatory taxa. Predicted short-chain fatty acid biosynthesis was highest in AIC and lowest in PSC, while predicted lipopolysaccharide biosynthesis trended higher in PSC. MRI showed similar edema resolution but higher macrophage-associated susceptibility signal in PSC animals. Conclusion: Intracranial catheter implantation produces material-dependent shifts in low-biomass brain-associated microbial signal that parallel differential neuroimmune activation. These findings suggest catheter material may shape a biologically relevant peri-catheter niche with implications for chronic gliosis and proximal shunt obstruction.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>How does longer temporal context enhance multimodal narrative video processing in the brain?</td>
<td style='padding: 6px;'>Prachi Jindal, Anant Khandelwal, Manish Gupta, Bapi S. Raju, Subba Reddy Oota, Tanmoy Chakraborty</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07570v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding how humans and artificial intelligence systems process complex narrative videos is a fundamental challenge at the intersection of neuroscience and machine learning. This study investigates how the temporal context length of video clips (3--12 s clips) and the narrative-task prompting shape brain-model alignment during naturalistic movie watching. Using fMRI recordings from participants viewing full-length movies, we examine how brain regions sensitive to narrative context dynamically represent information over varying timescales and how these neural patterns align with model-derived features. We find that increasing clip duration substantially improves brain alignment for multimodal large language models (MLLMs), whereas unimodal video models show little to no gain. Further, shorter temporal windows align with perceptual and early language regions, while longer windows preferentially align higher-order integrative regions, mirrored by a layer-to-cortex hierarchy in MLLMs. Finally, narrative-task prompts (multi-scene summary, narrative summary, character motivation, and event boundary detection) elicit task-specific, region-dependent brain alignment patterns and context-dependent shifts in clip-level tuning in higher-order regions. Together, our results position long-form narrative movies as a principled testbed for probing biologically relevant temporal integration and interpretable representations in long-context MLLMs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>Linguistic properties and model scale in brain encoding: from small to compressed language models</td>
<td style='padding: 6px;'>Subba Reddy Oota, Vijay Rowtula, Satya Sai Srinath Namburi, Khushbu Pahwa, Anant Khandelwal, Manish Gupta, Tanmoy Chakraborty, Bapi S. Raju</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07547v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent work has shown that scaling large language models (LLMs) improves their alignment with human brain activity, yet it remains unclear what drives these gains and which representational properties are responsible. Although larger models often yield better task performance and brain alignment, they are increasingly difficult to analyze mechanistically. This raises a fundamental question: what is the minimal model capacity required to capture brain-relevant representations? To address this question, we systematically investigate how constraining model scale and numerical precision affects brain alignment. We compare full-precision LLMs, small language models (SLMs), and compressed variants (quantized and pruned) by predicting fMRI responses during naturalistic language comprehension. Across model families up to 14B parameters, we find that 3B SLMs achieve brain predictivity indistinguishable from larger LLMs, whereas 1B models degrade substantially, particularly in semantic language regions. Brain alignment is remarkably robust to compression: most quantization and pruning methods preserve neural predictivity, with GPTQ as a consistent exception. Linguistic probing reveals a dissociation between task performance and brain predictivity: compression degrades discourse, syntax, and morphology, yet brain predictivity remains largely unchanged. Overall, brain alignment saturates at modest model scales and is resilient to compression, challenging common assumptions about neural scaling and motivating compact models for brain-aligned language modeling.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>Training-Driven Representational Geometry Modularization Predicts Brain Alignment in Language Models</td>
<td style='padding: 6px;'>Yixuan Liu, Zhiyuan Ma, Likai Tang, Runmin Gan, Xinche Zhang, Jinhao Li, Chao Xie, Sen Song</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07539v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>How large language models (LLMs) align with the neural representation and computation of human language is a central question in cognitive science. Using representational geometry as a mechanistic lens, we addressed this by tracking entropy, curvature, and fMRI encoding scores throughout Pythia (70M-1B) training. We identified a geometric modularization where layers self-organize into stable low- and high-complexity clusters. The low-complexity module, characterized by reduced entropy and curvature, consistently better predicted human language network activity. This alignment followed heterogeneous spatial-temporal trajectories: rapid and stable in temporal regions (AntTemp, PostTemp), but delayed and dynamic in frontal areas (IFG, IFGorb). Crucially, reduced curvature remained a robust predictor of model-brain alignment even after controlling for training progress, an effect that strengthened with model scale. These results links training-driven geometric reorganization to temporal-frontal functional specialization, suggesting that representational smoothing facilitates neural-like linguistic processing.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis</td>
<td style='padding: 6px;'>Md Sazidur Rahman, Kjersti Engan, Kathinka Dæhli Kurz, Mahdieh Khanmohammadi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07535v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>CA-YOLO: Cross Attention Empowered YOLO for Biomimetic Localization</td>
<td style='padding: 6px;'>Zhen Zhang, Qing Zhao, Xiuhe Li, Cheng Wang, Guoqiang Zhu, Yu Zhang, Yining Huo, Hongyi Yu, Yi Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07523v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In modern complex environments, achieving accurate and efficient target localization is essential in numerous fields. However, existing systems often face limitations in both accuracy and the ability to recognize small targets. In this study, we propose a bionic stabilized localization system based on CA-YOLO, designed to enhance both target localization accuracy and small target recognition capabilities. Acting as the "brain" of the system, the target detection algorithm emulates the visual focusing mechanism of animals by integrating bionic modules into the YOLO backbone network. These modules include the introduction of a small target detection head and the development of a Characteristic Fusion Attention Mechanism (CFAM). Furthermore, drawing inspiration from the human Vestibulo-Ocular Reflex (VOR), a bionic pan-tilt tracking control strategy is developed, which incorporates central positioning, stability optimization, adaptive control coefficient adjustment, and an intelligent recapture function. The experimental results show that CA-YOLO outperforms the original model on standard datasets (COCO and VisDrone), with average accuracy metrics improved by 3.94%and 4.90%, respectively.Further time-sensitive target localization experiments validate the effectiveness and practicality of this bionic stabilized localization system.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>Learning Brain Representation with Hierarchical Visual Embeddings</td>
<td style='padding: 6px;'>Jiawen Zheng, Haonan Jia, Ming Li, Yuhui Zheng, Yufeng Zeng, Yang Gao, Chen Liang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07495v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Decoding visual representations from brain signals has attracted significant attention in both neuroscience and artificial intelligence. However, the degree to which brain signals truly encode visual information remains unclear. Current visual decoding approaches explore various brain-image alignment strategies, yet most emphasize high-level semantic features while neglecting pixel-level details, thereby limiting our understanding of the human visual system. In this paper, we propose a brain-image alignment strategy that leverages multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multi-scale visual representations, while employing a contrastive learning objective to achieve effective alignment between brain signals and visual embeddings. Furthermore, we introduce a Fusion Prior, which learns a stable mapping on large-scale visual data and subsequently matches brain features to this pre-trained prior, thereby enhancing distributional consistency across modalities. Extensive quantitative and qualitative experiments demonstrate that our method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-06</td>
<td style='padding: 8px;'>Extracting Root-Causal Brain Activity Driving Psychopathology from Resting State fMRI</td>
<td style='padding: 6px;'>Eric V. Strobl</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07233v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroimaging studies of psychiatric disorders often correlate imaging patterns with diagnostic labels or composite symptom scores, yielding diffuse associations that obscure underlying mechanisms. We instead seek to identify root-causal maps -- localized BOLD disturbances that initiate pathological cascades -- and to link them selectively to symptom dimensions. We introduce a bilevel structural causal model that connects between-subject symptom structure to within-subject resting-state fMRI via independent latent sources with localized direct effects. Based on this model, we develop SOURCE (Symptom-Oriented Uncovering of Root-Causal Elements), a procedure that links interpretable symptom axes to a parsimonious set of localized drivers. Experiments show that SOURCE recovers localized maps consistent with root-causal BOLD drivers and increases interpretability and anatomical specificity relative to existing comparators.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-06</td>
<td style='padding: 8px;'>DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages</td>
<td style='padding: 6px;'>Yongheng Sun, Jun Shu, Jianhua Ma, Fan Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07174v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurate segmentation of brain tissues from MRI scans is critical for neuroscience and clinical applications, but achieving consistent performance across the human lifespan remains challenging due to dynamic, age-related changes in brain appearance and morphology. While prior work has sought to mitigate these shifts by using self-supervised regularization with paired longitudinal data, such data are often unavailable in practice. To address this, we propose \emph{DuMeta++}, a dual meta-learning framework that operates without paired longitudinal data. Our approach integrates: (1) meta-feature learning to extract age-agnostic semantic representations of spatiotemporally evolving brain structures, and (2) meta-initialization learning to enable data-efficient adaptation of the segmentation model. Furthermore, we propose a memory-bank-based class-aware regularization strategy to enforce longitudinal consistency without explicit longitudinal supervision. We theoretically prove the convergence of our DuMeta++, ensuring stability. Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings demonstrate that DuMeta++ outperforms existing methods in cross-age generalization. Code will be available at https://github.com/ladderlab-xjtu/DuMeta++.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-06</td>
<td style='padding: 8px;'>A first realization of reinforcement learning-based closed-loop EEG-TMS</td>
<td style='padding: 6px;'>Dania Humaidan, Jiahua Xu, Jing Chen, Christoph Zrenner, David Emanuel Vetter, Laura Marzetti, Paolo Belardinelli, Timo Roine, Risto J. Ilmoniemi, Gian Luca Romani, Ulf Zieman</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.06907v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Background: Transcranial magnetic stimulation (TMS) is a powerful tool to investigate neurophysiology of the human brain and treat brain disorders. Traditionally, therapeutic TMS has been applied in a one-size-fits-all approach, disregarding inter- and intra-individual differences. Brain state-dependent EEG-TMS, such as coupling TMS with a pre-specified phase of the sensorimotor mu-rhythm, enables the induction of differential neuroplastic effects depending on the targeted phase. But this approach is still user-dependent as it requires defining an a-priori target phase. Objectives: To present a first realization of a machine-learning-based, closed-loop real-time EEG-TMS setup to identify user-independently the individual mu-rhythm phase associated with high- vs. low-corticospinal excitability states. Methods: We applied EEG-TMS to 25 participants targeting the supplementary motor area-primary motor cortex network and used a reinforcement learning algorithm to identify the mu-rhythm phase associated with high- vs. low corticospinal excitability. We employed linear mixed effects models and Bayesian analysis to determine effects of reinforced learning on corticospinal excitability indexed by motor evoked potential amplitude, and functional connectivity indexed by the imaginary part of resting-state EEG coherence. Results: Reinforcement learning effectively identified the mu-rhythm phase associated with high- vs. low-excitability states, and their repetitive stimulation resulted in long-term increases vs. decreases in functional connectivity in the stimulated sensorimotor network. Conclusions: We demonstrated for the first time the feasibility of closed-loop EEG-TMS in humans, a critical step towards individualized treatment of brain disorders.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-06</td>
<td style='padding: 8px;'>EEG Emotion Classification Using an Enhanced Transformer-CNN-BiLSTM Architecture with Dual Attention Mechanisms</td>
<td style='padding: 6px;'>S M Rakib UI Karim, Wenyi Lu, Diponkor Bala, Rownak Ara Rasul, Sean Goggins</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.06411v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG)-based emotion recognition plays a critical role in affective computing and emerging decision-support systems, yet remains challenging due to high-dimensional, noisy, and subject-dependent signals. This study investigates whether hybrid deep learning architectures that integrate convolutional, recurrent, and attention-based components can improve emotion classification performance and robustness in EEG data. We propose an enhanced hybrid model that combines convolutional feature extraction, bidirectional temporal modeling, and self-attention mechanisms with regularization strategies to mitigate overfitting. Experiments conducted on a publicly available EEG dataset spanning three emotional states (neutral, positive, and negative) demonstrate that the proposed approach achieves state-of-the-art classification performance, significantly outperforming classical machine learning and neural baselines. Statistical tests confirm the robustness of these performance gains under cross-validation. Feature-level analyses further reveal that covariance-based EEG features contribute most strongly to emotion discrimination, highlighting the importance of inter-channel relationships in affective modeling. These findings suggest that carefully designed hybrid architectures can effectively balance predictive accuracy, robustness, and interpretability in EEG-based emotion recognition, with implications for applied affective computing and human-centered intelligent systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-06</td>
<td style='padding: 8px;'>AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders</td>
<td style='padding: 6px;'>Georgii Aparin, Tasnima Sadekova, Alexey Rukhovich, Assel Yermekova, Laida Kushnareva, Vadim Popov, Kristian Kuznetsov, Irina Piontkovskaya</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.05027v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-04</td>
<td style='padding: 8px;'>NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Image</td>
<td style='padding: 6px;'>Yan Chen, Jie Peng, Moajjem Hossain Chowdhury, Tianlong Chen, Yunmei Liu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.04769v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurate and timely seizure detection from Electroencephalography (EEG) is critical for clinical intervention, yet manual review of long-term recordings is labor-intensive. Recent efforts to encode EEG signals into large language models (LLMs) show promise in handling neural signals across diverse patients, but two significant challenges remain: (1) multi-channel heterogeneity, as seizure-relevant information varies substantially across EEG channels, and (2) computing inefficiency, as the EEG signals need to be encoded into a massive number of tokens for the prediction. To address these issues, we draw the EEG signal and propose the novel NeuroCanvas framework. Specifically, NeuroCanvas consists of two modules: (i) The Entropy-guided Channel Selector (ECS) selects the seizure-relevant channels input to LLM and (ii) the following Canvas of Neuron Signal (CNS) converts selected multi-channel heterogeneous EEG signals into structured visual representations. The ECS module alleviates the multi-channel heterogeneity issue, and the CNS uses compact visual tokens to represent the EEG signals that improve the computing efficiency. We evaluate NeuroCanvas across multiple seizure detection datasets, demonstrating a significant improvement of $20\%$ in F1 score and reductions of $88\%$ in inference latency. These results highlight NeuroCanvas as a scalable and effective solution for real-time and resource-efficient seizure detection in clinical practice.The code will be released at https://github.com/Yanchen30247/seizure_detect.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-04</td>
<td style='padding: 8px;'>HFMCA: Orthonormal Feature Learning for EEG-based Brain Decoding</td>
<td style='padding: 6px;'>Yinghao Wang, Lintao Xu, Shujian Yu, Enzo Tartaglione, Van-Tam Nguyen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.04681v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) analysis is critical for brain-computer interfaces and neuroscience, but the intrinsic noise and high dimensionality of EEG signals hinder effective feature learning. We propose a self-supervised framework based on the Hierarchical Functional Maximal Correlation Algorithm (HFMCA), which learns orthonormal EEG representations by enforcing feature decorrelation and reducing redundancy. This design enables robust capture of essential brain dynamics for various EEG recognition tasks. We validate HFMCA on two benchmark datasets, SEED and BCIC-2A, where pretraining with HFMCA consistently outperforms competitive self-supervised baselines, achieving notable gains in classification accuracy. Across diverse EEG tasks, our method demonstrates superior cross-subject generalization under leave-one-subject-out validation, advancing state-of-the-art by 2.71\% on SEED emotion recognition and 2.57\% on BCIC-2A motor imagery classification.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-04</td>
<td style='padding: 8px;'>A Multimodal fNIRS-EEG Dataset for Unilateral Limb Motor Imagery</td>
<td style='padding: 6px;'>Lufeng Feng, Baomin Xu, Haoran Zhang, Bihai Lin, Zuxuan Deng, Sidi Tao, Chenyu Liu, Shifan Jia, Li Duan, Ziyu Jia</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.04299v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Unilateral limb motor imagery (MI) plays an important role in upper-limb motor rehabilitation and precise control of external devices, and places higher demands on spatial resolution. However, most existing public datasets focus on binary- or four-class left-right limb paradigms that mainly exploit coarse hemispheric lateralization, and there is still a lack of multimodal datasets that simultaneously record EEG and fNIRS for unilateral multi-directional MI. To address this gap, we constructed MIND, a public motor imagery fNIRS-EEG dataset based on a four-class directional MI paradigm of the right upper limb. The dataset includes 64-channel EEG recordings (1000 Hz) and 51-channel fNIRS recordings (47.62 Hz) from 30 participants (12 females, 18 males; aged 19.0-25.0 years). We analyse the spatiotemporal characteristics of EEG spectral power and hemodynamic responses, and validate the potential advantages of hybrid fNIRS-EEG BCIs in terms of classification accuracy. We expect that this dataset will facilitate the evaluation and comparison of neuroimaging analysis and decoding methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-03</td>
<td style='padding: 8px;'>Heterogeneity dominates irreversibility in random Markov models</td>
<td style='padding: 6px;'>Faheem Mosam, Eric De Giuli</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.04905v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We introduce a two-parameter ensemble of random discrete-time Markov models that simultaneously captures critical slowing down and broken detailed balance. Extending a previously studied heterogeneous Markov ensemble, we incorporate correlations between forward and backward transition rates through a single asymmetry parameter $γ$, while heterogeneity is controlled by $ε$. Using results from random matrix theory, we identify a critical locus $ε_c(γ,N)$ at which relaxation times diverge and spectral universality breaks down. We characterize the behavior of entropy production, predictive information, and relaxation dynamics across the ensemble, showing that many observables depend strongly on heterogeneity but only weakly on asymmetry, except near the symmetric limit. Applying maximum-likelihood inference to human fMRI and EEG data, we find that both modalities operate near the predicted critical locus and occupy a similar region of the $ε-γ$ plane, supporting a super-universality of human brain dynamics. While ensemble averages are well captured by the null model, empirical data exhibit substantially enhanced variability, indicating subject-specific structure beyond random expectations. Our results unify criticality and nonequilibrium measures within a single framework and clarify their intertwined role in the analysis of complex biological dynamics.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-03</td>
<td style='padding: 8px;'>Cross-Frequency Bispectral EEG Analysis of Reach-to-Grasp Planning and Execution</td>
<td style='padding: 6px;'>Sima Ghafoori, Anna Cetera, Ali Rabiee, MH Farhadi, Rahul Singh, Mariusz Furmanek, Yalda Shahriari, Reza Abiri</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.04018v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neural control of grasping arises from nonlinear interactions across multiple brain rhythms, yet EEG-based motor decoding has largely relied on linear, second-order spectral features. Here, we examine whether higher-order cross-frequency dynamics distinguish motor planning from execution during natural reach-to-grasp behavior. EEG was recorded in a cue-based paradigm during executed precision and power grips, enabling stage-resolved analysis of preparatory and execution-related neural activity.   Cross-frequency bispectral analysis was used to compute bicoherence matrices across canonical frequency band pairs, from which magnitude- and phase-based features were extracted. Classification, permutation-based feature selection, and within-subject statistical testing showed that execution is characterized by substantially stronger and more discriminative nonlinear coupling than planning, with dominant contributions from beta- and gamma-driven interactions. In contrast, decoding of precision versus power grips achieved comparable performance during planning and execution, indicating that grasp-type representations emerge during planning and persist into execution.   Spatial and spectral analyses further revealed that informative bispectral features reflect coordinated activity across prefrontal, central, and occipital regions. Despite substantial feature redundancy, effective dimensionality reduction preserved decoding performance. Together, these findings demonstrate that nonlinear cross-frequency coupling provides an interpretable and robust marker of motor planning and execution, extending bispectral EEG analysis to ecologically valid grasping and supporting its relevance for brain-computer interfaces and neuroprosthetic control.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-03</td>
<td style='padding: 8px;'>A Multi-decoder Neural Tracking Method for Accurately Predicting Speech Intelligibility</td>
<td style='padding: 6px;'>Rien Sonck, Bernd Accou, Tom Francart, Jonas Vanthornhout</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.03624v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Objective: EEG-based methods can predict speech intelligibility, but their accuracy and robustness lag behind behavioral tests, which typically show test-retest differences under 1 dB. We introduce the multi-decoder method to predict speech reception thresholds (SRTs) from EEG recordings, enabling objective assessment for populations unable to perform behavioral tests; such as those with disorders of consciousness or during hearing aid fitting. Approach: The method aggregates data from hundreds of decoders, each trained on different speech features and EEG preprocessing setups to quantify neural tracking (NT) of speech signals. Using data from 39 participants (ages 18-24), we recorded 29 minutes of EEG per person while they listened to speech at six signal-to-noise ratios and a quiet story. NT values were combined into a high-dimensional feature vector per subject, and a support vector regression model was trained to predict SRTs from these vectors. Main Result: Predictions correlated significantly with behavioral SRTs (r = 0.647, p < 0.001; NRMSE = 0.19), with all differences under 1 dB. SHAP analysis showed theta/delta bands and early lags had slightly greater influence. Using pretrained subject-independent decoders reduced required EEG data collection to 15 minutes (3 minutes of story, 12 minutes across six SNR conditions) without losing accuracy.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-03</td>
<td style='padding: 8px;'>Systematic review of self-supervised foundation models for brain network representation using electroencephalography</td>
<td style='padding: 6px;'>Hannah Portmann, Yosuke Morishima</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.03269v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Automated analysis of electroencephalography (EEG) has recently undergone a paradigm shift. The introduction of transformer architectures and self-supervised pretraining (SSL) has led to the development of EEG foundation models. These models are pretrained on large amounts of unlabeled data and can be adapted to a range of downstream tasks. This systematic review summarizes recent SSL-trained EEG foundation models that learn whole-brain representations from multichannel EEG rather than representations derived from a single channel. We searched PubMed, IEEE Xplore, Scopus, and arXiv through July 21, 2025. Nineteen preprints and peer-reviewed articles met inclusion criteria. We extracted information regarding pretraining datasets, model architectures, pretraining SSL objectives, and downstream task applications. While pretraining data heavily relied on the Temple University EEG corpus, there was significant heterogeneity in model architecture and training objectives across studies. Transformer architectures were identified as the predominant pretraining architecture with state-space models such as MAMBA and S4 as emerging alternatives. Concerning SSL objectives, masked auto-encoding was most common, and other studies incorporate contrastive learning. Downstream tasks varied widely and implemented diverse fine-tuning strategies, which made direct comparison challenging. Furthermore, most studies used single-task fine-tuning, and a generalizable EEG foundation model remains lacking. In conclusion, the field is advancing rapidly but still limited by limited dataset diversity and the absence of standardized benchmarks. Progress will likely depend on larger and more diverse pretraining datasets, standardized evaluation protocols, and multi-task validation. The development will advance EEG foundation models towards robust and general-purpose relevant to both basic and clinical applications.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-04</td>
<td style='padding: 8px;'>A Multimodal fNIRS-EEG Dataset for Unilateral Limb Motor Imagery</td>
<td style='padding: 6px;'>Lufeng Feng, Baomin Xu, Haoran Zhang, Bihai Lin, Zuxuan Deng, Sidi Tao, Chenyu Liu, Shifan Jia, Li Duan, Ziyu Jia</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.04299v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Unilateral limb motor imagery (MI) plays an important role in upper-limb motor rehabilitation and precise control of external devices, and places higher demands on spatial resolution. However, most existing public datasets focus on binary- or four-class left-right limb paradigms that mainly exploit coarse hemispheric lateralization, and there is still a lack of multimodal datasets that simultaneously record EEG and fNIRS for unilateral multi-directional MI. To address this gap, we constructed MIND, a public motor imagery fNIRS-EEG dataset based on a four-class directional MI paradigm of the right upper limb. The dataset includes 64-channel EEG recordings (1000 Hz) and 51-channel fNIRS recordings (47.62 Hz) from 30 participants (12 females, 18 males; aged 19.0-25.0 years). We analyse the spatiotemporal characteristics of EEG spectral power and hemodynamic responses, and validate the potential advantages of hybrid fNIRS-EEG BCIs in terms of classification accuracy. We expect that this dataset will facilitate the evaluation and comparison of neuroimaging analysis and decoding methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-01</td>
<td style='padding: 8px;'>Inter- and Intra-Subject Variability in EEG: A Systematic Survey</td>
<td style='padding: 6px;'>Xuan-The Tran, Thien-Nhan Vo, Son-Tung Vu, Thoa-Thi Tran, Manh-Dat Nguyen, Thomas Do, Chin-Teng Lin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.01019v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) underpins neuroscience, clinical neurophysiology, and brain-computer interfaces (BCIs), yet pronounced inter- and intra-subject variability limits reliability, reproducibility, and translation. This systematic review studies that quantified or modeled EEG variability across resting-state, event-related potentials (ERPs), and task-related/BCI paradigms (including motor imagery and SSVEP) in healthy and clinical cohorts. Across paradigms, inter-subject differences are typically larger than within-subject fluctuations, but both affect inference and model generalization. Stability is feature-dependent: alpha-band measures and individual alpha peak frequency are often relatively reliable, whereas higher-frequency and many connectivity-derived metrics show more heterogeneous reliability; ERP reliability varies by component, with P300 measures frequently showing moderate-to-good stability. We summarize major sources of variability (biological, state-related, technical, and analytical), review common quantification and modeling approaches (e.g., ICC, CV, SNR, generalizability theory, and multivariate/learning-based methods), and provide recommendations for study design, reporting, and harmonization. Overall, EEG variability should be treated as both a practical constraint to manage and a meaningful signal to leverage for precision neuroscience and robust neurotechnology.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-29</td>
<td style='padding: 8px;'>Cognitive Load Estimation Using Brain Foundation Models and Interpretability for BCIs</td>
<td style='padding: 6px;'>Deeksha M. Shama, Dimitra Emmanouilidou, Ivan J. Tashev</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.21965v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurately monitoring cognitive load in real time is critical for Brain-Computer Interfaces (BCIs) that adapt to user engagement and support personalized learning. Electroencephalography (EEG) offers a non-invasive, cost-effective modality for capturing neural activity, though traditional methods often struggle with cross-subject variability and task-specific preprocessing. We propose leveraging Brain Foundation Models (BFMs), large pre-trained neural networks, to extract generalizable EEG features for cognitive load estimation. We adapt BFMs for long-term EEG monitoring and show that fine-tuning a small subset of layers yields improved accuracy over the state-of-the-art. Despite their scale, BFMs allow for real-time inference with a longer context window. To address often-overlooked interpretability challenges, we apply Partition SHAP (SHapley Additive exPlanations) to quantify feature importance. Our findings reveal consistent emphasis on prefrontal regions linked to cognitive control, while longitudinal trends suggest learning progression. These results position BFMs as efficient and interpretable tools for continuous cognitive load monitoring in real-world BCIs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-29</td>
<td style='padding: 8px;'>Rethinking Self-Training Based Cross-Subject Domain Adaptation for SSVEP Classification</td>
<td style='padding: 6px;'>Weiguang Wang, Yong Liu, Yingjie Gao, Guangyuan Xu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.21203v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Steady-state visually evoked potentials (SSVEP)-based brain-computer interfaces (BCIs) are widely used due to their high signal-to-noise ratio and user-friendliness. Accurate decoding of SSVEP signals is crucial for interpreting user intentions in BCI applications. However, signal variability across subjects and the costly user-specific annotation limit recognition performance. Therefore, we propose a novel cross-subject domain adaptation method built upon the self-training paradigm. Specifically, a Filter-Bank Euclidean Alignment (FBEA) strategy is designed to exploit frequency information from SSVEP filter banks. Then, we propose a Cross-Subject Self-Training (CSST) framework consisting of two stages: Pre-Training with Adversarial Learning (PTAL), which aligns the source and target distributions, and Dual-Ensemble Self-Training (DEST), which refines pseudo-label quality. Moreover, we introduce a Time-Frequency Augmented Contrastive Learning (TFA-CL) module to enhance feature discriminability across multiple augmented views. Extensive experiments on the Benchmark and BETA datasets demonstrate that our approach achieves state-of-the-art performance across varying signal lengths, highlighting its superiority.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-28</td>
<td style='padding: 8px;'>Assembling the Mind's Mosaic: Towards EEG Semantic Intent Decoding</td>
<td style='padding: 6px;'>Jiahe Li, Junru Chen, Fanqi Shen, Jialan Yang, Jada Li, Zhizhang Yuan, Baowen Cheng, Meng Li, Yang Yang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.20447v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Enabling natural communication through brain-computer interfaces (BCIs) remains one of the most profound challenges in neuroscience and neurotechnology. While existing frameworks offer partial solutions, they are constrained by oversimplified semantic representations and a lack of interpretability. To overcome these limitations, we introduce Semantic Intent Decoding (SID), a novel framework that translates neural activity into natural language by modeling meaning as a flexible set of compositional semantic units. SID is built on three core principles: semantic compositionality, continuity and expandability of semantic space, and fidelity in reconstruction. We present BrainMosaic, a deep learning architecture implementing SID. BrainMosaic decodes multiple semantic units from EEG/SEEG signals using set matching and then reconstructs coherent sentences through semantic-guided reconstruction. This approach moves beyond traditional pipelines that rely on fixed-class classification or unconstrained generation, enabling a more interpretable and expressive communication paradigm. Extensive experiments on multilingual EEG and clinical SEEG datasets demonstrate that SID and BrainMosaic offer substantial advantages over existing frameworks, paving the way for natural and effective BCI-mediated communication.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-28</td>
<td style='padding: 8px;'>A Pre-trained EEG-to-MEG Generative Framework for Enhancing BCI Decoding</td>
<td style='padding: 6px;'>Zhuo Li, Shuqiang Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.06990v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) and magnetoencephalography (MEG) play important and complementary roles in non-invasive brain-computer interface (BCI) decoding. However, compared to the low cost and portability of EEG, MEG is more expensive and less portable, which severely limits the practical application of MEG in BCI systems. To overcome this limitation, this study proposes the first cross-modal generation framework based on EEG-MEG spatiotemporal coupled representations to synthesize MEG signals cost-effectively. The framework first extracts general neural activity representations through a pre-trained EEG model. Building upon these representations, the framework effectively learns the lower spatial dispersion and higher high-frequency sensitivity of MEG via the spatial focus mapping module and the broadband spectral calibration module. Experimental results demonstrate that the synthesized MEG signals show high consistency with the real MEG in both time-frequency characteristics and source space activation patterns. More importantly, downstream BCI decoding experiments demonstrate that using synthesized MEG leads to performance enhancements not only on paired EEG-MEG datasets but also on independent EEG-only datasets. Overall, this framework opens a new avenue for overcoming data bottlenecks in BCI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-27</td>
<td style='padding: 8px;'>Cross-Session Decoding of Neural Spiking Data via Task-Conditioned Latent Alignment</td>
<td style='padding: 6px;'>Canyang Zhao, Bolin Peng, J. Patrick Mayo, Ce Ju, Bing Liu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.19963v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Cross-session nonstationarity in neural activity recorded by implanted electrodes is a major challenge for invasive Brain-computer interfaces (BCIs), as decoders trained on data from one session often fail to generalize to subsequent sessions. This issue is further exacerbated in practice, as retraining or adapting decoders becomes particularly challenging when only limited data are available from a new session. To address this challenge, we propose a Task-Conditioned Latent Alignment framework (TCLA) for cross-session neural decoding. Building upon an autoencoder architecture, TCLA first learns a low-dimensional representation of neural dynamics from a source session with sufficient data. For target sessions with limited data, TCLA then aligns target latent representations to the source in a task-conditioned manner, enabling effective transfer of learned neural dynamics. We evaluate TCLA on the macaque motor and oculomotor center-out dataset. Compared to baseline methods trained solely on target-session data, TCLA consistently improves decoding performance across datasets and decoding settings, with gains in the coefficient of determination of up to 0.386 for y coordinate velocity decoding in a motor dataset. These results suggest that TCLA provides an effective strategy for transferring knowledge from source to target sessions, enabling more robust neural decoding under conditions with limited data.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-27</td>
<td style='padding: 8px;'>A Personalized and Adaptable User Interface for a Speech and Cursor Brain-Computer Interface</td>
<td style='padding: 6px;'>Hamza Peracha, Carrina Iacobacci, Tyler Singer-Clark, Leigh R. Hochberg, Sergey D. Stavisky, David M. Brandman, Nicholas S. Card</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.19269v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Communication and computer interaction are important for autonomy in modern life. Unfortunately, these capabilities can be limited or inaccessible for the millions of people living with paralysis. While implantable brain-computer interfaces (BCIs) show promise for restoring these capabilities, little has been explored on designing BCI user interfaces (UIs) for sustained daily use. Here, we present a personalized UI for an intracortical BCI system that enables users with severe paralysis to communicate and interact with their computers independently. Through a 22-month longitudinal deployment with one participant, we used iterative co-design to develop a system for everyday at-home use and documented how it evolved to meet changing needs. Our findings highlight how personalization and adaptability enabled independence in daily life and provide design implications for developing future BCI assistive technologies.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-05</td>
<td style='padding: 8px;'>EEG Foundation Models: Progresses, Benchmarking, and Open Problems</td>
<td style='padding: 6px;'>Dingkun Liu, Yuheng Chen, Zhu Chen, Zhenyao Cui, Yaozhi Wen, Jiayu An, Jingwei Luo, Dongrui Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.17883v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-24</td>
<td style='padding: 8px;'>BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation</td>
<td style='padding: 6px;'>Yuhan Xie, Jinhan Liu, Xiaoyong Ni, Fei Tan, Icare Sakr, Thibault Collin, Shiqi Sun, Alejandro Rodriguez Guajardo, Demon Fanny, Charles-francois Vincent Latchoumane, Henri Lorach, Jocelyne Bloch, Gregoire Courtine, Mahsa Shoaran</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.17625v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce BrainDistill, a novel implantable motor decoding pipeline that integrates an implantable neural decoder (IND) with a task-specific knowledge distillation (TSKD) framework. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. Across multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>How does longer temporal context enhance multimodal narrative video processing in the brain?</td>
<td style='padding: 6px;'>Prachi Jindal, Anant Khandelwal, Manish Gupta, Bapi S. Raju, Subba Reddy Oota, Tanmoy Chakraborty</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07570v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding how humans and artificial intelligence systems process complex narrative videos is a fundamental challenge at the intersection of neuroscience and machine learning. This study investigates how the temporal context length of video clips (3--12 s clips) and the narrative-task prompting shape brain-model alignment during naturalistic movie watching. Using fMRI recordings from participants viewing full-length movies, we examine how brain regions sensitive to narrative context dynamically represent information over varying timescales and how these neural patterns align with model-derived features. We find that increasing clip duration substantially improves brain alignment for multimodal large language models (MLLMs), whereas unimodal video models show little to no gain. Further, shorter temporal windows align with perceptual and early language regions, while longer windows preferentially align higher-order integrative regions, mirrored by a layer-to-cortex hierarchy in MLLMs. Finally, narrative-task prompts (multi-scene summary, narrative summary, character motivation, and event boundary detection) elicit task-specific, region-dependent brain alignment patterns and context-dependent shifts in clip-level tuning in higher-order regions. Together, our results position long-form narrative movies as a principled testbed for probing biologically relevant temporal integration and interpretable representations in long-context MLLMs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>Linguistic properties and model scale in brain encoding: from small to compressed language models</td>
<td style='padding: 6px;'>Subba Reddy Oota, Vijay Rowtula, Satya Sai Srinath Namburi, Khushbu Pahwa, Anant Khandelwal, Manish Gupta, Tanmoy Chakraborty, Bapi S. Raju</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07547v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent work has shown that scaling large language models (LLMs) improves their alignment with human brain activity, yet it remains unclear what drives these gains and which representational properties are responsible. Although larger models often yield better task performance and brain alignment, they are increasingly difficult to analyze mechanistically. This raises a fundamental question: what is the minimal model capacity required to capture brain-relevant representations? To address this question, we systematically investigate how constraining model scale and numerical precision affects brain alignment. We compare full-precision LLMs, small language models (SLMs), and compressed variants (quantized and pruned) by predicting fMRI responses during naturalistic language comprehension. Across model families up to 14B parameters, we find that 3B SLMs achieve brain predictivity indistinguishable from larger LLMs, whereas 1B models degrade substantially, particularly in semantic language regions. Brain alignment is remarkably robust to compression: most quantization and pruning methods preserve neural predictivity, with GPTQ as a consistent exception. Linguistic probing reveals a dissociation between task performance and brain predictivity: compression degrades discourse, syntax, and morphology, yet brain predictivity remains largely unchanged. Overall, brain alignment saturates at modest model scales and is resilient to compression, challenging common assumptions about neural scaling and motivating compact models for brain-aligned language modeling.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>Training-Driven Representational Geometry Modularization Predicts Brain Alignment in Language Models</td>
<td style='padding: 6px;'>Yixuan Liu, Zhiyuan Ma, Likai Tang, Runmin Gan, Xinche Zhang, Jinhao Li, Chao Xie, Sen Song</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07539v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>How large language models (LLMs) align with the neural representation and computation of human language is a central question in cognitive science. Using representational geometry as a mechanistic lens, we addressed this by tracking entropy, curvature, and fMRI encoding scores throughout Pythia (70M-1B) training. We identified a geometric modularization where layers self-organize into stable low- and high-complexity clusters. The low-complexity module, characterized by reduced entropy and curvature, consistently better predicted human language network activity. This alignment followed heterogeneous spatial-temporal trajectories: rapid and stable in temporal regions (AntTemp, PostTemp), but delayed and dynamic in frontal areas (IFG, IFGorb). Crucially, reduced curvature remained a robust predictor of model-brain alignment even after controlling for training progress, an effect that strengthened with model scale. These results links training-driven geometric reorganization to temporal-frontal functional specialization, suggesting that representational smoothing facilitates neural-like linguistic processing.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-06</td>
<td style='padding: 8px;'>Extracting Root-Causal Brain Activity Driving Psychopathology from Resting State fMRI</td>
<td style='padding: 6px;'>Eric V. Strobl</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07233v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroimaging studies of psychiatric disorders often correlate imaging patterns with diagnostic labels or composite symptom scores, yielding diffuse associations that obscure underlying mechanisms. We instead seek to identify root-causal maps -- localized BOLD disturbances that initiate pathological cascades -- and to link them selectively to symptom dimensions. We introduce a bilevel structural causal model that connects between-subject symptom structure to within-subject resting-state fMRI via independent latent sources with localized direct effects. Based on this model, we develop SOURCE (Symptom-Oriented Uncovering of Root-Causal Elements), a procedure that links interpretable symptom axes to a parsimonious set of localized drivers. Experiments show that SOURCE recovers localized maps consistent with root-causal BOLD drivers and increases interpretability and anatomical specificity relative to existing comparators.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-05</td>
<td style='padding: 8px;'>SpARCD: A Spectral Graph Framework for Revealing Differential Functional Connectivity in fMRI Data</td>
<td style='padding: 6px;'>Shira Yoffe, Ziv Ben-Zion, Talma Hendler, Malka Gorfine, Ariel Jaffe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.05807v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Identifying brain regions that exhibit altered functional connectivity across cognitive or emotional states is a key problem in neuroscience. Existing methods, such as edge-wise testing, seed-based psychophysiological interaction (PPI) analysis, or correlation network comparison, typically suffer from low statistical power, arbitrary thresholding, and limited ability to capture distributed or nonlinear dependence patterns. We propose SpARCD (Spectral Analysis of Revealing Connectivity Differences), a novel statistical framework for detecting differences in brain connectivity between two experimental conditions. SpARCD leverages distance correlation, a dependence measure sensitive to both linear and nonlinear associations, to construct a weighted graph for each condition. It then constructs a differential operator via spectral filtering and uncovers connectivity changes by computing its leading eigenvectors. Inference is achieved via a permutation-based testing scheme that yields interpretable, region-level significance maps. Extensive simulation studies demonstrate that SpARCD achieves superior power relative to conventional edge-wise or univariate approaches, particularly in the presence of complex dependency structures. Application to fMRI data from 113 early PTSD patients performing an emotional face-matching task reveals distinct networks associated with emotional reactivity and regulatory processes. Overall, SpARCD provides a statistically rigorous and computationally efficient framework for comparing high-dimensional connectivity structures, with broad applicability to neuroimaging and other network-based scientific domains.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-05</td>
<td style='padding: 8px;'>Accelerating Benchmarking of Functional Connectivity Modeling via Structure-aware Core-set Selection</td>
<td style='padding: 6px;'>Ling Zhan, Zhen Li, Junjie Huang, Tao Jia</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.05667v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Benchmarking the hundreds of functional connectivity (FC) modeling methods on large-scale fMRI datasets is critical for reproducible neuroscience. However, the combinatorial explosion of model-data pairings makes exhaustive evaluation computationally prohibitive, preventing such assessments from becoming a routine pre-analysis step. To break this bottleneck, we reframe the challenge of FC benchmarking by selecting a small, representative core-set whose sole purpose is to preserve the relative performance ranking of FC operators. We formalize this as a ranking-preserving subset selection problem and propose Structure-aware Contrastive Learning for Core-set Selection (SCLCS), a self-supervised framework to select these core-sets. SCLCS first uses an adaptive Transformer to learn each sample's unique FC structure. It then introduces a novel Structural Perturbation Score (SPS) to quantify the stability of these learned structures during training, identifying samples that represent foundational connectivity archetypes. Finally, while SCLCS identifies stable samples via a top-k ranking, we further introduce a density-balanced sampling strategy as a necessary correction to promote diversity, ensuring the final core-set is both structurally robust and distributionally representative. On the large-scale REST-meta-MDD dataset, SCLCS preserves the ground-truth model ranking with just 10% of the data, outperforming state-of-the-art (SOTA) core-set selection methods by up to 23.2% in ranking consistency (nDCG@k). To our knowledge, this is the first work to formalize core-set selection for FC operator benchmarking, thereby making large-scale operators comparisons a feasible and integral part of computational neuroscience. Code is publicly available on https://github.com/lzhan94swu/SCLCS</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-04</td>
<td style='padding: 8px;'>BrainVista: Modeling Naturalistic Brain Dynamics as Multimodal Next-Token Prediction</td>
<td style='padding: 6px;'>Xuanhua Yin, Runkai Zhao, Lina Yao, Weidong Cai</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.04512v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Naturalistic fMRI characterizes the brain as a dynamic predictive engine driven by continuous sensory streams. However, modeling the causal forward evolution in realistic neural simulation is impeded by the timescale mismatch between multimodal inputs and the complex topology of cortical networks. To address these challenges, we introduce BrainVista, a multimodal autoregressive framework designed to model the causal evolution of brain states. BrainVista incorporates Network-wise Tokenizers to disentangle system-specific dynamics and a Spatial Mixer Head that captures inter-network information flow without compromising functional boundaries. Furthermore, we propose a novel Stimulus-to-Brain (S2B) masking mechanism to synchronize high-frequency sensory stimuli with hemodynamically filtered signals, enabling strict, history-only causal conditioning. We validate our framework on Algonauts 2025, CineBrain, and HAD, achieving state-of-the-art fMRI encoding performance. In long-horizon rollout settings, our model yields substantial improvements over baselines, increasing pattern correlation by 36.0\% and 33.3\% on relative to the strongest baseline Algonauts 2025 and CineBrain, respectively.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-03</td>
<td style='padding: 8px;'>Abstraction Induces the Brain Alignment of Language and Speech Models</td>
<td style='padding: 6px;'>Emily Cheng, Aditya R. Vaidya, Richard Antonello</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.04081v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Research has repeatedly demonstrated that intermediate hidden states extracted from large language models and speech audio models predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most effective for this unique and highly general transfer task? We give evidence that the correspondence between speech and language models and the brain derives from shared meaning abstraction and not their next-word prediction properties. In particular, models construct higher-order linguistic features in their middle layers, cued by a peak in the layerwise intrinsic dimension, a measure of feature complexity. We show that a layer's intrinsic dimension strongly predicts how well it explains fMRI and ECoG signals; that the relation between intrinsic dimension and brain predictivity arises over model pre-training; and finetuning models to better predict the brain causally increases both representations' intrinsic dimension and their semantic content. Results suggest that semantic richness, high intrinsic dimension, and brain predictivity mirror each other, and that the key driver of model-brain similarity is rich meaning abstraction of the inputs, where language modeling is a task sufficiently complex (but perhaps not the only) to require it.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-03</td>
<td style='padding: 8px;'>Heterogeneity dominates irreversibility in random Markov models</td>
<td style='padding: 6px;'>Faheem Mosam, Eric De Giuli</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.04905v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We introduce a two-parameter ensemble of random discrete-time Markov models that simultaneously captures critical slowing down and broken detailed balance. Extending a previously studied heterogeneous Markov ensemble, we incorporate correlations between forward and backward transition rates through a single asymmetry parameter $γ$, while heterogeneity is controlled by $ε$. Using results from random matrix theory, we identify a critical locus $ε_c(γ,N)$ at which relaxation times diverge and spectral universality breaks down. We characterize the behavior of entropy production, predictive information, and relaxation dynamics across the ensemble, showing that many observables depend strongly on heterogeneity but only weakly on asymmetry, except near the symmetric limit. Applying maximum-likelihood inference to human fMRI and EEG data, we find that both modalities operate near the predicted critical locus and occupy a similar region of the $ε-γ$ plane, supporting a super-universality of human brain dynamics. While ensemble averages are well captured by the null model, empirical data exhibit substantially enhanced variability, indicating subject-specific structure beyond random expectations. Our results unify criticality and nonequilibrium measures within a single framework and clarify their intertwined role in the analysis of complex biological dynamics.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-03</td>
<td style='padding: 8px;'>Accelerated Electromagnetic Simulation of MRI RF Interactions with Graphene Microtransistor-Based Neural Probes for Electrophysiology-fMRI Integration</td>
<td style='padding: 6px;'>Suchit Kumar, Alejandro Labastida Ramirez, Samuel M Flaherty, Anton Guimera Brunet, Nerea Alvarez de Eulate, Kostas Kostarelos, Ben Dickie, Rob C Wykes, Louis Lemieux</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.03437v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Implementing electrophysiological recordings within an MRI environment is challenging due to complex interactions between recording probes and MRI-generated fields, which can affect both safety and data quality. This study aims to develop and evaluate a hybrid electromagnetic (EM) simulation framework for efficient and accurate assessment of such interactions. Methods: A hybrid EM strategy integrating the Huygens' Box (HB) method with sub-gridding was implemented in an FDTD solver (Sim4Life). RF coil models for mouse and rat head were simulated with and without intracortical (IC) and epicortical (EC) graphene-based micro-transistor arrays. Three-dimensional multi-layered probe models were reconstructed from two-dimensional layouts, and transmit field ($B_{1}^{+}$), electric field ($E$), and specific absorption rate (SAR) distributions were evaluated. Performance was benchmarked against conventional full-wave multi-port (MP) simulations using Bland-Altman analysis and voxel-wise percentage differences. Results: HB simulations reduced computational time by approximately 70-80%, while preserving spatial patterns of $|B_{1}^{+}|$, $|E|$, and SAR, including transmit-field symmetry and localized high-field regions. Deviations from MP were minimal for $|B_{1}^{+}|$ (median $Δ$% 0.02-0.07% in mice, -3.7% to -1.7% in rats) and modest for $|E|$ and SAR, with absolute SAR values remaining well below human safety limits. Graphene-based arrays produced negligible effects on RF transmission and SAR deposition. Conclusion: The HB approach enables computationally efficient, high-resolution evaluation of EM interactions involving microscopic probes in MRI environments, supporting simulations that are otherwise impractical with full-wave MP modeling.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-04</td>
<td style='padding: 8px;'>Charged lepton flavor violating decays $Z\to \ell_α\ell_β$ in the inverse seesaw</td>
<td style='padding: 6px;'>Adrián González-Quiterio, Héctor Novales-Sánchez</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.04168v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>After confirmation of massiveness and mixing of neutrinos, by neutrino oscillation data, the origin of neutrino mass and the occurrence of charged-lepton-flavor non-conservation in nature have become two main objectives for the physics of elementary particles. Taking inspiration from both matters, we address the decays $Z\to\ell_α\ell_β$, with $\ell_α\ne\ell_β$, thus violating charged-lepton flavor. We calculate the set of contributing one-loop diagrams characterized by virtual neutral leptons, both light and heavy, emerged from the inverse seesaw mechanism for the generation of neutrino mass. By neglecting charged-lepton and light-neutrino masses, and then assuming that the mass spectrum of the heavy neutral leptons is degenerate, we find that a relation $\textrm{Br}\big( Z\to\ell_α\ell_β\big)\propto\big| η_{βα} \big|^2$, with $η$ the matrix describing non-unitarity effects in light-lepton mixing, is fulfilled. Our quantitative analysis, which considers both scenarios of degenerate and non-degenerate masses of heavy neutral leptons, takes into account upper bounds on $η_{μe}$, imposed by current constraints on the decay $μ\to eγ$ from the MEG II experiment, while projected future sensitivity of this experiment is considered as well. We find that, even though current constraints on $Z\to\ell_α\ell_β$, by the ATLAS Collaboration, remain far from inverse-seesaw contributions, improved sensitivity from in-plans machines, such as the Future Circular Collider and the Circular Electron Positron Collider, shall be able to probe this mass-generating mechanism through these decays.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-03</td>
<td style='padding: 8px;'>An Algorithm for Monitoring Edge-geodetic Sets in Chordal Graphs</td>
<td style='padding: 6px;'>Nacim Oijid, Clara Marcille</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.03288v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A monitoring edge-geodetic set (or meg-set for short) of a graph is a set of vertices $M$ such that if any edge is removed, then the distance between some two vertices of $M$ increases. This notion was introduced by Foucaud et al. in 2023 as a way to monitor networks for communication failures. As computing a minimal meg-set is hard in general, recent works aimed to find polynomial-time algorithms to compute minimal meg-sets when the input belongs to a restricted class of graphs. Most of these results are based on the property of some classes of graphs to admit a unique minimum meg-set, which is then easy to compute. In this work, we prove that chordal graphs also admit a unique minimal meg-set, answering a standing open question of Foucaud et al.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-02</td>
<td style='padding: 8px;'>MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training</td>
<td style='padding: 6px;'>Dulhan Jayalath, Oiwi Parker Jones</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.02494v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-28</td>
<td style='padding: 8px;'>A Pre-trained EEG-to-MEG Generative Framework for Enhancing BCI Decoding</td>
<td style='padding: 6px;'>Zhuo Li, Shuqiang Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.06990v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) and magnetoencephalography (MEG) play important and complementary roles in non-invasive brain-computer interface (BCI) decoding. However, compared to the low cost and portability of EEG, MEG is more expensive and less portable, which severely limits the practical application of MEG in BCI systems. To overcome this limitation, this study proposes the first cross-modal generation framework based on EEG-MEG spatiotemporal coupled representations to synthesize MEG signals cost-effectively. The framework first extracts general neural activity representations through a pre-trained EEG model. Building upon these representations, the framework effectively learns the lower spatial dispersion and higher high-frequency sensitivity of MEG via the spatial focus mapping module and the broadband spectral calibration module. Experimental results demonstrate that the synthesized MEG signals show high consistency with the real MEG in both time-frequency characteristics and source space activation patterns. More importantly, downstream BCI decoding experiments demonstrate that using synthesized MEG leads to performance enhancements not only on paired EEG-MEG datasets but also on independent EEG-only datasets. Overall, this framework opens a new avenue for overcoming data bottlenecks in BCI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-29</td>
<td style='padding: 8px;'>Scaling Next-Brain-Token Prediction for MEG</td>
<td style='padding: 6px;'>Richard Csaky</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.20138v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We present a large autoregressive model for source-space MEG that scales next-token prediction to long context across datasets and scanners: handling a corpus of over 500 hours and thousands of sessions across the three largest MEG datasets. A modified SEANet-style vector-quantizer reduces multichannel MEG into a flattened token stream on which we train a Qwen2.5-VL backbone from scratch to predict the next brain token and to recursively generate minutes of MEG from up to a minute of context. To evaluate long-horizon generation, we introduce task-matched tests: (i) on-manifold stability via generated-only drift compared to the time-resolved distribution of real sliding windows, and (ii) conditional specificity via correct context versus prompt-swap controls using a neurophysiologically grounded metric set. We train on CamCAN and Omega and run all analyses on held-out MOUS, establishing cross-dataset generalization. Across metrics, generations remain relatively stable over long rollouts and are closer to the correct continuation than swapped controls. Code available at: https://github.com/ricsinaruto/brain-gen.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-26</td>
<td style='padding: 8px;'>MEGnifying Emotion: Sentiment Analysis from Annotated Brain Data</td>
<td style='padding: 6px;'>Brian Liu, Oiwi Parker Jones</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.18792v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Decoding emotion from brain activity could unlock a deeper understanding of the human experience. While a number of existing datasets align brain data with speech and with speech transcripts, no datasets have annotated brain data with sentiment. To bridge this gap, we explore the use of pre-trained Text-to-Sentiment models to annotate non invasive brain recordings, acquired using magnetoencephalography (MEG), while participants listened to audiobooks. Having annotated the text, we employ force-alignment of the text and audio to align our sentiment labels with the brain recordings. It is straightforward then to train Brainto-Sentiment models on these data. Experimental results show an improvement in balanced accuracy for Brain-to-Sentiment compared to baseline, supporting the proposed approach as a proof-of-concept for leveraging existing MEG datasets and learning to decode sentiment directly from the brain.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-26</td>
<td style='padding: 8px;'>Human Cardiac Measurements with Diamond Magnetometers</td>
<td style='padding: 6px;'>Muhib Omar, Magnus Benke, Shaowen Zhang, Jixing Zhang, Michael Kuebler, Pouya Sharbati, Ara Rahimpour, Arno Gueck, Maryna Kapitonova, Devyani Kadam, Carlos Rene Izquierdo Geiser, Jens Haller, Arno Trautmann, Katharina Jag-Lauber, Robert Roelver, Thanh-Duc Nguyen, Leonardo Gizzi, Michelle Schweizer, Mena Abdelsayed, Ingo Wickenbrock, Andrew M. Edmonds, Matthew Markham, Peter A. Koss, Oliver Schnell, Ulrich G. Hofmann, Tonio Ball, Juergen Beck, Dmitry Budker, Joerg Wrachtrup, Arne Wickenbrock</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.18843v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We demonstrate direct, non-invasive and non-contact detection of human cardiac magnetic signals using quantum sensors based on nitrogen-vacancy (NV) centers in diamond. Three configurations were employed recording magnetocardiography (MCG) signals in various shielded and unshielded environments. The signals were averaged over a few hundreds up to several thousands of heart beats to detect the MCG traces. The compact room-temperature NV sensors exhibit sensitivities of 6-26 pT/Hz^(1/2) with active sensing volumes below 0.5 mm^3, defining the performance level of the demonstrated MCG measurements. While the present signals are obtained by averaging, this performance already indicates a clear path toward single-shot MCG sensing. To move beyond shielded environments toward practical clinical use, strong noise suppression is required. To this end, we implement NV-based gradiometry and achieve efficient common-mode noise rejection, enabled by the intrinsically small sensing volume of NV sensors. Together, these multi-platform results obtained across diverse magnetic environments provide a solid foundation for translating quantum sensors into human medical diagnostics such as MCG and magnetoencephalography (MEG).</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-23</td>
<td style='padding: 8px;'>Quantum Sensing MRI for Noninvasive Detection of Neuronal Electrical Activity in Human Brains</td>
<td style='padding: 6px;'>Yongxian Qian, Ying-Chia Lin, Seyedehsara Hejazi, Kamri Clarke, Kennedy Watson, Xingye Chen, Nahbila-Malikha Kumbella, Justin Quimbo, Abena Dinizulu, Simon Henin, Yulin Ge, Arjun Masurkar, Anli Liu, Yvonne W. Lui, Fernando E. Boada</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.16423v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuronal electrical activity underlies human cognition, yet its direct, noninvasive measurement in the living human brain remains a fundamental challenge. Existing neuroimaging techniques, including EEG, MEG, and fMRI, are limited by trade-offs in sensitivity and spatial or temporal resolution. Here we propose quantum sensing MRI (qsMRI), a noninvasive approach that enables direct detection of neuronal firing-induced magnetic fields using a clinical MRI system. qsMRI exploits endogenous proton (1H) nuclear spins in water molecules as intrinsic quantum sensors and decodes time-resolved phase information from free induction decay (FID) signals to infer neuronal magnetic fields. We validate qsMRI through simulations, phantom experiments, and human studies at rest and during motor tasks, and provide open experimental procedures to facilitate independent validation. We further present a case study demonstrating potential applications to neurological disorders. qsMRI represents a first-in-human application of quantum sensing on a clinical MRI platform, establishes a non-BOLD functional imaging modality, and enables interrogation of neuronal firing dynamics in both cortical and deep brain regions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-22</td>
<td style='padding: 8px;'>Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech</td>
<td style='padding: 6px;'>Soufiane Jhilal, Stéphanie Martin, Anne-Lise Giraud</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.15909v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-09</td>
<td style='padding: 8px;'>Cedalion Tutorial: A Python-based framework for comprehensive analysis of multimodal fNIRS & DOT from the lab to the everyday world</td>
<td style='padding: 6px;'>E. Middell, L. Carlton, S. Moradi, T. Codina, T. Fischer, J. Cutler, S. Kelley, J. Behrendt, T. Dissanayake, N. Harmening, M. A. Yücel, D. A. Boas, A. von Lühmann</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.05923v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional near-infrared spectroscopy (fNIRS) and diffuse optical tomography (DOT) are rapidly evolving toward wearable, multimodal, and data-driven, AI-supported neuroimaging in the everyday world. However, current analytical tools are fragmented across platforms, limiting reproducibility, interoperability, and integration with modern machine learning (ML) workflows. Cedalion is a Python-based open-source framework designed to unify advanced model-based and data-driven analysis of multimodal fNIRS and DOT data within a reproducible, extensible, and community-driven environment. Cedalion integrates forward modelling, photogrammetric optode co-registration, signal processing, GLM Analysis, DOT image reconstruction, and ML-based data-driven methods within a single standardized architecture based on the Python ecosystem. It adheres to SNIRF and BIDS standards, supports cloud-executable Jupyter notebooks, and provides containerized workflows for scalable, fully reproducible analysis pipelines that can be provided alongside original research publications. Cedalion connects established optical-neuroimaging pipelines with ML frameworks such as scikit-learn and PyTorch, enabling seamless multimodal fusion with EEG, MEG, and physiological data. It implements validated algorithms for signal-quality assessment, motion correction, GLM modelling, and DOT reconstruction, complemented by modules for simulation, data augmentation, and multimodal physiology analysis. Automated documentation links each method to its source publication, and continuous-integration testing ensures robustness. This tutorial paper provides seven fully executable notebooks that demonstrate core features. Cedalion offers an open, transparent, and community extensible foundation that supports reproducible, scalable, cloud- and ML-ready fNIRS/DOT workflows for laboratory-based and real-world neuroimaging.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-02</td>
<td style='padding: 8px;'>NeuroAI Temporal Neural Networks (NeuTNNs): Microarchitecture and Design Framework for Specialized Neuromorphic Processing Units</td>
<td style='padding: 6px;'>Shanmuga Venkatachalam, Prabhu Vellaisamy, Harideep Nair, Wei-Che Huang, Youngseok Na, Yuyang Kang, Quinn Jacobson, John Paul Shen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.01546v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Leading experts from both communities have suggested the need to (re)connect research in neuroscience and artificial intelligence (AI) to accelerate the development of next-generation AI innovations. They term this convergence as NeuroAI. Previous research has established temporal neural networks (TNNs) as a promising neuromorphic approach toward biological intelligence and efficiency. We fully embrace NeuroAI and propose a new category of TNNs we call NeuroAI TNNs (NeuTNNs) with greater capability and hardware efficiency by adopting neuroscience findings, including a neuron model with active dendrites and a hierarchy of distal and proximal segments. This work introduces a PyTorch-to-layout tool suite (NeuTNNGen) to design application-specific NeuTNNs. Compared to previous TNN designs, NeuTNNs achieve superior performance and efficiency. We demonstrate NeuTNNGen's capabilities using three example applications: 1) UCR time series benchmarks, 2) MNIST design exploration, and 3) Place Cells design for neocortical reference frames. We also explore using synaptic pruning to further reduce synapse counts and hardware costs by 30-50% while maintaining model precision across diverse sensory modalities. NeuTNNGen can facilitate the design of application-specific energy-efficient NeuTNNs for the next generation of NeuroAI computing systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-04</td>
<td style='padding: 8px;'>Governance at the Edge of Architecture: Regulating NeuroAI and Neuromorphic Systems</td>
<td style='padding: 6px;'>Afifah Kashif, Abdul Muhsin Hameed, Asim Iqbal</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.01503v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Current AI governance frameworks, including regulatory benchmarks for accuracy, latency, and energy efficiency, are built for static, centrally trained artificial neural networks on von Neumann hardware. NeuroAI systems, embodied in neuromorphic hardware and implemented via spiking neural networks, break these assumptions. This paper examines the limitations of current AI governance frameworks for NeuroAI, arguing that assurance and audit methods must co-evolve with these architectures, aligning traditional regulatory metrics with the physics, learning dynamics, and embodied efficiency of brain-inspired computation to enable technically grounded assurance.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-27</td>
<td style='padding: 8px;'>NeuroAI and Beyond</td>
<td style='padding: 6px;'>Jean-Marc Fellous, Gert Cauwenberghs, Cornelia Fermüller, Yulia Sandamisrkaya, Terrence Sejnowski</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.19955v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-24</td>
<td style='padding: 8px;'>When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics</td>
<td style='padding: 6px;'>Yiven, Zhu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.19548v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, "brain-based" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies "true" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-25</td>
<td style='padding: 8px;'>Dopamine-driven synaptic credit assignment in neural networks</td>
<td style='padding: 6px;'>Saranraj Nambusubramaniyan, Shervin Safavi, Raja Guru, Andreas Knoblauch</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.22178v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Solving the synaptic Credit Assignment Problem(CAP) is central to learning in both biological and artificial neural systems. Finding an optimal solution for synaptic CAP means setting the synaptic weights that assign credit to each neuron for influencing the final output and behavior of neural networks or animals. Gradient-based methods solve this problem in artificial neural networks using back-propagation, however, not in the most efficient way. For instance, back-propagation requires a chain of top-down gradient computations. This leads to an expensive optimization process in terms of computing power and memory linked with well-known weight transport and update locking problems. To address these shortcomings, we take a NeuroAI approach and draw inspiration from neural Reinforcement Learning to develop a derivative-free optimizer for training neural networks, Dopamine. Dopamine is developed for Weight Perturbation (WP) learning that exploits stochastic updating of weights towards optima. It achieves this by minimizing the regret, a form of Reward Prediction Error (RPE) between the expected outcome from the perturbed model and the actual outcome from the unperturbed model. We use this RPE to adjust the learning rate in the network (i.e., creating an adaptive learning rate strategy, similar to the role of dopamine in the brain). We tested the Dopamine optimizer for training multi-layered perceptrons for XOR tasks, and recurrent neural networks for chaotic time series forecasting. Dopamine-trained models demonstrate accelerated convergence and outperform standard WP, and give comparable performance to gradient-based algorithms, while consuming significantly less computation and memory. Overall, the Dopamine optimizer not only finds robust solutions and comparable performance to the state-of-the-art Machine Learning optimizers but is also neurobiologically more plausible.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>A Computational Perspective on NeuroAI and Synthetic Biological Intelligence</td>
<td style='padding: 6px;'>Dhruvik Patel, Md Sayed Tanveer, Jesus Gonzalez-Ferrer, Alon Loeffler, Brett J. Kagan, Mohammed A. Mostajo-Radji, Ge Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.23896v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>NeuroAI is an emerging field at the intersection of neuroscience and artificial intelligence, where insights from brain function guide the design of intelligent systems. A central area within this field is synthetic biological intelligence (SBI), which combines the adaptive learning properties of biological neural networks with engineered hardware and software. SBI systems provide a platform for modeling neural computation, developing biohybrid architectures, and enabling new forms of embodied intelligence. In this review, we organize the NeuroAI landscape into three interacting domains: hardware, software, and wetware. We outline computational frameworks that integrate biological and non-biological systems and highlight recent advances in organoid intelligence, neuromorphic computing, and neuro-symbolic learning. These developments collectively point toward a new class of systems that compute through interactions between living neural tissue and digital algorithms.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-07</td>
<td style='padding: 8px;'>Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers</td>
<td style='padding: 6px;'>Thomas Klein, Sascha Meyen, Wieland Brendel, Felix A. Wichmann, Kristof Meding</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.06645v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Benchmarking models is a key factor for the rapid progress in machine learning (ML) research. Thus, further progress depends on improving benchmarking metrics. A standard metric to measure the behavioral alignment between ML models and human observers is error consistency (EC). EC allows for more fine-grained comparisons of behavior than other metrics such as accuracy, and has been used in the influential Brain-Score benchmark to rank different DNNs by their behavioral consistency with humans. Previously, EC values have been reported without confidence intervals. However, empirically measured EC values are typically noisy -- thus, without confidence intervals, valid benchmarking conclusions are problematic. Here we improve on standard EC in two ways: First, we show how to obtain confidence intervals for EC using a bootstrapping technique, allowing us to derive significance tests for EC. Second, we propose a new computational model relating the EC between two classifiers to the implicit probability that one of them copies responses from the other. This view of EC allows us to give practical guidance to scientists regarding the number of trials required for sufficiently powerful, conclusive experiments. Finally, we use our methodology to revisit popular NeuroAI-results. We find that while the general trend of behavioral differences between humans and machines holds up to scrutiny, many reported differences between deep vision models are statistically insignificant. Our methodology enables researchers to design adequately powered experiments that can reliably detect behavioral differences between models, providing a foundation for more rigorous benchmarking of behavioral alignment.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-02</td>
<td style='padding: 8px;'>What Neuroscience Can Teach AI About Learning in Continuously Changing Environments</td>
<td style='padding: 6px;'>Daniel Durstewitz, Bruno Averbeck, Georgia Koppe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.02103v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-03</td>
<td style='padding: 8px;'>NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models</td>
<td style='padding: 6px;'>Luca Ghafourpour, Valentin Duruisseaux, Bahareh Tolooshams, Philip H. Wong, Costas A. Anastassiou, Anima Anandkumar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2506.04536v4' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Characterizing the cellular properties of neurons is fundamental to understanding their function in the brain. In this quest, the generation of bio-realistic models is central towards integrating multimodal cellular data sets and establishing causal relationships. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. The deterministic formalism of bio-realistic models currently precludes accounting for the natural variability observed experimentally. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on synthetic data generated from bio-realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE enables the efficient generation of synthetic neurons that closely resemble experimental data and exhibit trial-to-trial variability, offering a $4200\times$ speedup over the numerical solver. NOBLE is the first scaled-up deep learning framework that validates its generalization with real experimental data. To this end, NOBLE captures fundamental neural properties in a unique and emergent manner that opens the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-27</td>
<td style='padding: 8px;'>Decoding Cortical Microcircuits: A Generative Model for Latent Space Exploration and Controlled Synthesis</td>
<td style='padding: 6px;'>Xingyu Liu, Yubin Li, Guozhang Chen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2506.11062v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A central idea in understanding brains and building artificial intelligence is that structure determines function. Yet, how the brain's complex structure arises from a limited set of genetic instructions remains a key question. The ultra high-dimensional detail of neural connections vastly exceeds the information storage capacity of genes, suggesting a compact, low-dimensional blueprint must guide brain development. Our motivation is to uncover this blueprint. We introduce a generative model, to learn this underlying representation from detailed connectivity maps of mouse cortical microcircuits. Our model successfully captures the essential structural information of these circuits in a compressed latent space. We found that specific, interpretable directions within this space directly relate to understandable network properties. Building on this, we demonstrate a novel method to controllably generate new, synthetic microcircuits with desired structural features by navigating this latent space. This work offers a new way to investigate the design principles of neural circuits and explore how structure gives rise to function, potentially informing the development of more advanced artificial neural networks.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-08</td>
<td style='padding: 8px;'>WristMIR: Coarse-to-Fine Region-Aware Retrieval of Pediatric Wrist Radiographs with Radiology Report-Driven Learning</td>
<td style='padding: 6px;'>Mert Sonmezer, Serge Vasylechko, Duygu Atasoy, Seyda Ertekin, Sila Kurugol</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07872v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Retrieving wrist radiographs with analogous fracture patterns is challenging because clinically important cues are subtle, highly localized and often obscured by overlapping anatomy or variable imaging views. Progress is further limited by the scarcity of large, well-annotated datasets for case-based medical image retrieval. We introduce WristMIR, a region-aware pediatric wrist radiograph retrieval framework that leverages dense radiology reports and bone-specific localization to learn fine-grained, clinically meaningful image representations without any manual image-level annotations. Using MedGemma-based structured report mining to generate both global and region-level captions, together with pre-processed wrist images and bone-specific crops of the distal radius, distal ulna, and ulnar styloid, WristMIR jointly trains global and local contrastive encoders and performs a two-stage retrieval process: (1) coarse global matching to identify candidate exams, followed by (2) region-conditioned reranking aligned to a predefined anatomical bone region. WristMIR improves retrieval performance over strong vision-language baselines, raising image-to-text Recall@5 from 0.82% to 9.35%. Its embeddings also yield stronger fracture classification (AUROC 0.949, AUPRC 0.953). In region-aware evaluation, the two-stage design markedly improves retrieval-based fracture diagnosis, increasing mean $F_1$ from 0.568 to 0.753, and radiologists rate its retrieved cases as more clinically relevant, with mean scores rising from 3.36 to 4.35. These findings highlight the potential of anatomically guided retrieval to enhance diagnostic reasoning and support clinical decision-making in pediatric musculoskeletal imaging. The source code is publicly available at https://github.com/quin-med-harvard-edu/WristMIR.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-08</td>
<td style='padding: 8px;'>DINO-Mix: Distilling Foundational Knowledge with Cross-Domain CutMix for Semi-supervised Class-imbalanced Medical Image Segmentation</td>
<td style='padding: 6px;'>Xinyu Liu, Guolei Sun</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07819v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Semi-supervised learning (SSL) has emerged as a critical paradigm for medical image segmentation, mitigating the immense cost of dense annotations. However, prevailing SSL frameworks are fundamentally "inward-looking", recycling information and biases solely from within the target dataset. This design triggers a vicious cycle of confirmation bias under class imbalance, leading to the catastrophic failure to recognize minority classes. To dismantle this systemic issue, we propose a paradigm shift to a multi-level "outward-looking" framework. Our primary innovation is Foundational Knowledge Distillation (FKD), which looks outward beyond the confines of medical imaging by introducing a pre-trained visual foundation model, DINOv3, as an unbiased external semantic teacher. Instead of trusting the student's biased high confidence, our method distills knowledge from DINOv3's robust understanding of high semantic uniqueness, providing a stable, cross-domain supervisory signal that anchors the learning of minority classes. To complement this core strategy, we further look outward within the data by proposing Progressive Imbalance-aware CutMix (PIC), which creates a dynamic curriculum that adaptively forces the model to focus on minority classes in both labeled and unlabeled subsets. This layered strategy forms our framework, DINO-Mix, which breaks the vicious cycle of bias and achieves remarkable performance on challenging semi-supervised class-imbalanced medical image segmentation benchmarks Synapse and AMOS.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-08</td>
<td style='padding: 8px;'>The Development of a Preclinical Alpha Irradiation Platform with Versatile Control of Dose, Dose Rate, and Spatiotemporal Irradiation Patterns</td>
<td style='padding: 6px;'>Harsh Arya, Joshua Rajan, Varghese Chirayath, Mainul Arbar, Awat Lotfihagh, Sitmukhambetov Satzhan, Yan Chang, Alex Weiss, Zui Pan, Yujie Chi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07760v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Objectives. This study develops and validates a vacuum-based alpha irradiation platform to support preclinical radiobiology. We aim to demonstrate precise, independent control over incident energy, fluence rate, and spatiotemporal patterns, which are critical to the mechanisms underlying targeted alpha therapies and low-dose risk assessments.   Approach. A vacuum-based system with a radioactive alpha source was designed and fabricated. The platform provides independent modulation of: (i) temporal patterns via a programmable gate valve; (ii) fluence rate across two orders of magnitude by varying source-to-aperture distance (57 to 381 mm); (iii) incident energy (0 to 4.6 MeV) using adjustable absorption layers; and (iv) spatial distributions via a 3D motion stage. Temporal precision was assessed via synchronized audio-electronic recordings. Fluence rates and energies were validated using CR-39 detectors and Monte Carlo (MC) simulations. Spatial precision was verified through programmed continuous and discrete trajectories.   Main results. Validation experiments demonstrated high system fidelity. Measured irradiation durations deviated from programmed values by less than 0.3 s. Measured and computed fluence rates agreed within 3%. For energy validation, CR-39 track diameters matched MC model predictions within one standard deviation. Recorded spatial patterns and dimensions aligned well with programmed trajectories.   Significance. We successfully validated a versatile vacuum-based platform that overcomes energy-degradation constraints of gas-filled systems. By providing multi-parametric control over alpha-particle delivery, this system enables systematic investigation into how energy, dose rate, and spatiotemporal patterns influence radiobiological responses. This platform is poised to optimize targeted alpha therapies and refine radiation protection frameworks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>Dense Feature Learning via Linear Structure Preservation in Medical Data</td>
<td style='padding: 6px;'>Yuanyun Zhang, Mingxuan Zhang, Siyuan Li, Zihan Wang, Haoran Chen, Wenbo Zhou, Shi Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07706v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Deep learning models for medical data are typically trained using task specific objectives that encourage representations to collapse onto a small number of discriminative directions. While effective for individual prediction problems, this paradigm underutilizes the rich structure of clinical data and limits the transferability, stability, and interpretability of learned features. In this work, we propose dense feature learning, a representation centric framework that explicitly shapes the linear structure of medical embeddings. Our approach operates directly on embedding matrices, encouraging spectral balance, subspace consistency, and feature orthogonality through objectives defined entirely in terms of linear algebraic properties. Without relying on labels or generative reconstruction, dense feature learning produces representations with higher effective rank, improved conditioning, and greater stability across time. Empirical evaluations across longitudinal EHR data, clinical text, and multimodal patient representations demonstrate consistent improvements in downstream linear performance, robustness, and subspace alignment compared to supervised and self supervised baselines. These results suggest that learning to span clinical variation may be as important as learning to predict clinical outcomes, and position representation geometry as a first class objective in medical AI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>A hybrid Kolmogorov-Arnold network for medical image segmentation</td>
<td style='padding: 6px;'>Deep Bhattacharyya, Ali Ayub, A. Ben Hamza</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07702v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Medical image segmentation plays a vital role in diagnosis and treatment planning, but remains challenging due to the inherent complexity and variability of medical images, especially in capturing non-linear relationships within the data. We propose U-KABS, a novel hybrid framework that integrates the expressive power of Kolmogorov-Arnold Networks (KANs) with a U-shaped encoder-decoder architecture to enhance segmentation performance. The U-KABS model combines the convolutional and squeeze-and-excitation stage, which enhances channel-wise feature representations, and the KAN Bernstein Spline (KABS) stage, which employs learnable activation functions based on Bernstein polynomials and B-splines. This hybrid design leverages the global smoothness of Bernstein polynomials and the local adaptability of B-splines, enabling the model to effectively capture both broad contextual trends and fine-grained patterns critical for delineating complex structures in medical images. Skip connections between encoder and decoder layers support effective multi-scale feature fusion and preserve spatial details. Evaluated across diverse medical imaging benchmark datasets, U-KABS demonstrates superior performance compared to strong baselines, particularly in segmenting complex anatomical structures.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation</td>
<td style='padding: 6px;'>Avinash Kumar K M, Samarth S. Raut</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07658v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The accuracy of the 3D models created from medical scans depends on imaging hardware, segmentation methods and mesh processing techniques etc. The effects of geometry type, class imbalance, voxel and point cloud alignment on accuracy remain to be thoroughly explored. This work evaluates the errors across the reconstruction pipeline and explores the use of voxel and surface-based accuracy metrics for different segmentation algorithms and geometry types. A sphere, a facemask, and an AAA were printed using the SLA technique and scanned using a micro-CT machine. Segmentation was performed using GMM, Otsu and RG based methods. Segmented and reference models aligned using the KU algorithm, were quantitatively compared to evaluate metrics like Dice and Jaccard scores, precision. Surface meshes were registered with reference meshes using an ICP-based alignment process. Metrics like chamfer distance, and average Hausdorff distance were evaluated. The Otsu method was found to be the most suitable method for all the geometries. AAA yielded low overlap scores due to its small wall thickness and misalignment. The effect of class imbalance on specificity was observed the most for AAA. Surface-based accuracy metrics differed from the voxel-based trends. The RG method performed best for sphere, while GMM and Otsu perform better for AAA. The facemask surface was most error-prone, possibly due to misalignment during the ICP process. Segmentation accuracy is a cumulative sum of errors across different stages of the reconstruction process. High voxel-based accuracy metrics may be misleading in cases of high class imbalance and sensitivity to alignment. The Jaccard index is found to be more stringent than the Dice and more suitable for accuracy assessment for thin-walled structures. Voxel and point cloud alignment should be ensured to make any reliable assessment of the reconstruction pipeline.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation</td>
<td style='padding: 6px;'>Yichi Zhang, Feiyang Xiao, Le Xue, Wenbo Zhang, Gang Feng, Chenguang Zheng, Yuan Qi, Yuan Cheng, Zixin Hu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07643v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While emerging 3D medical foundation models are envisioned as versatile tools with offer general-purpose capabilities, their validation remains largely confined to regional and structural imaging, leaving a significant modality discrepancy unexplored. To provide a rigorous and objective assessment, we curate the UMD dataset comprising 490 whole-body PET/CT and 464 whole-body PET/MRI scans ($\sim$675k 2D images, $\sim$12k 3D organ annotations) and conduct a thorough and comprehensive evaluation of representative 3D segmentation foundation models. Through intra-subject controlled comparisons of paired scans, we isolate imaging modality as the primary independent variable to evaluate model robustness in real-world applications. Our evaluation reveals a stark discrepancy between literature-reported benchmarks and real-world efficacy, particularly when transitioning from structural to functional domains. Such systemic failures underscore that current 3D foundation models are far from achieving truly general-purpose status, necessitating a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility. This dataset and analysis establish a foundational cornerstone for future research to develop truly modality-agnostic medical foundation models.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>Photonuclear Production of $^{195m}$Pt for Medical Applications: Cross Sections and Energy Thresholds</td>
<td style='padding: 6px;'>J. Song, J. Nolen, D. Rotsch, R. Gampa, R. M. de Kruijff, T. Brossard, C. R. Howell, F. Krishichayan, S. W. Finch, Y. K. Wu, S. Mikhailov, M. W. Ahmed, R. V. F. Janssens</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07637v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Platinum radioisotopes are of growing interest for targeted cancer therapy and diagnostic imaging because their decay delivers highly localized radiation doses in tissue, herewith enabling precise DNA damage through Auger-electron emission. Developing production technologies that provide platinum isotopes with high specific activity is therefore essential, and photonuclear reactions on stable nuclei offer a viable accelerator-based route when supported by reliable cross-section data. We report photonuclear cross-section measurements for the $^{197}$Au($γ$,pn)$^{195m}$Pt reaction at incident $γ$-ray energies of 27, 29, and 31~MeV using the activation method. The measurements were performed by irradiating a stack of concentric-ring gold targets with a quasi-monoenergetic $γ$-ray beam provided by the High Intensity Gamma-ray Source (HI$γ$S). The induced $^{195m}$Pt activity was quantified using off-line $γ$-ray spectroscopy. The present results provide the first experimental information in the energy region where the $^{197}$Au($γ$,pn)$^{195m}$Pt reaction becomes significant and demonstrate the minimum energy requirements and feasibility of producing $^{195m}$Pt via photonuclear reactions using electron accelerator facilities. These measurements indicate that electron energies of $\ge$ 50 MeV are required to achieve practically meaningful production yields.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>First ex-vivo positronium imaging of tissues with modular J-PET scanner using $^{44}$Sc radionuclide</td>
<td style='padding: 6px;'>Karol Kubat, Manish Das, Sushil Sharma, Ermias Y. Beyene, Aleksander Bilewicz, Jarosław Choiński, Neha Chug, Catalina Curceanu, Eryk Czerwiński, Jakub Hajduga, Sharareh Jalali, Krzysztof Kacprzak, Tevfik Kaplanoglu, Łukasz Kapłon, Kamila Kasperska, Aleksander Khreptak, Grzegorz Korcyl, Tomasz Kozik, Deepak Kumar, Sumit Kumar Kundu, Anoop Kunimmal-Venadan, Bartosz Leszczyński, Edward Lisowski, Filip Lisowski, Justyna Mędrala-Sowa, Simbarashe Moyo, Wiktor Mryka, Szymon Niedźwiecki, Anand Pandey, Piyush Pandey, Szymon Parzych, Alessio Porcelli, Bartłomiej Rachwał, Martin Rädler, Magdalena Skurzok, Anna Stolarz, Tomasz Szumlak, Pooja Tanty, Keyvan Tayefi Ardebili, Satyam Tiwari, Kavya Valsan Eliyan, Rafał Walczak, Paweł Moskal, Ewa Ł. Stępień</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07580v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study presents the first ex-vivo positronium imaging of human tissues using the modular J-PET scanner with the $^{44}$Sc radionuclide. The $^{44}$Sc isotope was produced via the $^{44}$Ca(p, n)$^{44}$Sc nuclear reaction and used to perform positronium imaging of phantom composed of human adipose tissue, cardiac myxoma tissue, thrombi blood clot, and also porous polymer XAD4, and a certified reference material (CRM) made from fused silica. The experiment demonstrates the suitability of $^{44}$Sc as a positron source for positronium imaging. The performance of J-PET for positronium imaging with $^{44}$Sc was validated by proper reconstruction of the mean orthopositronium lifetime for CRM material and XAD-4 polymer. The mean ortho-positronium (oPs) lifetimes determined for adipose tissue, cardiac myxoma tissues and thrombi were consistent with results of previous experiments. The study highlights the potential $^{44}$Sc radionuclide for positronium lifetime imaging (PLI).</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-07</td>
<td style='padding: 8px;'>LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing</td>
<td style='padding: 6px;'>Huimin Yan, Liang Bai, Xian Yang, Long Chen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.07540v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.</td>
</tr>
</tbody>
</table>

