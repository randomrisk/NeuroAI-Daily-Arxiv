<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2025-01-08</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>Inverse receptive field attention for naturalistic image reconstruction from the brain</td>
<td style='padding: 6px;'>Lynn Le, Thirza Dado, Katja Seeliger, Paolo Papale, Antonio Lozano, Pieter Roelfsema, Yağmur Güçlütürk, Marcel van Gerven, Umut Güçlü</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.03051v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Visual perception in the brain largely depends on the organization of neuronal receptive fields. Although extensive research has delineated the coding principles of receptive fields, most studies have been constrained by their foundational assumptions. Moreover, while machine learning has successfully been used to reconstruct images from brain data, this approach faces significant challenges, including inherent feature biases in the model and the complexities of brain structure and function. In this study, we introduce an inverse receptive field attention (IRFA) model, designed to reconstruct naturalistic images from neurophysiological data in an end-to-end fashion. This approach aims to elucidate the tuning properties and representational transformations within the visual cortex. The IRFA model incorporates an attention mechanism that determines the inverse receptive field for each pixel, weighting neuronal responses across the visual field and feature spaces. This method allows for an examination of the dynamics of neuronal representations across stimuli in both spatial and feature dimensions. Our results show highly accurate reconstructions of naturalistic data, independent of pre-trained models. Notably, IRF models trained on macaque V1, V4, and IT regions yield remarkably consistent spatial receptive fields across different stimuli, while the features to which neuronal representations are selective exhibit significant variation. Additionally, we propose a data-driven method to explore representational clustering within various visual areas, further providing testable hypotheses.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>NeuroPMD: Neural Fields for Density Estimation on Product Manifolds</td>
<td style='padding: 6px;'>William Consagra, Zhiling Gu, Zhengwu Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02994v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We propose a novel deep neural network methodology for density estimation on product Riemannian manifold domains. In our approach, the network directly parameterizes the unknown density function and is trained using a penalized maximum likelihood framework, with a penalty term formed using manifold differential operators. The network architecture and estimation algorithm are carefully designed to handle the challenges of high-dimensional product manifold domains, effectively mitigating the curse of dimensionality that limits traditional kernel and basis expansion estimators, as well as overcoming the convergence issues encountered by non-specialized neural network methods. Extensive simulations and a real-world application to brain structural connectivity data highlight the clear advantages of our method over the competing alternatives.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>Key-value memory in the brain</td>
<td style='padding: 6px;'>Samuel J. Gershman, Ila Fiete, Kazuki Irie</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02950v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Classical models of memory in psychology and neuroscience rely on similarity-based retrieval of stored patterns, where similarity is a function of retrieval cues and the stored patterns. While parsimonious, these models do not allow distinct representations for storage and retrieval, despite their distinct computational demands. Key-value memory systems, in contrast, distinguish representations used for storage (values) and those used for retrieval (keys). This allows key-value memory systems to optimize simultaneously for fidelity in storage and discriminability in retrieval. We review the computational foundations of key-value memory, its role in modern machine learning systems, related ideas from psychology and neuroscience, applications to a number of empirical puzzles, and possible biological implementations.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>Integrating Language-Image Prior into EEG Decoding for Cross-Task Zero-Calibration RSVP-BCI</td>
<td style='padding: 6px;'>Xujin Li, Wei Wei, Shuang Qiu, Xinyi Zhang, Fu Li, Huiguang He</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02841v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Rapid Serial Visual Presentation (RSVP)-based Brain-Computer Interface (BCI) is an effective technology used for information detection by detecting Event-Related Potentials (ERPs). The current RSVP decoding methods can perform well in decoding EEG signals within a single RSVP task, but their decoding performance significantly decreases when directly applied to different RSVP tasks without calibration data from the new tasks. This limits the rapid and efficient deployment of RSVP-BCI systems for detecting different categories of targets in various scenarios. To overcome this limitation, this study aims to enhance the cross-task zero-calibration RSVP decoding performance. First, we design three distinct RSVP tasks for target image retrieval and build an open-source dataset containing EEG signals and corresponding stimulus images. Then we propose an EEG with Language-Image Prior fusion Transformer (ELIPformer) for cross-task zero-calibration RSVP decoding. Specifically, we propose a prompt encoder based on the language-image pre-trained model to extract language-image features from task-specific prompts and stimulus images as prior knowledge for enhancing EEG decoding. A cross bidirectional attention mechanism is also adopted to facilitate the effective feature fusion and alignment between the EEG and language-image features. Extensive experiments demonstrate that the proposed model achieves superior performance in cross-task zero-calibration RSVP decoding, which promotes the RSVP-BCI system from research to practical application.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>Multi-time scale and high performance in-material reservoir computing using graphene-based ion-gating reservoir</td>
<td style='padding: 6px;'>Daiki Nishioka, Hina Kitano, Wataru Namiki, Kazuya Terabe, Takashi Tsuchiya</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02813v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The rising energy demands of conventional AI systems underscore the need for efficient computing technologies like brain-inspired computing. Physical reservoir computing (PRC), leveraging the nonlinear dynamics of physical systems for information processing, has emerged as a promising approach for neuromorphic computing. However, current PRC systems are constrained by narrow operating timescales and limited performance. To address these challenges, an ion-gel/graphene electric double layer transistor-based ion-gating reservoir (IGR) was developed, offering adaptability across multi-time scales with an exceptionally wide operating range from 1 MHz to 20 Hz and high information processing capacity. The IGR achieved deep learning (DL)-level accuracy in chaotic time series prediction tasks while reducing computational resource requirements to 1/100 of those needed by DL. Principal component analysis reveals the IGR's superior performance stems from its high-dimensionality, driven by the ambipolar behavior of graphene and multiple relaxation processes. The proposed IGR represents a significant step forward in providing low-power, high-performance computing solutions, particularly for resource-constrained edge environments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-05</td>
<td style='padding: 8px;'>Quantum Cognition-Inspired EEG-based Recommendation via Graph Neural Networks</td>
<td style='padding: 6px;'>Jinkun Han, Wei Li, Yingshu Li, Zhipeng Cai</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02671v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Current recommendation systems recommend goods by considering users' historical behaviors, social relations, ratings, and other multi-modals. Although outdated user information presents the trends of a user's interests, no recommendation system can know the users' real-time thoughts indeed. With the development of brain-computer interfaces, it is time to explore next-generation recommenders that show users' real-time thoughts without delay. Electroencephalography (EEG) is a promising method of collecting brain signals because of its convenience and mobility. Currently, there is only few research on EEG-based recommendations due to the complexity of learning human brain activity. To explore the utility of EEG-based recommendation, we propose a novel neural network model, QUARK, combining Quantum Cognition Theory and Graph Convolutional Networks for accurate item recommendations. Compared with the state-of-the-art recommendation models, the superiority of QUARK is confirmed via extensive experiments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-05</td>
<td style='padding: 8px;'>LLMs Help Alleviate the Cross-Subject Variability in Brain Signal and Language Alignment</td>
<td style='padding: 6px;'>Yifei Liu, Hengwei Ye, Shuhang Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02621v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Decoding human activity from EEG signals has long been a popular research topic. While recent studies have increasingly shifted focus from single-subject to cross-subject analysis, few have explored the model's ability to perform zero-shot predictions on EEG signals from previously unseen subjects. This research aims to investigate whether deep learning methods can capture subject-independent semantic information inherent in human EEG signals. Such insights are crucial for Brain-Computer Interfaces (BCI) because, on one hand, they demonstrate the model's robustness against subject-specific temporal biases, and on the other, they significantly enhance the generalizability of downstream tasks. We employ Large Language Models (LLMs) as denoising agents to extract subject-independent semantic features from noisy EEG signals. Experimental results, including ablation studies, highlight the pivotal role of LLMs in decoding subject-independent semantic information from noisy EEG data. We hope our findings will contribute to advancing BCI research and assist both academia and industry in applying EEG signals to a broader range of applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-05</td>
<td style='padding: 8px;'>Decoding fMRI Data into Captions using Prefix Language Modeling</td>
<td style='padding: 6px;'>Vyacheslav Shen, Kassymzhomart Kunanbayev, Dae-Shik Kim</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02570v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>With the advancements in Large Language and Latent Diffusion models, brain decoding has achieved remarkable results in recent years. The works on the NSD dataset, with stimuli images from the COCO dataset, leverage the embeddings from the CLIP model for image reconstruction and GIT for captioning. However, the current captioning approach introduces the challenge of potential data contamination given that the GIT model was trained on the COCO dataset. In this work, we present an alternative method for decoding brain signals into image captions by predicting a DINOv2 model's embedding of an image from the corresponding fMRI signal and then providing its [CLS] token as the prefix to the GPT-2 language model which decreases computational requirements considerably. Additionally, instead of commonly used Linear Regression, we explore 3D Convolutional Neural Network mapping of fMRI signals to image embedding space for better accounting positional information of voxels.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-05</td>
<td style='padding: 8px;'>Reevaluating Specificity in Neuroimaging: Implications for the Salience Network and Methodological Rigor</td>
<td style='padding: 6px;'>Tommaso Costa, Franco Cauda</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02568v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The accurate assessment of neuroimaging specificity is critical for advancing our understanding of brain disorders. Current methodologies often rely on frequentist approaches and limited cross-pathology comparisons, leading to potential overestimations of specificity. This study critiques these limitations, highlighting the inherent shortcomings of frequentist methods in specificity calculations and the necessity of comprehensive control conditions. Through a review of the Bayesian framework, we demonstrate its superiority in evaluating specificity by incorporating probabilistic modeling and robust reverse inference. The work also emphasizes the pivotal role of well-defined control conditions in mitigating overlap among brain pathologies, particularly within shared networks like the salience network. By applying Bayesian tools such as BACON (Bayes fACtor mOdeliNg), we validate the ability to derive disease-specific patterns, contrasting these with the narrower findings of frequentist analyses. This paper underscores the importance of Bayesian methodologies and extensive meta-analytic datasets in overcoming existing challenges, ultimately paving the way for more precise neuroimaging studies.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-04</td>
<td style='padding: 8px;'>On The Causal Network Of Face-selective Regions In Human Brain During Movie Watching</td>
<td style='padding: 6px;'>Ali Bavafa, Gholam-Ali Hossein-Zadeh</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02333v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding the causal interactions in simple brain tasks, such as face detection, remains a challenging and ambiguous process for researchers. In this study, we address this issue by employing a novel causal discovery method -- Directed Acyclic Graphs via M-matrices for Acyclicity (DAGMA) -- to investigate the causal structure of the brain's face-selective network and gain deeper insights into its mechanism. Using natural movie stimuli, we extract causal network of face-selective regions and analyze how frames containing faces influence this network. Our findings reveal that the presence of faces in the stimuli have causal effect both on the number and strength of causal connections within the network. Additionally, our results highlight the crucial role of subcortical regions in satisfying causal sufficiency, emphasizing its importance in causal studies of brain. This study provides a new perspective on understanding the causal architecture of the face-selective network of the brain, motivating further research on neural causality.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>Integrating Language-Image Prior into EEG Decoding for Cross-Task Zero-Calibration RSVP-BCI</td>
<td style='padding: 6px;'>Xujin Li, Wei Wei, Shuang Qiu, Xinyi Zhang, Fu Li, Huiguang He</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02841v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Rapid Serial Visual Presentation (RSVP)-based Brain-Computer Interface (BCI) is an effective technology used for information detection by detecting Event-Related Potentials (ERPs). The current RSVP decoding methods can perform well in decoding EEG signals within a single RSVP task, but their decoding performance significantly decreases when directly applied to different RSVP tasks without calibration data from the new tasks. This limits the rapid and efficient deployment of RSVP-BCI systems for detecting different categories of targets in various scenarios. To overcome this limitation, this study aims to enhance the cross-task zero-calibration RSVP decoding performance. First, we design three distinct RSVP tasks for target image retrieval and build an open-source dataset containing EEG signals and corresponding stimulus images. Then we propose an EEG with Language-Image Prior fusion Transformer (ELIPformer) for cross-task zero-calibration RSVP decoding. Specifically, we propose a prompt encoder based on the language-image pre-trained model to extract language-image features from task-specific prompts and stimulus images as prior knowledge for enhancing EEG decoding. A cross bidirectional attention mechanism is also adopted to facilitate the effective feature fusion and alignment between the EEG and language-image features. Extensive experiments demonstrate that the proposed model achieves superior performance in cross-task zero-calibration RSVP decoding, which promotes the RSVP-BCI system from research to practical application.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-05</td>
<td style='padding: 8px;'>Towards Decoding Developer Cognition in the Age of AI Assistants</td>
<td style='padding: 6px;'>Ebtesam Al Haque, Chris Brown, Thomas D. LaToza, Brittany Johnson</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02684v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Background: The increasing adoption of AI assistants in programming has led to numerous studies exploring their benefits. While developers consistently report significant productivity gains from these tools, empirical measurements often show more modest improvements. While prior research has documented self-reported experiences with AI-assisted programming tools, little to no work has been done to understand their usage patterns and the actual cognitive load imposed in practice. Objective: In this exploratory study, we aim to investigate the role AI assistants play in developer productivity. Specifically, we are interested in how developers' expertise levels influence their AI usage patterns, and how these patterns impact their actual cognitive load and productivity during development tasks. We also seek to better understand how this relates to their perceived productivity. Method: We propose a controlled observational study combining physiological measurements (EEG and eye tracking) with interaction data to examine developers' use of AI-assisted programming tools. We will recruit professional developers to complete programming tasks both with and without AI assistance while measuring their cognitive load and task completion time. Through pre- and post-task questionnaires, we will collect data on perceived productivity and cognitive load using NASA-TLX.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-05</td>
<td style='padding: 8px;'>Quantum Cognition-Inspired EEG-based Recommendation via Graph Neural Networks</td>
<td style='padding: 6px;'>Jinkun Han, Wei Li, Yingshu Li, Zhipeng Cai</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02671v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Current recommendation systems recommend goods by considering users' historical behaviors, social relations, ratings, and other multi-modals. Although outdated user information presents the trends of a user's interests, no recommendation system can know the users' real-time thoughts indeed. With the development of brain-computer interfaces, it is time to explore next-generation recommenders that show users' real-time thoughts without delay. Electroencephalography (EEG) is a promising method of collecting brain signals because of its convenience and mobility. Currently, there is only few research on EEG-based recommendations due to the complexity of learning human brain activity. To explore the utility of EEG-based recommendation, we propose a novel neural network model, QUARK, combining Quantum Cognition Theory and Graph Convolutional Networks for accurate item recommendations. Compared with the state-of-the-art recommendation models, the superiority of QUARK is confirmed via extensive experiments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-05</td>
<td style='padding: 8px;'>LLMs Help Alleviate the Cross-Subject Variability in Brain Signal and Language Alignment</td>
<td style='padding: 6px;'>Yifei Liu, Hengwei Ye, Shuhang Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02621v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Decoding human activity from EEG signals has long been a popular research topic. While recent studies have increasingly shifted focus from single-subject to cross-subject analysis, few have explored the model's ability to perform zero-shot predictions on EEG signals from previously unseen subjects. This research aims to investigate whether deep learning methods can capture subject-independent semantic information inherent in human EEG signals. Such insights are crucial for Brain-Computer Interfaces (BCI) because, on one hand, they demonstrate the model's robustness against subject-specific temporal biases, and on the other, they significantly enhance the generalizability of downstream tasks. We employ Large Language Models (LLMs) as denoising agents to extract subject-independent semantic features from noisy EEG signals. Experimental results, including ablation studies, highlight the pivotal role of LLMs in decoding subject-independent semantic information from noisy EEG data. We hope our findings will contribute to advancing BCI research and assist both academia and industry in applying EEG signals to a broader range of applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-03</td>
<td style='padding: 8px;'>Aesthetic Matters in Music Perception for Image Stylization: A Emotion-driven Music-to-Visual Manipulation</td>
<td style='padding: 6px;'>Junjie Xu, Xingjiao Wu, Tanren Yao, Zihao Zhang, Jiayang Bei, Wu Wen, Liang He</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.01700v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Emotional information is essential for enhancing human-computer interaction and deepening image understanding. However, while deep learning has advanced image recognition, the intuitive understanding and precise control of emotional expression in images remain challenging. Similarly, music research largely focuses on theoretical aspects, with limited exploration of its emotional dimensions and their integration with visual arts. To address these gaps, we introduce EmoMV, an emotion-driven music-to-visual manipulation method that manipulates images based on musical emotions. EmoMV combines bottom-up processing of music elements-such as pitch and rhythm-with top-down application of these emotions to visual aspects like color and lighting. We evaluate EmoMV using a multi-scale framework that includes image quality metrics, aesthetic assessments, and EEG measurements to capture real-time emotional responses. Our results demonstrate that EmoMV effectively translates music's emotional content into visually compelling images, advancing multimodal emotional integration and opening new avenues for creative industries and interactive technologies.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-03</td>
<td style='padding: 8px;'>Improved Feature Extraction Network for Neuro-Oriented Target Speaker Extraction</td>
<td style='padding: 6px;'>Cunhang Fan, Youdian Gao, Zexu Pan, Jingjing Zhang, Hongyu Zhang, Jie Zhang, Zhao Lv</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.01673v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The recent rapid development of auditory attention decoding (AAD) offers the possibility of using electroencephalography (EEG) as auxiliary information for target speaker extraction. However, effectively modeling long sequences of speech and resolving the identity of the target speaker from EEG signals remains a major challenge. In this paper, an improved feature extraction network (IFENet) is proposed for neuro-oriented target speaker extraction, which mainly consists of a speech encoder with dual-path Mamba and an EEG encoder with Kolmogorov-Arnold Networks (KAN). We propose SpeechBiMamba, which makes use of dual-path Mamba in modeling local and global speech sequences to extract speech features. In addition, we propose EEGKAN to effectively extract EEG features that are closely related to the auditory stimuli and locate the target speaker through the subject's attention information. Experiments on the KUL and AVED datasets show that IFENet outperforms the state-of-the-art model, achieving 36\% and 29\% relative improvements in terms of scale-invariant signal-to-distortion ratio (SI-SDR) under an open evaluation condition.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-30</td>
<td style='padding: 8px;'>Human-AI Teaming Using Large Language Models: Boosting Brain-Computer Interfacing (BCI) and Brain Research</td>
<td style='padding: 6px;'>Maryna Kapitonova, Tonio Ball</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.01451v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recently, there is an increasing interest in using artificial intelligence (AI) to automate aspects of the research process, or even autonomously conduct the full research cycle from idea generation, over data analysis, to composing and evaluation of scientific manuscripts. Examples of working AI scientist systems have been demonstrated for computer science tasks and running molecular biology labs. While some approaches aim for full autonomy of the scientific AI, others rather aim for leveraging human-AI teaming. Here, we address how to adapt such approaches for boosting Brain-Computer Interface (BCI) development, as well as brain research resp. neuroscience at large. We argue that at this time, a strong emphasis on human-AI teaming, in contrast to fully autonomous AI BCI researcher will be the most promising way forward. We introduce the collaborative workspaces concept for human-AI teaming based on a set of Janusian design principles, looking both ways, to the human as well as to the AI side. Based on these principles, we present ChatBCI, a Python-based toolbox for enabling human-AI collaboration based on interaction with Large Language Models (LLMs), designed for BCI research and development projects. We show how ChatBCI was successfully used in a concrete BCI project on advancing motor imagery decoding from EEG signals. Our approach can be straightforwardly extended to broad neurotechnological and neuroscientific topics, and may by design facilitate human expert knowledge transfer to scientific AI systems in general.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-28</td>
<td style='padding: 8px;'>Improving SSVEP BCI Spellers With Data Augmentation and Language Models</td>
<td style='padding: 6px;'>Joseph Zhang, Ruiming Zhang, Kipngeno Koech, David Hill, Kateryna Shapovalenko</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.20052v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Steady-State Visual Evoked Potential (SSVEP) spellers are a promising communication tool for individuals with disabilities. This Brain-Computer Interface utilizes scalp potential data from (electroencephalography) EEG electrodes on a subject's head to decode specific letters or arbitrary targets the subject is looking at on a screen. However, deep neural networks for SSVEP spellers often suffer from low accuracy and poor generalizability to unseen subjects, largely due to the high variability in EEG data. In this study, we propose a hybrid approach combining data augmentation and language modeling to enhance the performance of SSVEP spellers. Using the Benchmark dataset from Tsinghua University, we explore various data augmentation techniques, including frequency masking, time masking, and noise injection, to improve the robustness of deep learning models. Additionally, we integrate a language model (CharRNN) with EEGNet to incorporate linguistic context, significantly enhancing word-level decoding accuracy. Our results demonstrate accuracy improvements of up to 2.9 percent over the baseline, with time masking and language modeling showing the most promise. This work paves the way for more accurate and generalizable SSVEP speller systems, offering improved communication solutions for individuals with disabilities.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-28</td>
<td style='padding: 8px;'>Comprehensive Review of EEG-to-Output Research: Decoding Neural Signals into Images, Videos, and Audio</td>
<td style='padding: 6px;'>Yashvir Sabharwal, Balaji Rama</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.19999v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) is an invaluable tool in neuroscience, offering insights into brain activity with high temporal resolution. Recent advancements in machine learning and generative modeling have catalyzed the application of EEG in reconstructing perceptual experiences, including images, videos, and audio. This paper systematically reviews EEG-to-output research, focusing on state-of-the-art generative methods, evaluation metrics, and data challenges. Using PRISMA guidelines, we analyze 1800 studies and identify key trends, challenges, and opportunities in the field. The findings emphasize the potential of advanced models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Transformers, while highlighting the pressing need for standardized datasets and cross-subject generalization. A roadmap for future research is proposed that aims to improve decoding accuracy and broadening real-world applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-27</td>
<td style='padding: 8px;'>EEG-Reptile: An Automatized Reptile-Based Meta-Learning Library for BCIs</td>
<td style='padding: 6px;'>Daniil A. Berdyshev, Artem M. Grachev, Sergei L. Shishkin, Bogdan L. Kozyrskiy</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.19725v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Meta-learning, i.e., "learning to learn", is a promising approach to enable efficient BCI classifier training with limited amounts of data. It can effectively use collections of in some way similar classification tasks, with rapid adaptation to new tasks where only minimal data are available. However, applying meta-learning to existing classifiers and BCI tasks requires significant effort. To address this issue, we propose EEG-Reptile, an automated library that leverages meta-learning to improve classification accuracy of neural networks in BCIs and other EEG-based applications. It utilizes the Reptile meta-learning algorithm to adapt neural network classifiers of EEG data to the inter-subject domain, allowing for more efficient fine-tuning for a new subject on a small amount of data. The proposed library incorporates an automated hyperparameter tuning module, a data management pipeline, and an implementation of the Reptile meta-learning algorithm. EEG-Reptile automation level allows using it without deep understanding of meta-learning. We demonstrate the effectiveness of EEG-Reptile on two benchmark datasets (BCI IV 2a, Lee2019 MI) and three neural network architectures (EEGNet, FBCNet, EEG-Inception). Our library achieved improvement in both zero-shot and few-shot learning scenarios compared to traditional transfer learning approaches.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>Integrating Language-Image Prior into EEG Decoding for Cross-Task Zero-Calibration RSVP-BCI</td>
<td style='padding: 6px;'>Xujin Li, Wei Wei, Shuang Qiu, Xinyi Zhang, Fu Li, Huiguang He</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02841v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Rapid Serial Visual Presentation (RSVP)-based Brain-Computer Interface (BCI) is an effective technology used for information detection by detecting Event-Related Potentials (ERPs). The current RSVP decoding methods can perform well in decoding EEG signals within a single RSVP task, but their decoding performance significantly decreases when directly applied to different RSVP tasks without calibration data from the new tasks. This limits the rapid and efficient deployment of RSVP-BCI systems for detecting different categories of targets in various scenarios. To overcome this limitation, this study aims to enhance the cross-task zero-calibration RSVP decoding performance. First, we design three distinct RSVP tasks for target image retrieval and build an open-source dataset containing EEG signals and corresponding stimulus images. Then we propose an EEG with Language-Image Prior fusion Transformer (ELIPformer) for cross-task zero-calibration RSVP decoding. Specifically, we propose a prompt encoder based on the language-image pre-trained model to extract language-image features from task-specific prompts and stimulus images as prior knowledge for enhancing EEG decoding. A cross bidirectional attention mechanism is also adopted to facilitate the effective feature fusion and alignment between the EEG and language-image features. Extensive experiments demonstrate that the proposed model achieves superior performance in cross-task zero-calibration RSVP decoding, which promotes the RSVP-BCI system from research to practical application.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-05</td>
<td style='padding: 8px;'>LLMs Help Alleviate the Cross-Subject Variability in Brain Signal and Language Alignment</td>
<td style='padding: 6px;'>Yifei Liu, Hengwei Ye, Shuhang Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02621v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Decoding human activity from EEG signals has long been a popular research topic. While recent studies have increasingly shifted focus from single-subject to cross-subject analysis, few have explored the model's ability to perform zero-shot predictions on EEG signals from previously unseen subjects. This research aims to investigate whether deep learning methods can capture subject-independent semantic information inherent in human EEG signals. Such insights are crucial for Brain-Computer Interfaces (BCI) because, on one hand, they demonstrate the model's robustness against subject-specific temporal biases, and on the other, they significantly enhance the generalizability of downstream tasks. We employ Large Language Models (LLMs) as denoising agents to extract subject-independent semantic features from noisy EEG signals. Experimental results, including ablation studies, highlight the pivotal role of LLMs in decoding subject-independent semantic information from noisy EEG data. We hope our findings will contribute to advancing BCI research and assist both academia and industry in applying EEG signals to a broader range of applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-03</td>
<td style='padding: 8px;'>Subject Specific Deep Learning Model for Motor Imagery Direction Decoding</td>
<td style='padding: 6px;'>Praveen K. Parashiva, Sagila Gangadaran, A. P. Vinod</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.01725v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Hemispheric strokes impair motor control in contralateral body parts, necessitating effective rehabilitation strategies. Motor Imagery-based Brain-Computer Interfaces (MI-BCIs) promote neuroplasticity, aiding the recovery of motor functions. While deep learning has shown promise in decoding MI actions for stroke rehabilitation, existing studies largely focus on bilateral MI actions and are limited to offline evaluations. Decoding directional information from unilateral MI, however, offers a more natural control interface with greater degrees of freedom but remains challenging due to spatially overlapping neural activity. This work proposes a novel deep learning framework for online decoding of binary directional MI signals from the dominant hand of 20 healthy subjects. The proposed method employs EEGNet-based convolutional filters to extract temporal and spatial features. The EEGNet model is enhanced by Squeeze-and-Excitation (SE) layers that rank the electrode importance and feature maps. A subject-independent model is initially trained using calibration data from multiple subjects and fine-tuned for subject-specific adaptation. The performance of the proposed method is evaluated using subject-specific online session data. The proposed method achieved an average right vs left binary direction decoding accuracy of 58.7 +\- 8% for unilateral MI tasks, outperforming the existing deep learning models. Additionally, the SE-layer ranking offers insights into electrode contribution, enabling potential subject-specific BCI optimization. The findings highlight the efficacy of the proposed method in advancing MI-BCI applications for a more natural and effective control of BCI systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-30</td>
<td style='padding: 8px;'>Human-AI Teaming Using Large Language Models: Boosting Brain-Computer Interfacing (BCI) and Brain Research</td>
<td style='padding: 6px;'>Maryna Kapitonova, Tonio Ball</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.01451v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recently, there is an increasing interest in using artificial intelligence (AI) to automate aspects of the research process, or even autonomously conduct the full research cycle from idea generation, over data analysis, to composing and evaluation of scientific manuscripts. Examples of working AI scientist systems have been demonstrated for computer science tasks and running molecular biology labs. While some approaches aim for full autonomy of the scientific AI, others rather aim for leveraging human-AI teaming. Here, we address how to adapt such approaches for boosting Brain-Computer Interface (BCI) development, as well as brain research resp. neuroscience at large. We argue that at this time, a strong emphasis on human-AI teaming, in contrast to fully autonomous AI BCI researcher will be the most promising way forward. We introduce the collaborative workspaces concept for human-AI teaming based on a set of Janusian design principles, looking both ways, to the human as well as to the AI side. Based on these principles, we present ChatBCI, a Python-based toolbox for enabling human-AI collaboration based on interaction with Large Language Models (LLMs), designed for BCI research and development projects. We show how ChatBCI was successfully used in a concrete BCI project on advancing motor imagery decoding from EEG signals. Our approach can be straightforwardly extended to broad neurotechnological and neuroscientific topics, and may by design facilitate human expert knowledge transfer to scientific AI systems in general.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-30</td>
<td style='padding: 8px;'>Effective and Efficient Intracortical Brain Signal Decoding with Spiking Neural Networks</td>
<td style='padding: 6px;'>Haotian Fu, Peng Zhang, Song Yang, Herui Zhang, Ziwei Wang, Dongrui Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.20714v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A brain-computer interface (BCI) facilitates direct interaction between the brain and external devices. To concurrently achieve high decoding accuracy and low energy consumption in invasive BCIs, we propose a novel spiking neural network (SNN) framework incorporating local synaptic stabilization (LSS) and channel-wise attention (CA), termed LSS-CA-SNN. LSS optimizes neuronal membrane potential dynamics, boosting classification performance, while CA refines neuronal activation, effectively reducing energy consumption. Furthermore, we introduce SpikeDrop, a data augmentation strategy designed to expand the training dataset thus enhancing model generalizability. Experiments on invasive spiking datasets recorded from two rhesus macaques demonstrated that LSS-CA-SNN surpassed state-of-the-art artificial neural networks (ANNs) in both decoding accuracy and energy efficiency, achieving 0.80-3.87% performance gains and 14.78-43.86 times energy saving. This study highlights the potential of LSS-CA-SNN and SpikeDrop in advancing invasive BCI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-28</td>
<td style='padding: 8px;'>Improving SSVEP BCI Spellers With Data Augmentation and Language Models</td>
<td style='padding: 6px;'>Joseph Zhang, Ruiming Zhang, Kipngeno Koech, David Hill, Kateryna Shapovalenko</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.20052v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Steady-State Visual Evoked Potential (SSVEP) spellers are a promising communication tool for individuals with disabilities. This Brain-Computer Interface utilizes scalp potential data from (electroencephalography) EEG electrodes on a subject's head to decode specific letters or arbitrary targets the subject is looking at on a screen. However, deep neural networks for SSVEP spellers often suffer from low accuracy and poor generalizability to unseen subjects, largely due to the high variability in EEG data. In this study, we propose a hybrid approach combining data augmentation and language modeling to enhance the performance of SSVEP spellers. Using the Benchmark dataset from Tsinghua University, we explore various data augmentation techniques, including frequency masking, time masking, and noise injection, to improve the robustness of deep learning models. Additionally, we integrate a language model (CharRNN) with EEGNet to incorporate linguistic context, significantly enhancing word-level decoding accuracy. Our results demonstrate accuracy improvements of up to 2.9 percent over the baseline, with time masking and language modeling showing the most promise. This work paves the way for more accurate and generalizable SSVEP speller systems, offering improved communication solutions for individuals with disabilities.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-27</td>
<td style='padding: 8px;'>EEG-Reptile: An Automatized Reptile-Based Meta-Learning Library for BCIs</td>
<td style='padding: 6px;'>Daniil A. Berdyshev, Artem M. Grachev, Sergei L. Shishkin, Bogdan L. Kozyrskiy</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.19725v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Meta-learning, i.e., "learning to learn", is a promising approach to enable efficient BCI classifier training with limited amounts of data. It can effectively use collections of in some way similar classification tasks, with rapid adaptation to new tasks where only minimal data are available. However, applying meta-learning to existing classifiers and BCI tasks requires significant effort. To address this issue, we propose EEG-Reptile, an automated library that leverages meta-learning to improve classification accuracy of neural networks in BCIs and other EEG-based applications. It utilizes the Reptile meta-learning algorithm to adapt neural network classifiers of EEG data to the inter-subject domain, allowing for more efficient fine-tuning for a new subject on a small amount of data. The proposed library incorporates an automated hyperparameter tuning module, a data management pipeline, and an implementation of the Reptile meta-learning algorithm. EEG-Reptile automation level allows using it without deep understanding of meta-learning. We demonstrate the effectiveness of EEG-Reptile on two benchmark datasets (BCI IV 2a, Lee2019 MI) and three neural network architectures (EEGNet, FBCNet, EEG-Inception). Our library achieved improvement in both zero-shot and few-shot learning scenarios compared to traditional transfer learning approaches.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-24</td>
<td style='padding: 8px;'>Low count of optically pumped magnetometers furnishes a reliable real-time access to sensorimotor rhythm</td>
<td style='padding: 6px;'>Nikita Fedosov, Daria Medvedeva, Oleg Shevtsov, Alexei Ossadtchi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.18353v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study presents an analysis of sensorimotor rhythms using an advanced, optically-pumped magnetoencephalography (OPM-MEG) system - a novel and rapidly developing technology. We conducted real-movement and motor imagery experiments with nine participants across two distinct magnetically-shielded environments: one featuring an analog active suppression system and the other a digital implementation. Our findings demonstrate that, under optimal recording conditions, OPM sensors provide highly informative signals, suitable for use in practical motor imagery brain-computer interface (BCI) applications. We further examine the feasibility of a portable, low-sensor-count OPM-based BCI under varied experimental setups, highlighting its potential for real-time control of external devices via user intentions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-20</td>
<td style='padding: 8px;'>MarkovType: A Markov Decision Process Strategy for Non-Invasive Brain-Computer Interfaces Typing Systems</td>
<td style='padding: 6px;'>Elifnur Sunger, Yunus Bicer, Deniz Erdogmus, Tales Imbiriba</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.15862v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-Computer Interfaces (BCIs) help people with severe speech and motor disabilities communicate and interact with their environment using neural activity. This work focuses on the Rapid Serial Visual Presentation (RSVP) paradigm of BCIs using noninvasive electroencephalography (EEG). The RSVP typing task is a recursive task with multiple sequences, where users see only a subset of symbols in each sequence. Extensive research has been conducted to improve classification in the RSVP typing task, achieving fast classification. However, these methods struggle to achieve high accuracy and do not consider the typing mechanism in the learning procedure. They apply binary target and non-target classification without including recursive training. To improve performance in the classification of symbols while controlling the classification speed, we incorporate the typing setup into training by proposing a Partially Observable Markov Decision Process (POMDP) approach. To the best of our knowledge, this is the first work to formulate the RSVP typing task as a POMDP for recursive classification. Experiments show that the proposed approach, MarkovType, results in a more accurate typing system compared to competitors. Additionally, our experiments demonstrate that while there is a trade-off between accuracy and speed, MarkovType achieves the optimal balance between these factors compared to other methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-20</td>
<td style='padding: 8px;'>Predicting Artificial Neural Network Representations to Learn Recognition Model for Music Identification from Brain Recordings</td>
<td style='padding: 6px;'>Taketo Akama, Zhuohao Zhang, Pengcheng Li, Kotaro Hongo, Hiroaki Kitano, Shun Minamikawa, Natalia Polouliakh</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.15560v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent studies have demonstrated that the representations of artificial neural networks (ANNs) can exhibit notable similarities to cortical representations when subjected to identical auditory sensory inputs. In these studies, the ability to predict cortical representations is probed by regressing from ANN representations to cortical representations. Building upon this concept, our approach reverses the direction of prediction: we utilize ANN representations as a supervisory signal to train recognition models using noisy brain recordings obtained through non-invasive measurements. Specifically, we focus on constructing a recognition model for music identification, where electroencephalography (EEG) brain recordings collected during music listening serve as input. By training an EEG recognition model to predict ANN representations-representations associated with music identification-we observed a substantial improvement in classification accuracy. This study introduces a novel approach to developing recognition models for brain recordings in response to external auditory stimuli. It holds promise for advancing brain-computer interfaces (BCI), neural decoding techniques, and our understanding of music cognition. Furthermore, it provides new insights into the relationship between auditory brain activity and ANN representations.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-05</td>
<td style='padding: 8px;'>Decoding fMRI Data into Captions using Prefix Language Modeling</td>
<td style='padding: 6px;'>Vyacheslav Shen, Kassymzhomart Kunanbayev, Dae-Shik Kim</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02570v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>With the advancements in Large Language and Latent Diffusion models, brain decoding has achieved remarkable results in recent years. The works on the NSD dataset, with stimuli images from the COCO dataset, leverage the embeddings from the CLIP model for image reconstruction and GIT for captioning. However, the current captioning approach introduces the challenge of potential data contamination given that the GIT model was trained on the COCO dataset. In this work, we present an alternative method for decoding brain signals into image captions by predicting a DINOv2 model's embedding of an image from the corresponding fMRI signal and then providing its [CLS] token as the prefix to the GPT-2 language model which decreases computational requirements considerably. Additionally, instead of commonly used Linear Regression, we explore 3D Convolutional Neural Network mapping of fMRI signals to image embedding space for better accounting positional information of voxels.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-01</td>
<td style='padding: 8px;'>Sparse identification of evolution equations via Bayesian model selection</td>
<td style='padding: 6px;'>Tim W. Kroll, Oliver Kamps</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.01476v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The quantitative formulation of evolution equations is the backbone for prediction, control, and understanding of dynamical systems across diverse scientific fields. Besides deriving differential equations for dynamical systems based on basic scientific reasoning or prior knowledge in recent times a growing interest emerged to infer these equations purely from data. In this article, we introduce a novel method for the sparse identification of nonlinear dynamical systems from observational data, based on the observation how the key challenges of the quality of time derivatives and sampling rates influence this problem. Our approach combines system identification based on thresholded least squares minimization with additional error measures that account for both the deviation between the model and the time derivative of the data, and the integrated performance of the model in forecasting dynamics. Specifically, we integrate a least squares error as well as the Wasserstein metric for estimated models and combine them within a Bayesian optimization framework to efficiently determine optimal hyperparameters for thresholding and weighting of the different error norms. Additionally, we employ distinct regularization parameters for each differential equation in the system, enhancing the method's precision and flexibility. We demonstrate the capabilities of our approach through applications to dynamical fMRI data and the prototypical example of a wake flow behind a cylinder. In the wake flow problem, our method identifies a sparse, accurate model that correctly captures transient dynamics, oscillation periods, and phase information, outperforming existing methods. In the fMRI example, we show how our approach extracts insights from a trained recurrent neural network, offering a novel avenue for explainable AI by inferring differential equations that capture potentially causal relationships.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>The Algonauts Project 2025 Challenge: How the Human Brain Makes Sense of Multimodal Movies</td>
<td style='padding: 6px;'>Alessandro T. Gifford, Domenic Bersch, Marie St-Laurent, Basile Pinsard, Julie Boyle, Lune Bellec, Aude Oliva, Gemma Roig, Radoslaw M. Cichy</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.00504v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>There is growing symbiosis between artificial and biological intelligence sciences: neural principles inspire new intelligent machines, which are in turn used to advance our theoretical understanding of the brain. To promote further collaboration between biological and artificial intelligence researchers, we introduce the 2025 edition of the Algonauts Project challenge: How the Human Brain Makes Sense of Multimodal Movies (https://algonautsproject.com/). In collaboration with the Courtois Project on Neuronal Modelling (CNeuroMod), this edition aims to bring forth a new generation of brain encoding models that are multimodal and that generalize well beyond their training distribution, by training them on the largest dataset of fMRI responses to movie watching available to date. Open to all, the 2025 challenge provides transparent, directly comparable results through a public leaderboard that is updated automatically after each submission to facilitate rapid model assessment and guide development. The challenge will end with a session at the 2025 Cognitive Computational Neuroscience (CCN) conference that will feature winning models. We welcome researchers interested in collaborating with the Algonauts Project by contributing ideas and datasets for future challenges.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-31</td>
<td style='padding: 8px;'>STARFormer: A Novel Spatio-Temporal Aggregation Reorganization Transformer of FMRI for Brain Disorder Diagnosis</td>
<td style='padding: 6px;'>Wenhao Dong, Yueyang Li, Weiming Zeng, Lei Chen, Hongjie Yan, Wai Ting Siok, Nizhuan Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.00378v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Many existing methods that use functional magnetic resonance imaging (fMRI) classify brain disorders, such as autism spectrum disorder (ASD) and attention deficit hyperactivity disorder (ADHD), often overlook the integration of spatial and temporal dependencies of the blood oxygen level-dependent (BOLD) signals, which may lead to inaccurate or imprecise classification results. To solve this problem, we propose a Spatio-Temporal Aggregation eorganization ransformer (STARFormer) that effectively captures both spatial and temporal features of BOLD signals by incorporating three key modules. The region of interest (ROI) spatial structure analysis module uses eigenvector centrality (EC) to reorganize brain regions based on effective connectivity, highlighting critical spatial relationships relevant to the brain disorder. The temporal feature reorganization module systematically segments the time series into equal-dimensional window tokens and captures multiscale features through variable window and cross-window attention. The spatio-temporal feature fusion module employs a parallel transformer architecture with dedicated temporal and spatial branches to extract integrated features. The proposed STARFormer has been rigorously evaluated on two publicly available datasets for the classification of ASD and ADHD. The experimental results confirm that the STARFormer achieves state-of-the-art performance across multiple evaluation metrics, providing a more accurate and reliable tool for the diagnosis of brain disorders and biomedical research. The codes will be available at: https://github.com/NZWANG/STARFormer.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-27</td>
<td style='padding: 8px;'>Signatures of prediction during natural listening in MEG data?</td>
<td style='padding: 6px;'>Sahel Azizpour, Britta U. Westner, Jakub Szewczyk, Umut Güçlü, Linda Geerligs</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.19622v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The brain uses contextual information and prior knowledge to anticipate upcoming content during language comprehension. Recent research has shown predictive signals can be revealed in pre-onset ECoG activity during naturalistic narrative listening, by building encoding models based on word embeddings from Large Language Models (LLMs). Similarly, evidence for long-range predictive encoding has been observed in fMRI data, where incorporating embeddings for multiple upcoming words in a narrative improves alignment with brain activity. This study examines whether similar predictive information can be detected in MEG, a technique with higher temporal resolution than fMRI but a lower signal-to-noise ratio than ECoG. Our findings indicate that MEG captures pre-onset representations up to 1 second before word onset, consistent with ECoG results. However, unlike fMRI findings, incorporating future word embeddings did not enhance MEG encoding, even for one word into the future, which suggests that the pre-onset encoding may not reflect predictive processing. This work demonstrates that MEG combined with LLMs is a valuable approach for studying language processing in naturalistic narratives and highlights the need to study further what constitutes evidence for prediction during natural listening.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-27</td>
<td style='padding: 8px;'>UniBrain: A Unified Model for Cross-Subject Brain Decoding</td>
<td style='padding: 6px;'>Zicheng Wang, Zhen Zhao, Luping Zhou, Parashkev Nachev</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.19487v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain decoding aims to reconstruct original stimuli from fMRI signals, providing insights into interpreting mental content. Current approaches rely heavily on subject-specific models due to the complex brain processing mechanisms and the variations in fMRI signals across individuals. Therefore, these methods greatly limit the generalization of models and fail to capture cross-subject commonalities. To address this, we present UniBrain, a unified brain decoding model that requires no subject-specific parameters. Our approach includes a group-based extractor to handle variable fMRI signal lengths, a mutual assistance embedder to capture cross-subject commonalities, and a bilevel feature alignment scheme for extracting subject-invariant features. We validate our UniBrain on the brain decoding benchmark, achieving comparable performance to current state-of-the-art subject-specific models with extremely fewer parameters. We also propose a generalization benchmark to encourage the community to emphasize cross-subject commonalities for more general brain decoding. Our code is available at https://github.com/xiaoyao3302/UniBrain.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-25</td>
<td style='padding: 8px;'>Unveiling Secrets of Brain Function With Generative Modeling: Motion Perception in Primates & Cortical Network Organization in Mice</td>
<td style='padding: 6px;'>Hadi Vafaii</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.19845v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This Dissertation is comprised of two main projects, addressing questions in neuroscience through applications of generative modeling.   Project #1 (Chapter 4) explores how neurons encode features of the external world. I combine Helmholtz's "Perception as Unconscious Inference" -- paralleled by modern generative models like variational autoencoders (VAE) -- with the hierarchical structure of the visual cortex. This combination leads to the development of a hierarchical VAE model, which I test for its ability to mimic neurons from the primate visual cortex in response to motion stimuli. Results show that the hierarchical VAE perceives motion similar to the primate brain. Additionally, the model identifies causal factors of retinal motion inputs, such as object- and self-motion, in a completely unsupervised manner. Collectively, these results suggest that hierarchical inference underlines the brain's understanding of the world, and hierarchical VAEs can effectively model this understanding.   Project #2 (Chapter 5) investigates the spatiotemporal structure of spontaneous brain activity and its reflection of brain states like rest. Using simultaneous fMRI and wide-field Ca2+ imaging data, this project demonstrates that the mouse cortex can be decomposed into overlapping communities, with around half of the cortical regions belonging to multiple communities. Comparisons reveal similarities and differences between networks inferred from fMRI and Ca2+ signals.   The introduction (Chapter 1) is divided similarly to this abstract: sections 1.1 to 1.8 provide background information about Project #1, and sections 1.9 to 1.13 are related to Project #2. Chapter 2 includes historical background, Chapter 3 provides the necessary mathematical background, and finally, Chapter 6 contains concluding remarks and future directions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-23</td>
<td style='padding: 8px;'>BrainMAP: Learning Multiple Activation Pathways in Brain Networks</td>
<td style='padding: 6px;'>Song Wang, Zhenyu Lei, Zhen Tan, Jiaqi Ding, Xinyu Zhao, Yushun Dong, Guorong Wu, Tianlong Chen, Chen Chen, Aiying Zhang, Jundong Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.17404v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional Magnetic Resonance Image (fMRI) is commonly employed to study human brain activity, since it offers insight into the relationship between functional fluctuations and human behavior. To enhance analysis and comprehension of brain activity, Graph Neural Networks (GNNs) have been widely applied to the analysis of functional connectivities (FC) derived from fMRI data, due to their ability to capture the synergistic interactions among brain regions. However, in the human brain, performing complex tasks typically involves the activation of certain pathways, which could be represented as paths across graphs. As such, conventional GNNs struggle to learn from these pathways due to the long-range dependencies of multiple pathways. To address these challenges, we introduce a novel framework BrainMAP to learn Multiple Activation Pathways in Brain networks. BrainMAP leverages sequential models to identify long-range correlations among sequentialized brain regions and incorporates an aggregation module based on Mixture of Experts (MoE) to learn from multiple pathways. Our comprehensive experiments highlight BrainMAP's superior performance. Furthermore, our framework enables explanatory analyses of crucial brain regions involved in tasks. Our code is provided at https://github.com/LzyFischer/Graph-Mamba.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-21</td>
<td style='padding: 8px;'>Multi-atlas Ensemble Graph Neural Network Model For Major Depressive Disorder Detection Using Functional MRI Data</td>
<td style='padding: 6px;'>Nojod M. Alotaibi, Areej M. Alhothali, Manar S. Ali</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.19833v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Major depressive disorder (MDD) is one of the most common mental disorders, with significant impacts on many daily activities and quality of life. It stands as one of the most common mental disorders globally and ranks as the second leading cause of disability. The current diagnostic approach for MDD primarily relies on clinical observations and patient-reported symptoms, overlooking the diverse underlying causes and pathophysiological factors contributing to depression. Therefore, scientific researchers and clinicians must gain a deeper understanding of the pathophysiological mechanisms involved in MDD. There is growing evidence in neuroscience that depression is a brain network disorder, and the use of neuroimaging, such as magnetic resonance imaging (MRI), plays a significant role in identifying and treating MDD. Rest-state functional MRI (rs-fMRI) is among the most popular neuroimaging techniques used to study MDD. Deep learning techniques have been widely applied to neuroimaging data to help with early mental health disorder detection. Recent years have seen a rise in interest in graph neural networks (GNNs), which are deep neural architectures specifically designed to handle graph-structured data like rs-fMRI. This research aimed to develop an ensemble-based GNN model capable of detecting discriminative features from rs-fMRI images for the purpose of diagnosing MDD. Specifically, we constructed an ensemble model by combining features from multiple brain region segmentation atlases to capture brain complexity and detect distinct features more accurately than single atlas-based models. Further, the effectiveness of our model is demonstrated by assessing its performance on a large multi-site MDD dataset. The best performing model among all folds achieved an accuracy of 75.80%, a sensitivity of 88.89%, a specificity of 61.84%, a precision of 71.29%, and an F1-score of 79.12%.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-19</td>
<td style='padding: 8px;'>Accessing the topological properties of human brain functional sub-circuits in Echo State Networks</td>
<td style='padding: 6px;'>Bach Nguyen, Tianlong Chen, Shu Yang, Bojian Hou, Li Shen, Duy Duong-Tran</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.14999v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent years have witnessed an emerging trend in neuromorphic computing that centers around the use of brain connectomics as a blueprint for artificial neural networks. Connectomics-based neuromorphic computing has primarily focused on embedding human brain large-scale structural connectomes (SCs), as estimated from diffusion Magnetic Resonance Imaging (dMRI) modality, to echo-state networks (ESNs). A critical step in ESN embedding requires pre-determined read-in and read-out layers constructed by the induced subgraphs of the embedded reservoir. As \textit{a priori} set of functional sub-circuits are derived from functional MRI (fMRI) modality, it is unknown, till this point, whether the embedding of fMRI-induced sub-circuits/networks onto SCs is well justified from the neuro-physiological perspective and ESN performance across a variety of tasks. This paper proposes a pipeline to implement and evaluate ESNs with various embedded topologies and processing/memorization tasks. To this end, we showed that different performance optimums highly depend on the neuro-physiological characteristics of these pre-determined fMRI-induced sub-circuits. In general, fMRI-induced sub-circuit-embedded ESN outperforms simple bipartite and various null models with feed-forward properties commonly seen in MLP for different tasks and reservoir criticality conditions. We provided a thorough analysis of the topological properties of pre-determined fMRI-induced sub-circuits and highlighted their graph-theoretical properties that play significant roles in determining ESN performance.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-27</td>
<td style='padding: 8px;'>Signatures of prediction during natural listening in MEG data?</td>
<td style='padding: 6px;'>Sahel Azizpour, Britta U. Westner, Jakub Szewczyk, Umut Güçlü, Linda Geerligs</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.19622v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The brain uses contextual information and prior knowledge to anticipate upcoming content during language comprehension. Recent research has shown predictive signals can be revealed in pre-onset ECoG activity during naturalistic narrative listening, by building encoding models based on word embeddings from Large Language Models (LLMs). Similarly, evidence for long-range predictive encoding has been observed in fMRI data, where incorporating embeddings for multiple upcoming words in a narrative improves alignment with brain activity. This study examines whether similar predictive information can be detected in MEG, a technique with higher temporal resolution than fMRI but a lower signal-to-noise ratio than ECoG. Our findings indicate that MEG captures pre-onset representations up to 1 second before word onset, consistent with ECoG results. However, unlike fMRI findings, incorporating future word embeddings did not enhance MEG encoding, even for one word into the future, which suggests that the pre-onset encoding may not reflect predictive processing. This work demonstrates that MEG combined with LLMs is a valuable approach for studying language processing in naturalistic narratives and highlights the need to study further what constitutes evidence for prediction during natural listening.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-24</td>
<td style='padding: 8px;'>Low count of optically pumped magnetometers furnishes a reliable real-time access to sensorimotor rhythm</td>
<td style='padding: 6px;'>Nikita Fedosov, Daria Medvedeva, Oleg Shevtsov, Alexei Ossadtchi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.18353v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study presents an analysis of sensorimotor rhythms using an advanced, optically-pumped magnetoencephalography (OPM-MEG) system - a novel and rapidly developing technology. We conducted real-movement and motor imagery experiments with nine participants across two distinct magnetically-shielded environments: one featuring an analog active suppression system and the other a digital implementation. Our findings demonstrate that, under optimal recording conditions, OPM sensors provide highly informative signals, suitable for use in practical motor imagery brain-computer interface (BCI) applications. We further examine the feasibility of a portable, low-sensor-count OPM-based BCI under varied experimental setups, highlighting its potential for real-time control of external devices via user intentions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-12</td>
<td style='padding: 8px;'>LV-CadeNet: Long View Feature Convolution-Attention Fusion Encoder-Decoder Network for Clinical MEG Spike Detection</td>
<td style='padding: 6px;'>Kuntao Xiao, Xiongfei Wang, Pengfei Teng, Yi Sun, Wanli Yang, Liang Zhang, Hanyang Dong, Guoming Luan, Shurong Sheng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.08896v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>It is widely acknowledged that the epileptic foci can be pinpointed by source localizing interictal epileptic discharges (IEDs) via Magnetoencephalography (MEG). However, manual detection of IEDs, which appear as spikes in MEG data, is extremely labor intensive and requires considerable professional expertise, limiting the broader adoption of MEG technology. Numerous studies have focused on automatic detection of MEG spikes to overcome this challenge, but these efforts often validate their models on synthetic datasets with balanced positive and negative samples. In contrast, clinical MEG data is highly imbalanced, raising doubts on the real-world efficacy of these models. To address this issue, we introduce LV-CadeNet, a Long View feature Convolution-Attention fusion Encoder-Decoder Network, designed for automatic MEG spike detection in real-world clinical scenarios. Beyond addressing the disparity between training data distribution and clinical test data through semi-supervised learning, our approach also mimics human specialists by constructing long view morphological input data. Moreover, we propose an advanced convolution-attention module to extract temporal and spatial features from the input data. LV-CadeNet significantly improves the accuracy of MEG spike detection, boosting it from 42.31\% to 54.88\% on a novel clinical dataset sourced from Sanbo Brain Hospital Capital Medical University. This dataset, characterized by a highly imbalanced distribution of positive and negative samples, accurately represents real-world clinical scenarios.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-11</td>
<td style='padding: 8px;'>Decoding individual words from non-invasive brain recordings across 723 participants</td>
<td style='padding: 6px;'>Stéphane d'Ascoli, Corentin Bel, Jérémy Rapin, Hubert Banville, Yohann Benchetrit, Christophe Pallier, Jean-Rémi King</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.17829v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Deep learning has recently enabled the decoding of language from the neural activity of a few participants with electrodes implanted inside their brain. However, reliably decoding words from non-invasive recordings remains an open challenge. To tackle this issue, we introduce a novel deep learning pipeline to decode individual words from non-invasive electro- (EEG) and magneto-encephalography (MEG) signals. We train and evaluate our approach on an unprecedentedly large number of participants (723) exposed to five million words either written or spoken in English, French or Dutch. Our model outperforms existing methods consistently across participants, devices, languages, and tasks, and can decode words absent from the training set. Our analyses highlight the importance of the recording device and experimental protocol: MEG and reading are easier to decode than EEG and listening, respectively, and it is preferable to collect a large amount of data per participant than to repeat stimuli across a large number of participants. Furthermore, decoding performance consistently increases with the amount of (i) data used for training and (ii) data used for averaging during testing. Finally, single-word predictions show that our model effectively relies on word semantics but also captures syntactic and surface properties such as part-of-speech, word length and even individual letters, especially in the reading condition. Overall, our findings delineate the path and remaining challenges towards building non-invasive brain decoders for natural language.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-06</td>
<td style='padding: 8px;'>Measuring Goal-Directedness</td>
<td style='padding: 6px;'>Matt MacDermott, James Fox, Francesco Belardinelli, Tom Everitt</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.04758v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We define maximum entropy goal-directedness (MEG), a formal measure of goal-directedness in causal models and Markov decision processes, and give algorithms for computing it. Measuring goal-directedness is important, as it is a critical element of many concerns about harm from AI. It is also of philosophical interest, as goal-directedness is a key aspect of agency. MEG is based on an adaptation of the maximum causal entropy framework used in inverse reinforcement learning. It can measure goal-directedness with respect to a known utility function, a hypothesis class of utility functions, or a set of random variables. We prove that MEG satisfies several desiderata and demonstrate our algorithms with small-scale experiments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-29</td>
<td style='padding: 8px;'>Neuroplasticity and Psychedelics: a comprehensive examination of classic and non-classic compounds in pre and clinical models</td>
<td style='padding: 6px;'>Claudio Agnorelli, Meg Spriggs, Kate Godfrey, Gabriela Sawicka, Bettina Bohl, Hannah Douglass, Andrea Fagiolini, Hashemi Parastoo, Robin Carhart-Harris, David Nutt, David Erritzoe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19840v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroplasticity, the ability of the nervous system to adapt throughout an organism's lifespan, offers potential as both a biomarker and treatment target for neuropsychiatric conditions. Psychedelics, a burgeoning category of drugs, are increasingly prominent in psychiatric research, prompting inquiries into their mechanisms of action. Distinguishing themselves from traditional medications, psychedelics demonstrate rapid and enduring therapeutic effects after a single or few administrations, believed to stem from their neuroplasticity-enhancing properties. This review examines how classic psychedelics (e.g., LSD, psilocybin, N,N-DMT) and non-classic psychedelics (e.g., ketamine, MDMA) influence neuroplasticity. Drawing from preclinical and clinical studies, we explore the molecular, structural, and functional changes triggered by these agents. Animal studies suggest psychedelics induce heightened sensitivity of the nervous system to environmental stimuli (meta-plasticity), re-opening developmental windows for long-term structural changes (hyper-plasticity), with implications for mood and behavior. Translating these findings to humans faces challenges due to limitations in current imaging techniques. Nonetheless, promising new directions for human research are emerging, including the employment of novel positron-emission tomography (PET) radioligands, non-invasive brain stimulation methods, and multimodal approaches. By elucidating the interplay between psychedelics and neuroplasticity, this review informs the development of targeted interventions for neuropsychiatric disorders and advances understanding of psychedelics' therapeutic potential.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-29</td>
<td style='padding: 8px;'>On Monitoring Edge-Geodetic Sets of Dynamic Graph</td>
<td style='padding: 6px;'>Zin Mar Myint, Ashish Saxena</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19800v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The concept of a monitoring edge-geodetic set (MEG-set) in a graph $G$, denoted $MEG(G)$, refers to a subset of vertices $MEG(G)\subseteq V(G)$ such that every edge $e$ in $G$ is monitored by some pair of vertices $ u, v \in MEG(G)$, where $e$ lies on all shortest paths between $u$ and $v$. The minimum number of vertices required to form such a set is called the monitoring edge-geodetic number, denoted $meg(G)$. The primary motivation for studying $MEG$-sets in previous works arises from scenarios in which certain edges are removed from $G$. In these cases, the vertices of the $MEG$-set are responsible for detecting these deletions. Such detection is crucial for identifying which edges have been removed from $G$ and need to be repaired. In real life, repairing these edges may be costly, or sometimes it is impossible to repair edges. In this case, the original $MEG$-set may no longer be effective in monitoring the modified graph. This highlights the importance of reassessing and adapting the $MEG$-set after edge deletions. This work investigates the monitoring edge-geodetic properties of graphs, focusing on how the removal of $k$ edges affects the structure of a graph and influences its monitoring capabilities. Specifically, we explore how the monitoring edge-geodetic number $meg(G)$ changes when $k$ edges are removed. The study aims to compare the monitoring properties of the original graph with those of the modified graph and to understand the impact of edge deletions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-14</td>
<td style='padding: 8px;'>Towards Neural Foundation Models for Vision: Aligning EEG, MEG, and fMRI Representations for Decoding, Encoding, and Modality Conversion</td>
<td style='padding: 6px;'>Matteo Ferrante, Tommaso Boccato, Grigorii Rashkov, Nicola Toschi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.09723v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper presents a novel approach towards creating a foundational model for aligning neural data and visual stimuli across multimodal representationsof brain activity by leveraging contrastive learning. We used electroencephalography (EEG), magnetoencephalography (MEG), and functional magnetic resonance imaging (fMRI) data. Our framework's capabilities are demonstrated through three key experiments: decoding visual information from neural data, encoding images into neural representations, and converting between neural modalities. The results highlight the model's ability to accurately capture semantic information across different brain imaging techniques, illustrating its potential in decoding, encoding, and modality conversion tasks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-12</td>
<td style='padding: 8px;'>Search for the X17 particle in $^{7}\mathrm{Li}(\mathrm{p},\mathrm{e}^+ \mathrm{e}^{-}) ^{8}\mathrm{Be}$ processes with the MEG II detector</td>
<td style='padding: 6px;'>The MEG II collaboration, K. Afanaciev, A. M. Baldini, S. Ban, H. Benmansour, G. Boca, P. W. Cattaneo, G. Cavoto, F. Cei, M. Chiappini, A. Corvaglia, G. Dal Maso, A. De Bari, M. De Gerone, L. Ferrari Barusso, M. Francesconi, L. Galli, G. Gallucci, F. Gatti, L. Gerritzen, F. Grancagnolo, E. G. Grandoni, M. Grassi, D. N. Grigoriev, M. Hildebrandt, F. Ignatov, F. Ikeda, T. Iwamoto, S. Karpov, P. -R. Kettle, N. Khomutov, A. Kolesnikov, N. Kravchuk, V. Krylov, N. Kuchinskiy, F. Leonetti, W. Li, V. Malyshev, A. Matsushita, M. Meucci, S. Mihara, W. Molzon, T. Mori, D. Nicolò, H. Nishiguchi, A. Ochi, W. Ootani, A. Oya, D. Palo, M. Panareo, A. Papa, V. Pettinacci, A. Popov, F. Renga, S. Ritt, M. Rossella, A. Rozhdestvensky. S. Scarpellini, P. Schwendimann, G. Signorelli, M. Takahashi, Y. Uchiyama, A. Venturini, B. Vitali, C. Voena, K. Yamamoto, R. Yokota, T. Yonemoto</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.07994v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The observation of a resonance structure in the opening angle of the electron-positron pairs in the $^{7}$Li(p,\ee) $^{8}$Be reaction was claimed and interpreted as the production and subsequent decay of a hypothetical particle (X17). Similar excesses, consistent with this particle, were later observed in processes involving $^{4}$He and $^{12}$C nuclei with the same experimental technique. The MEG II apparatus at PSI, designed to search for the $\mu^+ \rightarrow \mathrm{e}^+ \gamma$ decay, can be exploited to investigate the existence of this particle and study its nature. Protons from a Cockroft-Walton accelerator, with an energy up to 1.1 MeV, were delivered on a dedicated Li-based target. The $\gamma$ and the e$^{+}$e$^{-}$ pair emerging from the $^8\mathrm{Be}^*$ transitions were studied with calorimeters and a spectrometer, featuring a broader angular acceptance than previous experiments. We present in this paper the analysis of a four-week data-taking in 2023 with a beam energy of 1080 keV, resulting in the excitation of two different resonances with Q-value \SI{17.6}{\mega\electronvolt} and \SI{18.1}{\mega\electronvolt}. No significant signal was found, and limits at \SI{90}{\percent} C.L. on the branching ratios (relative to the $\gamma$ emission) of the two resonances to X17 were set, $R_{17.6} < 1.8 \times 10^{-6} $ and $R_{18.1} < 1.2 \times 10^{-5} $.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-07</td>
<td style='padding: 8px;'>MEG: Medical Knowledge-Augmented Large Language Models for Question Answering</td>
<td style='padding: 6px;'>Laura Cabello, Carmen Martin-Turrero, Uchenna Akujuobi, Anders Søgaard, Carlos Bobed</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.03883v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Question answering is a natural language understanding task that involves reasoning over both explicit context and unstated, relevant domain knowledge. Large language models (LLMs), which underpin most contemporary question answering systems, struggle to induce how concepts relate in specialized domains such as medicine. Existing medical LLMs are also costly to train. In this work, we present MEG, a parameter-efficient approach for medical knowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate graph embeddings into the LLM, enabling it to leverage external knowledge in a cost-effective way. We evaluate our method on four popular medical multiple-choice datasets and show that LLMs greatly benefit from the factual grounding provided by knowledge graph embeddings. MEG attains an average of +10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized models like BioMistral. We also show results based on Llama-3. Finally, we show that MEG's performance remains robust to the choice of graph encoder.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-04</td>
<td style='padding: 8px;'>Asynchronous Hebbian/anti-Hebbian networks</td>
<td style='padding: 6px;'>Henrique Reis Aguiar, Matthias H. Hennig</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02402v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-27</td>
<td style='padding: 8px;'>NeuroAI for AI Safety</td>
<td style='padding: 6px;'>Patrick Mineault, Niccolò Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.18526v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-21</td>
<td style='padding: 8px;'>Evaluating Representational Similarity Measures from the Lens of Functional Correspondence</td>
<td style='padding: 6px;'>Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.14633v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-25</td>
<td style='padding: 8px;'>A prescriptive theory for brain-like inference</td>
<td style='padding: 6px;'>Hadi Vafaii, Dekel Galor, Jacob L. Yates</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.19315v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The Evidence Lower Bound (ELBO) is a widely used objective for training deep generative models, such as Variational Autoencoders (VAEs). In the neuroscience literature, an identical objective is known as the variational free energy, hinting at a potential unified framework for brain function and machine learning. Despite its utility in interpreting generative models, including diffusion models, ELBO maximization is often seen as too broad to offer prescriptive guidance for specific architectures in neuroscience or machine learning. In this work, we show that maximizing ELBO under Poisson assumptions for general sequence data leads to a spiking neural network that performs Bayesian posterior inference through its membrane potential dynamics. The resulting model, the iterative Poisson VAE (iP-VAE), has a closer connection to biological neurons than previous brain-inspired predictive coding models based on Gaussian assumptions. Compared to amortized and iterative VAEs, iP-VAElearns sparser representations and exhibits superior generalization to out-of-distribution samples. These findings suggest that optimizing ELBO, combined with Poisson assumptions, provides a solid foundation for developing prescriptive theories in NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-09</td>
<td style='padding: 8px;'>Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models</td>
<td style='padding: 6px;'>Emily Cheng, Richard J. Antonello</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.05771v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first "composition" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-07-22</td>
<td style='padding: 8px;'>Predictive Coding Networks and Inference Learning: Tutorial and Survey</td>
<td style='padding: 6px;'>Björn van Zwol, Ro Jefferson, Egon L. van den Broek</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2407.04117v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent years have witnessed a growing call for renewed emphasis on neuroscience-inspired approaches in artificial intelligence research, under the banner of NeuroAI. A prime example of this is predictive coding networks (PCNs), based on the neuroscientific framework of predictive coding. This framework views the brain as a hierarchical Bayesian inference model that minimizes prediction errors through feedback connections. Unlike traditional neural networks trained with backpropagation (BP), PCNs utilize inference learning (IL), a more biologically plausible algorithm that explains patterns of neural activity that BP cannot. Historically, IL has been more computationally intensive, but recent advancements have demonstrated that it can achieve higher efficiency than BP with sufficient parallelization. Furthermore, PCNs can be mathematically considered a superset of traditional feedforward neural networks (FNNs), significantly extending the range of trainable architectures. As inherently probabilistic (graphical) latent variable models, PCNs provide a versatile framework for both supervised learning and unsupervised (generative) modeling that goes beyond traditional artificial neural networks. This work provides a comprehensive review and detailed formal specification of PCNs, particularly situating them within the context of modern ML methods. Additionally, we introduce a Python library (PRECO) for practical implementation. This positions PC as a promising framework for future ML innovations.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-10-29</td>
<td style='padding: 8px;'>Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis</td>
<td style='padding: 6px;'>Mitchell Ostrow, Adam Eisen, Leo Kozachkov, Ila Fiete</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2306.10168v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-05-25</td>
<td style='padding: 8px;'>Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture</td>
<td style='padding: 6px;'>Galen Pogoncheff, Jacob Granley, Michael Beyeler</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2305.11275v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-09</td>
<td style='padding: 8px;'>A Deep Probabilistic Spatiotemporal Framework for Dynamic Graph Representation Learning with Application to Brain Disorder Identification</td>
<td style='padding: 6px;'>Sin-Yee Yap, Junn Yong Loo, Chee-Ming Ting, Fuad Noman, Raphael C. -W. Phan, Adeel Razi, David L. Dowe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2302.07243v4' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent applications of pattern recognition techniques on brain connectome classification using functional connectivity (FC) are shifting towards acknowledging the non-Euclidean topology and dynamic aspects of brain connectivity across time. In this paper, a deep spatiotemporal variational Bayes (DSVB) framework is proposed to learn time-varying topological structures in dynamic FC networks for identifying autism spectrum disorder (ASD) in human participants. The framework incorporates a spatial-aware recurrent neural network with an attention-based message passing scheme to capture rich spatiotemporal patterns across dynamic FC networks. To overcome model overfitting on limited training datasets, an adversarial training strategy is introduced to learn graph embedding models that generalize well to unseen brain networks. Evaluation on the ABIDE resting-state functional magnetic resonance imaging dataset shows that our proposed framework substantially outperforms state-of-the-art methods in identifying patients with ASD. Dynamic FC analyses with DSVB-learned embeddings reveal apparent group differences between ASD and healthy controls in brain network connectivity patterns and switching dynamics of brain states. The code is available at https://github.com/Monash-NeuroAI/Deep-Spatiotemporal-Variational-Bayes.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-03-11</td>
<td style='padding: 8px;'>Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks</td>
<td style='padding: 6px;'>Feng-Lei Fan, Yingxin Li, Hanchuan Peng, Tieyong Zeng, Fei Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2301.09245v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Throughout history, the development of artificial intelligence, particularly artificial neural networks, has been open to and constantly inspired by the increasingly deepened understanding of the brain, such as the inspiration of neocognitron, which is the pioneering work of convolutional neural networks. Per the motives of the emerging field: NeuroAI, a great amount of neuroscience knowledge can help catalyze the next generation of AI by endowing a network with more powerful capabilities. As we know, the human brain has numerous morphologically and functionally different neurons, while artificial neural networks are almost exclusively built on a single neuron type. In the human brain, neuronal diversity is an enabling factor for all kinds of biological intelligent behaviors. Since an artificial network is a miniature of the human brain, introducing neuronal diversity should be valuable in terms of addressing those essential problems of artificial networks such as efficiency, interpretability, and memory. In this Primer, we first discuss the preliminaries of biological neuronal diversity and the characteristics of information transmission and processing in a biological neuron. Then, we review studies of designing new neurons for artificial networks. Next, we discuss what gains can neuronal diversity bring into artificial networks and exemplary applications in several important fields. Lastly, we discuss the challenges and future directions of neuronal diversity to explore the potential of NeuroAI.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>Statistical Reconstruction For Anisotropic X-ray Dark-Field Tomography</td>
<td style='padding: 6px;'>David Frank, Cederik Höfs, Tobias Lasser</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.03160v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Anisotropic X-ray Dark-Field Tomography (AXDT) is a novel imaging technology that enables the extraction of fiber structures on the micrometer scale, far smaller than standard X-ray Computed Tomography (CT) setups. Directional and structural information is relevant in medical diagnostics and material testing. Compared to existing solutions, AXDT could prove a viable alternative. Reconstruction methods in AXDT have so far been driven by practicality. Improved methods could make AXDT more accessible. We contribute numerically stable implementations and validation of advanced statistical reconstruction methods that incorporate the statistical noise behavior of the imaging system. We further provide a new statistical reconstruction formulation that retains the advanced noise assumptions of the imaging setup while being efficient and easy to optimize. Finally, we provide a detailed analysis of the optimization behavior for all models regarding AXDT. Our experiments show that statistical reconstruction outperforms the previously used model, and particularly the noise performance is superior. While the previously proposed statistical method is effective, it is computationally expensive, and our newly proposed formulation proves highly efficient with identical performance. Our theoretical analysis opens the possibility to new and more advanced reconstruction algorithms, which in turn enable future research in AXDT.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>Assessing the impact of external factors on the occurrence of emergencies</td>
<td style='padding: 6px;'>Félicien Hêche, Philipp Schiller, Oussama Barakat, Thibaut Desmettre, Stephan Robert-Nicoud</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.03111v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study investigates the impact of 19 external factors, related to weather, road traffic conditions, air quality, and time, on the occurrence of emergencies using historical data provided by the dispatch center of the Centre Hospitalier Universitaire Vaudois (CHUV). This center is responsible for managing Emergency Medical Service (EMS) resources in the majority of the French-speaking part of Switzerland. First, classical statistical methods, such as correlation, Chi-squared test, Student's $t$-test, and information value, are employed to identify dependencies between the occurrence of emergencies and the considered parameters. Additionally, SHapley Additive exPlanations (SHAP) values and permutation importance are computed using eXtreme Gradient Boosting (XGBoost) and Multilayer Perceptron (MLP) models. The results indicate that the hour of the day, along with correlated parameters, plays a crucial role in the occurrence of emergencies. Conversely, other factors do not significantly influence emergency occurrences. Subsequently, a simplified model that considers only the hour of the day is compared with our XGBoost and MLP models. These comparisons reveal no significant difference between the three models in terms of performance, supporting the use of the basic model in this context. These observations provide valuable insights for EMS resource relocation strategies, benefit predictive modeling efforts, and inform decision-making in the context of EMS. The implications extend to enhancing EMS quality, making this research essential.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>AIF-SFDA: Autonomous Information Filter-driven Source-Free Domain Adaptation for Medical Image Segmentation</td>
<td style='padding: 6px;'>Haojin Li, Heng Li, Jianyu Chen, Rihan Zhong, Ke Niu, Huazhu Fu, Jiang Liu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.03074v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Decoupling domain-variant information (DVI) from domain-invariant information (DII) serves as a prominent strategy for mitigating domain shifts in the practical implementation of deep learning algorithms. However, in medical settings, concerns surrounding data collection and privacy often restrict access to both training and test data, hindering the empirical decoupling of information by existing methods. To tackle this issue, we propose an Autonomous Information Filter-driven Source-free Domain Adaptation (AIF-SFDA) algorithm, which leverages a frequency-based learnable information filter to autonomously decouple DVI and DII. Information Bottleneck (IB) and Self-supervision (SS) are incorporated to optimize the learnable frequency filter. The IB governs the information flow within the filter to diminish redundant DVI, while SS preserves DII in alignment with the specific task and image modality. Thus, the autonomous information filter can overcome domain shifts relying solely on target data. A series of experiments covering various medical image modalities and segmentation tasks were conducted to demonstrate the benefits of AIF-SFDA through comparisons with leading algorithms and ablation studies. The code is available at https://github.com/JingHuaMan/AIF-SFDA.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>Design and implementation of tools to build an ontology of Security Requirements for Internet of Medical Things</td>
<td style='padding: 6px;'>Daniel Naro, Jaime Delgado, Silvia Llorente, Amanda Palomo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.03067v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>When developing devices, architectures and services for the Internet of Medical Things (IoMT) world, manufacturers or integrators must be aware of the security requirements expressed by both laws and specifications. To provide tools guiding through these requirements and to assure a third party of the correct compliance, an ontology charting the relevant laws and specifications (for the European context) is very useful. We here address the development of this ontology. Due to the very high number and size of the considered specification documents, we have put in place a methodology and tools to simplify the transition from natural text to an ontology. The first step is a manual highlighting of relevant concepts in the corpus, then a manual translation to XML/XSD is operated. We have developed a tool allowing us to convert this semi-structured data into an ontology. Because the different specifications use similar but different wording, our approach favors the creation of similar instances in the ontology. To improve the ontology simplification through instance merging, we consider the use of LLMs. The responses of the LLMs are compared against our manually defined correct responses. The quality of the responses of the automated system does not prove to be good enough to be trusted blindly, and should only be used as a starting point for a manual correction.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>A Passive Mechanical Add-on for Treadmill Exercise (P-MATE) in Stroke Rehabilitation</td>
<td style='padding: 6px;'>Irene L. Y. Beck, Belle C. Hopmans, Bram Haanen, Levi Kieft, Heike Vallery, Laura Marchal-Crespo, Katherine L. Poggensee</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.03054v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Robotic rehabilitation can deliver high-dose gait therapy and improve motor function after a stroke. However, for many devices, high costs and lengthy setup times limit clinical adoption. Thus, we designed, built, and evaluated the Passive Mechanical Add-on for Treadmill Exercise (P-MATE), a low-cost passive end-effector add-on for treadmills that couples the movement of the paretic and non-paretic legs via a reciprocating system of elastic cables and pulleys. Two human-device mechanical interfaces were designed to attach the elastic cables to the user. The P-MATE and two interface prototypes were tested with a physical therapist and eight unimpaired participants. Biomechanical data, including kinematics and interaction forces, were collected alongside standardized questionnaires to assess usability and user experience. Both interfaces were quick and easy to attach, though user experience differed, highlighting the need for personalization. We also identified areas for future improvement, including pretension adjustments, tendon derailing prevention, and understanding long-term impacts on user gait. Our preliminary findings underline the potential of the P-MATE to provide effective, accessible, and sustainable stroke gait rehabilitation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>Dr. Tongue: Sign-Oriented Multi-label Detection for Remote Tongue Diagnosis</td>
<td style='padding: 6px;'>Yiliang Chen, Steven SC Ho, Cheng Xu, Yao Jie Xie, Wing-Fai Yeung, Shengfeng He, Jing Qin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.03053v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Tongue diagnosis is a vital tool in Western and Traditional Chinese Medicine, providing key insights into a patient's health by analyzing tongue attributes. The COVID-19 pandemic has heightened the need for accurate remote medical assessments, emphasizing the importance of precise tongue attribute recognition via telehealth. To address this, we propose a Sign-Oriented multi-label Attributes Detection framework. Our approach begins with an adaptive tongue feature extraction module that standardizes tongue images and mitigates environmental factors. This is followed by a Sign-oriented Network (SignNet) that identifies specific tongue attributes, emulating the diagnostic process of experienced practitioners and enabling comprehensive health evaluations. To validate our methodology, we developed an extensive tongue image dataset specifically designed for telemedicine. Unlike existing datasets, ours is tailored for remote diagnosis, with a comprehensive set of attribute labels. This dataset will be openly available, providing a valuable resource for research. Initial tests have shown improved accuracy in detecting various tongue attributes, highlighting our framework's potential as an essential tool for remote medical assessments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>A Trust-Guided Approach to MR Image Reconstruction with Side Information</td>
<td style='padding: 6px;'>Arda Atalık, Sumit Chopra, Daniel K. Sodickson</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.03021v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Reducing MRI scan times can improve patient care and lower healthcare costs. Many acceleration methods are designed to reconstruct diagnostic-quality images from limited sets of acquired $\textit{k}$-space data. This task can be framed as a linear inverse problem (LIP), where, as a result of undersampling, the forward operator may become rank-deficient or exhibit small singular values. This results in ambiguities in reconstruction, in which multiple generally incorrect or non-diagnostic images can map to the same acquired data. To address such ambiguities, it is crucial to incorporate prior knowledge, for example in the form of regularization. Another form of prior knowledge less commonly used in medical imaging is contextual side information garnered from other sources than the current acquisition. Here, we propose the $\textbf{T}$rust-$\textbf{G}$uided $\textbf{V}$ariational $\textbf{N}$etwork $\textbf{(TGVN)}$, a novel end-to-end deep learning framework that effectively integrates side information into LIPs. TGVN eliminates undesirable solutions from the ambiguous space of the forward operator while remaining faithful to the acquired data. We demonstrate its effectiveness in multi-coil, multi-contrast MR image reconstruction, where incomplete or low-quality measurements from one contrast are used as side information to reconstruct high-quality images of another contrast from heavily under-sampled data. Our method is robust across different contrasts, anatomies, and field strengths. Compared to baselines that also utilize side information, TGVN achieves superior image quality at challenging under-sampling levels, drastically speeding up acquisition while minimizing hallucinations. Our approach is also versatile enough to incorporate many different types of side information (including previous scans or even text) into any LIP.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>SurgRIPE challenge: Benchmark of Surgical Robot Instrument Pose Estimation</td>
<td style='padding: 6px;'>Haozheng Xu, Alistair Weld, Chi Xu, Alfie Roddan, Joao Cartucho, Mert Asim Karaoglu, Alexander Ladikos, Yangke Li, Yiping Li, Daiyun Shen, Shoujie Yang, Geonhee Lee, Seyeon Park, Jongho Shin, Young-Gon Kim, Lucy Fothergill, Dominic Jones, Pietro Valdastri, Duygu Sarikaya, Stamatia Giannarou</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02990v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurate instrument pose estimation is a crucial step towards the future of robotic surgery, enabling applications such as autonomous surgical task execution. Vision-based methods for surgical instrument pose estimation provide a practical approach to tool tracking, but they often require markers to be attached to the instruments. Recently, more research has focused on the development of marker-less methods based on deep learning. However, acquiring realistic surgical data, with ground truth instrument poses, required for deep learning training, is challenging. To address the issues in surgical instrument pose estimation, we introduce the Surgical Robot Instrument Pose Estimation (SurgRIPE) challenge, hosted at the 26th International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) in 2023. The objectives of this challenge are: (1) to provide the surgical vision community with realistic surgical video data paired with ground truth instrument poses, and (2) to establish a benchmark for evaluating markerless pose estimation methods. The challenge led to the development of several novel algorithms that showcased improved accuracy and robustness over existing methods. The performance evaluation study on the SurgRIPE dataset highlights the potential of these advanced algorithms to be integrated into robotic surgery systems, paving the way for more precise and autonomous surgical procedures. The SurgRIPE challenge has successfully established a new benchmark for the field, encouraging further research and development in surgical robot instrument pose estimation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>Label-free Concept Based Multiple Instance Learning for Gigapixel Histopathology</td>
<td style='padding: 6px;'>Susu Sun, Leslie Tessier, Frédérique Meeuwsen, Clément Grisi, Dominique van Midden, Geert Litjens, Christian F. Baumgartner</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02922v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Multiple Instance Learning (MIL) methods allow for gigapixel Whole-Slide Image (WSI) analysis with only slide-level annotations. Interpretability is crucial for safely deploying such algorithms in high-stakes medical domains. Traditional MIL methods offer explanations by highlighting salient regions. However, such spatial heatmaps provide limited insights for end users. To address this, we propose a novel inherently interpretable WSI-classification approach that uses human-understandable pathology concepts to generate explanations. Our proposed Concept MIL model leverages recent advances in vision-language models to directly predict pathology concepts based on image features. The model's predictions are obtained through a linear combination of the concepts identified on the top-K patches of a WSI, enabling inherent explanations by tracing each concept's influence on the prediction. In contrast to traditional concept-based interpretable models, our approach eliminates the need for costly human annotations by leveraging the vision-language model. We validate our method on two widely used pathology datasets: Camelyon16 and PANDA. On both datasets, Concept MIL achieves AUC and accuracy scores over 0.9, putting it on par with state-of-the-art models. We further find that 87.1\% (Camelyon16) and 85.3\% (PANDA) of the top 20 patches fall within the tumor region. A user study shows that the concepts identified by our model align with the concepts used by pathologists, making it a promising strategy for human-interpretable WSI classification.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>Comprehensive Pathological Image Segmentation via Teacher Aggregation for Tumor Microenvironment Analysis</td>
<td style='padding: 6px;'>Daisuke Komura, Maki Takao, Mieko Ochi, Takumi Onoyama, Hiroto Katoh, Hiroyuki Abe, Hiroyuki Sano, Teppei Konishi, Toshio Kumasaka, Tomoyuki Yokose, Yohei Miyagi, Tetsuo Ushiku, Shumpei Ishikawa</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02909v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The tumor microenvironment (TME) plays a crucial role in cancer progression and treatment response, yet current methods for its comprehensive analysis in H&E-stained tissue slides face significant limitations in the diversity of tissue cell types and accuracy. Here, we present PAGET (Pathological image segmentation via AGgrEgated Teachers), a new knowledge distillation approach that integrates multiple segmentation models while considering the hierarchical nature of cell types in the TME. By leveraging a unique dataset created through immunohistochemical restaining techniques and existing segmentation models, PAGET enables simultaneous identification and classification of 14 key TME components. We demonstrate PAGET's ability to perform rapid, comprehensive TME segmentation across various tissue types and medical institutions, advancing the quantitative analysis of tumor microenvironments. This method represents a significant step forward in enhancing our understanding of cancer biology and supporting precise clinical decision-making from large-scale histopathology images.</td>
</tr>
</tbody>
</table>

