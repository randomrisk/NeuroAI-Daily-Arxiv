<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2025-09-19</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Multi-Source Neural Activity Indices and Spatial Filters for EEG/MEG Inverse Problem: An Extension to MNE-Python</td>
<td style='padding: 6px;'>Julia Jurkowska, Joanna Dreszer, Monika Lewandowska, Krzysztof To≈Çpa, Tomasz Piotrowski</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.14118v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurate EEG/MEG source localization is essential for understanding brain function, yet remains challenging because the inverse problem is inherently ill-posed. In spatial filtering (beamforming) approaches, single-source LCMV spatial filters, though widely used, suffer from source cancellation when sources are correlated - a common experimental scenario. Multi-source frameworks, such as the multi-source minimum-variance pseudo-unbiased reduced-rank (MV-PURE) method, offer improved reconstruction and robust neural activity indices, yet their adoption has been limited by incomplete theory and lack of accessible implementations. In this paper, we present a rigorous derivation of multi-source neural activity indices and spatial filters, establishing a complete analytical framework with automated parameter selection. The resulting compact algebraic forms enable straightforward implementation. To facilitate adoption, we provide a full implementation extending MNE-Python, along with an accompanying tutorial, and demonstrate its utility on EEG experimental data, highlighting the practical advantages of multi-source spatial filtering for source localization and reconstruction.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques</td>
<td style='padding: 6px;'>Robiul Islam, Dmitry I. Ignatov, Karl Kaberg, Roman Nabatchikov</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.14078v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study investigates classifier performance across EEG frequency bands using various optimizers and evaluates efficient class prediction for the left and right hemispheres. Three neural network architectures - a deep dense network, a shallow three-layer network, and a convolutional neural network (CNN) - are implemented and compared using the TensorFlow and PyTorch frameworks. Results indicate that the Adagrad and RMSprop optimizers consistently perform well across different frequency bands, with Adadelta exhibiting robust performance in cross-model evaluations. Specifically, Adagrad excels in the beta band, while RMSprop achieves superior performance in the gamma band. Conversely, SGD and FTRL exhibit inconsistent performance. Among the models, the CNN demonstrates the second highest accuracy, particularly in capturing spatial features of EEG data. The deep dense network shows competitive performance in learning complex patterns, whereas the shallow three-layer network, sometimes being less accurate, provides computational efficiency. SHAP (Shapley Additive Explanations) plots are employed to identify efficient class prediction, revealing nuanced contributions of EEG frequency bands to model accuracy. Overall, the study highlights the importance of optimizer selection, model architecture, and EEG frequency band analysis in enhancing classifier performance and understanding feature importance in neuroimaging-based classification tasks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>EEG-Based Cognitive Load Classification During Landmark-Based VR Navigation</td>
<td style='padding: 6px;'>Jiahui An, Bingjie Cheng, Dmitriy Rudyka, Elisa Donati, Sara Fabrikant</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.14056v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain computer interfaces enable real-time monitoring of cognitive load, but their effectiveness in dynamic navigation contexts is not well established. Using an existing VR navigation dataset, we examined whether EEG signals can classify cognitive load during map-based wayfinding and whether classification accuracy depends more on task complexity or on individual traits. EEG recordings from forty-six participants navigating routes with 3, 5, or 7 map landmarks were analyzed with a nested cross-validation framework across multiple machine learning models. Classification achieved mean accuracies up to 90.8% for binary contrasts (3 vs. 7 landmarks) and 78.7% for the three-class problem, both well above chance. Demographic and cognitive variables (age, gender, spatial ability, working memory) showed no significant influence. These findings demonstrate that task demands outweigh individual differences in shaping classification performance, highlighting the potential for task-adaptive navigation systems that dynamically adjust map complexity in response to real-time cognitive states.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification</td>
<td style='padding: 6px;'>Kaniz Fatema, Emad A. Mohammed, Sukhjit Singh Sehra</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13687v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Effective and interpretable classification of medical images is a challenge in computer-aided diagnosis, especially in resource-limited clinical settings. This study introduces spline-based Kolmogorov-Arnold Networks (KANs) for accurate medical image classification with limited, diverse datasets. The models include SBTAYLOR-KAN, integrating B-splines with Taylor series; SBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN, embedding B-splines in Morlet wavelet transforms. These approaches leverage spline-based function approximation to capture both local and global nonlinearities. The models were evaluated on brain MRI, chest X-rays, tuberculosis X-rays, and skin lesion images without preprocessing, demonstrating the ability to learn directly from raw data. Extensive experiments, including cross-dataset validation and data reduction analysis, showed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93% accuracy, with a balanced F1-score, maintaining over 86% accuracy using only 30% of the training data across three datasets. Despite class imbalance in the skin cancer dataset, experiments on both imbalanced and balanced versions showed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy. Unlike traditional CNNs, which require millions of parameters (e.g., ResNet50 with 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872 trainable parameters, making it more suitable for constrained medical environments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used for interpretability, highlighting relevant regions in medical images. This framework provides a lightweight, interpretable, and generalizable solution for medical image classification, addressing the challenges of limited datasets and data-scarce scenarios in clinical AI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring</td>
<td style='padding: 6px;'>Zhenqi Wu, Abhinav Modi, Angelos Mavrogiannis, Kaustubh Joshi, Nikhil Chopra, Yiannis Aloimonos, Nare Karapetyan, Ioannis Rekleitis, Xiaomin Lin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13666v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The ocean is warming and acidifying, increasing the risk of mass mortality events for temperature-sensitive shellfish such as oysters. This motivates the development of long-term monitoring systems. However, human labor is costly and long-duration underwater work is highly hazardous, thus favoring robotic solutions as a safer and more efficient option. To enable underwater robots to make real-time, environment-aware decisions without human intervention, we must equip them with an intelligent "brain." This highlights the need for persistent,wide-area, and low-cost benthic monitoring. To this end, we present DREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term underwater exploration and habitat monitoring. The results show that our framework is highly efficient in finding and exploring target objects (e.g., oysters, shipwrecks) without prior location information. In the oyster-monitoring task, our framework takes 31.5% less time than the previous baseline with the same amount of oysters. Compared to the vanilla VLM, it uses 23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our framework successfully explores and maps the wreck without collisions, requiring 27.5% fewer steps than the vanilla model and achieving 100% coverage, while the vanilla model achieves 60.23% average coverage in our shipwreck environments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Sensitivity of literature $T_1$ mapping methods to the underlying magnetization transfer parameters</td>
<td style='padding: 6px;'>Jakob Assl√§nder</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13644v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Purpose: Magnetization transfer (MT) has been identified as the principal source of $T_1$ variability in the MRI literature. This study assesses the sensitivity of established $T_1$ mapping techniques to variations in the underlying MT parameters.   Methods: For each $T_1$-mapping method, the observed $T_1$ was simulated as a function of the underlying MT parameters $p_i^\text{MT}$, corresponding to different brain regions of interest (ROIs) at 3T. As measures of sensitivity, the derivatives $\partial T_1^\text{observed} / \partial p_i^\text{MT}$ were computed and analyzed with a linear mixed-effects model as a function of $p_i^\text{MT}$, ROI, pulse sequence type (e.g., inversion recovery, variable flip angle), and the individual sequences.   Results: The analyzed $T_1$-mapping sequences have a considerable sensitivity to changes in the semi-solid spin pool size $m_0^\text{s}$, $T_1^\text{f}$ of the free, $T_1^\text{s}$ of the semi-solid spin pool, and the (inverse) exchange rate $T_\text{x}$. All derivatives vary considerably with the underlying MT parameters and between pulse sequences. The derivatives can, in general, not be determined by the sequence type, but rather depend on implementation details of the sequence. One notable exception is that variable-flip-angle methods are, in general, more sensitive to the exchange rate than inversion-recovery methods.   Conclusion: Variations in $T_1$ measurements can be caused by several underlying MT parameters, and the sensitivity to each parameter depends on both the underlying MT parameters and the sequence.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Rest2Visual: Predicting Visually Evoked fMRI from Resting-State Scans</td>
<td style='padding: 6px;'>Chuyang Zhou, Ziao Ji, Daochang Liu, Dongang Wang, Chenyu Wang, Chang Xu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13612v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding how spontaneous brain activity relates to stimulus-driven neural responses is a fundamental challenge in cognitive neuroscience. While task-based functional magnetic resonance imaging (fMRI) captures localized stimulus-evoked brain activation, its acquisition is costly, time-consuming, and difficult to scale across populations. In contrast, resting-state fMRI (rs-fMRI) is task-free and abundant, but lacks direct interpretability. We introduce Rest2Visual, a conditional generative model that predicts visually evoked fMRI (ve-fMRI) from resting-state input and 2D visual stimuli. It follows a volumetric encoder--decoder design, where multiscale 3D features from rs-fMRI are modulated by image embeddings via adaptive normalization, enabling spatially accurate, stimulus-specific activation synthesis. To enable model training, we construct a large-scale triplet dataset from the Natural Scenes Dataset (NSD), aligning each rs-fMRI volume with stimulus images and their corresponding ve-fMRI activation maps. Quantitative evaluation shows that the predicted activations closely match ground truth across standard similarity and representational metrics, and support successful image reconstruction in downstream decoding. Notably, the predicted maps preserve subject-specific structure, demonstrating the model's capacity to generate individualized functional surrogates. Our results provide compelling evidence that individualized spontaneous neural activity can be transformed into stimulus-aligned representations, opening new avenues for scalable, task-free functional brain modeling.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-16</td>
<td style='padding: 8px;'>Complex-valued Phase Synchrony Reveals Directional Coupling in FMRI and Tracks Medication Effects</td>
<td style='padding: 6px;'>Sir-Lord Wiafe, Najme Soleimani, Masoud Seraji, Bradley Baker, Robyn Miller, Ashkan Faghiri, Vince D. Calhoun</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13481v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding interactions in complex systems requires capturing the directionality of coupling, not only its strength. Phase synchronization captures this timing, yet most methods either reduce phase to its cosine or collapse it into scaler indices such as phase-locking value, discarding directionality. We propose a complex-valued phase synchrony (CVPS) framework that estimates phase with an adaptive Gabor wavelet and preserves both cosine and sine components. Simulations confirm that CVPS recovers true phase offsets and tracks non-stationary dynamics more faithfully than Hilbert-based methods. Because antipsychotics are known to modulate the timing of cortical interactions, they provide a rigorous context to evaluate whether CVPS can capture such pharmacological effects. CVPS further reveals cortical neuro-hemodynamic drivers, with occipital-to-parietal and prefrontal-to-striatal lead-lag flows consistent with known receptor targets, confirming its ability to capture pharmacological timing. CVPS, therefore, offers a robust and generalizable framework for detecting directional coupling in complex systems such as the brain.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-16</td>
<td style='padding: 8px;'>Why all roads don't lead to Rome: Representation geometry varies across the human visual cortical hierarchy</td>
<td style='padding: 6px;'>Arna Ghosh, Zahraa Chorghay, Shahab Bakhtiari, Blake A. Richards</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13459v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Biological and artificial intelligence systems navigate the fundamental efficiency-robustness tradeoff for optimal encoding, i.e., they must efficiently encode numerous attributes of the input space while also being robust to noise. This challenge is particularly evident in hierarchical processing systems like the human brain. With a view towards understanding how systems navigate the efficiency-robustness tradeoff, we turned to a population geometry framework for analyzing representations in the human visual cortex alongside artificial neural networks (ANNs). In the ventral visual stream, we found general-purpose, scale-free representations characterized by a power law-decaying eigenspectrum in most areas. However, in certain higher-order visual areas did not have scale-free representations, indicating that scale-free geometry is not a universal property of the brain. In parallel, ANNs trained with a self-supervised learning objective also exhibited free-free geometry, but not after fine-tune on a specific task. Based on these empirical results and our analytical insights, we posit that a system's representation geometry is not a universal property and instead depends upon the computational objective.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-16</td>
<td style='padding: 8px;'>Model Predictive Control with Reference Learning for Soft Robotic Intracranial Pressure Waveform Modulation</td>
<td style='padding: 6px;'>Fabian Fl√ºrenbrock, Yanick B√ºchel, Johannes K√∂hler, Marianne Schmid Daners, Melanie N. Zeilinger</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13109v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper introduces a learning-based control framework for a soft robotic actuator system designed to modulate intracranial pressure (ICP) waveforms, which is essential for studying cerebrospinal fluid dynamics and pathological processes underlying neurological disorders. A two-layer framework is proposed to safely achieve a desired ICP waveform modulation. First, a model predictive controller (MPC) with a disturbance observer is used for offset-free tracking of the system's motor position reference trajectory under safety constraints. Second, to address the unknown nonlinear dependence of ICP on the motor position, we employ a Bayesian optimization (BO) algorithm used for online learning of a motor position reference trajectory that yields the desired ICP modulation. The framework is experimentally validated using a test bench with a brain phantom that replicates realistic ICP dynamics in vitro. Compared to a previously employed proportional-integral-derivative controller, the MPC reduces mean and maximum motor position reference tracking errors by 83 % and 73 %, respectively. In less than 20 iterations, the BO algorithm learns a motor position reference trajectory that yields an ICP waveform with the desired mean and amplitude.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Multi-Source Neural Activity Indices and Spatial Filters for EEG/MEG Inverse Problem: An Extension to MNE-Python</td>
<td style='padding: 6px;'>Julia Jurkowska, Joanna Dreszer, Monika Lewandowska, Krzysztof To≈Çpa, Tomasz Piotrowski</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.14118v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurate EEG/MEG source localization is essential for understanding brain function, yet remains challenging because the inverse problem is inherently ill-posed. In spatial filtering (beamforming) approaches, single-source LCMV spatial filters, though widely used, suffer from source cancellation when sources are correlated - a common experimental scenario. Multi-source frameworks, such as the multi-source minimum-variance pseudo-unbiased reduced-rank (MV-PURE) method, offer improved reconstruction and robust neural activity indices, yet their adoption has been limited by incomplete theory and lack of accessible implementations. In this paper, we present a rigorous derivation of multi-source neural activity indices and spatial filters, establishing a complete analytical framework with automated parameter selection. The resulting compact algebraic forms enable straightforward implementation. To facilitate adoption, we provide a full implementation extending MNE-Python, along with an accompanying tutorial, and demonstrate its utility on EEG experimental data, highlighting the practical advantages of multi-source spatial filtering for source localization and reconstruction.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques</td>
<td style='padding: 6px;'>Robiul Islam, Dmitry I. Ignatov, Karl Kaberg, Roman Nabatchikov</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.14078v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study investigates classifier performance across EEG frequency bands using various optimizers and evaluates efficient class prediction for the left and right hemispheres. Three neural network architectures - a deep dense network, a shallow three-layer network, and a convolutional neural network (CNN) - are implemented and compared using the TensorFlow and PyTorch frameworks. Results indicate that the Adagrad and RMSprop optimizers consistently perform well across different frequency bands, with Adadelta exhibiting robust performance in cross-model evaluations. Specifically, Adagrad excels in the beta band, while RMSprop achieves superior performance in the gamma band. Conversely, SGD and FTRL exhibit inconsistent performance. Among the models, the CNN demonstrates the second highest accuracy, particularly in capturing spatial features of EEG data. The deep dense network shows competitive performance in learning complex patterns, whereas the shallow three-layer network, sometimes being less accurate, provides computational efficiency. SHAP (Shapley Additive Explanations) plots are employed to identify efficient class prediction, revealing nuanced contributions of EEG frequency bands to model accuracy. Overall, the study highlights the importance of optimizer selection, model architecture, and EEG frequency band analysis in enhancing classifier performance and understanding feature importance in neuroimaging-based classification tasks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>EEG-Based Cognitive Load Classification During Landmark-Based VR Navigation</td>
<td style='padding: 6px;'>Jiahui An, Bingjie Cheng, Dmitriy Rudyka, Elisa Donati, Sara Fabrikant</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.14056v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain computer interfaces enable real-time monitoring of cognitive load, but their effectiveness in dynamic navigation contexts is not well established. Using an existing VR navigation dataset, we examined whether EEG signals can classify cognitive load during map-based wayfinding and whether classification accuracy depends more on task complexity or on individual traits. EEG recordings from forty-six participants navigating routes with 3, 5, or 7 map landmarks were analyzed with a nested cross-validation framework across multiple machine learning models. Classification achieved mean accuracies up to 90.8% for binary contrasts (3 vs. 7 landmarks) and 78.7% for the three-class problem, both well above chance. Demographic and cognitive variables (age, gender, spatial ability, working memory) showed no significant influence. These findings demonstrate that task demands outweigh individual differences in shaping classification performance, highlighting the potential for task-adaptive navigation systems that dynamically adjust map complexity in response to real-time cognitive states.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-15</td>
<td style='padding: 8px;'>Quantifying Mental States in Work Environment: Mathematical Perspectives</td>
<td style='padding: 6px;'>Aymen Balti, Assane Wade, Abdelatif Oujbara, M. A., Aziz-Alaoui, Hicham Bellarabi, Frederic Dutertre, Benjamin Ambrosio</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.12162v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This article presents a study involving 87 participants exposed to a stressful scenario in a virtual reality (VR) environment. An algorithm was developed to assign a positive or negative valence based on questionnaire responses. EEG signals were recorded, and a k-nearest neighbors (KNN) algorithm was trained to infer emotional valence from these signals. Our objective is to further develop mathematical models capable of describing the dynamic evolution of emotional and mental states.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-15</td>
<td style='padding: 8px;'>NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired Geometric Priors for Robust Facial Emotion Recognition</td>
<td style='padding: 6px;'>Zilin Li, Weiwei Xu, Xuanqi Zhao, Yiran Zhu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.11916v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Facial emotion recognition (FER) models trained only on pixels often fail to generalize across datasets because facial appearance is an indirect and biased proxy for underlying affect. We present NeuroGaze-Distill, a cross-modal distillation framework that transfers brain-informed priors into an image-only FER student via static Valence/Arousal (V/A) prototypes and a depression-inspired geometric prior (D-Geo). A teacher trained on EEG topographic maps from DREAMER (with MAHNOB-HCI as unlabeled support) produces a consolidated 5x5 V/A prototype grid that is frozen and reused; no EEG-face pairing and no non-visual signals at deployment are required. The student (ResNet-18/50) is trained on FERPlus with conventional CE/KD and two lightweight regularizers: (i) Proto-KD (cosine) aligns student features to the static prototypes; (ii) D-Geo softly shapes the embedding geometry in line with affective findings often reported in depression research (e.g., anhedonia-like contraction in high-valence regions). We evaluate both within-domain (FERPlus validation) and cross-dataset protocols (AffectNet-mini; optional CK+), reporting standard 8-way scores alongside present-only Macro-F1 and balanced accuracy to fairly handle label-set mismatch. Ablations attribute consistent gains to prototypes and D-Geo, and favor 5x5 over denser grids for stability. The method is simple, deployable, and improves robustness without architectural complexity.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-16</td>
<td style='padding: 8px;'>A funny companion: Distinct neural responses to perceived AI- versus human-generated humor</td>
<td style='padding: 6px;'>Xiaohui Rao, Hanlin Wu, Zhenguang G. Cai</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.10847v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As AI companions become capable of human-like communication, including telling jokes, understanding how people cognitively and emotionally respond to AI humor becomes increasingly important. This study used electroencephalography (EEG) to compare how people process humor from AI versus human sources. Behavioral analysis revealed that participants rated AI and human humor as comparably funny. However, neurophysiological data showed that AI humor elicited a smaller N400 effect, suggesting reduced cognitive effort during the processing of incongruity. This was accompanied by a larger Late Positive Potential (LPP), indicating a greater degree of surprise and emotional response. This enhanced LPP likely stems from the violation of low initial expectations regarding AI's comedic capabilities. Furthermore, a key temporal dynamic emerged: human humor showed habituation effects, marked by an increasing N400 and a decreasing LPP over time. In contrast, AI humor demonstrated increasing processing efficiency and emotional reward, with a decreasing N400 and an increasing LPP. This trajectory reveals how the brain can dynamically update its predictive model of AI capabilities. This process of cumulative reinforcement challenges "algorithm aversion" in humor, as it demonstrates how cognitive adaptation to AI's language patterns can lead to an intensified emotional reward. Additionally, participants' social attitudes toward AI modulated these neural responses, with higher perceived AI trustworthiness correlating with enhanced emotional engagement. These findings indicate that the brain responds to AI humor with surprisingly positive and intense reactions, highlighting humor's potential for fostering genuine engagement in human-AI social interaction.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-12</td>
<td style='padding: 8px;'>FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation Domain Adaptation for Fetal Sleep Stage Classification</td>
<td style='padding: 6px;'>Weitao Tang, Johann Vargas-Calixto, Nasim Katebi, Nhi Tran, Sharmony B. Kelly, Gari D. Clifford, Robert Galinsky, Faezeh Marzbanrad</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.10082v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Introduction: This study presents FetalSleepNet, the first published deep learning approach to classifying sleep states from the ovine electroencephalogram (EEG). Fetal EEG is complex to acquire and difficult and laborious to interpret consistently. However, accurate sleep stage classification may aid in the early detection of abnormal brain maturation associated with pregnancy complications (e.g. hypoxia or intrauterine growth restriction).   Methods: EEG electrodes were secured onto the ovine dura over the parietal cortices of 24 late gestation fetal sheep. A lightweight deep neural network originally developed for adult EEG sleep staging was trained on the ovine EEG using transfer learning from adult EEG. A spectral equalisation-based domain adaptation strategy was used to reduce cross-domain mismatch.   Results: We demonstrated that while direct transfer performed poorly, full fine tuning combined with spectral equalisation achieved the best overall performance (accuracy: 86.6 percent, macro F1-score: 62.5), outperforming baseline models.   Conclusions: To the best of our knowledge, FetalSleepNet is the first deep learning framework specifically developed for automated sleep staging from the fetal EEG. Beyond the laboratory, the EEG-based sleep stage classifier functions as a label engine, enabling large scale weak/semi supervised labeling and distillation to facilitate training on less invasive signals that can be acquired in the clinic, such as Doppler Ultrasound or electrocardiogram data. FetalSleepNet's lightweight design makes it well suited for deployment in low power, real time, and wearable fetal monitoring systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-12</td>
<td style='padding: 8px;'>The nature of alpha modulation through neurofeedback</td>
<td style='padding: 6px;'>Jacob Maaz, Laurent Waroquier, Alexandra Dia, V√©ronique Paban, Arnaud Rey</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.10046v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalographic neurofeedback (EEG-NF) has been proposed as a promising technique to modulate brain activity through real-time EEG-based feedback. Alpha neurofeedback in particular is believed to induce rapid self-regulation of brain rhythms, with applications in cognitive enhancement and clinical treatment. However, whether this modulation reflects specific volitional control or non-specific influences remains unresolved. In a preregistered, double-blind, sham-controlled study, we evaluated alpha upregulation in healthy participants receiving either genuine or sham EEG-NF during a single-session design. A third arm composed of a passive control group was also included to differentiate between non-specific influences related or not to the active engagement in EEG-NF. Throughout the session, alpha power increased robustly, yet independently of feedback veracity, engagement in self-regulation, or feedback update frequency. Parallel increases in theta and sensorimotor rhythms further suggest broadband non-specific modulation. Importantly, these results challenge the foundational assumption of EEG-NF: that feedback enables volitional EEG control. Instead, they point to spontaneous repetition-related processes as primary drivers, calling for a critical reassessment of neurofeedback efficacy and its underlying mechanisms.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-11</td>
<td style='padding: 8px;'>Improved Riemannian potato field: an Automatic Artifact Rejection Method for EEG</td>
<td style='padding: 6px;'>Davoud Hajhassani, Quentin Barth√©lemy, J√©r√©mie Mattout, Marco Congedo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.09264v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) signal cleaning has long been a critical challenge in the research community. The presence of artifacts can significantly degrade EEG data quality, complicating analysis and potentially leading to erroneous interpretations. While various artifact rejection methods have been proposed, the gold standard remains manual visual inspection by human experts-a process that is time-consuming, subjective, and impractical for large-scale EEG studies. Existing techniques are often hindered by a strong reliance on manual hyperparameter tuning, sensitivity to outliers, and high computational costs. In this paper, we introduce the improved Riemannian Potato Field (iRPF), a fast and fully automated method for EEG artifact rejection that addresses key limitations of current approaches. We evaluate iRPF against several state-of-the-art artifact rejection methods, using two publicly available EEG databases, labeled for various artifact types, comprising 226 EEG recordings. Our results demonstrate that iRPF outperforms all competitors across multiple metrics, with gains of up to 22% in recall, 102% in specificity, 54% in precision, and 24% in F1-score, compared to Isolation Forest, Autoreject, Riemannian Potato, and Riemannian Potato Field, respectively. Statistical analysis confirmed the significance of these improvements (p < 0.001) with large effect sizes (Cohen's d > 0.8) in most comparisons. Additionally, on a typical EEG recording iRPF performs artifact cleaning in under 8 milliseconds per epoch using a standard laptop, highlighting its efficiency for large-scale EEG data processing and real-time applications. iRPF offers a robust and data-driven artifact rejection solution for high-quality EEG pre-processing in brain-computer interfaces and clinical neuroimaging applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-10</td>
<td style='padding: 8px;'>ADHDeepNet From Raw EEG to Diagnosis: Improving ADHD Diagnosis through Temporal-Spatial Processing, Adaptive Attention Mechanisms, and Explainability in Raw EEG Signals</td>
<td style='padding: 6px;'>Ali Amini, Mohammad Alijanpour, Behnam Latifi, Ali Motie Nasrabadi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.08779v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Attention Deficit Hyperactivity Disorder (ADHD) is a common brain disorder in children that can persist into adulthood, affecting social, academic, and career life. Early diagnosis is crucial for managing these impacts on patients and the healthcare system but is often labor-intensive and time-consuming. This paper presents a novel method to improve ADHD diagnosis precision and timeliness by leveraging Deep Learning (DL) approaches and electroencephalogram (EEG) signals. We introduce ADHDeepNet, a DL model that utilizes comprehensive temporal-spatial characterization, attention modules, and explainability techniques optimized for EEG signals. ADHDeepNet integrates feature extraction and refinement processes to enhance ADHD diagnosis. The model was trained and validated on a dataset of 121 participants (61 ADHD, 60 Healthy Controls), employing nested cross-validation for robust performance. The proposed two-stage methodology uses a 10-fold cross-subject validation strategy. Initially, each iteration optimizes the model's hyper-parameters with inner 2-fold cross-validation. Then, Additive Gaussian Noise (AGN) with various standard deviations and magnification levels is applied for data augmentation. ADHDeepNet achieved 100% sensitivity and 99.17% accuracy in classifying ADHD/HC subjects. To clarify model explainability and identify key brain regions and frequency bands for ADHD diagnosis, we analyzed the learned weights and activation patterns of the model's primary layers. Additionally, t-distributed Stochastic Neighbor Embedding (t-SNE) visualized high-dimensional data, aiding in interpreting the model's decisions. This study highlights the potential of DL and EEG in enhancing ADHD diagnosis accuracy and efficiency.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-09</td>
<td style='padding: 8px;'>An Enactivist Approach to Human-Computer Interaction: Bridging the Gap Between Human Agency and Affordances</td>
<td style='padding: 6px;'>Angjelin Hila</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.07871v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Emerging paradigms in XR, AI, and BCI contexts necessitate novel theoretical frameworks for understanding human autonomy and agency in HCI. Drawing from enactivist theories of cognition, we conceptualize human agents as self-organizing, operationally closed systems that actively enact their cognitive domains through dynamic interaction with their environments. To develop measurable variables aligned with this framework, we introduce "feelings of agency" (FoA) as an alternative to the established construct of "sense of agency" (SoA), refining Synofzyk's multifactorial weighting model and offering a novel conceptual pathway for overcoming gaps in the dominant comparator model. We define FoA as comprising two subconstructs: affective engagement and volitional attention, which we operationalize through integrated neurodynamic indicators (valence, arousal, cross frequency coupling within the dorsal attention system) and first-person phenomenological reports. We argue that these neurophenomenological indicators provide richer, more actionable insights for digital affordance design, particularly in XR, BCI, Human AI Interaction (HAX), and generative AI environments. Our framework aims to inform and inspire design parameters that significantly enhance human agency in rapidly evolving interactive domains.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-09</td>
<td style='padding: 8px;'>NeuroGaze: A Hybrid EEG and Eye-Tracking Brain-Computer Interface for Hands-Free Interaction in Virtual Reality</td>
<td style='padding: 6px;'>Kyle Coutray, Wanyea Barbel, Zack Groth, Joseph J LaViola Jr</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.07863v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-Computer Interfaces (BCIs) have traditionally been studied in clinical and laboratory contexts, but the rise of consumer-grade devices now allows exploration of their use in daily activities. Virtual reality (VR) provides a particularly relevant domain, where existing input methods often force trade-offs between speed, accuracy, and physical effort. This study introduces NeuroGaze, a hybrid interface combining electroencephalography (EEG) with eye tracking to enable hands-free interaction in immersive VR. Twenty participants completed a 360{\deg} cube-selection task using three different input methods: VR controllers, gaze combined with a pinch gesture, and NeuroGaze. Performance was measured by task completion time and error rate, while workload was evaluated using the NASA Task Load Index (NASA-TLX). NeuroGaze successfully supported target selection with off-the-shelf hardware, producing fewer errors than the alternative methods but requiring longer completion times, reflecting a classic speed-accuracy tradeoff. Workload analysis indicated reduced physical demand for NeuroGaze compared to controllers, though overall ratings and user preferences were mixed. These findings demonstrate the feasibility of hybrid EEG+gaze systems for everyday VR use, highlighting their ergonomic benefits and inclusivity potential. Although not yet competitive in speed, NeuroGaze points toward a practical role for consumer-grade BCIs in accessibility and long-duration applications, and underscores the need for improved EEG signal processing and adaptive multimodal integration to enhance future performance.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-07</td>
<td style='padding: 8px;'>DRDCAE-STGNN: An End-to-End Discrimina-tive Autoencoder with Spatio-Temporal Graph Learning for Motor Imagery Classification</td>
<td style='padding: 6px;'>Yi Wang, Haodong Zhang, Hongqi Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.05943v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Motor imagery (MI) based brain-computer interfaces (BCIs) hold significant potential for assistive technologies and neurorehabilitation. However, the precise and efficient decoding of MI remains challenging due to their non-stationary nature and low signal-to-noise ratio. This paper introduces a novel end-to-end deep learning framework of Discriminative Residual Dense Convolutional Autoencoder with Spatio-Temporal Graph Neural Network (DRDCAE-STGNN) to enhance the MI feature learning and classification. Specifically, the DRDCAE module leverages residual-dense connections to learn discriminative latent representations through joint reconstruction and classifica-tion, while the STGNN module captures dynamic spatial dependencies via a learnable graph adjacency matrix and models temporal dynamics using bidirectional long short-term memory (LSTM). Extensive evaluations on BCI Competition IV 2a, 2b, and PhysioNet datasets demonstrate state-of-the-art performance, with average accuracies of 95.42%, 97.51%, and 90.15%, respectively. Ablation studies confirm the contribution of each component, and interpreta-bility analysis reveals neurophysiologically meaningful connectivity patterns. Moreover, despite its complexity, the model maintains a feasible parameter count and an inference time of 0.32 ms per sample. These results indicate that our method offers a robust, accurate, and interpretable solution for MI-EEG decoding, with strong generalizability across subjects and tasks and meeting the requirements for potential real-time BCI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Handwriting Imagery EEG Classification based on Convolutional Neural Networks</td>
<td style='padding: 6px;'>Hao Yang, Guang Ouyang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03111v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Handwriting imagery has emerged as a promising paradigm for brain-computer interfaces (BCIs) aimed at translating brain activity into text output. Compared with invasively recorded electroencephalography (EEG), non-invasive recording offers a more practical and feasible approach to capturing brain signals for BCI. This study explores the limit of decoding non-invasive EEG associated with handwriting imagery into English letters using deep neural networks. To this end, five participants were instructed to imagine writing the 26 English letters with their EEG being recorded from the scalp. A measurement of EEG similarity across letters was conducted to investigate letter-specific patterns in the dataset. Subsequently, four convolutional neural network (CNN) models were trained for EEG classification. Descriptively, the EEG data clearly exhibited letter-specific patterns serving as a proof-of-concept for EEG-to-text translation. Under the chance level of accuracy at 3.85%, the CNN classifiers trained on each participant reached the highest limit of around 20%. This study marks the first attempt to decode non-invasive EEG associated with handwriting imagery. Although the achieved accuracy is not sufficient for a usable brain-to-text BCI, the model's performance is noteworthy in revealing the potential for translating non-invasively recorded brain signals into text outputs and establishing a baseline for future research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-01</td>
<td style='padding: 8px;'>MATL-DC: A Multi-domain Aggregation Transfer Learning Framework for EEG Emotion Recognition with Domain-Class Prototype under Unseen Targets</td>
<td style='padding: 6px;'>Guangli Li, Canbiao Wu, Zhehao Zhou, Na Tian, Zhen Liang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.01135v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Emotion recognition based on electroencephalography (EEG) signals is increasingly becoming a key research hotspot in affective Brain-Computer Interfaces (aBCIs). However, the current transfer learning model greatly depends on the source domain and target domain data, which hinder the practical application of emotion recognition. Therefore, we propose a Multi-domain Aggregation Transfer Learning framework for EEG emotion recognition with Domain-Class prototype under unseen targets (MATL-DC). We design the feature decoupling module to decouple class-invariant domain features from domain-invariant class features from shallow features. In the model training stage, the multi-domain aggregation mechanism aggregates the domain feature space to form a superdomain, which enhances the characteristics of emotional EEG signals. In each superdomain, we further extract the class prototype representation by class features. In addition, we adopt the pairwise learning strategy to transform the sample classification problem into the similarity problem between sample pairs, which effectively alleviates the influence of label noise. It is worth noting that the target domain is completely unseen during the training process. In the inference stage, we use the trained domain-class prototypes for inference, and then realize emotion recognition. We rigorously validate it on the publicly available databases (SEED, SEED-IV and SEED-V). The results show that the accuracy of MATL-DC model is 84.70\%, 68.11\% and 61.08\%, respectively. MATL-DC achieves comparable or even better performance than methods that rely on both source and target domains. The source code is available at https://github.com/WuCB-BCI/MATL-DC.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-01</td>
<td style='padding: 8px;'>IMU-Enhanced EEG Motion Artifact Removal with Fine-Tuned Large Brain Models</td>
<td style='padding: 6px;'>Yuhong Zhang, Xusheng Zhu, Yuchen Xu, ChiaEn Lu, Hsinyu Shih, Gert Cauwenberghs, Tzyy-Ping Jung</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.01073v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) is a non-invasive method for measuring brain activity with high temporal resolution; however, EEG signals often exhibit low signal-to-noise ratios because of contamination from physiological and environmental artifacts. One of the major challenges hindering the real-world deployment of brain-computer interfaces (BCIs) involves the frequent occurrence of motion-related EEG artifacts. Most prior studies on EEG motion artifact removal rely on single-modality approaches, such as Artifact Subspace Reconstruction (ASR) and Independent Component Analysis (ICA), without incorporating simultaneously recorded modalities like inertial measurement units (IMUs), which directly capture the extent and dynamics of motion. This work proposes a fine-tuned large brain model (LaBraM)-based correlation attention mapping method that leverages spatial channel relationships in IMU data to identify motion-related artifacts in EEG signals. The fine-tuned model contains approximately 9.2 million parameters and uses 5.9 hours of EEG and IMU recordings for training, just 0.2346\% of the 2500 hours used to train the base model. We compare our results against the established ASR-ICA benchmark across varying time scales and motion activities, showing that incorporating IMU reference signals significantly improves robustness under diverse motion scenarios.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-31</td>
<td style='padding: 8px;'>PyNoetic: A modular python framework for no-code development of EEG brain-computer interfaces</td>
<td style='padding: 6px;'>Gursimran Singh, Aviral Chharia, Rahul Upadhyay, Vinay Kumar, Luca Longo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.00670v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG)-based Brain-Computer Interfaces (BCIs) have emerged as a transformative technology with applications spanning robotics, virtual reality, medicine, and rehabilitation. However, existing BCI frameworks face several limitations, including a lack of stage-wise flexibility essential for experimental research, steep learning curves for researchers without programming expertise, elevated costs due to reliance on proprietary software, and a lack of all-inclusive features leading to the use of multiple external tools affecting research outcomes. To address these challenges, we present PyNoetic, a modular BCI framework designed to cater to the diverse needs of BCI research. PyNoetic is one of the very few frameworks in Python that encompasses the entire BCI design pipeline, from stimulus presentation and data acquisition to channel selection, filtering, feature extraction, artifact removal, and finally simulation and visualization. Notably, PyNoetic introduces an intuitive and end-to-end GUI coupled with a unique pick-and-place configurable flowchart for no-code BCI design, making it accessible to researchers with minimal programming experience. For advanced users, it facilitates the seamless integration of custom functionalities and novel algorithms with minimal coding, ensuring adaptability at each design stage. PyNoetic also includes a rich array of analytical tools such as machine learning models, brain-connectivity indices, systematic testing functionalities via simulation, and evaluation methods of novel paradigms. PyNoetic's strengths lie in its versatility for both offline and real-time BCI development, which streamlines the design process, allowing researchers to focus on more intricate aspects of BCI development and thus accelerate their research endeavors. Project Website: https://neurodiag.github.io/PyNoetic</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-29</td>
<td style='padding: 8px;'>A Single Subject Machine Learning Based Classification of Motor Imagery EEGs</td>
<td style='padding: 6px;'>Dario Sanalitro, Marco Finocchiaro, Pasquale Memmolo, Emanuela Cutuli, Maide Bucolo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.21724v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Motor Imagery-Based Brain-Computer Interfaces (MI-BCIs) are systems that detect and interpret brain activity patterns linked to the mental visualization of movement, and then translate these into instructions for controlling external robotic or domotic devices. Such devices have the potential to be useful in a broad variety of applications. While implementing a system that would help individuals restore some freedom levels, the interpretation of (Electroencephalography) EEG data remains a complex and unsolved problem. In the literature, the classification of left and right imagined movements has been extensively studied. This study introduces a novel pipeline that makes use of machine learning techniques for classifying MI EEG data. The entire framework is capable of accurately categorizing left and imagined motions, as well as rest phases, for a set of 52 subjects who performed a MI task. We trained a within subject model on each individual subject. The methodology has been offline evaluated and compared to four studies that are currently the state-of-the-art regarding the specified dataset. The results show that our proposed framework could be used with MI-BCI systems in light of its failsafe classification performances, i.e. 99.5% in accuracy</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-27</td>
<td style='padding: 8px;'>Alljoined-1.6M: A Million-Trial EEG-Image Dataset for Evaluating Affordable Brain-Computer Interfaces</td>
<td style='padding: 6px;'>Jonathan Xu, Ugo Bruzadin Nunes, Wangshu Jiang, Samuel Ryther, Jordan Pringle, Paul S. Scotti, Arnaud Delorme, Reese Kneeland</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.18571v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We present a new large-scale electroencephalography (EEG) dataset as part of the THINGS initiative, comprising over 1.6 million visual stimulus trials collected from 20 participants, and totaling more than twice the size of the most popular current benchmark dataset, THINGS-EEG2. Crucially, our data was recorded using a 32-channel consumer-grade wet electrode system costing ~$2.2k, around 27x cheaper than research-grade EEG systems typically used in cognitive neuroscience labs. Our work is one of the first open-source, large-scale EEG resource designed to closely reflect the quality of hardware that is practical to deploy in real-world, downstream applications of brain-computer interfaces (BCIs). We aim to explore the specific question of whether deep neural network-based BCI research and semantic decoding methods can be effectively conducted with such affordable systems, filling an important gap in current literature that is extremely relevant for future research. In our analysis, we not only demonstrate that decoding of high-level semantic information from EEG of visualized images is possible at consumer-grade hardware, but also that our data can facilitate effective EEG-to-Image reconstruction even despite significantly lower signal-to-noise ratios. In addition to traditional benchmarks, we also conduct analyses of EEG-to-Image models that demonstrate log-linear decoding performance with increasing data volume on our data, and discuss the trade-offs between hardware cost, signal fidelity, and the scale of data collection efforts in increasing the size and utility of currently available datasets. Our contributions aim to pave the way for large-scale, cost-effective EEG research with widely accessible equipment, and position our dataset as a unique resource for the democratization and development of effective deep neural models of visual cognition.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-25</td>
<td style='padding: 8px;'>DLGE: Dual Local-Global Encoding for Generalizable Cross-BCI-Paradigm</td>
<td style='padding: 6px;'>Jingyuan Wang, Junhua Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.07991v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Deep learning models have been frequently used to decode a single brain-computer interface (BCI) paradigm based on electroencephalography (EEG). It is challenging to decode multiple BCI paradigms using one model due to diverse barriers, such as different channel configurations and disparate task-related representations. In this study, we propose Dual Local-Global Encoder (DLGE), enabling the classification across different BCI paradigms. To address the heterogeneity in EEG channel configurations across paradigms, we employ an anatomically inspired brain-region partitioning and padding strategy to standardize EEG channel configuration. In the proposed model, the local encoder is designed to learn shared features across BCI paradigms within each brain region based on time-frequency information, which integrates temporal attention on individual channels with spatial attention among channels for each brain region. These shared features are subsequently aggregated in the global encoder to form respective paradigm-specific feature representations. Three BCI paradigms (motor imagery, resting state, and driving fatigue) were used to evaluate the proposed model. The results demonstrate that our model is capable of processing diverse BCI paradigms without retraining and retuning, achieving average macro precision, recall, and F1-score of 60.16\%, 59.88\%, and 59.56\%, respectively. We made an initial attempt to develop a general model for cross-BCI-paradigm classification, avoiding retraining or redevelopment for each paradigm. This study paves the way for the development of an effective but simple model for cross-BCI-paradigm decoding, which might benefit the design of portable devices for universal BCI decoding.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Rest2Visual: Predicting Visually Evoked fMRI from Resting-State Scans</td>
<td style='padding: 6px;'>Chuyang Zhou, Ziao Ji, Daochang Liu, Dongang Wang, Chenyu Wang, Chang Xu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13612v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding how spontaneous brain activity relates to stimulus-driven neural responses is a fundamental challenge in cognitive neuroscience. While task-based functional magnetic resonance imaging (fMRI) captures localized stimulus-evoked brain activation, its acquisition is costly, time-consuming, and difficult to scale across populations. In contrast, resting-state fMRI (rs-fMRI) is task-free and abundant, but lacks direct interpretability. We introduce Rest2Visual, a conditional generative model that predicts visually evoked fMRI (ve-fMRI) from resting-state input and 2D visual stimuli. It follows a volumetric encoder--decoder design, where multiscale 3D features from rs-fMRI are modulated by image embeddings via adaptive normalization, enabling spatially accurate, stimulus-specific activation synthesis. To enable model training, we construct a large-scale triplet dataset from the Natural Scenes Dataset (NSD), aligning each rs-fMRI volume with stimulus images and their corresponding ve-fMRI activation maps. Quantitative evaluation shows that the predicted activations closely match ground truth across standard similarity and representational metrics, and support successful image reconstruction in downstream decoding. Notably, the predicted maps preserve subject-specific structure, demonstrating the model's capacity to generate individualized functional surrogates. Our results provide compelling evidence that individualized spontaneous neural activity can be transformed into stimulus-aligned representations, opening new avenues for scalable, task-free functional brain modeling.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-16</td>
<td style='padding: 8px;'>Complex-valued Phase Synchrony Reveals Directional Coupling in FMRI and Tracks Medication Effects</td>
<td style='padding: 6px;'>Sir-Lord Wiafe, Najme Soleimani, Masoud Seraji, Bradley Baker, Robyn Miller, Ashkan Faghiri, Vince D. Calhoun</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13481v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding interactions in complex systems requires capturing the directionality of coupling, not only its strength. Phase synchronization captures this timing, yet most methods either reduce phase to its cosine or collapse it into scaler indices such as phase-locking value, discarding directionality. We propose a complex-valued phase synchrony (CVPS) framework that estimates phase with an adaptive Gabor wavelet and preserves both cosine and sine components. Simulations confirm that CVPS recovers true phase offsets and tracks non-stationary dynamics more faithfully than Hilbert-based methods. Because antipsychotics are known to modulate the timing of cortical interactions, they provide a rigorous context to evaluate whether CVPS can capture such pharmacological effects. CVPS further reveals cortical neuro-hemodynamic drivers, with occipital-to-parietal and prefrontal-to-striatal lead-lag flows consistent with known receptor targets, confirming its ability to capture pharmacological timing. CVPS, therefore, offers a robust and generalizable framework for detecting directional coupling in complex systems such as the brain.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Prediction and Causality of functional MRI and synthetic signal using a Zero-Shot Time-Series Foundation Model</td>
<td style='padding: 6px;'>Alessandro Crimi, Andrea Brovelli</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.12497v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Time-series forecasting and causal discovery are central in neuroscience, as predicting brain activity and identifying causal relationships between neural populations and circuits can shed light on the mechanisms underlying cognition and disease. With the rise of foundation models, an open question is how they compare to traditional methods for brain signal forecasting and causality analysis, and whether they can be applied in a zero-shot setting. In this work, we evaluate a foundation model against classical methods for inferring directional interactions from spontaneous brain activity measured with functional magnetic resonance imaging (fMRI) in humans. Traditional approaches often rely on Wiener-Granger causality. We tested the forecasting ability of the foundation model in both zero-shot and fine-tuned settings, and assessed causality by comparing Granger-like estimates from the model with standard Granger causality. We validated the approach using synthetic time series generated from ground-truth causal models, including logistic map coupling and Ornstein-Uhlenbeck processes. The foundation model achieved competitive zero-shot forecasting fMRI time series (mean absolute percentage error of 0.55 in controls and 0.27 in patients). Although standard Granger causality did not show clear quantitative differences between models, the foundation model provided a more precise detection of causal interactions.   Overall, these findings suggest that foundation models offer versatility, strong zero-shot performance, and potential utility for forecasting and causal discovery in time-series data.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-11</td>
<td style='padding: 8px;'>LITcoder: A General-Purpose Library for Building and Comparing Encoding Models</td>
<td style='padding: 6px;'>Taha Binhuraib, Ruimin Gao, Anna A. Ivanova</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.09152v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We introduce LITcoder, an open-source library for building and benchmarking neural encoding models. Designed as a flexible backend, LITcoder provides standardized tools for aligning continuous stimuli (e.g., text and speech) with brain data, transforming stimuli into representational features, mapping those features onto brain data, and evaluating the predictive performance of the resulting model on held-out data. The library implements a modular pipeline covering a wide array of methodological design choices, so researchers can easily compose, compare, and extend encoding models without reinventing core infrastructure. Such choices include brain datasets, brain regions, stimulus feature (both neural-net-based and control, such as word rate), downsampling approaches, and many others. In addition, the library provides built-in logging, plotting, and seamless integration with experiment tracking platforms such as Weights & Biases (W&B). We demonstrate the scalability and versatility of our framework by fitting a range of encoding models to three story listening datasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore the methodological choices critical for building encoding models for continuous fMRI data, illustrating the importance of accounting for all tokens in a TR scan (as opposed to just taking the last one, even when contextualized), incorporating hemodynamic lag effects, using train-test splits that minimize information leakage, and accounting for head motion effects on encoding model predictivity. Overall, LITcoder lowers technical barriers to encoding model implementation, facilitates systematic comparisons across models and datasets, fosters methodological rigor, and accelerates the development of high-quality high-performance predictive models of brain activity.   Project page: https://litcoder-brain.github.io</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-10</td>
<td style='padding: 8px;'>VoxelFormer: Parameter-Efficient Multi-Subject Visual Decoding from fMRI</td>
<td style='padding: 6px;'>Chenqian Le, Yilin Zhao, Nikasadat Emami, Kushagra Yadav, Xujin "Chris" Liu, Xupeng Chen, Yao Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.09015v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent advances in fMRI-based visual decoding have enabled compelling reconstructions of perceived images. However, most approaches rely on subject-specific training, limiting scalability and practical deployment. We introduce \textbf{VoxelFormer}, a lightweight transformer architecture that enables multi-subject training for visual decoding from fMRI. VoxelFormer integrates a Token Merging Transformer (ToMer) for efficient voxel compression and a query-driven Q-Former that produces fixed-size neural representations aligned with the CLIP image embedding space. Evaluated on the 7T Natural Scenes Dataset, VoxelFormer achieves competitive retrieval performance on subjects included during training with significantly fewer parameters than existing methods. These results highlight token merging and query-based transformers as promising strategies for parameter-efficient neural decoding.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-09</td>
<td style='padding: 8px;'>Identifying Neural Signatures from fMRI using Hybrid Principal Components Regression</td>
<td style='padding: 6px;'>Jared Rieck, Julia Wrobel, Joshua L. Gowin, Yue Wang, Martin Paulus, Ryan Peterson</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.07300v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent advances in neuroimaging analysis have enabled accurate decoding of mental state from brain activation patterns during functional magnetic resonance imaging scans. A commonly applied tool for this purpose is principal components regression regularized with the least absolute shrinkage and selection operator (LASSO PCR), a type of multi-voxel pattern analysis (MVPA). This model presumes that all components are equally likely to harbor relevant information, when in fact the task-related signal may be concentrated in specific components. In such cases, the model will fail to select the optimal set of principal components that maximizes the total signal relevant to the cognitive process under study. Here, we present modifications to LASSO PCR that allow for a regularization penalty tied directly to the index of the principal component, reflecting a prior belief that task-relevant signal is more likely to be concentrated in components explaining greater variance. Additionally, we propose a novel hybrid method, Joint Sparsity-Ranked LASSO (JSRL), which integrates component-level and voxel-level activity under an information parity framework and imposes ranked sparsity to guide component selection. We apply the models to brain activation during risk taking, monetary incentive, and emotion regulation tasks. Results demonstrate that incorporating sparsity ranking into LASSO PCR produces models with enhanced classification performance, with JSRL achieving up to 51.7\% improvement in cross-validated deviance $R^2$ and 7.3\% improvement in cross-validated AUC. Furthermore, sparsity-ranked models perform as well as or better than standard LASSO PCR approaches across all classification tasks and allocate predictive weight to brain regions consistent with their established functional roles, offering a robust alternative for MVPA.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-07</td>
<td style='padding: 8px;'>Assessment of 7T task fMRI value over 3T task fMRI</td>
<td style='padding: 6px;'>Dalton H Bermudez</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.05873v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Ultra-high field 7T fMRI offers notable advantages over 3T fMRI, including higher signal-to-noise and contrast-to-noise ratios, enabling finer spatial and temporal resolution. This study explores the differences in activation maps from Human Connectome Project datasets between 7T and 3T field strengths, focusing on visual identified using the Glasser atlas. Functional tasks for each scanner were designed to include visual stimuli, with data processed uniformly to ensure comparability. Results showed significantly higher beta coefficients for common regions of activation, such as V3A and V3B, in 7T compared to 3T datasets. This suggests that 7T fMRI data more accurately reflect the idealized time course of task-related conditions, likely due to improved sensitivity to blood oxygenation level-dependent (BOLD) signals. However, variations in experimental design and acquisition parameters between scanners complicate the direct comparison of beta coefficients.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-04</td>
<td style='padding: 8px;'>Data-Efficient Psychiatric Disorder Detection via Self-supervised Learning on Frequency-enhanced Brain Networks</td>
<td style='padding: 6px;'>Mujie Liu, Mengchu Zhu, Qichao Dong, Ting Dang, Jiangang Ma, Jing Ren, Feng Xia</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.10524v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Psychiatric disorders involve complex neural activity changes, with functional magnetic resonance imaging (fMRI) data serving as key diagnostic evidence. However, data scarcity and the diverse nature of fMRI information pose significant challenges. While graph-based self-supervised learning (SSL) methods have shown promise in brain network analysis, they primarily focus on time-domain representations, often overlooking the rich information embedded in the frequency domain. To overcome these limitations, we propose Frequency-Enhanced Network (FENet), a novel SSL framework specially designed for fMRI data that integrates time-domain and frequency-domain information to improve psychiatric disorder detection in small-sample datasets. FENet constructs multi-view brain networks based on the inherent properties of fMRI data, explicitly incorporating frequency information into the learning process of representation. Additionally, it employs domain-specific encoders to capture temporal-spectral characteristics, including an efficient frequency-domain encoder that highlights disease-relevant frequency features. Finally, FENet introduces a domain consistency-guided learning objective, which balances the utilization of diverse information and generates frequency-enhanced brain graph representations. Experiments on two real-world medical datasets demonstrate that FENet outperforms state-of-the-art methods while maintaining strong performance in minimal data conditions. Furthermore, we analyze the correlation between various frequency-domain features and psychiatric disorders, emphasizing the critical role of high-frequency information in disorder detection.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>RTGMFF: Enhanced fMRI-based Brain Disorder Diagnosis via ROI-driven Text Generation and Multimodal Feature Fusion</td>
<td style='padding: 6px;'>Junhao Jia, Yifei Sun, Yunyou Liu, Cheng Yang, Changmiao Wang, Feiwei Qin, Yong Peng, Wenwen Min</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03214v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional magnetic resonance imaging (fMRI) is a powerful tool for probing brain function, yet reliable clinical diagnosis is hampered by low signal-to-noise ratios, inter-subject variability, and the limited frequency awareness of prevailing CNN- and Transformer-based models. Moreover, most fMRI datasets lack textual annotations that could contextualize regional activation and connectivity patterns. We introduce RTGMFF, a framework that unifies automatic ROI-level text generation with multimodal feature fusion for brain-disorder diagnosis. RTGMFF consists of three components: (i) ROI-driven fMRI text generation deterministically condenses each subject's activation, connectivity, age, and sex into reproducible text tokens; (ii) Hybrid frequency-spatial encoder fuses a hierarchical wavelet-mamba branch with a cross-scale Transformer encoder to capture frequency-domain structure alongside long-range spatial dependencies; and (iii) Adaptive semantic alignment module embeds the ROI token sequence and visual features in a shared space, using a regularized cosine-similarity loss to narrow the modality gap. Extensive experiments on the ADHD-200 and ABIDE benchmarks show that RTGMFF surpasses current methods in diagnostic accuracy, achieving notable gains in sensitivity, specificity, and area under the ROC curve. Code is available at https://github.com/BeistMedAI/RTGMFF.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-04</td>
<td style='padding: 8px;'>Data-driven mean-field within whole-brain models</td>
<td style='padding: 6px;'>Martin Breyton, Viktor Sip, Marmaduke Woodman, Meysam Hashemi, Spase Petkoski, Viktor Jirsa</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.02799v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Mean-field models provide a link between microscopic neuronal activity and macroscopic brain dynamics. Their derivation depends on simplifying assumptions, such as all-to-all connectivity, limiting their biological realism. To overcome this, we introduce a data-driven framework in which a multi-layer perceptron (MLP) learns the macroscopic dynamics directly from simulations of a network of spiking neurons. The network connection probability serves here as a new parameter, inaccessible to purely analytical treatment, which is validated against ground truth analytical solutions. Through bifurcation analysis on the trained MLP, we demonstrate the existence of new cusp bifurcation that systematically reshapes the system's phase diagram in a degenerate manner with synaptic coupling. By integrating this data-driven mean-field model into a whole-brain computational framework, we show that it extends beyond the macroscopic emergent dynamics generated by the analytical model. For validation, we use simulation-based inference on synthetic functional magnetic resonance imaging (fMRI) data and demonstrate accurate parameter recovery for the novel mean-field model, while the current state-of-the-art models lead to biased estimates. This work presents a flexible and generic framework for building more realistic whole-brain models, bridging the gap between microscale mechanisms and macroscopic brain recordings.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Multi-Source Neural Activity Indices and Spatial Filters for EEG/MEG Inverse Problem: An Extension to MNE-Python</td>
<td style='padding: 6px;'>Julia Jurkowska, Joanna Dreszer, Monika Lewandowska, Krzysztof To≈Çpa, Tomasz Piotrowski</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.14118v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurate EEG/MEG source localization is essential for understanding brain function, yet remains challenging because the inverse problem is inherently ill-posed. In spatial filtering (beamforming) approaches, single-source LCMV spatial filters, though widely used, suffer from source cancellation when sources are correlated - a common experimental scenario. Multi-source frameworks, such as the multi-source minimum-variance pseudo-unbiased reduced-rank (MV-PURE) method, offer improved reconstruction and robust neural activity indices, yet their adoption has been limited by incomplete theory and lack of accessible implementations. In this paper, we present a rigorous derivation of multi-source neural activity indices and spatial filters, establishing a complete analytical framework with automated parameter selection. The resulting compact algebraic forms enable straightforward implementation. To facilitate adoption, we provide a full implementation extending MNE-Python, along with an accompanying tutorial, and demonstrate its utility on EEG experimental data, highlighting the practical advantages of multi-source spatial filtering for source localization and reconstruction.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-07</td>
<td style='padding: 8px;'>MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning</td>
<td style='padding: 6px;'>Jiarui Chen, Yikeng Chen, Yingshuang Zou, Ye Huang, Peng Wang, Yuan Liu, Yujing Sun, Wenping Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.07021v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis technique, but its high memory consumption severely limits its applicability on edge devices. A growing number of 3DGS compression methods have been proposed to make 3DGS more efficient, yet most only focus on storage compression and fail to address the critical bottleneck of rendering memory. To address this problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that tackles this challenge by jointly optimizing two key factors: the total primitive number and the parameters per primitive, achieving unprecedented memory compression. Specifically, we replace the memory-intensive spherical harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our color representations. More importantly, we propose a unified soft pruning framework that models primitive-number and lobe-number pruning as a single constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a 50% static VRAM reduction and a 40% rendering VRAM reduction compared to existing methods, while maintaining comparable rendering quality.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-15</td>
<td style='padding: 8px;'>A fast machine learning tool to predict the composition of astronomical ices from infrared absorption spectra</td>
<td style='padding: 6px;'>Andr√©s Meg√≠as, Izaskun Jim√©nez-Serra, Fran√ßois Dulieu, Julie Vitorino, Bel√©n Mat√©, David Ciudad, Will R. M. Rocha, Marcos Mart√≠nez Jim√©nez, Jacobo Aguirre</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.04331v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Current observations taken by James Webb Space Telescope (JWST) allow us to observe the absorption features of icy mantles that cover interstellar dust grains, which are mainly composed of $\mathrm{H_2O}$, $\mathrm{CO}$, and $\mathrm{CO_2}$, along with other minor species. Thanks to its sensitivity and spectral resolution, JWST has the potential to observe ice features towards hundreds of sources at different stages along the process of star formation. However, identifying the spectral features of the different species and quantifying the ice composition is not trivial and requires complex spectroscopic analysis. We present Automatic Ice Composition Estimator (AICE), a new tool based on artificial neural networks. Based on the infrared (IR) ice absorption spectrum between 2.5 and 10 microns, AICE predicts the ice fractional composition in terms of $\mathrm{H_2O}$, $\mathrm{CO}$, $\mathrm{CO_2}$, $\mathrm{CH_3OH}$, $\mathrm{NH_3}$, and $\mathrm{CH_4}$. To train the model, we used hundreds of laboratory experiments of ice mixtures from different databases, which were reprocessed with baseline subtraction and normalisation. Once trained, AICE takes less than one second on a conventional computer to predict the ice composition associated with the observed IR absorption spectrum, with typical errors of $\sim$3 $\%$ in the species fraction. We tested its performance on two spectra reported towards the NIR38 and J110621 background stars observed within the JWST Ice Age program, demonstrating a good agreement with previous estimations of the ice composition. The fast and accurate performance of AICE enables the systematic analysis of hundreds of different ice spectra with a modest time investment. In addition, this model can be enhanced and re-trained with more laboratory data, improving the precision of the predictions and expanding the list of predicted species.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Towards a 384-channel magnetoencephalography system based on optically pumped magnetometers</td>
<td style='padding: 6px;'>Holly Schofield, Ryan M. Hill, Lukas Rier, Ewan Kennett, Gonzalo Reina Rivero, Joseph Gibson, Ashley Tyler, Zoe Tanner, Frank Worcester, Tyler Hayward, James Osborne, Cody Doyle, Vishal Shah, Elena Boto, Niall Holmes, Matthew J. Brookes</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03107v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Magnetoencephalography using optically pumped magnetometers (OPM-MEG) is gaining significant traction as a neuroimaging tool, with the potential for improved performance and practicality compared to conventional instrumentation. However, OPM-MEG has so far lagged conventional-MEG in terms of the number of independent measurements of magnetic field that can be made across the scalp (i.e. the number of channels). This is important since increasing channel count offers improvements in sensitivity, spatial resolution and coverage. Unfortunately, increasing channel count also poses significant technical and practical challenges. Here, we describe a new OPM-MEG array which exploits triaxial sensors and integrated miniaturised electronic control units to measure MEG data from up to 384 channels. We also introduce a high-speed calibration method to ensure the that the fields measured by this array are high fidelity. The system was validated using a phantom, with results showing that dipole localisation accuracy is better than 1 mm, and correlation between the measured magnetic fields and a dipole model is >0.998. We then demonstrate utility in human MEG acquisition: via measurement of visual gamma oscillations we demonstrate the improvements in sensitivity that are afforded by high channel density, and via a moviewatching paradigm we quantify improvements in spatial resolution. In sum, we show the first OPM-MEG system with a channel count larger than that of typical conventional MEG devices. This represents a significant step on the path towards OPMs becoming the sensor of choice for MEG measurement.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-06</td>
<td style='padding: 8px;'>Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention Mechanisms for Visual Prostheses</td>
<td style='padding: 6px;'>Ganxi Xu, Jinyi Long, Jia Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.00787v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Visual prostheses have shown great potential in restoring vision for blind individuals. However, while researchers have successfully utilized M/EEG signals to evoke visual perceptions during the brain decoding stage of visual prostheses, the complementary process-converting images to M/EEG signals in the brain encoding stage-remains largely unexplored. Thus, we present the first image-to-brain signal (M/EEG) framework based on denoising diffusion probabilistic models enhanced with cross-attention mechanisms. Our framework consists of two key architectural components: a pre-trained CLIP visual encoder that extracts rich semantic representations from input images, and a cross-attention enhanced U-Net diffusion model that learns to reconstruct biologically plausible brain signals through iterative denoising. Unlike conventional generative models that rely on simple concatenation for conditioning, our cross-attention modules enable dynamic interaction between visual features and brain signal representations, facilitating fine-grained alignment during the generation process. Furthermore, we evaluate our framework on two multimodal datasets (THINGS-EEG2 and THINGS-MEG) to demonstrate its effectiveness in generating biologically plausible brain signals. Additionally, we pioneer the visualization of M/EEG topographies across all subjects in both datasets, providing intuitive demonstrations of intra-subject and inter-subject variations in brain signals.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-29</td>
<td style='padding: 8px;'>Quantile Function-Based Models for Neuroimaging Classification Using Wasserstein Regression</td>
<td style='padding: 6px;'>Jie Li, Gary Green, Jian Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.21523v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We propose a novel quantile function-based approach for neuroimaging classification using Wasserstein-Fr\'echet regression, specifically applied to the detection of mild traumatic brain injury (mTBI) based on the MEG and MRI data. Conventional neuroimaging classification methods for mTBI detection typically extract summary statistics from brain signals across the different epochs, which may result in the loss of important distributional information, such as variance, skewness, kurtosis, etc. Our approach treats complete probability density functions of epoch space results as functional response variables within a Wasserstein-Fr\'echet regression framework, thereby preserving the full distributional characteristics of epoch results from $L_{1}$ minimum norm solutions. The global Wasserstein-Fr\'echet regression model incorporating covariates (age and gender) allows us to directly compare the distributional patterns between healthy control subjects and mTBI patients. The classification procedure computes Wasserstein distances between estimated quantile functions from control and patient groups, respectively. These distances are then used as the basis for diagnostic decisions. This framework offers a statistically principled approach to improving diagnostic accuracy in mTBI detection. In practical applications, the test accuracy on unseen data from Innovision IP's dataset achieves up to 98\%.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-25</td>
<td style='padding: 8px;'>Disentangling the Factors of Convergence between Brains and Computer Vision Models</td>
<td style='padding: 6px;'>Jos√©phine Raugel, Marc Szafraniec, Huy V. Vo, Camille Couprie, Patrick Labatut, Piotr Bojanowski, Valentin Wyart, Jean-R√©mi King</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.18226v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Many AI models trained on natural images develop representations that resemble those of the human brain. However, the factors that drive this brain-model similarity remain poorly understood. To disentangle how the model, training and data independently lead a neural network to develop brain-like representations, we trained a family of self-supervised vision transformers (DINOv3) that systematically varied these different factors. We compare their representations of images to those of the human brain recorded with both fMRI and MEG, providing high resolution in spatial and temporal analyses. We assess the brain-model similarity with three complementary metrics focusing on overall representational similarity, topographical organization, and temporal dynamics. We show that all three factors - model size, training amount, and image type - independently and interactively impact each of these brain similarity metrics. In particular, the largest DINOv3 models trained with the most human-centric images reach the highest brain-similarity. This emergence of brain-like representations in AI models follows a specific chronology during training: models first align with the early representations of the sensory cortices, and only align with the late and prefrontal representations of the brain with considerably more training. Finally, this developmental trajectory is indexed by both structural and functional properties of the human cortex: the representations that are acquired last by the models specifically align with the cortical areas with the largest developmental expansion, thickness, least myelination, and slowest timescales. Overall, these findings disentangle the interplay between architecture and experience in shaping how artificial neural networks come to see the world as humans do, thus offering a promising framework to understand how the human brain comes to represent its visual world.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-21</td>
<td style='padding: 8px;'>Probing $0ŒΩŒ≤Œ≤$ and $Œº\to eŒ≥$ via Fully Determined Dirac Mass Terms in LRSM with Double Seesaw</td>
<td style='padding: 6px;'>Pratik Adarsh, Rajrupa Banerjee, Purushottam Sahu, Utkarsh Patel, Sudhanwa Patra</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.15893v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neutrinoless double beta decay ($0\nu\beta\beta$) and charged lepton flavor violation (cLFV) experiments provide promising avenues to probe new physics contributions from extended neutrino sectors in beyond Standard Model (BSM) scenarios. We consider a Left--Right Symmetric Model (LRSM) extended with three generations of sterile neutrinos to realize a double type-I seesaw mechanism for light neutrino mass generation. The double seesaw induces maximal lepton number violation in the right-handed sector and facilitates enhanced Majorana masses for right-handed neutrinos, thereby leading to their dominant contributions in both cLFV and $0\nu\beta\beta$ processes. We perform a comprehensive exploration of the parameter space for new-physics contributions to the cLFV decay $\mu \to e \gamma$ and to $0\nu\beta\beta$, considering two different textures for the Dirac mass matrices: one proportional to the identity matrix and another fully determined by the model framework. A detailed analysis of the common parameter regions accessible to current experiments like KamLAND-Zen and LEGEND-200, and upcoming experiments, such as MEG-II and LEGEND-1000, is presented, underscoring the phenomenological relevance of this framework. Our results aim to provide optimistic benchmarks for future searches targeting right-handed current-mediated neutrino interactions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-19</td>
<td style='padding: 8px;'>Reduction of Electromagnetic Interference in ultra-low noise Bimodal MEG & EEG</td>
<td style='padding: 6px;'>Jim Barnes, Lukasz Radzinski, Soudabeh Arsalani, Gunnar Waterstraat, Gabriel Curio, Jens Haueisen, Rainer K√∂rber</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.13758v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Single-channel SQUID system technology, operating at a noise level of 100s of aT/$\sqrt{\textrm{Hz}}$, enables the non-invasive detection of synchronized spiking activity at the single-trial level via magnetoencephalography (MEG). However, when combined with simultaneous electroencephalography (EEG) recordings, the noise performance of the ultrasensitive MEG system can be greatly diminished. This issue negates some of the complementary qualities of these two recording methods. In addition, typical electrical components required for electrical stimulation of peripheral nerves, a common method for evoking specific brain responses, are also observed to have a detrimental influence on ultra-low MEG noise performance. These effects are caused by electromagnetic interference (EMI) and typically preclude single-trial detection. This work outlines, how careful design allows a significant reduction of the impact of EMI when these different electronic systems are operated concurrently. This optimization enabled the simultaneous single-trial detection of synchronized spiking activity using these two highly sensitive recording modalities.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-31</td>
<td style='padding: 8px;'>Wave Turbulence and Cortical Dynamics</td>
<td style='padding: 6px;'>Gerald Kaushallye Cooray</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.23525v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Cortical activity recorded through EEG and MEG reflects complex dynamics that span multiple temporal and spatial scales. Spectral analyses of these signals consistently reveal power-law behaviour, a hallmark of turbulent systems. In this paper, we derive a kinetic equation for neural field activity based on wave turbulence theory, highlighting how quantities such as energy and pseudo-particle density flow through wave-space (k-space) via direct and inverse cascades. We explore how different forms of nonlinearity, particularly 3-wave and 4-wave interactions, shape spectral features, including harmonic generation, spectral dispersion, and transient dynamics. While the observed power-law decays in empirical data are broadly consistent with turbulent cascades, variations across studies, such as the presence of dual decay rates or harmonic structures, point to a diversity of underlying mechanisms. We argue that although no single model fully explains all spectral observations, key constraints emerge: namely, that cortical dynamics exhibit features consistent with turbulent wave systems involving both single and dual cascades and a mixture of 3- and 4-wave interactions. This turbulence-based framework offers a principled and unifying approach to interpreting large-scale brain activity, including state transitions and seizure dynamics.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-09</td>
<td style='padding: 8px;'>Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers</td>
<td style='padding: 6px;'>Thomas Klein, Sascha Meyen, Wieland Brendel, Felix A. Wichmann, Kristof Meding</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.06645v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Benchmarking models is a key factor for the rapid progress in machine learning (ML) research. Thus, further progress depends on improving benchmarking metrics. A standard metric to measure the behavioral alignment between ML models and human observers is error consistency (EC). EC allows for more fine-grained comparisons of behavior than other metrics such as e.g. accuracy, and has been used in the influential Brain-Score benchmark to rank different DNNs by their behavioral consistency with humans. Previously, EC values have been reported without confidence intervals. However, empirically measured EC values are typically noisy -- thus, without confidence intervals, valid benchmarking conclusions are problematic. Here we improve on standard EC in two ways: First, we show how to obtain confidence intervals for EC using a bootstrapping technique, allowing us to derive significance tests for EC. Second, we propose a new computational model relating the EC between two classifiers to the implicit probability that one of them copies responses from the other. This view of EC allows us to give practical guidance to scientists regarding the number of trials required for sufficiently powerful, conclusive experiments. Finally, we use our methodology to revisit popular NeuroAI-results. We find that while the general trend of behavioral differences between humans and machines holds up to scrutiny, many reported differences between deep vision models are statistically insignificant. Our methodology enables researchers to design adequately powered experiments that can reliably detect behavioral differences between models, providing a foundation for more rigorous benchmarking of behavioral alignment.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-02</td>
<td style='padding: 8px;'>What Neuroscience Can Teach AI About Learning in Continuously Changing Environments</td>
<td style='padding: 6px;'>Daniel Durstewitz, Bruno Averbeck, Georgia Koppe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.02103v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-06-12</td>
<td style='padding: 8px;'>NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models</td>
<td style='padding: 6px;'>Luca Ghafourpour, Valentin Duruisseaux, Bahareh Tolooshams, Philip H. Wong, Costas A. Anastassiou, Anima Anandkumar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2506.04536v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Characterizing the diverse computational properties of human neurons via multimodal electrophysiological, transcriptomic, and morphological data provides the foundation for constructing and validating bio-realistic neuron models that can advance our understanding of fundamental mechanisms underlying brain function. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. To capture variability, ensembles of deterministic models are often used, but are difficult to scale as model generation requires repeating computationally expensive optimization for each neuron. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on data generated from biophysically realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE is the first scaled-up deep learning framework validated on real experimental data, enabling efficient generation of synthetic neurons that exhibit trial-to-trial variability and achieve a $4200\times$ speedup over numerical solvers. To this end, NOBLE captures fundamental neural properties, opening the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-21</td>
<td style='padding: 8px;'>SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation</td>
<td style='padding: 6px;'>Jiayue Liu, Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Kuo Yang, Xu Wang, Yang Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.16080v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-02-22</td>
<td style='padding: 8px;'>Brain-Model Evaluations Need the NeuroAI Turing Test</td>
<td style='padding: 6px;'>Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2502.16238v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-04</td>
<td style='padding: 8px;'>Asynchronous Hebbian/anti-Hebbian networks</td>
<td style='padding: 6px;'>Henrique Reis Aguiar, Matthias H. Hennig</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02402v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-03</td>
<td style='padding: 8px;'>NeuroAI for AI Safety</td>
<td style='padding: 6px;'>Patrick Mineault, Niccol√≤ Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.18526v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-14</td>
<td style='padding: 8px;'>Evaluating Representational Similarity Measures from the Lens of Functional Correspondence</td>
<td style='padding: 6px;'>Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.14633v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-09</td>
<td style='padding: 8px;'>Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models</td>
<td style='padding: 6px;'>Emily Cheng, Richard J. Antonello</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.05771v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first "composition" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-07-22</td>
<td style='padding: 8px;'>Predictive Coding Networks and Inference Learning: Tutorial and Survey</td>
<td style='padding: 6px;'>Bj√∂rn van Zwol, Ro Jefferson, Egon L. van den Broek</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2407.04117v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent years have witnessed a growing call for renewed emphasis on neuroscience-inspired approaches in artificial intelligence research, under the banner of NeuroAI. A prime example of this is predictive coding networks (PCNs), based on the neuroscientific framework of predictive coding. This framework views the brain as a hierarchical Bayesian inference model that minimizes prediction errors through feedback connections. Unlike traditional neural networks trained with backpropagation (BP), PCNs utilize inference learning (IL), a more biologically plausible algorithm that explains patterns of neural activity that BP cannot. Historically, IL has been more computationally intensive, but recent advancements have demonstrated that it can achieve higher efficiency than BP with sufficient parallelization. Furthermore, PCNs can be mathematically considered a superset of traditional feedforward neural networks (FNNs), significantly extending the range of trainable architectures. As inherently probabilistic (graphical) latent variable models, PCNs provide a versatile framework for both supervised learning and unsupervised (generative) modeling that goes beyond traditional artificial neural networks. This work provides a comprehensive review and detailed formal specification of PCNs, particularly situating them within the context of modern ML methods. Additionally, we introduce a Python library (PRECO) for practical implementation. This positions PC as a promising framework for future ML innovations.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training</td>
<td style='padding: 6px;'>Julia S. Dollis, Iago A. Brito, Fernanda B. F√§rber, Pedro S. F. B. Ribeiro, Rafael T. Sousa, Arlindo R. Galv√£o Filho</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.14132v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While virtual reality (VR) excels at simulating physical environments, its effectiveness for training complex interpersonal skills is limited by a lack of psychologically plausible virtual humans. This is a critical gap in high-stakes domains like medical education, where communication is a core competency. This paper introduces a framework that integrates large language models (LLMs) into immersive VR to create medically coherent virtual patients with distinct, consistent personalities, built on a modular architecture that decouples personality from clinical data. We evaluated our system in a mixed-method, within-subjects study with licensed physicians who engaged in simulated consultations. Results demonstrate that the approach is not only feasible but is also perceived by physicians as a highly rewarding and effective training enhancement. Furthermore, our analysis uncovers critical design principles, including a ``realism-verbosity paradox" where less communicative agents can seem more artificial, and the need for challenges to be perceived as authentic to be instructive. This work provides a validated framework and key insights for developing the next generation of socially intelligent VR training environments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-18</td>
<td style='padding: 8px;'>AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration</td>
<td style='padding: 6px;'>Jingyi Yuan, Jianxiong Ye, Wenkang Chen, Chenqiang Gao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.14084v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary novel categories, offering a scalable and annotation-efficient solution. Traditionally, most ZSAD works have been based on the CLIP model, which performs anomaly detection by calculating the similarity between visual and text embeddings. Recently, vision foundation models such as DINOv3 have demonstrated strong transferable representation capabilities. In this work, we are the first to adapt DINOv3 for ZSAD. However, this adaptation presents two key challenges: (i) the domain bias between large-scale pretraining data and anomaly detection tasks leads to feature misalignment; and (ii) the inherent bias toward global semantics in pretrained representations often leads to subtle anomalies being misinterpreted as part of the normal foreground objects, rather than being distinguished as abnormal regions. To overcome these challenges, we introduce AD-DINOv3, a novel vision-language multimodal framework designed for ZSAD. Specifically, we formulate anomaly detection as a multimodal contrastive learning problem, where DINOv3 is employed as the visual backbone to extract patch tokens and a CLS token, and the CLIP text encoder provides embeddings for both normal and abnormal prompts. To bridge the domain gap, lightweight adapters are introduced in both modalities, enabling their representations to be recalibrated for the anomaly detection task. Beyond this baseline alignment, we further design an Anomaly-Aware Calibration Module (AACM), which explicitly guides the CLS token to attend to anomalous regions rather than generic foreground semantics, thereby enhancing discriminability. Extensive experiments on eight industrial and medical benchmarks demonstrate that AD-DINOv3 consistently matches or surpasses state-of-the-art methods.The code will be available at https://github.com/Kaisor-Yuan/AD-DINOv3.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Time-smoothed inverse probability weighted estimation of effects of generalized time-varying treatment strategies on repeated outcomes truncated by death</td>
<td style='padding: 6px;'>Sean McGrath, Takuya Kawahara, Joshua Petimar, Sheryl L. Rifas-Shiman, Iv√°n D√≠az, Jason P. Block, Jessica G. Young</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13971v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Researchers are often interested in estimating effects of generalized time-varying treatment strategies on the mean of an outcome at one or more selected follow-up times of interest. For example, the Medications and Weight Gain in PCORnet (MedWeight) study aimed to estimate effects of adhering to flexible medication regimes on future weight change using electronic health records (EHR) data. This problem presents several methodological challenges that have not been jointly addressed in the prior literature. First, this setting involves treatment strategies that vary over time and depend dynamically and non-deterministically on measured confounder history. Second, the outcome is repeatedly, non-monotonically, informatively, and sparsely measured in the data source. Third, some individuals die during follow-up, rendering the outcome of interest undefined at the follow-up time of interest. In this article, we pose a range of inverse probability weighted (IPW) estimators targeting effects of generalized time-varying treatment strategies in truncation by death settings that allow time-smoothing for precision gain. We conducted simulation studies that confirm precision gains of the time-smoothed IPW approaches over more conventional IPW approaches that do not leverage the repeated outcome measurements. We illustrate an application of the IPW approaches to estimate comparative effects of adhering to flexible antidepressant medication strategies on future weight change. The methods are implemented in the accompanying R package, smoothedIPW.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification</td>
<td style='padding: 6px;'>Mariano Barone, Antonio Romano, Giuseppe Riccio, Marco Postiglione, Vincenzo Moscato</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13888v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Misinformation in healthcare, from vaccine hesitancy to unproven treatments, poses risks to public health and trust in medical systems. While machine learning and natural language processing have advanced automated fact-checking, validating biomedical claims remains uniquely challenging due to complex terminology, the need for domain expertise, and the critical importance of grounding in scientific evidence. We introduce CER (Combining Evidence and Reasoning), a novel framework for biomedical fact-checking that integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. By integrating the text-generation capabilities of large language models with advanced retrieval techniques for high-quality biomedical scientific evidence, CER effectively mitigates the risk of hallucinations, ensuring that generated outputs are grounded in verifiable, evidence-based sources. Evaluations on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising cross-dataset generalization. Code and data are released for transparency and reproducibility: https://github.com/PRAISELab-PicusLab/CER</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Combining Evidence and Reasoning for Biomedical Fact-Checking</td>
<td style='padding: 6px;'>Mariano Barone, Antonio Romano, Giuseppe Riccio, Marco Postiglione, Vincenzo Moscato</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13879v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Misinformation in healthcare, from vaccine hesitancy to unproven treatments, poses risks to public health and trust in medical systems. While machine learning and natural language processing have advanced automated fact-checking, validating biomedical claims remains uniquely challenging due to complex terminology, the need for domain expertise, and the critical importance of grounding in scientific evidence. We introduce CER (Combining Evidence and Reasoning), a novel framework for biomedical fact-checking that integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. By integrating the text-generation capabilities of large language models with advanced retrieval techniques for high-quality biomedical scientific evidence, CER effectively mitigates the risk of hallucinations, ensuring that generated outputs are grounded in verifiable, evidence-based sources. Evaluations on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising cross-dataset generalization. Code and data are released for transparency and reproducibility: https: //github.com/PRAISELab-PicusLab/CER.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation</td>
<td style='padding: 6px;'>Puru Vaish, Felix Meister, Tobias Heimann, Christoph Brune, Jelmer M. Wolterink</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13846v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Many recent approaches in representation learning implicitly assume that uncorrelated views of a data point are sufficient to learn meaningful representations for various downstream tasks. In this work, we challenge this assumption and demonstrate that meaningful structure in the latent space does not emerge naturally. Instead, it must be explicitly induced. We propose a method that aligns representations from different views of the data to align complementary information without inducing false positives. Our experiments show that our proposed self-supervised learning method, Consistent View Alignment, improves performance for downstream tasks, highlighting the critical role of structured view alignment in learning effective representations. Our method achieved first and second place in the MICCAI 2025 SSL3D challenge when using a Primus vision transformer and ResEnc convolutional neural network, respectively. The code and pretrained model weights are released at https://github.com/Tenbatsu24/LatentCampus.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs</td>
<td style='padding: 6px;'>Edward Phillips, Sean Wu, Soheila Molaei, Danielle Belgrave, Anshul Thakur, David Clifton</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13813v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large language models demonstrate impressive results across diverse tasks but are still known to hallucinate, generating linguistically plausible but incorrect answers to questions. Uncertainty quantification has been proposed as a strategy for hallucination detection, but no existing black-box approach provides estimates for both global and local uncertainty. The former attributes uncertainty to a batch of responses, while the latter attributes uncertainty to individual responses. Current local methods typically rely on white-box access to internal model states, whilst black-box methods only provide global uncertainty estimates. We introduce a geometric framework to address this, based on archetypal analysis of batches of responses sampled with only black-box model access. At the global level, we propose Geometric Volume, which measures the convex hull volume of archetypes derived from response embeddings. At the local level, we propose Geometric Suspicion, which ranks responses by reliability and enables hallucination reduction through preferential response selection. Unlike prior dispersion methods which yield only a single global score, our approach provides semantic boundary points which have utility for attributing reliability to individual responses. Experiments show that our framework performs comparably to or better than prior methods on short form question-answering datasets, and achieves superior results on medical datasets where hallucinations carry particularly critical risks. We also provide theoretical justification by proving a link between convex hull volume and entropy.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Data-Efficient Spectral Classification of Hyperspectral Data Using MiniROCKET and HDC-MiniROCKET</td>
<td style='padding: 6px;'>Nick Theisen, Kenny Schlegel, Dietrich Paulus, Peer Neubert</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13809v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The classification of pixel spectra of hyperspectral images, i.e. spectral classification, is used in many fields ranging from agricultural, over medical to remote sensing applications and is currently also expanding to areas such as autonomous driving. Even though for full hyperspectral images the best-performing methods exploit spatial-spectral information, performing classification solely on spectral information has its own advantages, e.g. smaller model size and thus less data required for training. Moreover, spectral information is complementary to spatial information and improvements on either part can be used to improve spatial-spectral approaches in the future. Recently, 1D-Justo-LiuNet was proposed as a particularly efficient model with very few parameters, which currently defines the state of the art in spectral classification. However, we show that with limited training data the model performance deteriorates. Therefore, we investigate MiniROCKET and HDC-MiniROCKET for spectral classification to mitigate that problem. The model extracts well-engineered features without trainable parameters in the feature extraction part and is therefore less vulnerable to limited training data. We show that even though MiniROCKET has more parameters it outperforms 1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the general case</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes</td>
<td style='padding: 6px;'>Iyadh Ben Cheikh Larbi, Ajay Madhavan Ravichandran, Aljoscha Burchardt, Roland Roller</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13696v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large language models (LLMs) excel at text generation, but their ability to handle clinical classification tasks involving structured data, such as time series, remains underexplored. In this work, we adapt instruction-tuned LLMs using DSPy-based prompt optimization to process clinical notes and structured EHR inputs jointly. Our results show that this approach achieves performance on par with specialized multimodal systems while requiring less complexity and offering greater adaptability across tasks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification</td>
<td style='padding: 6px;'>Kaniz Fatema, Emad A. Mohammed, Sukhjit Singh Sehra</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.13687v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Effective and interpretable classification of medical images is a challenge in computer-aided diagnosis, especially in resource-limited clinical settings. This study introduces spline-based Kolmogorov-Arnold Networks (KANs) for accurate medical image classification with limited, diverse datasets. The models include SBTAYLOR-KAN, integrating B-splines with Taylor series; SBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN, embedding B-splines in Morlet wavelet transforms. These approaches leverage spline-based function approximation to capture both local and global nonlinearities. The models were evaluated on brain MRI, chest X-rays, tuberculosis X-rays, and skin lesion images without preprocessing, demonstrating the ability to learn directly from raw data. Extensive experiments, including cross-dataset validation and data reduction analysis, showed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93% accuracy, with a balanced F1-score, maintaining over 86% accuracy using only 30% of the training data across three datasets. Despite class imbalance in the skin cancer dataset, experiments on both imbalanced and balanced versions showed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy. Unlike traditional CNNs, which require millions of parameters (e.g., ResNet50 with 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872 trainable parameters, making it more suitable for constrained medical environments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used for interpretability, highlighting relevant regions in medical images. This framework provides a lightweight, interpretable, and generalizable solution for medical image classification, addressing the challenges of limited datasets and data-scarce scenarios in clinical AI applications.</td>
</tr>
</tbody>
</table>

