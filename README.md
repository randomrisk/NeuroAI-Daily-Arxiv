<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2024-10-11</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>Stochastic Process Turing Machines</td>
<td style='padding: 6px;'>David Wolpert, Jordan Scharnhorst</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.07131v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Computer science theory provides many different measures of complexity of a system including Kolmogorov complexity, logical depth, computational depth, and Levin complexity. However, these measures are all defined only for deterministic Turing machines, i.e., deterministic dynamics of the underlying generative process whose output we are interested in. Therefore, by construction they cannot capture complexity of the output of stochastic processes - like those in the real world. Motivated by this observation, we combine probabilistic Turing machines with a prior over the inputs to the Turing machine to define a complete stochastic process of Turing machines. We call this a stochastic process Turing machine. Stochastic process Turing machines allow us to formalize a stochastic generalization of logical depth called stochastic depth, and also to apply stochastic thermodynamics to the analysis of Turing machines. Stochastic process Turing machines and stochastic depth allow us to study the complex, stochastic systems like the human brain, societies, and evolution all from within the framework of formal computation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>Causal Representation Learning in Temporal Data via Single-Parent Decoding</td>
<td style='padding: 6px;'>Philippe Brouillard, Sébastien Lachapelle, Julia Kaltenborn, Yaniv Gurwicz, Dhanya Sridhar, Alexandre Drouin, Peer Nowack, Jakob Runge, David Rolnick</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.07013v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Scientific research often seeks to understand the causal structure underlying high-level variables in a system. For example, climate scientists study how phenomena, such as El Ni\~no, affect other climate processes at remote locations across the globe. However, scientists typically collect low-level measurements, such as geographically distributed temperature readings. From these, one needs to learn both a mapping to causally-relevant latent variables, such as a high-level representation of the El Ni\~no phenomenon and other processes, as well as the causal model over them. The challenge is that this task, called causal representation learning, is highly underdetermined from observational data alone, requiring other constraints during learning to resolve the indeterminacies. In this work, we consider a temporal model with a sparsity assumption, namely single-parent decoding: each observed low-level variable is only affected by a single latent variable. Such an assumption is reasonable in many scientific applications that require finding groups of low-level variables, such as extracting regions from geographically gridded measurement data in climate research or capturing brain regions from neural activity data. We demonstrate the identifiability of the resulting model and propose a differentiable method, Causal Discovery with Single-parent Decoding (CDSD), that simultaneously learns the underlying latents and a causal graph over them. We assess the validity of our theoretical results using simulated data and showcase the practical validity of our method in an application to real-world data from the climate science field.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>Structure and Control of Biology-inspired Networks</td>
<td style='padding: 6px;'>Zexin Sun, John Baillieul</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06990v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>There is increasing interest in developing the theoretical foundations of networked control systems that illuminate how brain networks function so as to enable sensory perception, control of movement, memory and all the operations that are needed for animals to survive. The present paper proposes a biologically inspired network model featuring dynamic connections regulated by Hebbian learning. Drawing on the machinery of graph theory and classical control we show that our novel nonlinear model exhibits such biologically plausible features as bounded evolution, stability, resilience, and a kind of structural stability -- meaning that perturbations of the model parameters leave the essential properties of the model in tact. The proposed network model involves generalized cactus graphs with multiple control input nodes, and it is shown that the properties of the network are resilient to various changes in network topology provided these changes preserve the generalized cactus structure. A particular example described in what follows is an idealized network model of the visual system of a macaque monkey. The model displays resilience to network disruptions such as might occur in a living organism due to disease or injury. A different model of the same type provides an example of a system that can perform data classification.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>The multiscale self-similarity of the weighted human brain connectome</td>
<td style='padding: 6px;'>Laia Barjuan, Muhua Zheng, M. Ángeles Serrano</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06739v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Anatomical connectivity between different regions in the brain can be mapped to a network representation, the connectome, where the intensities of the links, the weights, influence its structural resilience and the functional processes it sustains. Yet, many features associated with the weights in the human brain connectome are not fully understood, particularly their multiscale organization. In this paper, we elucidate the architecture of weights, including weak ties, in multiscale hierarchical human brain connectomes reconstructed from empirical data. Our findings reveal multiscale self-similarity in the weighted statistical properties, including the ordering of weak ties, that remain consistent across the analyzed length scales of every individual and the group representatives. This phenomenon is effectively captured by a renormalization of the weighted structure applied to hyperbolic embeddings of the connectomes, based on a unique weighted geometric model that integrates links of all weights across all length scales. This eliminates the need for separate generative weighted connectivity rules for each scale or to replicate weak and strong ties at specific scales in brain connectomes. The observed symmetry represents a distinct signature of criticality in the weighted connectivity of human brain connectomes, aligning with the fractality observed in their topology, and raises important questions for future research, like the existence of a resolution threshold where the observed symmetry breaks, or whether it is preserved in cases of neurodegenerative disease or psychiatric disorder.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>EEG-estimated functional connectivity, and not behavior, differentiates Parkinson's patients from health controls during the Simon conflict task</td>
<td style='padding: 6px;'>Xiaoxiao Sun, Chongkun Zhao, Sharath Koorathota, Paul Sajda</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06534v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neural biomarkers that can classify or predict disease are of broad interest to the neurological and psychiatric communities. Such biomarkers can be informative of disease state or treatment efficacy, even before there are changes in symptoms and/or behavior. This work investigates EEG-estimated functional connectivity (FC) as a Parkinson's Disease (PD) biomarker. Specifically, we investigate FC mediated via neural oscillations and consider such activity during the Simons conflict task. This task yields sensory-motor conflict, and one might expect differences in behavior between PD patients and healthy controls (HCs). In addition to considering spatially focused approaches, such as FC, as a biomarker, we also consider temporal biomarkers, which are more sensitive to ongoing changes in neural activity. We find that FC, estimated from delta (1-4Hz) and theta (4-7Hz) oscillations, yields spatial FC patterns significantly better at distinguishing PD from HC than temporal features or behavior. This study reinforces that FC in spectral bands is informative of differences in brain-wide processes and can serve as a biomarker distinguishing normal brain function from that seen in disease.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>OpenEarable ExG: Open-Source Hardware for Ear-Based Biopotential Sensing Applications</td>
<td style='padding: 6px;'>Philipp Lepold, Tobias Röddiger, Tobias King, Kai Kunze, Christoph Maurer, Michael Beigl</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06533v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While traditional earphones primarily offer private audio spaces, so-called "earables" emerged to offer a variety of sensing capabilities. Pioneering platforms like OpenEarable have introduced novel sensing platforms targeted at the ears, incorporating various sensors. The proximity of the ears to the eyes, brain, and facial muscles has also sparked investigation into sensing biopotentials. However, currently there is no platform available that is targeted at the ears to sense biopotentials. To address this gap, we introduce OpenEarable ExG - an open-source hardware platform designed to measure biopotentials in and around the ears. OpenEarable ExG can be freely configured and has up to 7 sensing channels. We initially validate OpenEarable ExG in a study with a left-right in-ear dual-electrode montage setup with 3 participants. Our results demonstrate the successful detection of smooth pursuit eye movements via Electrooculography (EOG), alpha brain activity via Electroencephalography (EEG), and jaw clenching via Electromyography (EMG). OpenEarable ExG is part of the OpenEarable initiative and is fully open-source under MIT license.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>Dynamics of Adaptive Continuous Attractor Neural Networks</td>
<td style='padding: 6px;'>Yujun Li, Tianhao Chu, Si Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06517v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Attractor neural networks consider that neural information is stored as stationary states of a dynamical system formed by a large number of interconnected neurons. The attractor property empowers a neural system to encode information robustly, but it also incurs the difficulty of rapid update of network states, which can impair information update and search in the brain. To overcome this difficulty, a solution is to include adaptation in the attractor network dynamics, whereby the adaptation serves as a slow negative feedback mechanism to destabilize which are otherwise permanently stable states. In such a way, the neural system can, on one hand, represent information reliably using attractor states, and on the other hand, perform computations wherever rapid state updating is involved. Previous studies have shown that continuous attractor neural networks with adaptation (A-CANNs) exhibits rich dynamical behaviors accounting for various brain functions. In this paper, we present a comprehensive view of the rich diverse dynamics of A-CANNs. Moreover, we provide a unified mathematical framework to understand these different dynamical behaviors, and briefly discuss about their biological implications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-08</td>
<td style='padding: 8px;'>Stochastic Sparse Sampling: A Framework for Variable-Length Medical Time Series Classification</td>
<td style='padding: 6px;'>Xavier Mootoo, Alan A. Díaz-Montiel, Milad Lankarany, Hina Tabassum</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06412v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While the majority of time series classification research has focused on modeling fixed-length sequences, variable-length time series classification (VTSC) remains critical in healthcare, where sequence length may vary among patients and events. To address this challenge, we propose $\textbf{S}$tochastic $\textbf{S}$parse $\textbf{S}$ampling (SSS), a novel VTSC framework developed for medical time series. SSS manages variable-length sequences by sparsely sampling fixed windows to compute local predictions, which are then aggregated and calibrated to form a global prediction. We apply SSS to the task of seizure onset zone (SOZ) localization, a critical VTSC problem requiring identification of seizure-inducing brain regions from variable-length electrophysiological time series. We evaluate our method on the Epilepsy iEEG Multicenter Dataset, a heterogeneous collection of intracranial electroencephalography (iEEG) recordings obtained from four independent medical centers. SSS demonstrates superior performance compared to state-of-the-art (SOTA) baselines across most medical centers, and superior performance on all out-of-distribution (OOD) unseen medical centers. Additionally, SSS naturally provides post-hoc insights into local signal characteristics related to the SOZ, by visualizing temporally averaged local predictions throughout the signal.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-08</td>
<td style='padding: 8px;'>A convex formulation of covariate-adjusted Gaussian graphical models via natural parametrization</td>
<td style='padding: 6px;'>Ruobin Liu, Guo Yu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06326v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Gaussian graphical models (GGMs) are widely used for recovering the conditional independence structure among random variables. Recently, several key advances have been made to exploit an additional set of variables for better estimating the GGMs of the variables of interest. For example, in co-expression quantitative trait locus (eQTL) studies, both the mean expression level of genes as well as their pairwise conditional independence structure may be adjusted by genetic variants local to those genes. Existing methods to estimate covariate-adjusted GGMs either allow only the mean to depend on covariates or suffer from poor scaling assumptions due to the inherent non-convexity of simultaneously estimating the mean and precision matrix. In this paper, we propose a convex formulation that jointly estimates the covariate-adjusted mean and precision matrix by utilizing the natural parametrization of the multivariate Gaussian likelihood. This convexity yields theoretically better performance as the sparsity and dimension of the covariates grow large relative to the number of samples. We verify our theoretical results with numerical simulations and perform a reanalysis of an eQTL study of glioblastoma multiforme (GBM), an aggressive form of brain cancer.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-08</td>
<td style='padding: 8px;'>Exploring Growing Complex Systems with Higher-Order Interactions</td>
<td style='padding: 6px;'>Soo Min Oh, Yongsun Lee, Byungnam Kahng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06279v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A complex system with many interacting individuals can be represented by a network consisting of nodes and links representing individuals and pairwise interactions, respectively. However, real-world systems grow with time and include many higher-order interactions. Such systems with higher-order interactions can be well described by a simplicial complex (SC), which is a type of hypergraph, consisting of simplexes representing sets of multiple interacting nodes. Here, capturing the properties of growing real-world systems, we propose a growing random SC (GRSC) model where a node is added and a higher dimensional simplex is established among nodes at each time step. We then rigorously derive various percolation properties in the GRSC. Finally, we confirm that the system exhibits an infinite-order phase transition as higher-order interactions accelerate the growth of the system and result in the advanced emergence of a giant cluster. This work can pave the way for interpreting growing complex systems with higher-order interactions such as social, biological, brain, and technological systems.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>EEG-estimated functional connectivity, and not behavior, differentiates Parkinson's patients from health controls during the Simon conflict task</td>
<td style='padding: 6px;'>Xiaoxiao Sun, Chongkun Zhao, Sharath Koorathota, Paul Sajda</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06534v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neural biomarkers that can classify or predict disease are of broad interest to the neurological and psychiatric communities. Such biomarkers can be informative of disease state or treatment efficacy, even before there are changes in symptoms and/or behavior. This work investigates EEG-estimated functional connectivity (FC) as a Parkinson's Disease (PD) biomarker. Specifically, we investigate FC mediated via neural oscillations and consider such activity during the Simons conflict task. This task yields sensory-motor conflict, and one might expect differences in behavior between PD patients and healthy controls (HCs). In addition to considering spatially focused approaches, such as FC, as a biomarker, we also consider temporal biomarkers, which are more sensitive to ongoing changes in neural activity. We find that FC, estimated from delta (1-4Hz) and theta (4-7Hz) oscillations, yields spatial FC patterns significantly better at distinguishing PD from HC than temporal features or behavior. This study reinforces that FC in spectral bands is informative of differences in brain-wide processes and can serve as a biomarker distinguishing normal brain function from that seen in disease.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>OpenEarable ExG: Open-Source Hardware for Ear-Based Biopotential Sensing Applications</td>
<td style='padding: 6px;'>Philipp Lepold, Tobias Röddiger, Tobias King, Kai Kunze, Christoph Maurer, Michael Beigl</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06533v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While traditional earphones primarily offer private audio spaces, so-called "earables" emerged to offer a variety of sensing capabilities. Pioneering platforms like OpenEarable have introduced novel sensing platforms targeted at the ears, incorporating various sensors. The proximity of the ears to the eyes, brain, and facial muscles has also sparked investigation into sensing biopotentials. However, currently there is no platform available that is targeted at the ears to sense biopotentials. To address this gap, we introduce OpenEarable ExG - an open-source hardware platform designed to measure biopotentials in and around the ears. OpenEarable ExG can be freely configured and has up to 7 sensing channels. We initially validate OpenEarable ExG in a study with a left-right in-ear dual-electrode montage setup with 3 participants. Our results demonstrate the successful detection of smooth pursuit eye movements via Electrooculography (EOG), alpha brain activity via Electroencephalography (EEG), and jaw clenching via Electromyography (EMG). OpenEarable ExG is part of the OpenEarable initiative and is fully open-source under MIT license.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-08</td>
<td style='padding: 8px;'>Closed-Loop phase selection in EEG-TMS using Bayesian Optimization</td>
<td style='padding: 6px;'>Miriam Kirchhoff, Dania Humaidan, Ulf Ziemann</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.05747v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Research on transcranial magnetic stimulation (TMS) combined with encephalography feedback (EEG-TMS) has shown that the phase of the sensorimotor mu rhythm is predictive of corticospinal excitability. Thus, if the subject-specific optimal phase is known, stimulation can be timed to be more efficient. In this paper, we present a closed-loop algorithm to determine the optimal phase linked to the highest excitability with few trials. We used Bayesian optimization as an automated, online search tool in an EEG-TMS simulation experiment. From a sample of 38 participants, we selected all participants with a significant single-subject phase effect (N = 5) for simulation. We then simulated 1000 experimental sessions per participant where we used Bayesian optimization to find the optimal phase. We tested two objective functions: Fitting a sinusoid in Bayesian linear regression or Gaussian Process (GP) regression. We additionally tested adaptive sampling using a knowledge gradient as the acquisition function compared with random sampling. We evaluated the algorithm's performance in a fast optimization (100 trials) and a long-term optimization (1000 trials). For fast optimization, the Bayesian linear regression in combination with adaptive sampling gives the best results with a mean phase location accuracy of 79 % after 100 trials. With either sampling approach, Bayesian linear regression performs better than GP regression in the fast optimization. In the long-term optimization, Bayesian regression with random sampling shows the best trajectory, with a rather steep improvement and good final performance of 87 % mean phase location accuracy. We show the suitability of closed-loop Bayesian optimization for phase selection. We could increase the speed and accuracy by using prior knowledge about the expected function shape compared with traditional Bayesian optimization with GP regression.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-07</td>
<td style='padding: 8px;'>Pre-Ictal Seizure Prediction Using Personalized Deep Learning</td>
<td style='padding: 6px;'>Shriya Jaddu, Sidh Jaddu, Camilo Gutierrez, Quincy K. Tran</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.05491v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Introduction: Approximately 23 million or 30% of epilepsy patients worldwide suffer from drug-resistant epilepsy (DRE). The unpredictability of seizure occurrences, which causes safety issues as well as social concerns, restrict the lifestyles of DRE patients. Surgical solutions and EEG-based solutions are very expensive, unreliable, invasive or impractical. The goal of this research was to employ improved technologies and methods to epilepsy patient physiological data and predict seizures up to two hours before onset, enabling non-invasive, affordable seizure prediction for DRE patients.   Methods: This research used a 1D Convolutional Neural Network-Based Bidirectional Long Short-Term Memory network that was trained on a diverse set of epileptic patient physiological data to predict seizures. Transfer learning was further utilized to personalize and optimize predictions for specific patients. Clinical data was retrospectively obtained for nine epilepsy patients via wearable devices over a period of about three to five days from a prospectively maintained database. The physiological data included 54 seizure occurrences and included heart rate, blood volume pulse, accelerometry, body temperature, and electrodermal activity.   Results and Conclusion: A general deep-learning model trained on the physiological data with randomly sampled test data achieved an accuracy of 91.94%. However, such a generalized deep learning model had varied performances on data from unseen patients. When the general model was personalized (further trained) with patient-specific data, the personalized model achieved significantly improved performance with accuracies as high as 97%. This preliminary research shows that patient-specific personalization may be a viable approach to achieve affordable, non-invasive seizure prediction that can improve the quality of life for DRE patients.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-07</td>
<td style='padding: 8px;'>NeuroBOLT: Resting-state EEG-to-fMRI Synthesis with Multi-dimensional Feature Mapping</td>
<td style='padding: 6px;'>Yamin Li, Ange Lou, Ziyuan Xu, Shengchao Zhang, Shiyu Wang, Dario J. Englot, Soheil Kolouri, Daniel Moyer, Roza G. Bayrak, Catie Chang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.05341v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional magnetic resonance imaging (fMRI) is an indispensable tool in modern neuroscience, providing a non-invasive window into whole-brain dynamics at millimeter-scale spatial resolution. However, fMRI is constrained by issues such as high operation costs and immobility. With the rapid advancements in cross-modality synthesis and brain decoding, the use of deep neural networks has emerged as a promising solution for inferring whole-brain, high-resolution fMRI features directly from electroencephalography (EEG), a more widely accessible and portable neuroimaging modality. Nonetheless, the complex projection from neural activity to fMRI hemodynamic responses and the spatial ambiguity of EEG pose substantial challenges both in modeling and interpretability. Relatively few studies to date have developed approaches for EEG-fMRI translation, and although they have made significant strides, the inference of fMRI signals in a given study has been limited to a small set of brain areas and to a single condition (i.e., either resting-state or a specific task). The capability to predict fMRI signals in other brain areas, as well as to generalize across conditions, remain critical gaps in the field. To tackle these challenges, we introduce a novel and generalizable framework: NeuroBOLT, i.e., Neuro-to-BOLD Transformer, which leverages multi-dimensional representation learning from temporal, spatial, and spectral domains to translate raw EEG data to the corresponding fMRI activity signals across the brain. Our experiments demonstrate that NeuroBOLT effectively reconstructs resting-state fMRI signals from primary sensory, high-level cognitive areas, and deep subcortical brain regions, achieving state-of-the-art accuracy and significantly advancing the integration of these two modalities.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-04</td>
<td style='padding: 8px;'>From Epilepsy Seizures Classification to Detection: A Deep Learning-based Approach for Raw EEG Signals</td>
<td style='padding: 6px;'>Davy Darankoum, Manon Villalba, Clelia Allioux, Baptiste Caraballo, Carine Dumont, Eloise Gronlier, Corinne Roucard, Yann Roche, Chloe Habermacher, Sergei Grudinin, Julien Volle</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.03385v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Epilepsy represents the most prevalent neurological disease in the world. One-third of people suffering from mesial temporal lobe epilepsy (MTLE) exhibit drug resistance, urging the need to develop new treatments. A key part in anti-seizure medication (ASM) development is the capability of detecting and quantifying epileptic seizures occurring in electroencephalogram (EEG) signals, which is crucial for treatment efficacy evaluation. In this study, we introduced a seizure detection pipeline based on deep learning models applied to raw EEG signals. This pipeline integrates: a new pre-processing technique which segments continuous raw EEG signals without prior distinction between seizure and seizure-free activities; a post-processing algorithm developed to reassemble EEG segments and allow the identification of seizures start/end; and finally, a new evaluation procedure based on a strict seizure events comparison between predicted and real labels. Models training have been performed using a data splitting strategy which addresses the potential for data leakage. We demonstrated the fundamental differences between a seizure classification and a seizure detection task and showed the differences in performance between the two tasks. Finally, we demonstrated the generalization capabilities across species of our best architecture, combining a Convolutional Neural Network and a Transformer encoder. The model was trained on animal EEGs and tested on human EEGs with a F1-score of 93% on a balanced Bonn dataset.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-04</td>
<td style='padding: 8px;'>Simulated Eyeblink Artifact Removal with ICA: Effect of Measurement Uncertainty</td>
<td style='padding: 6px;'>Jennie Couchman, Orestis Kaparounakis, Chatura Samarakoon, Phillip Stanley-Marbell</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.03261v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Independent Component Analysis (ICA) is commonly-used in electroencephalogram (EEG) signal processing to remove non-cerebral artifacts from cerebral data. Despite the ubiquity of ICA, the effect of measurement uncertainty on the artifact removal process has not been thoroughly investigated. We first characterize the measurement uncertainty distribution of a common ADC and show that it quantitatively conforms to a Gaussian distribution. We then evaluate the effect of measurement uncertainty on the artifact identification process through several computer simulations. These computer simulations evaluate the performance of two different ICA algorithms, FastICA and Infomax, in removing eyeblink artifacts from five different electrode configurations with varying levels of measurement uncertainty. FastICA and Infomax show similar performance in identifying the eyeblink artifacts for a given uncertainty level and electrode configuration. We quantify the correlation performance degradation with respect to SNR and show that in general, an SNR of greater than 15 dB results in less than a 5% degradation in performance. The biggest difference in performance between the two algorithms is in their execution time. FastICA's execution time is dependent on the amount of measurement uncertainty, with a 50% to 85% reduction in execution time over an SNR range of 20 dB. This contrasts with Infomax's execution time, which is unaffected by measurement uncertainty.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>Nested Deep Learning Model Towards A Foundation Model for Brain Signal Data</td>
<td style='padding: 6px;'>Fangyi Wei, Jiajie Mo, Kai Zhang, Haipeng Shen, Srikantan Nagarajan, Fei Jiang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.03191v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Epilepsy affects over 50 million people globally, with EEG/MEG-based spike detection playing a crucial role in diagnosis and treatment. Manual spike identification is time-consuming and requires specialized training, limiting the number of professionals available to analyze EEG/MEG data. To address this, various algorithmic approaches have been developed. However, current methods face challenges in handling varying channel configurations and in identifying the specific channels where spikes originate. This paper introduces a novel Nested Deep Learning (NDL) framework designed to overcome these limitations. NDL applies a weighted combination of signals across all channels, ensuring adaptability to different channel setups, and allows clinicians to identify key channels more accurately. Through theoretical analysis and empirical validation on real EEG/MEG datasets, NDL demonstrates superior accuracy in spike detection and channel localization compared to traditional methods. The results show that NDL improves prediction accuracy, supports cross-modality data integration, and can be fine-tuned for various neurophysiological applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-04</td>
<td style='padding: 8px;'>How to evaluate your medical time series classification?</td>
<td style='padding: 6px;'>Yihe Wang, Taida Li, Yujun Yan, Wenzhan Song, Xiang Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.03057v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Medical time series (MedTS) play a critical role in many healthcare applications, such as vital sign monitoring and the diagnosis of brain and heart diseases. However, the existence of subject-specific features poses unique challenges in MedTS evaluation. Inappropriate evaluation setups that either exploit or overlook these features can lead to artificially inflated classification performance (by up to 50% in accuracy on ADFTD dataset): this concern has received little attention in current research. Here, we categorize the existing evaluation setups into two primary categories: subject-dependent and subject-independent. We show the subject-independent setup is more appropriate for different datasets and tasks. Our theoretical analysis explores the feature components of MedTS, examining how different evaluation setups influence the features that a model learns. Through experiments on six datasets (spanning EEG, ECG, and fNIRS modalities) using four different methods, we demonstrate step-by-step how subject-dependent utilizes subject-specific features as a shortcut for classification and leads to a deceptive high performance, suggesting that the subject-independent setup is more precise and practicable evaluation setup in real-world. This comprehensive analysis aims to establish clearer guidelines for evaluating MedTS models in different healthcare applications. Code to reproduce this work in \url{https://github.com/DL4mHealth/MedTS_Evaluation}.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-03</td>
<td style='padding: 8px;'>E2H: A Two-Stage Non-Invasive Neural Signal Driven Humanoid Robotic Whole-Body Control Framework</td>
<td style='padding: 6px;'>Yiqun Duan, Jinzhao Zhou, Xiaowei Jiang, Qiang Zhang, Jingkai Sun, Jiahang Cao, Jiaxu Wang, Yiqian Yang, Wen Zhao, Gang Han, Yijie Guo, Chin-Teng Lin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.02141v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent advancements in humanoid robotics, including the integration of hierarchical reinforcement learning-based control and the utilization of LLM planning, have significantly enhanced the ability of robots to perform complex tasks. In contrast to the highly developed humanoid robots, the human factors involved remain relatively unexplored. Directly controlling humanoid robots with the brain has already appeared in many science fiction novels, such as Pacific Rim and Gundam. In this work, we present E2H (EEG-to-Humanoid), an innovative framework that pioneers the control of humanoid robots using high-frequency non-invasive neural signals. As the none-invasive signal quality remains low in decoding precise spatial trajectory, we decompose the E2H framework in an innovative two-stage formation: 1) decoding neural signals (EEG) into semantic motion keywords, 2) utilizing LLM facilitated motion generation with a precise motion imitation control policy to realize humanoid robotics control. The method of directly driving robots with brainwave commands offers a novel approach to human-machine collaboration, especially in situations where verbal commands are impractical, such as in cases of speech impairments, space exploration, or underwater exploration, unlocking significant potential. E2H offers an exciting glimpse into the future, holding immense potential for human-computer interaction.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-08</td>
<td style='padding: 8px;'>Bayesian model of individual learning to control a motor imagery BCI</td>
<td style='padding: 6px;'>Côme Annicchiarico, Fabien Lotte, Jérémie Mattout</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.05926v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The cognitive mechanisms underlying subjects' self-regulation in Brain-Computer Interface (BCI) and neurofeedback (NF) training remain poorly understood. Yet, a mechanistic computational model of each individual learning trajectory is required to improve the reliability of BCI applications. The few existing attempts mostly rely on model-free (reinforcement learning) approaches. Hence, they cannot capture the strategy developed by each subject and neither finely predict their learning curve. In this study, we propose an alternative, model-based approach rooted in cognitive skill learning within the Active Inference framework. We show how BCI training may be framed as an inference problem under high uncertainties. We illustrate the proposed approach on a previously published synthetic Motor Imagery ERD laterality training. We show how simple changes in model parameters allow us to qualitatively match experimental results and account for various subject. In the near future, this approach may provide a powerful computational to model individual skill learning and thus optimize and finely characterize BCI training.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-02</td>
<td style='padding: 8px;'>SHAP-CAT: A interpretable multi-modal framework enhancing WSI classification via virtual staining and shapley-value-based multimodal fusion</td>
<td style='padding: 6px;'>Jun Wang, Yu Mao, Nan Guan, Chun Jason Xue</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.01408v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The multimodal model has demonstrated promise in histopathology. However, most multimodal models are based on H\&E and genomics, adopting increasingly complex yet black-box designs. In our paper, we propose a novel interpretable multimodal framework named SHAP-CAT, which uses a Shapley-value-based dimension reduction technique for effective multimodal fusion. Starting with two paired modalities -- H\&E and IHC images, we employ virtual staining techniques to enhance limited input data by generating a new clinical-related modality. Lightweight bag-level representations are extracted from image modalities and a Shapley-value-based mechanism is used for dimension reduction. For each dimension of the bag-level representation, attribution values are calculated to indicate how changes in the specific dimensions of the input affect the model output. In this way, we select a few top important dimensions of bag-level representation for each image modality to late fusion. Our experimental results demonstrate that the proposed SHAP-CAT framework incorporating synthetic modalities significantly enhances model performance, yielding a 5\% increase in accuracy for the BCI, an 8\% increase for IHC4BC-ER, and an 11\% increase for the IHC4BC-PR dataset.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-30</td>
<td style='padding: 8px;'>Professor X: Manipulating EEG BCI with Invisible and Robust Backdoor Attack</td>
<td style='padding: 6px;'>Xuan-Hao Liu, Xinhao Song, Dexuan He, Bao-Liang Lu, Wei-Long Zheng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.20158v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While electroencephalogram (EEG) based brain-computer interface (BCI) has been widely used for medical diagnosis, health care, and device control, the safety of EEG BCI has long been neglected. In this paper, we propose Professor X, an invisible and robust "mind-controller" that can arbitrarily manipulate the outputs of EEG BCI through backdoor attack, to alert the EEG community of the potential hazard. However, existing EEG attacks mainly focus on single-target class attacks, and they either require engaging the training stage of the target BCI, or fail to maintain high stealthiness. Addressing these limitations, Professor X exploits a three-stage clean label poisoning attack: 1) selecting one trigger for each class; 2) learning optimal injecting EEG electrodes and frequencies strategy with reinforcement learning for each trigger; 3) generating poisoned samples by injecting the corresponding trigger's frequencies into poisoned data for each class by linearly interpolating the spectral amplitude of both data according to previously learned strategies. Experiments on datasets of three common EEG tasks demonstrate the effectiveness and robustness of Professor X, which also easily bypasses existing backdoor defenses.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-27</td>
<td style='padding: 8px;'>AM-MTEEG: Multi-task EEG classification based on impulsive associative memory</td>
<td style='padding: 6px;'>Junyan Li, Bin Hu, Zhi-Hong Guan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.18375v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalogram-based brain-computer interface (BCI) has potential applications in various fields, but their development is hindered by limited data and significant cross-individual variability. Inspired by the principles of learning and memory in the human hippocampus, we propose a multi-task (MT) classification model, called AM-MTEEG, which combines learning-based impulsive neural representations with bidirectional associative memory (AM) for cross-individual BCI classification tasks. The model treats the EEG classification of each individual as an independent task and facilitates feature sharing across individuals. Our model consists of an impulsive neural population coupled with a convolutional encoder-decoder to extract shared features and a bidirectional associative memory matrix to map features to class. Experimental results in two BCI competition datasets show that our model improves average accuracy compared to state-of-the-art models and reduces performance variance across individuals, and the waveforms reconstructed by the bidirectional associative memory provide interpretability for the model's classification results. The neuronal firing patterns in our model are highly coordinated, similarly to the neural coding of hippocampal neurons, indicating that our model has biological similarities.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-26</td>
<td style='padding: 8px;'>Towards Forever Access for Implanted Brain-Computer Interfaces</td>
<td style='padding: 6px;'>Muhammed Ugur, Raghavendra Pradyumna Pothukuchi, Abhishek Bhattacharjee</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.17496v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Designs for implanted brain-computer interfaces (BCIs) have increased significantly in recent years. Each device promises better clinical outcomes and quality-of-life improvements, yet due to severe and inflexible safety constraints, progress requires tight co-design from materials to circuits and all the way up the stack to applications and algorithms. This trend has become more aggressive over time, forcing clinicians and patients to rely on vendor-specific hardware and software for deployment, maintenance, upgrades, and replacement. This over-reliance is ethically problematic, especially if companies go out-of-business or business objectives diverge from clinical promises. Device heterogeneity additionally burdens clinicians and healthcare facilities, adding complexity and costs for in-clinic visits, monitoring, and continuous access.   Reliability, interoperability, portability, and future-proofed design is needed, but this unfortunately comes at a cost. These system features sap resources that would have otherwise been allocated to reduce power/energy and improve performance. Navigating this trade-off in a systematic way is critical to providing patients with forever access to their implants and reducing burdens placed on healthcare providers and caretakers. We study the integration of on-device storage to highlight the sensitivity of this trade-off and establish other points of interest within BCI design that require careful investigation. In the process, we revisit relevant problems in computer architecture and medical devices from the current era of hardware specialization and modern neurotechnology.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-26</td>
<td style='padding: 8px;'>The Interplay of Computing, Ethics, and Policy in Brain-Computer Interface Design</td>
<td style='padding: 6px;'>Muhammed Ugur, Raghavendra Pradyumna Pothukuchi, Abhishek Bhattacharjee</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.17445v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-computer interfaces (BCIs) connect biological neurons in the brain with external systems like prosthetics and computers. They are increasingly incorporating processing capabilities to analyze and stimulate neural activity, and consequently, pose unique design challenges related to ethics, law, and policy. For the first time, this paper articulates how ethical, legal, and policy considerations can shape BCI architecture design, and how the decisions that architects make constrain or expand the ethical, legal, and policy frameworks that can be applied to them.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-25</td>
<td style='padding: 8px;'>Sense of Agency in Closed-loop Muscle Stimulation</td>
<td style='padding: 6px;'>Lukas Gehrke, Leonie Terfurth, Klaus Gramann</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.16896v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>To maintain a user's sense of agency (SoA) when working with a physical motor augmentation device, the actuation must align with the user's intentions. In experiments, this is often achieved using stimulus-response paradigms where the motor augmentation can be optimally timed. However, in the everyday world users primarily act at their own volition. We designed a closed-loop system for motor augmentation using an EEG-based brain-computer interface (BCI) to cue users' volitional finger tapping. Relying on the readiness potentials, the system autonomously cued the finger movement at the time of the intent to interact via electrical muscle stimulation (EMS). The prototype discriminated pre-movement from idle EEG segments with an average F1 score of 0.7. However, we found only weak evidence for a maintained SoA. Still, participants reported a higher level of control when working with the system instead of being passively moved.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-25</td>
<td style='padding: 8px;'>Translating Mental Imaginations into Characters with Codebooks and Dynamics-Enhanced Decoding</td>
<td style='padding: 6px;'>Jingyuan Li, Yansen Wang, Nie Lin, Dongsheng Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.16816v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Advancements in non-invasive electroencephalogram (EEG)-based Brain-Computer Interface (BCI) technology have enabled communication through brain activity, offering significant potential for individuals with motor impairments. Existing methods for decoding characters or words from EEG recordings either rely on continuous external stimulation for high decoding accuracy or depend on direct intention imagination, which suffers from reduced discrimination ability. To overcome these limitations, we introduce a novel EEG paradigm based on mental tasks that achieves high discrimination accuracy without external stimulation. Specifically, we propose a codebook in which each letter or number is associated with a unique code that integrates three mental tasks, interleaved with eye-open and eye-closed states. This approach allows individuals to internally reference characters without external stimuli while maintaining reasonable accuracy. For enhanced decoding performance, we apply a Temporal-Spatial-Latent-Dynamics (TSLD) network to capture latent dynamics of spatiotemporal EEG signals. Experimental results demonstrate the effectiveness of our proposed EEG paradigm which achieves five times higher accuracy over direct imagination. Additionally, the TSLD network improves baseline methods by approximately 8.5%. Further more, we observe consistent performance improvement throughout the data collection process, suggesting that the proposed paradigm has potential for further optimization with continued use.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-24</td>
<td style='padding: 8px;'>Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition</td>
<td style='padding: 6px;'>Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, Xiangmin Xu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.16081v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Utilizing functional near-infrared spectroscopy (fNIRS) signals for emotion recognition is a significant advancement in understanding human emotions. However, due to the lack of artificial intelligence data and algorithms in this field, current research faces the following challenges: 1) The portable wearable devices have higher requirements for lightweight models; 2) The objective differences of physiology and psychology among different subjects aggravate the difficulty of emotion recognition. To address these challenges, we propose a novel cross-subject fNIRS emotion recognition method, called the Online Multi-level Contrastive Representation Distillation framework (OMCRD). Specifically, OMCRD is a framework designed for mutual learning among multiple lightweight student networks. It utilizes multi-level fNIRS feature extractor for each sub-network and conducts multi-view sentimental mining using physiological signals. The proposed Inter-Subject Interaction Contrastive Representation (IS-ICR) facilitates knowledge transfer for interactions between student models, enhancing cross-subject emotion recognition performance. The optimal student network can be selected and deployed on a wearable device. Some experimental results demonstrate that OMCRD achieves state-of-the-art results in emotional perception and affective imagery tasks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-18</td>
<td style='padding: 8px;'>Accelerated Algorithms for Source Orientation Detection (AORI) and Spatiotemporal LCMV (ALCMV) Beamforming in EEG Source Localization</td>
<td style='padding: 6px;'>Ava Yektaeian Vaziri, Bahador Makkiabadi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.11751v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper illustrates the development of two efficient source localization algorithms for electroencephalography (EEG) data, aimed at enhancing real-time brain signal reconstruction while addressing the computational challenges of traditional methods. Accurate EEG source localization is crucial for applications in cognitive neuroscience, neurorehabilitation, and brain-computer interfaces (BCIs). To make significant progress toward precise source orientation detection and improved signal reconstruction, we introduce the Accelerated Linear Constrained Minimum Variance (ALCMV) beamforming toolbox and the Accelerated Brain Source Orientation Detection (AORI) toolbox. The ALCMV algorithm speeds up EEG source reconstruction by utilizing recursive covariance matrix calculations, while AORI simplifies source orientation detection from three dimensions to one, reducing computational load by 66% compared to conventional methods. Using both simulated and real EEG data, we demonstrate that these algorithms maintain high accuracy, with orientation errors below 0.2% and signal reconstruction accuracy within 2%. These findings suggest that the proposed toolboxes represent a substantial advancement in the efficiency and speed of EEG source localization, making them well-suited for real-time neurotechnological applications.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-08</td>
<td style='padding: 8px;'>Contrastive Learning to Fine-Tune Feature Extraction Models for the Visual Cortex</td>
<td style='padding: 6px;'>Alex Mulrooney, Austin J. Brockmeier</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06067v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Predicting the neural response to natural images in the visual cortex requires extracting relevant features from the images and relating those feature to the observed responses. In this work, we optimize the feature extraction in order to maximize the information shared between the image features and the neural response across voxels in a given region of interest (ROI) extracted from the BOLD signal measured by fMRI. We adapt contrastive learning (CL) to fine-tune a convolutional neural network, which was pretrained for image classification, such that a mapping of a given image's features are more similar to the corresponding fMRI response than to the responses to other images. We exploit the recently released Natural Scenes Dataset (Allen et al., 2022) as organized for the Algonauts Project (Gifford et al., 2023), which contains the high-resolution fMRI responses of eight subjects to tens of thousands of naturalistic images. We show that CL fine-tuning creates feature extraction models that enable higher encoding accuracy in early visual ROIs as compared to both the pretrained network and a baseline approach that uses a regression loss at the output of the network to tune it for fMRI response encoding. We investigate inter-subject transfer of the CL fine-tuned models, including subjects from another, lower-resolution dataset (Gong et al., 2023). We also pool subjects for fine-tuning to further improve the encoding performance. Finally, we examine the performance of the fine-tuned models on common image classification tasks, explore the landscape of ROI-specific models by applying dimensionality reduction on the Bhattacharya dissimilarity matrix created using the predictions on those tasks (Mao et al., 2024), and investigate lateralization of the processing for early visual ROIs using salience maps of the classifiers built on the CL-tuned models.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-08</td>
<td style='padding: 8px;'>Node-reconfiguring multilayer networks of human brain function</td>
<td style='padding: 6px;'>Tarmo Nurmi, Pietro De Luca, Mikko Kivelä, Onerva Korhonen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.05972v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The properties of functional brain networks are heavily influenced by how the network nodes are defined. A common approach uses Regions of Interest (ROIs), which are predetermined collections of fMRI measurement voxels, as network nodes. Their definition is always a compromise, as static ROIs cannot capture the dynamics and the temporal reconfigurations of the brain areas. Consequently, the ROIs do not align with the functionally homogeneous regions, which can explain the very low functional homogeneity values observed for the ROIs. This is in violation of the underlying homogeneity assumption in functional brain network analysis pipelines and it can cause serious problems such as spurious network structure. We introduce the node-reconfiguring multilayer network model, where nodes represent ROIs with boundaries optimized for high functional homogeneity in each time window. In this representation, network layers correspond to time windows, intralayer links depict functional connectivity between ROIs, and interlayer link weights quantify overlap between ROIs on different layers. The ROI optimization approach increases functional homogeneity notably, yielding an over 10-fold increase in the fraction of ROIs with high homogeneity compared to static ROIs from the Brainnetome atlas. The optimized ROIs reorganize non-trivially at short time scales of consecutive time windows and across several windows. The amount of reorganization across time windows is connected to intralayer hubness: ROIs that show intermediate levels of reorganization have stronger intralayer links than extremely stable or unstable ROIs. Our results demonstrate that reconfiguring parcellations yield more accurate network models of brain function. This supports the ongoing paradigm shift towards the chronnectome that sees the brain as a set of sources with continuously reconfiguring spatial and connectivity profiles.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-08</td>
<td style='padding: 8px;'>Vector-ICL: In-context Learning with Continuous Vector Representations</td>
<td style='padding: 6px;'>Yufan Zhuang, Chandan Singh, Liyuan Liu, Jingbo Shang, Jianfeng Gao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.05629v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large language models (LLMs) have shown remarkable in-context learning (ICL) capabilities on textual data. We explore whether these capabilities can be extended to continuous vectors from diverse domains, obtained from black-box pretrained encoders. By aligning input data with an LLM's embedding space through lightweight projectors, we observe that LLMs can effectively process and learn from these projected vectors, which we term Vector-ICL. In particular, we find that pretraining projectors with general language modeling objectives enables Vector-ICL, while task-specific finetuning further enhances performance. In our experiments across various tasks and modalities, including text reconstruction, numerical function regression, text classification, summarization, molecule captioning, time-series classification, graph classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL and domain-specific model or tuning. We further conduct analyses and case studies, indicating the potential of LLMs to process vector representations beyond traditional token-based paradigms.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-07</td>
<td style='padding: 8px;'>Multi-Stage Graph Learning for fMRI Analysis to Diagnose Neuro-Developmental Disorders</td>
<td style='padding: 6px;'>Wenjing Gao, Yuanyuan Yang, Jianrui Wei, Xuntao Yin, Xinhan Di</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.05342v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The insufficient supervision limit the performance of the deep supervised models for brain disease diagnosis. It is important to develop a learning framework that can capture more information in limited data and insufficient supervision. To address these issues at some extend, we propose a multi-stage graph learning framework which incorporates 1) pretrain stage : self-supervised graph learning on insufficient supervision of the fmri data 2) fine-tune stage : supervised graph learning for brain disorder diagnosis. Experiment results on three datasets, Autism Brain Imaging Data Exchange ABIDE I, ABIDE II and ADHD with AAL1,demonstrating the superiority and generalizability of the proposed framework compared to the state of art of models.(ranging from 0.7330 to 0.9321,0.7209 to 0.9021,0.6338 to 0.6699)</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-07</td>
<td style='padding: 8px;'>NeuroBOLT: Resting-state EEG-to-fMRI Synthesis with Multi-dimensional Feature Mapping</td>
<td style='padding: 6px;'>Yamin Li, Ange Lou, Ziyuan Xu, Shengchao Zhang, Shiyu Wang, Dario J. Englot, Soheil Kolouri, Daniel Moyer, Roza G. Bayrak, Catie Chang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.05341v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional magnetic resonance imaging (fMRI) is an indispensable tool in modern neuroscience, providing a non-invasive window into whole-brain dynamics at millimeter-scale spatial resolution. However, fMRI is constrained by issues such as high operation costs and immobility. With the rapid advancements in cross-modality synthesis and brain decoding, the use of deep neural networks has emerged as a promising solution for inferring whole-brain, high-resolution fMRI features directly from electroencephalography (EEG), a more widely accessible and portable neuroimaging modality. Nonetheless, the complex projection from neural activity to fMRI hemodynamic responses and the spatial ambiguity of EEG pose substantial challenges both in modeling and interpretability. Relatively few studies to date have developed approaches for EEG-fMRI translation, and although they have made significant strides, the inference of fMRI signals in a given study has been limited to a small set of brain areas and to a single condition (i.e., either resting-state or a specific task). The capability to predict fMRI signals in other brain areas, as well as to generalize across conditions, remain critical gaps in the field. To tackle these challenges, we introduce a novel and generalizable framework: NeuroBOLT, i.e., Neuro-to-BOLD Transformer, which leverages multi-dimensional representation learning from temporal, spatial, and spectral domains to translate raw EEG data to the corresponding fMRI activity signals across the brain. Our experiments demonstrate that NeuroBOLT effectively reconstructs resting-state fMRI signals from primary sensory, high-level cognitive areas, and deep subcortical brain regions, achieving state-of-the-art accuracy and significantly advancing the integration of these two modalities.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-06</td>
<td style='padding: 8px;'>BrainCodec: Neural fMRI codec for the decoding of cognitive brain states</td>
<td style='padding: 6px;'>Yuto Nishimura, Masataka Sawayama, Ayumu Yamashita, Hideki Nakayama, Kaoru Amano</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.04383v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recently, leveraging big data in deep learning has led to significant performance improvements, as confirmed in applications like mental state decoding using fMRI data. However, fMRI datasets remain relatively small in scale, and the inherent issue of low signal-to-noise ratios (SNR) in fMRI data further exacerbates these challenges. To address this, we apply compression techniques as a preprocessing step for fMRI data. We propose BrainCodec, a novel fMRI codec inspired by the neural audio codec. We evaluated BrainCodec's compression capability in mental state decoding, demonstrating further improvements over previous methods. Furthermore, we analyzed the latent representations obtained through BrainCodec, elucidating the similarities and differences between task and resting state fMRI, highlighting the interpretability of BrainCodec. Additionally, we demonstrated that fMRI reconstructions using BrainCodec can enhance the visibility of brain activity by achieving higher SNR, suggesting its potential as a novel denoising method. Our study shows that BrainCodec not only enhances performance over previous methods but also offers new analytical possibilities for neuroscience. Our codes, dataset, and model weights are available at https://github.com/amano-k-lab/BrainCodec.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-05</td>
<td style='padding: 8px;'>TV-based Deep 3D Self Super-Resolution for fMRI</td>
<td style='padding: 6px;'>Fernando Pérez-Bueno, Hongwei Bran Li, Shahin Nasr, Cesar Caballero-Gaudes, Juan Eugenio Iglesias</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.04097v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While functional Magnetic Resonance Imaging (fMRI) offers valuable insights into cognitive processes, its inherent spatial limitations pose challenges for detailed analysis of the fine-grained functional architecture of the brain. More specifically, MRI scanner and sequence specifications impose a trade-off between temporal resolution, spatial resolution, signal-to-noise ratio, and scan time. Deep Learning (DL) Super-Resolution (SR) methods have emerged as a promising solution to enhance fMRI resolution, generating high-resolution (HR) images from low-resolution (LR) images typically acquired with lower scanning times. However, most existing SR approaches depend on supervised DL techniques, which require training ground truth (GT) HR data, which is often difficult to acquire and simultaneously sets a bound for how far SR can go. In this paper, we introduce a novel self-supervised DL SR model that combines a DL network with an analytical approach and Total Variation (TV) regularization. Our method eliminates the need for external GT images, achieving competitive performance compared to supervised DL techniques and preserving the functional maps.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-04</td>
<td style='padding: 8px;'>ECHOPulse: ECG controlled echocardio-grams video generation</td>
<td style='padding: 6px;'>Yiwei Li, Sekeun Kim, Zihao Wu, Hanqi Jiang, Yi Pan, Pengfei Jin, Sifan Song, Yucheng Shi, Tianze Yang, Tianming Liu, Quanzheng Li, Xiang Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.03143v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Echocardiography (ECHO) is essential for cardiac assessments, but its video quality and interpretation heavily relies on manual expertise, leading to inconsistent results from clinical and portable devices. ECHO video generation offers a solution by improving automated monitoring through synthetic data and generating high-quality videos from routine health data. However, existing models often face high computational costs, slow inference, and rely on complex conditional prompts that require experts' annotations. To address these challenges, we propose ECHOPULSE, an ECG-conditioned ECHO video generation model. ECHOPULSE introduces two key advancements: (1) it accelerates ECHO video generation by leveraging VQ-VAE tokenization and masked visual token modeling for fast decoding, and (2) it conditions on readily accessible ECG signals, which are highly coherent with ECHO videos, bypassing complex conditional prompts. To the best of our knowledge, this is the first work to use time-series prompts like ECG signals for ECHO video generation. ECHOPULSE not only enables controllable synthetic ECHO data generation but also provides updated cardiac function information for disease monitoring and prediction beyond ECG alone. Evaluations on three public and private datasets demonstrate state-of-the-art performance in ECHO video generation across both qualitative and quantitative measures. Additionally, ECHOPULSE can be easily generalized to other modality generation tasks, such as cardiac MRI, fMRI, and 3D CT generation. Demo can seen from \url{https://github.com/levyisthebest/ECHOPulse_Prelease}.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-03</td>
<td style='padding: 8px;'>BSNMani: Bayesian Scalar-on-network Regression with Manifold Learning</td>
<td style='padding: 6px;'>Yijun Li, Ki Sueng Choi, Boadie W. Dunlop, Wade Edward Craighead, Helen S. Mayberg, Lana Garmire, Ying Guo, Jian Kang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.02965v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain connectivity analysis is crucial for understanding brain structure and neurological function, shedding light on the mechanisms of mental illness. To study the association between individual brain connectivity networks and the clinical characteristics, we develop BSNMani: a Bayesian scalar-on-network regression model with manifold learning. BSNMani comprises two components: the network manifold learning model for brain connectivity networks, which extracts shared connectivity structures and subject-specific network features, and the joint predictive model for clinical outcomes, which studies the association between clinical phenotypes and subject-specific network features while adjusting for potential confounding covariates. For posterior computation, we develop a novel two-stage hybrid algorithm combining Metropolis-Adjusted Langevin Algorithm (MALA) and Gibbs sampling. Our method is not only able to extract meaningful subnetwork features that reveal shared connectivity patterns, but can also reveal their association with clinical phenotypes, further enabling clinical outcome prediction. We demonstrate our method through simulations and through its application to real resting-state fMRI data from a study focusing on Major Depressive Disorder (MDD). Our approach sheds light on the intricate interplay between brain connectivity and clinical features, offering insights that can contribute to our understanding of psychiatric and neurological disorders, as well as mental health.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-02</td>
<td style='padding: 8px;'>HyperBrain: Anomaly Detection for Temporal Hypergraph Brain Networks</td>
<td style='padding: 6px;'>Sadaf Sadeghian, Xiaoxiao Li, Margo Seltzer</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.02087v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Identifying unusual brain activity is a crucial task in neuroscience research, as it aids in the early detection of brain disorders. It is common to represent brain networks as graphs, and researchers have developed various graph-based machine learning methods for analyzing them. However, the majority of existing graph learning tools for the brain face a combination of the following three key limitations. First, they focus only on pairwise correlations between regions of the brain, limiting their ability to capture synchronized activity among larger groups of regions. Second, they model the brain network as a static network, overlooking the temporal changes in the brain. Third, most are designed only for classifying brain networks as healthy or disordered, lacking the ability to identify abnormal brain activity patterns linked to biomarkers associated with disorders. To address these issues, we present HyperBrain, an unsupervised anomaly detection framework for temporal hypergraph brain networks. HyperBrain models fMRI time series data as temporal hypergraphs capturing dynamic higher-order interactions. It then uses a novel customized temporal walk (BrainWalk) and neural encodings to detect abnormal co-activations among brain regions. We evaluate the performance of HyperBrain in both synthetic and real-world settings for Autism Spectrum Disorder and Attention Deficit Hyperactivity Disorder(ADHD). HyperBrain outperforms all other baselines on detecting abnormal co-activations in brain networks. Furthermore, results obtained from HyperBrain are consistent with clinical research on these brain disorders. Our findings suggest that learning temporal and higher-order connections in the brain provides a promising approach to uncover intricate connectivity patterns in brain networks, offering improved diagnosis.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>Nested Deep Learning Model Towards A Foundation Model for Brain Signal Data</td>
<td style='padding: 6px;'>Fangyi Wei, Jiajie Mo, Kai Zhang, Haipeng Shen, Srikantan Nagarajan, Fei Jiang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.03191v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Epilepsy affects over 50 million people globally, with EEG/MEG-based spike detection playing a crucial role in diagnosis and treatment. Manual spike identification is time-consuming and requires specialized training, limiting the number of professionals available to analyze EEG/MEG data. To address this, various algorithmic approaches have been developed. However, current methods face challenges in handling varying channel configurations and in identifying the specific channels where spikes originate. This paper introduces a novel Nested Deep Learning (NDL) framework designed to overcome these limitations. NDL applies a weighted combination of signals across all channels, ensuring adaptability to different channel setups, and allows clinicians to identify key channels more accurately. Through theoretical analysis and empirical validation on real EEG/MEG datasets, NDL demonstrates superior accuracy in spike detection and channel localization compared to traditional methods. The results show that NDL improves prediction accuracy, supports cross-modality data integration, and can be fine-tuned for various neurophysiological applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-16</td>
<td style='padding: 8px;'>MEGS: Morphological Evaluation of Galactic Structure</td>
<td style='padding: 6px;'>Ufuk Çakır, Tobias Buck</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.10346v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding the morphology of galaxies is a critical aspect of astrophysics research, providing insight into the formation, evolution, and physical properties of these vast cosmic structures. Various observational and computational methods have been developed to quantify galaxy morphology, and with the advent of large galaxy simulations, the need for automated and effective classification methods has become increasingly important. This paper investigates the use of Principal Component Analysis (PCA) as an interpretable dimensionality reduction algorithm for galaxy morphology using the IllustrisTNG cosmological simulation dataset with the aim of developing a generative model for galaxies. We first generate a dataset of 2D images and 3D cubes of galaxies from the IllustrisTNG simulation, focusing on the mass, metallicity, and stellar age distribution of each galaxy. PCA is then applied to this data, transforming it into a lower-dimensional image space, where closeness of data points corresponds to morphological similarity. We find that PCA can effectively capture the key morphological features of galaxies, with a significant proportion of the variance in the data being explained by a small number of components. With our method we achieve a dimensionality reduction by a factor of $\sim200$ for 2D images and $\sim3650$ for 3D cubes at a reconstruction accuracy below five percent. Our results illustrate the potential of PCA in compressing large cosmological simulations into an interpretable generative model for galaxies that can easily be used in various downstream tasks such as galaxy classification and analysis.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-13</td>
<td style='padding: 8px;'>Characterizing the Molecular Gas in Infrared Bright Galaxies with CARMA</td>
<td style='padding: 6px;'>Katherine Alatalo, Andreea O. Petric, Lauranne Lanz, Kate Rowlands, Vivian U, Kirsten L. Larson, Lee Armus, Loreto Barcos-Muñoz, Aaron S. Evans, Jin Koda, Yuanze Luo, Anne M. Medling, Kristina E. Nyland, Justin A. Otter, Pallavi Patil, Fernando Peñaloza, Diane Salim, David B. Sanders, Elizaveta Sazonova, Maya Skarbinski, Yiqing Song, Ezequiel Treister, C. Meg Urry</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.09116v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We present the CO(1-0) maps of 28 infrared-bright galaxies from the Great Observatories All-Sky Luminous Infrared Galaxy Survey (GOALS) taken with the Combined Array for Research in Millimeter Astronomy (CARMA). We detect 100GHz continuum in 16 of 28 galaxies, which trace both active galactic nuclei (AGNs) and compact star-forming cores. The GOALS galaxies show a variety of molecular gas morphologies, though in the majority of cases, the average velocity fields show a gradient consistent with rotation. We fit the full continuum SEDs of each of the source using either MAGPHYS or SED3FIT (if there are signs of an AGN) to derive the total stellar mass, dust mass, and star formation rates of each object. We adopt a value determined from luminous and ultraluminous infrared galaxies (LIRGs and ULIRGs) of $\alpha_{\rm CO}=1.5^{+1.3}_{-0.8}~M_\odot$ (K km s$^{-1}$ pc$^2)^{-1}$, which leads to more physical values for $f_{\rm mol}$ and the gas-to-dust ratio. Mergers tend to have the highest gas-to-dust ratios. We assume the cospatiality of the molecular gas and star formation, and plot the sample on the Schmidt-Kennicutt relation, we find that they preferentially lie above the line set by normal star-forming galaxies. This hyper-efficiency is likely due to the increased turbulence in these systems, which decreases the freefall time compared to star-forming galaxies, leading to "enhanced" star formation efficiency. Line wings are present in a non-negligible subsample (11/28) of the CARMA GOALS sources and are likely due to outflows driven by AGNs or star formation, gas inflows, or additional decoupled gas components.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-06</td>
<td style='padding: 8px;'>Lepton-flavor changing decays and non-unitarity in the inverse seesaw mechanism</td>
<td style='padding: 6px;'>Adrián González-Quiterio, Héctor Novales-Sánchez</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.03952v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The pursuit of the genuine fundamental description, governing nature at some high-energy scale, must invariably consider the yet-unknown mechanism behind the generation of neutrino mass. Lepton-flavor violating decays $l_\alpha\to\gamma\,l_\beta$, allowed in the presence of neutrino mass and mixing, provide a mean to look for physics beyond the Standard Model. In the present work we consider the inverse seesaw mechanism and then revisit the calculation of its contributions to the branching ratios of the aforementioned decay processes, among which we find $\mu\to\gamma\,e$ to be more promising, in the light of current bounds by the MEG Collaboration. Deviations from unitarity in the mixing of light neutrinos are related to the branching ratios ${\rm Br}\big( l_\alpha\to\gamma\,l_\beta \big)$ in a simple manner, which we address, then finding that, while experimental data are consistent with current bounds on non-unitarity effects, the upcoming MEG II update shall be able to improve restrictions on such effects by a factor $\sim\frac{1}{3}$.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-10</td>
<td style='padding: 8px;'>Bayesian Inference General Procedures for A Single-subject Test Study</td>
<td style='padding: 6px;'>Jie Li, Gary Green, Sarah J. A. Carr, Peng Liu, Jian Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.15419v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Abnormality detection in the identification of a single-subject which deviates from the majority of the dataset that comes from a control group is a critical problem. A common approach is to assume that the control group can be characterised in terms of standard Normal statistics and the detection of single abnormal subject is in that context. But in many situations the control group can not be described in terms of Normal statistics and the use of standard statistics is inappropriate. This paper presents a Bayesian Inference General Procedures for A Single-Subject Test (BIGPAST), designed to mitigate the effects of skewness under the assumption that the dataset of control group comes from the skewed Student's \( t \) distribution. BIGPAST operates under the null hypothesis that the single-subject follows the same distribution as the control group. We assess BIGPAST's performance against other methods through a series of simulation studies. The results demonstrate that BIGPAST is robust against deviations from normality and outperforms the existing approaches in terms of accuracy. This is because BIGPAST can effectively reduce model misspecification errors under the skewed Student's \( t \) assumption. We apply BIGPAST to a MEG dataset consisting of an individual with mild traumatic brain injury and an age and gender-matched control group, demonstrating its effectiveness in detecting abnormalities in the single-subject.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-23</td>
<td style='padding: 8px;'>Enabling Distributed Generative Artificial Intelligence in 6G: Mobile Edge Generation</td>
<td style='padding: 6px;'>Ruikang Zhong, Xidong Mu, Mona Jaber, Yuanwei Liu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.05870v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Mobile edge generation (MEG) is an emerging technology that allows the network to meet the challenging traffic load expectations posed by the rise of generative artificial intelligence~(GAI). A novel MEG model is proposed for deploying GAI models on edge servers (ES) and user equipment~(UE) to jointly complete text-to-image generation tasks. In the generation task, the ES and UE will cooperatively generate the image according to the text prompt given by the user. To enable the MEG, a pre-trained latent diffusion model (LDM) is invoked to generate the latent feature, and an edge-inferencing MEG protocol is employed for data transmission exchange between the ES and the UE. A compression coding technique is proposed for compressing the latent features to produce seeds. Based on the above seed-enabled MEG model, an image quality optimization problem with transmit power constraint is formulated. The transmitting power of the seed is dynamically optimized by a deep reinforcement learning agent over the fading channel. The proposed MEG enabled text-to-image generation system is evaluated in terms of image quality and transmission overhead. The numerical results indicate that, compared to the conventional centralized generation-and-downloading scheme, the symbol number of the transmission of MEG is materially reduced. In addition, the proposed compression coding approach can improve the quality of generated images under low signal-to-noise ratio (SNR) conditions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-09</td>
<td style='padding: 8px;'>Towards improving Alzheimer's intervention: a machine learning approach for biomarker detection through combining MEG and MRI pipelines</td>
<td style='padding: 6px;'>Alwani Liyana Ahmad, Jose Sanchez-Bornot, Roberto C. Sotero, Damien Coyle, Zamzuri Idris, Ibrahima Faye</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.04815v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>MEG are non invasive neuroimaging techniques with excellent temporal and spatial resolution, crucial for studying brain function in dementia and Alzheimer Disease. They identify changes in brain activity at various Alzheimer stages, including preclinical and prodromal phases. MEG may detect pathological changes before clinical symptoms, offering potential biomarkers for intervention. This study evaluates classification techniques using MEG features to distinguish between healthy controls and mild cognitive impairment participants from the BioFIND study. We compare MEG based biomarkers with MRI based anatomical features, both independently and combined. We used 3 Tesla MRI and MEG data from 324 BioFIND participants;158 MCI and 166 HC. Analyses were performed using MATLAB with SPM12 and OSL toolboxes. Machine learning analyses, including 100 Monte Carlo replications of 10 fold cross validation, were conducted on sensor and source spaces. Combining MRI with MEG features achieved the best performance; 0.76 accuracy and AUC of 0.82 for GLMNET using LCMV source based MEG. MEG only analyses using LCMV and eLORETA also performed well, suggesting that combining uncorrected MEG with z-score-corrected MRI features is optimal.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-05</td>
<td style='padding: 8px;'>Classification of Raw MEG/EEG Data with Detach-Rocket Ensemble: An Improved ROCKET Algorithm for Multivariate Time Series Analysis</td>
<td style='padding: 6px;'>Adrià Solana, Erik Fransén, Gonzalo Uribarri</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.02760v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Multivariate Time Series Classification (MTSC) is a ubiquitous problem in science and engineering, particularly in neuroscience, where most data acquisition modalities involve the simultaneous time-dependent recording of brain activity in multiple brain regions. In recent years, Random Convolutional Kernel models such as ROCKET and MiniRocket have emerged as highly effective time series classification algorithms, capable of achieving state-of-the-art accuracy results with low computational load. Despite their success, these types of models face two major challenges when employed in neuroscience: 1) they struggle to deal with high-dimensional data such as EEG and MEG, and 2) they are difficult to interpret. In this work, we present a novel ROCKET-based algorithm, named Detach-Rocket Ensemble, that is specifically designed to address these two problems in MTSC. Our algorithm leverages pruning to provide an integrated estimation of channel importance, and ensembles to achieve better accuracy and provide a label probability. Using a synthetic multivariate time series classification dataset in which we control the amount of information carried by each of the channels, we first show that our algorithm is able to correctly recover the channel importance for classification. Then, using two real-world datasets, a MEG dataset and an EEG dataset, we show that Detach-Rocket Ensemble is able to provide both interpretable channel relevance and competitive classification accuracy, even when applied directly to the raw brain data, without the need for feature engineering.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-02</td>
<td style='padding: 8px;'>Hotspots and Trends in Magnetoencephalography Research (2013-2022): A Bibliometric Analysis</td>
<td style='padding: 6px;'>Shen Liu, Jingwen Zhao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.08877v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study aimed to utilize bibliometric methods to analyze trends in international Magnetoencephalography (MEG) research from 2013 to 2022. Due to the limited volume of domestic literature on MEG, this analysis focuses solely on the global research landscape, providing insights from the past decade as a representative sample. This study utilized bibliometric methods to explore and analyze the progress, hotspots and developmental trends in international MEG research spanning from 1995 to 2022. The results indicated a dynamic and steady growth trend in the overall number of publications in MEG. Ryusuke Kakigi emerged as the most prolific author, while Neuroimage led as the most prolific journal. Current hotspots in MEG research encompass resting state, networks, functional connectivity, phase dynamics, oscillation, and more. Future trends in MEG research are poised to advance across three key aspects: disease treatment and practical applications, experimental foundations and technical advancements, and fundamental and advanced human cognition. In the future, there should be a focus on enhancing cross-integration and utilization of MEG with other instruments to diversify research methodologies in this field</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-02</td>
<td style='padding: 8px;'>Gemma 2: Improving Open Language Models at a Practical Size</td>
<td style='padding: 6px;'>Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozińska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucińska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju-yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Cogan, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, Alek Andreev</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.00118v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3 times bigger. We release all our models to the community.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-09</td>
<td style='padding: 8px;'>Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models</td>
<td style='padding: 6px;'>Emily Cheng, Richard J. Antonello</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.05771v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first "composition" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-07-22</td>
<td style='padding: 8px;'>Predictive Coding Networks and Inference Learning: Tutorial and Survey</td>
<td style='padding: 6px;'>Björn van Zwol, Ro Jefferson, Egon L. van den Broek</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2407.04117v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent years have witnessed a growing call for renewed emphasis on neuroscience-inspired approaches in artificial intelligence research, under the banner of NeuroAI. A prime example of this is predictive coding networks (PCNs), based on the neuroscientific framework of predictive coding. This framework views the brain as a hierarchical Bayesian inference model that minimizes prediction errors through feedback connections. Unlike traditional neural networks trained with backpropagation (BP), PCNs utilize inference learning (IL), a more biologically plausible algorithm that explains patterns of neural activity that BP cannot. Historically, IL has been more computationally intensive, but recent advancements have demonstrated that it can achieve higher efficiency than BP with sufficient parallelization. Furthermore, PCNs can be mathematically considered a superset of traditional feedforward neural networks (FNNs), significantly extending the range of trainable architectures. As inherently probabilistic (graphical) latent variable models, PCNs provide a versatile framework for both supervised learning and unsupervised (generative) modeling that goes beyond traditional artificial neural networks. This work provides a comprehensive review and detailed formal specification of PCNs, particularly situating them within the context of modern ML methods. Additionally, we introduce a Python library (PRECO) for practical implementation. This positions PC as a promising framework for future ML innovations.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-10-29</td>
<td style='padding: 8px;'>Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis</td>
<td style='padding: 6px;'>Mitchell Ostrow, Adam Eisen, Leo Kozachkov, Ila Fiete</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2306.10168v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-05-25</td>
<td style='padding: 8px;'>Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture</td>
<td style='padding: 6px;'>Galen Pogoncheff, Jacob Granley, Michael Beyeler</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2305.11275v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-03-11</td>
<td style='padding: 8px;'>Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks</td>
<td style='padding: 6px;'>Feng-Lei Fan, Yingxin Li, Hanchuan Peng, Tieyong Zeng, Fei Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2301.09245v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Throughout history, the development of artificial intelligence, particularly artificial neural networks, has been open to and constantly inspired by the increasingly deepened understanding of the brain, such as the inspiration of neocognitron, which is the pioneering work of convolutional neural networks. Per the motives of the emerging field: NeuroAI, a great amount of neuroscience knowledge can help catalyze the next generation of AI by endowing a network with more powerful capabilities. As we know, the human brain has numerous morphologically and functionally different neurons, while artificial neural networks are almost exclusively built on a single neuron type. In the human brain, neuronal diversity is an enabling factor for all kinds of biological intelligent behaviors. Since an artificial network is a miniature of the human brain, introducing neuronal diversity should be valuable in terms of addressing those essential problems of artificial networks such as efficiency, interpretability, and memory. In this Primer, we first discuss the preliminaries of biological neuronal diversity and the characteristics of information transmission and processing in a biological neuron. Then, we review studies of designing new neurons for artificial networks. Next, we discuss what gains can neuronal diversity bring into artificial networks and exemplary applications in several important fields. Lastly, we discuss the challenges and future directions of neuronal diversity to explore the potential of NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2022-12-08</td>
<td style='padding: 8px;'>A Rubric for Human-like Agents and NeuroAI</td>
<td style='padding: 6px;'>Ida Momennejad</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2212.04401v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Researchers across cognitive, neuro-, and computer sciences increasingly reference human-like artificial intelligence and neuroAI. However, the scope and use of the terms are often inconsistent. Contributed research ranges widely from mimicking behaviour, to testing machine learning methods as neurally plausible hypotheses at the cellular or functional levels, or solving engineering problems. However, it cannot be assumed nor expected that progress on one of these three goals will automatically translate to progress in others. Here a simple rubric is proposed to clarify the scope of individual contributions, grounded in their commitments to human-like behaviour, neural plausibility, or benchmark/engineering goals. This is clarified using examples of weak and strong neuroAI and human-like agents, and discussing the generative, corroborate, and corrective ways in which the three dimensions interact with one another. The author maintains that future progress in artificial intelligence will need strong interactions across the disciplines, with iterative feedback loops and meticulous validity tests, leading to both known and yet-unknown advances that may span decades to come.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-02-22</td>
<td style='padding: 8px;'>Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolution</td>
<td style='padding: 6px;'>Anthony Zador, Sean Escola, Blake Richards, Bence Ölveczky, Yoshua Bengio, Kwabena Boahen, Matthew Botvinick, Dmitri Chklovskii, Anne Churchland, Claudia Clopath, James DiCarlo, Surya Ganguli, Jeff Hawkins, Konrad Koerding, Alexei Koulakov, Yann LeCun, Timothy Lillicrap, Adam Marblestone, Bruno Olshausen, Alexandre Pouget, Cristina Savin, Terrence Sejnowski, Eero Simoncelli, Sara Solla, David Sussillo, Andreas S. Tolias, Doris Tsao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2210.08340v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience has long been an essential driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI. A core component of this is the embodied Turing test, which challenges AI animal models to interact with the sensorimotor world at skill levels akin to their living counterparts. The embodied Turing test shifts the focus from those capabilities like game playing and language that are especially well-developed or uniquely human to those capabilities, inherited from over 500 million years of evolution, that are shared with all animals. Building models that can pass the embodied Turing test will provide a roadmap for the next generation of AI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2022-04-11</td>
<td style='padding: 8px;'>Social Neuro AI: Social Interaction as the "dark matter" of AI</td>
<td style='padding: 6px;'>Samuele Bolotta, Guillaume Dumas</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2112.15459v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This article introduces a three-axis framework indicating how AI can be informed by biological examples of social learning mechanisms. We argue that the complex human cognitive architecture owes a large portion of its expressive power to its ability to engage in social and cultural learning. However, the field of AI has mostly embraced a solipsistic perspective on intelligence. We thus argue that social interactions not only are largely unexplored in this field but also are an essential element of advanced cognitive ability, and therefore constitute metaphorically the dark matter of AI. In the first section, we discuss how social learning plays a key role in the development of intelligence. We do so by discussing social and cultural learning theories and empirical findings from social neuroscience. Then, we discuss three lines of research that fall under the umbrella of Social NeuroAI and can contribute to developing socially intelligent embodied agents in complex environments. First, neuroscientific theories of cognitive architecture, such as the global workspace theory and the attention schema theory, can enhance biological plausibility and help us understand how we could bridge individual and social theories of intelligence. Second, intelligence occurs in time as opposed to over time, and this is naturally incorporated by dynamical systems. Third, embodiment has been demonstrated to provide more sophisticated array of communicative signals. To conclude, we discuss the example of active inference, which offers powerful insights for developing agents that possess biological realism, can self-organize in time, and are socially embodied.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2021-10-23</td>
<td style='padding: 8px;'>Predictive Coding, Variational Autoencoders, and Biological Connections</td>
<td style='padding: 6px;'>Joseph Marino</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2011.07464v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper reviews predictive coding, from theoretical neuroscience, and variational autoencoders, from machine learning, identifying the common origin and mathematical framework underlying both areas. As each area is prominent within its respective field, more firmly connecting these areas could prove useful in the dialogue between neuroscience and machine learning. After reviewing each area, we discuss two possible correspondences implied by this perspective: cortical pyramidal dendrites as analogous to (non-linear) deep networks and lateral inhibition as analogous to normalizing flows. These connections may provide new directions for further investigations in each field.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2019-09-13</td>
<td style='padding: 8px;'>Additive function approximation in the brain</td>
<td style='padding: 6px;'>Kameron Decker Harris</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/1909.02603v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Many biological learning systems such as the mushroom body, hippocampus, and cerebellum are built from sparsely connected networks of neurons. For a new understanding of such networks, we study the function spaces induced by sparse random features and characterize what functions may and may not be learned. A network with $d$ inputs per neuron is found to be equivalent to an additive model of order $d$, whereas with a degree distribution the network combines additive terms of different orders. We identify three specific advantages of sparsity: additive function approximation is a powerful inductive bias that limits the curse of dimensionality, sparse networks are stable to outlier noise in the inputs, and sparse random features are scalable. Thus, even simple brain architectures can be powerful function approximators. Finally, we hope that this work helps popularize kernel theories of networks among computational neuroscientists.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>Large-scale self-assembled nanophotonic scintillators for X-ray imaging</td>
<td style='padding: 6px;'>Louis Martin-Monier, Simo Pajovic, Muluneh G. Abebe, Joshua Chen, Sachin Vaidya, Seokhwan Min, Seou Choi, Steven E. Kooi, Bjorn Maes, Juejun Hu, Marin Soljacic, Charles Roques-Carmes</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.07141v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Scintillators are essential for converting X-ray energy into visible light in imaging technologies. Their widespread application in imaging technologies has been enabled by scalable, high-quality, and affordable manufacturing methods. Nanophotonic scintillators, which feature nanostructures at the scale of their emission wavelength, provide a promising approach to enhance emission properties like light yield, decay time, and directionality. However, scalable fabrication of such nanostructured scintillators has been a significant challenge, impeding their widespread adoption. Here, we present a scalable fabrication method for large-area nanophotonic scintillators based on the self-assembly of chalcogenide glass photonic crystals. This technique enables the production of nanophotonic scintillators over wafer-scale areas, achieving a six-fold enhancement in light yield compared to unpatterned scintillators. We demonstrate this approach using a conventional X-ray scintillator material, cerium-doped yttrium aluminum garnet (YAG:Ce). By analyzing the influence of surface nanofabrication disorder, we establish its effect on imaging performance and provide a route towards large-scale scintillation enhancements without decrease in spatial resolution. Finally, we demonstrate the practical applicability of our nanophotonic scintillators through X-ray imaging of biological and inorganic specimens. Our results indicate that this scalable fabrication technique could enable the industrial implementation of a new generation of nanophotonic-enhanced scintillators, with significant implications for advancements in medical imaging, security screening, and nondestructive testing.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation Models Without Human Feedback</td>
<td style='padding: 6px;'>Dennis Hein, Zhihong Chen, Sophie Ostmeier, Justin Xu, Maya Varma, Eduardo Pontes Reis, Arne Edward Michalson, Christian Bluethgen, Hyun Joo Shin, Curtis Langlotz, Akshay S Chaudhari</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.07025v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Radiologists play a crucial role by translating medical images into medical reports. However, the field faces staffing shortages and increasing workloads. While automated approaches using vision-language models (VLMs) show promise as assistants, they require exceptionally high accuracy. Most current VLMs in radiology rely solely on supervised fine-tuning (SFT). Meanwhile, in the general domain, additional preference fine-tuning has become standard practice. The challenge in radiology lies in the prohibitive cost of obtaining radiologist feedback. We propose a scalable automated preference alignment technique for VLMs in radiology, focusing on chest X-ray (CXR) report generation. Our method leverages publicly available datasets with an LLM-as-a-Judge mechanism, eliminating the need for additional expert radiologist feedback. We evaluate and benchmark five direct alignment algorithms (DAAs). Our results show up to a 57.4% improvement in average GREEN scores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase in an average across six metrics (domain specific and general), compared to the SFT baseline. We study reward overoptimization via length exploitation, with reports lengthening by up to 3.2x. To assess a potential alignment tax, we benchmark on six additional diverse tasks, finding no significant degradations. A reader study involving four board-certified radiologists indicates win rates of up to 0.62 over the SFT baseline, while significantly penalizing verbosity. Our analysis provides actionable insights for the development of VLMs in high-stakes fields like radiology.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>A Diffusion-based Xray2MRI Model: Generating Pseudo-MRI Volumes From one Single X-ray</td>
<td style='padding: 6px;'>Zhe Wang, Rachid Jennane, Aladine Chetouani, Mohamed Jarraya</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06997v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Knee osteoarthritis (KOA) is a prevalent musculoskeletal disorder, and X-rays are commonly used for its diagnosis due to their cost-effectiveness. Magnetic Resonance Imaging (MRI), on the other hand, offers detailed soft tissue visualization and has become a valuable supplementary diagnostic tool for KOA. Unfortunately, the high cost and limited accessibility of MRI hinder its widespread use, leaving many patients with KOA reliant solely on X-ray imaging. In this study, we introduce a novel diffusion-based Xray2MRI model capable of generating pseudo-MRI volumes from one single X-ray image. In addition to using X-rays as conditional input, our model integrates target depth, KOA probability distribution, and image intensity distribution modules to guide the synthesis process, ensuring that the generated corresponding slices accurately correspond to the anatomical structures. Experimental results demonstrate that by integrating information from X-rays with additional input data, our proposed approach is capable of generating pseudo-MRI sequences that approximate real MRI scans. Moreover, by increasing the inference times, the model achieves effective interpolation, further improving the continuity and smoothness of the generated MRI sequences, representing one promising initial attempt for cost-effective medical imaging solutions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>Selecting the Best Sequential Transfer Path for Medical Image Segmentation with Limited Labeled Data</td>
<td style='padding: 6px;'>Jingyun Yang, Jingge Wang, Guoqing Zhang, Yang Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06892v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The medical image processing field often encounters the critical issue of scarce annotated data. Transfer learning has emerged as a solution, yet how to select an adequate source task and effectively transfer the knowledge to the target task remains challenging. To address this, we propose a novel sequential transfer scheme with a task affinity metric tailored for medical images. Considering the characteristics of medical image segmentation tasks, we analyze the image and label similarity between tasks and compute the task affinity scores, which assess the relatedness among tasks. Based on this, we select appropriate source tasks and develop an effective sequential transfer strategy by incorporating intermediate source tasks to gradually narrow the domain discrepancy and minimize the transfer cost. Thereby we identify the best sequential transfer path for the given target task. Extensive experiments on three MRI medical datasets, FeTS 2022, iSeg-2019, and WMH, demonstrate the efficacy of our method in finding the best source sequence. Compared with directly transferring from a single source task, the sequential transfer results underline a significant improvement in target task performance, achieving an average of 2.58% gain in terms of segmentation Dice score, notably, 6.00% for FeTS 2022. Code is available at the git repository.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>Decentralized Clinical Trials in the Era of Real-World Evidence: A Statistical Perspective</td>
<td style='padding: 6px;'>Jie Chen, Junrui Di, Nadia Daizadeh, Ying Lu, Hongwei Wang, Yuan-Li Shen, Jennifer Kirk, Frank W. Rockhold, Herbert Pang, Jing Zhao, Weili He, Andrew Potter, Hana Lee</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06591v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>There has been a growing trend that activities relating to clinical trials take place at locations other than traditional trial sites (hence decentralized clinical trials or DCTs), some of which are at settings of real-world clinical practice. Although there are numerous benefits of DCTs, this also brings some implications on a number of issues relating to the design, conduct, and analysis of DCTs. The Real-World Evidence Scientific Working Group of the American Statistical Association Biopharmaceutical Section has been reviewing the field of DCTs and provides in this paper considerations for decentralized trials from a statistical perspective. This paper first discusses selected critical decentralized elements that may have statistical implications on the trial and then summarizes regulatory guidance, framework, and initiatives on DCTs. More discussions are presented by focusing on the design (including construction of estimand), implementation, statistical analysis plan (including missing data handling), and reporting of safety events. Some additional considerations (e.g., ethical considerations, technology infrastructure, study oversight, data security and privacy, and regulatory compliance) are also briefly discussed. This paper is intended to provide statistical considerations for decentralized trials of medical products to support regulatory decision-making.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>Use of Real-World Data and Real-World Evidence in Rare Disease Drug Development: A Statistical Perspective</td>
<td style='padding: 6px;'>Jie Chen, Susan Gruber, Hana Lee, Haitao Chu, Shiowjen Lee, Haijun Tian, Yan Wang, Weili He, Thomas Jemielita, Yang Song, Roy Tamura, Lu Tian, Yihua Zhao, Yong Chen, Mark van der Laan, Lei Nie</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06586v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Real-world data (RWD) and real-world evidence (RWE) have been increasingly used in medical product development and regulatory decision-making, especially for rare diseases. After outlining the challenges and possible strategies to address the challenges in rare disease drug development (see the accompanying paper), the Real-World Evidence (RWE) Scientific Working Group of the American Statistical Association Biopharmaceutical Section reviews the roles of RWD and RWE in clinical trials for drugs treating rare diseases. This paper summarizes relevant guidance documents and frameworks by selected regulatory agencies and the current practice on the use of RWD and RWE in natural history studies and the design, conduct, and analysis of rare disease clinical trials. A targeted learning roadmap for rare disease trials is described, followed by case studies on the use of RWD and RWE to support a natural history study and marketing applications in various settings.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>Challenges and Possible Strategies to Address Them in Rare Disease Drug Development: A Statistical Perspective</td>
<td style='padding: 6px;'>Jie Chen, Lei Nie, Shiowjen Lee, Haitao Chu, Haijun Tian, Yan Wang, Weili He, Thomas Jemielita, Susan Gruber, Yang Song, Roy Tamura, Lu Tian, Yihua Zhao, Yong Chen, Mark van der Laan, Hana Lee</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06585v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Developing drugs for rare diseases presents unique challenges from a statistical perspective. These challenges may include slowly progressive diseases with unmet medical needs, poorly understood natural history, small population size, diversified phenotypes and geneotypes within a disorder, and lack of appropriate surrogate endpoints to measure clinical benefits. The Real-World Evidence (RWE) Scientific Working Group of the American Statistical Association Biopharmaceutical Section has assembled a research team to assess the landscape including challenges and possible strategies to address these challenges and the role of real-world data (RWD) and RWE in rare disease drug development. This paper first reviews the current regulations by regulatory agencies worldwide and then discusses in more details the challenges from a statistical perspective in the design, conduct, and analysis of rare disease clinical trials. After outlining an overall development pathway for rare disease drugs, corresponding strategies to address the aforementioned challenges are presented. Other considerations are also discussed for generating relevant evidence for regulatory decision-making on drugs for rare diseases. The accompanying paper discusses how RWD and RWE can be used to improve the efficiency of rare disease drug development.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>Detecting Bias and Enhancing Diagnostic Accuracy in Large Language Models for Healthcare</td>
<td style='padding: 6px;'>Pardis Sadat Zahraei, Zahra Shakeri</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06566v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Biased AI-generated medical advice and misdiagnoses can jeopardize patient safety, making the integrity of AI in healthcare more critical than ever. As Large Language Models (LLMs) take on a growing role in medical decision-making, addressing their biases and enhancing their accuracy is key to delivering safe, reliable care. This study addresses these challenges head-on by introducing new resources designed to promote ethical and precise AI in healthcare. We present two datasets: BiasMD, featuring 6,007 question-answer pairs crafted to evaluate and mitigate biases in health-related LLM outputs, and DiseaseMatcher, with 32,000 clinical question-answer pairs spanning 700 diseases, aimed at assessing symptom-based diagnostic accuracy. Using these datasets, we developed the EthiClinician, a fine-tuned model built on the ChatDoctor framework, which outperforms GPT-4 in both ethical reasoning and clinical judgment. By exposing and correcting hidden biases in existing models for healthcare, our work sets a new benchmark for safer, more reliable patient outcomes.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>Ventilator pressure prediction using recurrent neural network</td>
<td style='padding: 6px;'>Su Diao, Changsong Wei, Junyu Wang, Yizhou Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06552v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper presents a recurrent neural network approach to simulating mechanical ventilator pressure. The traditional mechanical ventilator has a control pressure that is monitored by a medical practitioner and can behave incorrectly if the proper pressure is not applied. This paper takes advantage of recent research and develops a simulator based on a deep sequence model to predict airway pressure in the respiratory circuit during the inspiratory phase of a breath given a time series of control parameters and lung attributes. This method demonstrates the effectiveness of neural network-based controllers in tracking pressure wave forms significantly better than the current industry standard and provides insights into the development of effective and robust pressure-controlled mechanical ventilators. The paper will measure as the mean absolute error between the predicted and actual pressures during the inspiratory phase of each breath.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>MedImageInsight: An Open-Source Embedding Model for General Domain Medical Imaging</td>
<td style='padding: 6px;'>Noel C. F. Codella, Ying Jin, Shrey Jain, Yu Gu, Ho Hin Lee, Asma Ben Abacha, Alberto Santamaria-Pang, Will Guyman, Naiteek Sangani, Sheng Zhang, Hoifung Poon, Stephanie Hyland, Shruthi Bannur, Javier Alvarez-Valle, Xue Li, John Garrett, Alan McMillan, Gaurav Rajguru, Madhu Maddi, Nilesh Vijayrania, Rehaan Bhimai, Nick Mecklenburg, Rupal Jain, Daniel Holstein, Naveen Gaur, Vijay Aski, Jenq-Neng Hwang, Thomas Lin, Ivan Tarapov, Matthew Lungren, Mu Wei</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06542v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In this work, we present MedImageInsight, an open-source medical imaging embedding model. MedImageInsight is trained on medical images with associated text and labels across a diverse collection of domains, including X-Ray, CT, MRI, dermoscopy, OCT, fundus photography, ultrasound, histopathology, and mammography. Rigorous evaluations demonstrate MedImageInsight's ability to achieve state-of-the-art (SOTA) or human expert level performance across classification, image-image search, and fine-tuning tasks. Specifically, on public datasets, MedImageInsight achieves SOTA in CT 3D medical image retrieval, as well as SOTA in disease classification and search for chest X-ray, dermatology, and OCT imaging. Furthermore, MedImageInsight achieves human expert performance in bone age estimation (on both public and partner data), as well as AUC above 0.9 in most other domains. When paired with a text decoder, MedImageInsight achieves near SOTA level single image report findings generation with less than 10\% the parameters of other models. Compared to fine-tuning GPT-4o with only MIMIC-CXR data for the same task, MedImageInsight outperforms in clinical metrics, but underperforms on lexical metrics where GPT-4o sets a new SOTA. Importantly for regulatory purposes, MedImageInsight can generate ROC curves, adjust sensitivity and specificity based on clinical need, and provide evidence-based decision support through image-image search (which can also enable retrieval augmented generation). In an independent clinical evaluation of image-image search in chest X-ray, MedImageInsight outperformed every other publicly available foundation model evaluated by large margins (over 6 points AUC), and significantly outperformed other models in terms of AI fairness (across age and gender). We hope releasing MedImageInsight will help enhance collective progress in medical imaging AI research and development.</td>
</tr>
</tbody>
</table>

