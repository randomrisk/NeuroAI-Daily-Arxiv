<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2024-12-19</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>A Knowledge-enhanced Pathology Vision-language Foundation Model for Cancer Diagnosis</td>
<td style='padding: 6px;'>Xiao Zhou, Luoyi Sun, Dexuan He, Wenbin Guan, Ruifen Wang, Lifeng Wang, Xin Sun, Kun Sun, Ya Zhang, Yanfeng Wang, Weidi Xie</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.13126v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Deep learning has enabled the development of highly robust foundation models for various pathological tasks across diverse diseases and patient cohorts. Among these models, vision-language pre-training, which leverages large-scale paired data to align pathology image and text embedding spaces, and provides a novel zero-shot paradigm for downstream tasks. However, existing models have been primarily data-driven and lack the incorporation of domain-specific knowledge, which limits their performance in cancer diagnosis, especially for rare tumor subtypes. To address this limitation, we establish a Knowledge-enhanced Pathology (KEEP) foundation model that harnesses disease knowledge to facilitate vision-language pre-training. Specifically, we first construct a disease knowledge graph (KG) that covers 11,454 human diseases with 139,143 disease attributes, including synonyms, definitions, and hypernym relations. We then systematically reorganize the millions of publicly available noisy pathology image-text pairs, into 143K well-structured semantic groups linked through the hierarchical relations of the disease KG. To derive more nuanced image and text representations, we propose a novel knowledge-enhanced vision-language pre-training approach that integrates disease knowledge into the alignment within hierarchical semantic groups instead of unstructured image-text pairs. Validated on 18 diverse benchmarks with more than 14,000 whole slide images (WSIs), KEEP achieves state-of-the-art performance in zero-shot cancer diagnostic tasks. Notably, for cancer detection, KEEP demonstrates an average sensitivity of 89.8% at a specificity of 95.0% across 7 cancer types. For cancer subtyping, KEEP achieves a median balanced accuracy of 0.456 in subtyping 30 rare brain cancers, indicating strong generalizability for diagnosing rare tumors.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>Multi-View Incremental Learning with Structured Hebbian Plasticity for Enhanced Fusion Efficiency</td>
<td style='padding: 6px;'>Yuhong Chen, Ailin Song, Huifeng Yin, Shuai Zhong, Fuhai Chen, Qi Xu, Shiping Wang, Mingkun Xu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12801v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The rapid evolution of multimedia technology has revolutionized human perception, paving the way for multi-view learning. However, traditional multi-view learning approaches are tailored for scenarios with fixed data views, falling short of emulating the intricate cognitive procedures of the human brain processing signals sequentially. Our cerebral architecture seamlessly integrates sequential data through intricate feed-forward and feedback mechanisms. In stark contrast, traditional methods struggle to generalize effectively when confronted with data spanning diverse domains, highlighting the need for innovative strategies that can mimic the brain's adaptability and dynamic integration capabilities. In this paper, we propose a bio-neurologically inspired multi-view incremental framework named MVIL aimed at emulating the brain's fine-grained fusion of sequentially arriving views. MVIL lies two fundamental modules: structured Hebbian plasticity and synaptic partition learning. The structured Hebbian plasticity reshapes the structure of weights to express the high correlation between view representations, facilitating a fine-grained fusion of view representations. Moreover, synaptic partition learning is efficient in alleviating drastic changes in weights and also retaining old knowledge by inhibiting partial synapses. These modules bionically play a central role in reinforcing crucial associations between newly acquired information and existing knowledge repositories, thereby enhancing the network's capacity for generalization. Experimental results on six benchmark datasets show MVIL's effectiveness over state-of-the-art methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>Activating Distributed Visual Region within LLMs for Efficient and Effective Vision-Language Training and Inference</td>
<td style='padding: 6px;'>Siyuan Wang, Dianyi Wang, Chengxing Zhou, Zejun Li, Zhihao Fan, Xuanjing Huang, Zhongyu Wei</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12785v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large Vision-Language Models (LVLMs) typically learn visual capacity through visual instruction tuning, involving updates to both a projector and their LLM backbones. Drawing inspiration from the concept of visual region in the human brain, we investigate the existence of an analogous \textit{visual region} within LLMs that functions as a cognitive core, and explore the possibility of efficient training of LVLMs via selective layers tuning. We use Bunny-Llama-3-8B-V for detailed experiments and LLaVA-1.5-7B and LLaVA-1.5-13B for validation across a range of visual and textual tasks. Our findings reveal that selectively updating 25\% of LLMs layers, when sparsely and uniformly distributed, can preserve nearly 99\% of visual performance while maintaining or enhancing textual task results, and also effectively reducing training time. Based on this targeted training approach, we further propose a novel visual region-based pruning paradigm, removing non-critical layers outside the visual region, which can achieve minimal performance loss. This study offers an effective and efficient strategy for LVLM training and inference by activating a layer-wise visual region within LLMs, which is consistently effective across different models and parameter scales.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>Noise-based Local Learning using Stochastic Magnetic Tunnel Junctions</td>
<td style='padding: 6px;'>Kees Koenders, Leo Schnitzpan, Fabian Kammerbauer, Sinan Shu, Gerhard Jakob, Mathis Kl√§ui, Johan Mentink, Nasir Ahmad, Marcel van Gerven</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12783v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-inspired learning in physical hardware has enormous potential to learn fast at minimal energy expenditure. One of the characteristics of biological learning systems is their ability to learn in the presence of various noise sources. Inspired by this observation, we introduce a novel noise-based learning approach for physical systems implementing multi-layer neural networks. Simulation results show that our approach allows for effective learning whose performance approaches that of the conventional effective yet energy-costly backpropagation algorithm. Using a spintronics hardware implementation, we demonstrate experimentally that learning can be achieved in a small network composed of physical stochastic magnetic tunnel junctions. These results provide a path towards efficient learning in general physical systems which embraces rather than mitigates the noise inherent in physical devices.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>ALADE-SNN: Adaptive Logit Alignment in Dynamically Expandable Spiking Neural Networks for Class Incremental Learning</td>
<td style='padding: 6px;'>Wenyao Ni, Jiangrong Shen, Qi Xu, Huajin Tang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12696v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Inspired by the human brain's ability to adapt to new tasks without erasing prior knowledge, we develop spiking neural networks (SNNs) with dynamic structures for Class Incremental Learning (CIL). Our comparative experiments reveal that limited datasets introduce biases in logits distributions among tasks. Fixed features from frozen past-task extractors can cause overfitting and hinder the learning of new tasks. To address these challenges, we propose the ALADE-SNN framework, which includes adaptive logit alignment for balanced feature representation and OtoN suppression to manage weights mapping frozen old features to new classes during training, releasing them during fine-tuning. This approach dynamically adjusts the network architecture based on analytical observations, improving feature extraction and balancing performance between new and old tasks. Experiment results show that ALADE-SNN achieves an average incremental accuracy of 75.42 on the CIFAR100-B0 benchmark over 10 incremental steps. ALADE-SNN not only matches the performance of DNN-based methods but also surpasses state-of-the-art SNN-based continual learning algorithms. This advancement enhances continual learning in neuromorphic computing, offering a brain-inspired, energy-efficient solution for real-time data processing.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>Design and Performance Analysis of an Ultra-Low Power Integrate-and-Fire Neuron Circuit Using Nanoscale Side-contacted Field Effect Diode Technology</td>
<td style='padding: 6px;'>Seyedmohamadjavad Motaman, Sarah Sharif, Yaser Banad</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12443v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Enhancing power efficiency and performance in neuromorphic computing systems is critical for next-generation artificial intelligence applications. We propose the Nanoscale Side-contacted Field Effect Diode (S-FED), a novel solution that significantly lowers power usage and improves circuit speed, facilitating efficient neuron circuit design. Our innovative integrate-and-fire (IF) neuron model demonstrates exceptional performance metrics: 44 nW power consumption (85% lower than current designs), 0.964 fJ energy per spike (36% improvement over state-of-the-art), and 20 MHz spiking frequency. The architecture exhibits robust stability across process-voltage-temperature (PVT) variations, maintaining consistent performance with less than 7% spike amplitude variation for channel lengths from 7.5nm to 15nm, supply voltages from 0.8V to 1.2V, and temperatures from -40{\deg}C to 120{\deg}C. The model features tunable thresholds from 0.8V to 1.4V and reliable operation across input spike pulse widths from 0.5 ns to 2 ns. This significant advancement in neuromorphic hardware paves the way for more efficient brain-inspired computing systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>Predicting Workload in Virtual Flight Simulations using EEG Features (Including Post-hoc Analysis in Appendix)</td>
<td style='padding: 6px;'>Bas Verkennis, Evy van Weelden, Francesca L. Marogna, Maryam Alimardani, Travis J. Wiltshire, Max M. Louwerse</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12428v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Effective cognitive workload management has a major impact on the safety and performance of pilots. Integrating brain-computer interfaces (BCIs) presents an opportunity for real-time workload assessment. Leveraging cognitive workload data from immersive, high-fidelity virtual reality (VR) flight simulations enhances ecological validity and allows for dynamic adjustments to training scenarios based on individual cognitive states. While prior studies have predominantly concentrated on EEG spectral power for workload prediction, delving into inter-brain connectivity may yield deeper insights. This study assessed the predictive value of EEG spectral and connectivity features in distinguishing high vs. low workload periods during simulated flight in VR and Desktop conditions. EEG data were collected from 52 non-pilot participants conducting flight tasks in an aircraft simulation, after which they reported cognitive workload using the NASA Task Load Index. Using an ensemble approach, a stacked classifier was trained to predict workload using two feature sets extracted from the EEG data: 1) spectral features (Baseline model), and 2) a combination of spectral and connectivity features (Connectivity model), both within the alpha, beta, and theta band ranges. Results showed that the performance of the Connectivity model surpassed the Baseline model. Additionally, Recursive Feature Elimination (RFE) provided insights into the most influential workload-predicting features, highlighting the potential dominance of parietal-directed connectivity in managing cognitive workload during simulated flight. Further research on other connectivity metrics and alternative models (such as deep learning) in a large sample of pilots is essential to validate the possibility of a real-time BCI for the prediction of workload under safety-critical operational conditions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-16</td>
<td style='padding: 8px;'>The lack of asymmetry of the Maxwell centroids, and of ocular dominance, in persons with dyslexia</td>
<td style='padding: 6px;'>Albert Le Floch, Guy Ropars</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12053v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While the existence of an asymmetry between the two Maxwell centroids at the centre of the two foveas recorded using a foveascope, leads to the ocular dominance in good readers, the lack of asymmetry in most of the observers with dyslexia leads to their non-dominance and their difficulties in reading and writing. Indeed, the lack of asymmetry between the two main roads to the brain, i.e. the two optical nerves, leads to perturbations in the brain central connectivity, namely between the two hemispheres inducing too robust interhemispheric visual connections, beyond the critical period of 7-8 years. The symmetrical mirror-connections like b-d (observed for about 60% of children with dyslexia) or the non-symmetrical connections like b-b (35%) induce confusions and duplications in observers with dyslexia but remain erasable thanks to pulsed systems (glasses, lamps, screens. . .), using Hebbian processes in primary cortex synapses. The effect is instantaneous, non-invasive and compensates the lack of asymmetry. These systems help most of the observers with dyslexia to overcome their difficulties. The orthoptists and speech therapists can acquire and improve the mechanism to help the children to catch up, and perhaps be able to use the foveascope for an early diagnosis with young children.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-16</td>
<td style='padding: 8px;'>Ensemble Learning and 3D Pix2Pix for Comprehensive Brain Tumor Analysis in Multimodal MRI</td>
<td style='padding: 6px;'>Ramy A. Zeineldin, Franziska Mathis-Ullrich</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.11849v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Motivated by the need for advanced solutions in the segmentation and inpainting of glioma-affected brain regions in multi-modal magnetic resonance imaging (MRI), this study presents an integrated approach leveraging the strengths of ensemble learning with hybrid transformer models and convolutional neural networks (CNNs), alongside the innovative application of 3D Pix2Pix Generative Adversarial Network (GAN). Our methodology combines robust tumor segmentation capabilities, utilizing axial attention and transformer encoders for enhanced spatial relationship modeling, with the ability to synthesize biologically plausible brain tissue through 3D Pix2Pix GAN. This integrated approach addresses the BraTS 2023 cluster challenges by offering precise segmentation and realistic inpainting, tailored for diverse tumor types and sub-regions. The results demonstrate outstanding performance, evidenced by quantitative evaluations such as the Dice Similarity Coefficient (DSC), Hausdorff Distance (HD95) for segmentation, and Structural Similarity Index Measure (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Mean-Square Error (MSE) for inpainting. Qualitative assessments further validate the high-quality, clinically relevant outputs. In conclusion, this study underscores the potential of combining advanced machine learning techniques for comprehensive brain tumor analysis, promising significant advancements in clinical decision-making and patient care within the realm of medical imaging.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-16</td>
<td style='padding: 8px;'>A Mapper Algorithm with implicit intervals and its optimization</td>
<td style='padding: 6px;'>Yuyang Tao, Shufei Ge</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.11631v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The Mapper algorithm is an essential tool for visualizing complex, high dimensional data in topology data analysis (TDA) and has been widely used in biomedical research. It outputs a combinatorial graph whose structure implies the shape of the data. However,the need for manual parameter tuning and fixed intervals, along with fixed overlapping ratios may impede the performance of the standard Mapper algorithm. Variants of the standard Mapper algorithms have been developed to address these limitations, yet most of them still require manual tuning of parameters. Additionally, many of these variants, including the standard version found in the literature, were built within a deterministic framework and overlooked the uncertainty inherent in the data. To relax these limitations, in this work, we introduce a novel framework that implicitly represents intervals through a hidden assignment matrix, enabling automatic parameter optimization via stochastic gradient descent. In this work, we develop a soft Mapper framework based on a Gaussian mixture model(GMM) for flexible and implicit interval construction. We further illustrate the robustness of the soft Mapper algorithm by introducing the Mapper graph mode as a point estimation for the output graph. Moreover, a stochastic gradient descent algorithm with a specific topological loss function is proposed for optimizing parameters in the model. Both simulation and application studies demonstrate its effectiveness in capturing the underlying topological structures. In addition, the application to an RNA expression dataset obtained from the Mount Sinai/JJ Peters VA Medical Center Brain Bank (MSBB) successfully identifies a distinct subgroup of Alzheimer's Disease.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>Identification of Epileptic Spasms (ESES) Phases Using EEG Signals: A Vision Transformer Approach</td>
<td style='padding: 6px;'>Wei Gong, Yaru Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.13028v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This work introduces a new approach to the Epileptic Spasms (ESES) detection based on the EEG signals using Vision Transformers (ViT). Classic ESES detection approaches have usually been performed with manual processing or conventional algorithms, suffering from poor sample sizes, single-channel-based analyses, and low generalization abilities. In contrast, the proposed ViT model overcomes these limitations by using the attention mechanism to focus on the important features in multi-channel EEG data, which is contributing to both better accuracy and efficiency. The model processes frequency-domain representations of EEG signals, such as spectrograms, as image data to capture long-range dependencies and complex patterns in the signal. The model demonstrates high performance with an accuracy of 97% without requiring intensive data preprocessing, thus rendering it suitable for real-time clinical applications on a large scale. The method represents a significant development in the advancement of neurological disorders such as ESES in detection and analysis.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>Predicting Workload in Virtual Flight Simulations using EEG Features (Including Post-hoc Analysis in Appendix)</td>
<td style='padding: 6px;'>Bas Verkennis, Evy van Weelden, Francesca L. Marogna, Maryam Alimardani, Travis J. Wiltshire, Max M. Louwerse</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12428v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Effective cognitive workload management has a major impact on the safety and performance of pilots. Integrating brain-computer interfaces (BCIs) presents an opportunity for real-time workload assessment. Leveraging cognitive workload data from immersive, high-fidelity virtual reality (VR) flight simulations enhances ecological validity and allows for dynamic adjustments to training scenarios based on individual cognitive states. While prior studies have predominantly concentrated on EEG spectral power for workload prediction, delving into inter-brain connectivity may yield deeper insights. This study assessed the predictive value of EEG spectral and connectivity features in distinguishing high vs. low workload periods during simulated flight in VR and Desktop conditions. EEG data were collected from 52 non-pilot participants conducting flight tasks in an aircraft simulation, after which they reported cognitive workload using the NASA Task Load Index. Using an ensemble approach, a stacked classifier was trained to predict workload using two feature sets extracted from the EEG data: 1) spectral features (Baseline model), and 2) a combination of spectral and connectivity features (Connectivity model), both within the alpha, beta, and theta band ranges. Results showed that the performance of the Connectivity model surpassed the Baseline model. Additionally, Recursive Feature Elimination (RFE) provided insights into the most influential workload-predicting features, highlighting the potential dominance of parietal-directed connectivity in managing cognitive workload during simulated flight. Further research on other connectivity metrics and alternative models (such as deep learning) in a large sample of pilots is essential to validate the possibility of a real-time BCI for the prediction of workload under safety-critical operational conditions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-16</td>
<td style='padding: 8px;'>CiTrus: Squeezing Extra Performance out of Low-data Bio-signal Transfer Learning</td>
<td style='padding: 6px;'>Eloy Geenjaar, Lie Lu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.11695v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Transfer learning for bio-signals has recently become an important technique to improve prediction performance on downstream tasks with small bio-signal datasets. Recent works have shown that pre-training a neural network model on a large dataset (e.g. EEG) with a self-supervised task, replacing the self-supervised head with a linear classification head, and fine-tuning the model on different downstream bio-signal datasets (e.g., EMG or ECG) can dramatically improve the performance on those datasets. In this paper, we propose a new convolution-transformer hybrid model architecture with masked auto-encoding for low-data bio-signal transfer learning, introduce a frequency-based masked auto-encoding task, employ a more comprehensive evaluation framework, and evaluate how much and when (multimodal) pre-training improves fine-tuning performance. We also introduce a dramatically more performant method of aligning a downstream dataset with a different temporal length and sampling rate to the original pre-training dataset. Our findings indicate that the convolution-only part of our hybrid model can achieve state-of-the-art performance on some low-data downstream tasks. The performance is often improved even further with our full model. In the case of transformer-based models we find that pre-training especially improves performance on downstream datasets, multimodal pre-training often increases those gains further, and our frequency-based pre-training performs the best on average for the lowest and highest data regimes.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-16</td>
<td style='padding: 8px;'>Privacy-Preserving Brain-Computer Interfaces: A Systematic Review</td>
<td style='padding: 6px;'>K. Xia, W. Duch, Y. Sun, K. Xu, W. Fang, H. Luo, Y. Zhang, D. Sang, X. Xu, F-Y Wang, D. Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.11394v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A brain-computer interface (BCI) establishes a direct communication pathway between the human brain and a computer. It has been widely used in medical diagnosis, rehabilitation, education, entertainment, etc. Most research so far focuses on making BCIs more accurate and reliable, but much less attention has been paid to their privacy. Developing a commercial BCI system usually requires close collaborations among multiple organizations, e.g., hospitals, universities, and/or companies. Input data in BCIs, e.g., electroencephalogram (EEG), contain rich privacy information, and the developed machine learning model is usually proprietary. Data and model transmission among different parties may incur significant privacy threats, and hence privacy protection in BCIs must be considered. Unfortunately, there does not exist any contemporary and comprehensive review on privacy-preserving BCIs. This paper fills this gap, by describing potential privacy threats and protection strategies in BCIs. It also points out several challenges and future research directions in developing privacy-preserving BCIs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-16</td>
<td style='padding: 8px;'>Accurate, Robust and Privacy-Preserving Brain-Computer Interface Decoding</td>
<td style='padding: 6px;'>Xiaoqing Chen, Tianwang Jia, Dongrui Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.11390v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>An electroencephalogram (EEG) based brain-computer interface (BCI) enables direct communication between the brain and external devices. However, EEG-based BCIs face at least three major challenges in real-world applications: data scarcity and individual differences, adversarial vulnerability, and data privacy. While previous studies have addressed one or two of these issues, simultaneous accommodation of all three challenges remains challenging and unexplored. This paper fills this gap, by proposing an Augmented Robustness Ensemble (ARE) algorithm and integrating it into three privacy protection scenarios (centralized source-free transfer, federated source-free transfer, and source data perturbation), achieving simultaneously accurate decoding, adversarial robustness, and privacy protection of EEG-based BCIs. Experiments on three public EEG datasets demonstrated that our proposed approach outperformed over 10 classic and state-of-the-art approaches in both accuracy and robustness in all three privacy-preserving scenarios, even outperforming state-of-the-art transfer learning approaches that do not consider privacy protection at all. This is the first time that three major challenges in EEG-based BCIs can be addressed simultaneously, significantly improving the practicalness of EEG decoding in real-world BCIs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-13</td>
<td style='padding: 8px;'>KenCoh: A Ranked-Based Canonical Coherence</td>
<td style='padding: 6px;'>Mara Sherlin D. Talento, Sarbojit Roy, Hernando C. Ombao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.10521v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In this paper, we consider the problem of characterizing a robust global dependence between two brain regions where each region may contain several voxels or channels. This work is driven by experiments to investigate the dependence between two cortical regions and to identify differences in brain networks between brain states, e.g., alert and drowsy states. The most common approach to explore dependence between two groups of variables (or signals) is via canonical correlation analysis (CCA). However, it is limited to only capturing linear associations and is sensitive to outlier observations. These limitations are crucial because brain network connectivity is likely to be more complex than linear and that brain signals may exhibit heavy-tailed properties. To overcome these limitations, we develop a robust method, Kendall canonical coherence (KenCoh), for learning monotonic connectivity structure among neuronal signals filtered at given frequency bands. Furthermore, we propose the KenCoh-based permutation test to investigate the differences in brain network connectivity between two different states. Our simulation study demonstrates that KenCoh is competitive to the traditional variance-covariance estimator and outperforms the later when the underlying distributions are heavy-tailed. We apply our method to EEG recordings from a virtual-reality driving experiment. Our proposed method led to further insights on the differences of frontal-parietal cross-dependence network when the subject is alert and when the subject is drowsy and that left-parietal channel drives this dependence at the beta-band.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-13</td>
<td style='padding: 8px;'>CognitionCapturer: Decoding Visual Stimuli From Human EEG Signal With Multimodal Information</td>
<td style='padding: 6px;'>Kaifan Zhang, Lihuo He, Xin Jiang, Wen Lu, Di Wang, Xinbo Gao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.10489v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalogram (EEG) signals have attracted significant attention from researchers due to their non-invasive nature and high temporal sensitivity in decoding visual stimuli. However, most recent studies have focused solely on the relationship between EEG and image data pairs, neglecting the valuable ``beyond-image-modality" information embedded in EEG signals. This results in the loss of critical multimodal information in EEG. To address this limitation, we propose CognitionCapturer, a unified framework that fully leverages multimodal data to represent EEG signals. Specifically, CognitionCapturer trains Modality Expert Encoders for each modality to extract cross-modal information from the EEG modality. Then, it introduces a diffusion prior to map the EEG embedding space to the CLIP embedding space, followed by using a pretrained generative model, the proposed framework can reconstruct visual stimuli with high semantic and structural fidelity. Notably, the framework does not require any fine-tuning of the generative models and can be extended to incorporate more modalities. Through extensive experiments, we demonstrate that CognitionCapturer outperforms state-of-the-art methods both qualitatively and quantitatively. Code: https://github.com/XiaoZhangYES/CognitionCapturer.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-13</td>
<td style='padding: 8px;'>Active Poisoning: Efficient Backdoor Attacks on Transfer Learning-Based Brain-Computer Interfaces</td>
<td style='padding: 6px;'>X. Jiang, L. Meng, S. Li, D. Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.09933v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Transfer learning (TL) has been widely used in electroencephalogram (EEG)-based brain-computer interfaces (BCIs) for reducing calibration efforts. However, backdoor attacks could be introduced through TL. In such attacks, an attacker embeds a backdoor with a specific pattern into the machine learning model. As a result, the model will misclassify a test sample with the backdoor trigger into a prespecified class while still maintaining good performance on benign samples. Accordingly, this study explores backdoor attacks in the TL of EEG-based BCIs, where source-domain data are poisoned by a backdoor trigger and then used in TL. We propose several active poisoning approaches to select source-domain samples, which are most effective in embedding the backdoor pattern, to improve the attack success rate and efficiency. Experiments on four EEG datasets and three deep learning models demonstrate the effectiveness of the approaches. To our knowledge, this is the first study about backdoor attacks on TL models in EEG-based BCIs. It exposes a serious security risk in BCIs, which should be immediately addressed.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-13</td>
<td style='padding: 8px;'>User Identity Protection in EEG-based Brain-Computer Interfaces</td>
<td style='padding: 6px;'>L. Meng, X. Jiang, J. Huang, W. Li, H. Luo, D. Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.09854v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A brain-computer interface (BCI) establishes a direct communication pathway between the brain and an external device. Electroencephalogram (EEG) is the most popular input signal in BCIs, due to its convenience and low cost. Most research on EEG-based BCIs focuses on the accurate decoding of EEG signals; however, EEG signals also contain rich private information, e.g., user identity, emotion, and so on, which should be protected. This paper first exposes a serious privacy problem in EEG-based BCIs, i.e., the user identity in EEG data can be easily learned so that different sessions of EEG data from the same user can be associated together to more reliably mine private information. To address this issue, we further propose two approaches to convert the original EEG data into identity-unlearnable EEG data, i.e., removing the user identity information while maintaining the good performance on the primary BCI task. Experiments on seven EEG datasets from five different BCI paradigms showed that on average the generated identity-unlearnable EEG data can reduce the user identification accuracy from 70.01\% to at most 21.36\%, greatly facilitating user privacy protection in EEG-based BCIs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-13</td>
<td style='padding: 8px;'>Sequential Change Point Detection in High-dimensional Vector Auto-regressive Models</td>
<td style='padding: 6px;'>Yuhan Tian, Abolfazl Safikhani</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.09794v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Sequential (online) change-point detection involves continuously monitoring time-series data and triggering an alarm when shifts in the data distribution are detected. We propose an algorithm for real-time identification of alterations in the transition matrices of high-dimensional vector autoregressive models. The algorithm estimates transition matrices and error term variances using regularization techniques applied to training data, then computes a specific test statistic to detect changes in transition matrices as new data batches arrive. We establish the asymptotic normality of the test statistic under the scenario of no change points, subject to mild conditions. An alarm is raised when the calculated test statistic exceeds a predefined quantile of the standard normal distribution. We demonstrate that, as the size of the change (jump size) increases, the test power approaches one. The effectiveness of the algorithm is validated empirically across various simulation scenarios. Finally, we present two applications of the proposed methodology: analyzing shocks in S&P 500 data and detecting the timing of seizures in EEG data.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>Predicting Workload in Virtual Flight Simulations using EEG Features (Including Post-hoc Analysis in Appendix)</td>
<td style='padding: 6px;'>Bas Verkennis, Evy van Weelden, Francesca L. Marogna, Maryam Alimardani, Travis J. Wiltshire, Max M. Louwerse</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12428v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Effective cognitive workload management has a major impact on the safety and performance of pilots. Integrating brain-computer interfaces (BCIs) presents an opportunity for real-time workload assessment. Leveraging cognitive workload data from immersive, high-fidelity virtual reality (VR) flight simulations enhances ecological validity and allows for dynamic adjustments to training scenarios based on individual cognitive states. While prior studies have predominantly concentrated on EEG spectral power for workload prediction, delving into inter-brain connectivity may yield deeper insights. This study assessed the predictive value of EEG spectral and connectivity features in distinguishing high vs. low workload periods during simulated flight in VR and Desktop conditions. EEG data were collected from 52 non-pilot participants conducting flight tasks in an aircraft simulation, after which they reported cognitive workload using the NASA Task Load Index. Using an ensemble approach, a stacked classifier was trained to predict workload using two feature sets extracted from the EEG data: 1) spectral features (Baseline model), and 2) a combination of spectral and connectivity features (Connectivity model), both within the alpha, beta, and theta band ranges. Results showed that the performance of the Connectivity model surpassed the Baseline model. Additionally, Recursive Feature Elimination (RFE) provided insights into the most influential workload-predicting features, highlighting the potential dominance of parietal-directed connectivity in managing cognitive workload during simulated flight. Further research on other connectivity metrics and alternative models (such as deep learning) in a large sample of pilots is essential to validate the possibility of a real-time BCI for the prediction of workload under safety-critical operational conditions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-16</td>
<td style='padding: 8px;'>Privacy-Preserving Brain-Computer Interfaces: A Systematic Review</td>
<td style='padding: 6px;'>K. Xia, W. Duch, Y. Sun, K. Xu, W. Fang, H. Luo, Y. Zhang, D. Sang, X. Xu, F-Y Wang, D. Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.11394v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A brain-computer interface (BCI) establishes a direct communication pathway between the human brain and a computer. It has been widely used in medical diagnosis, rehabilitation, education, entertainment, etc. Most research so far focuses on making BCIs more accurate and reliable, but much less attention has been paid to their privacy. Developing a commercial BCI system usually requires close collaborations among multiple organizations, e.g., hospitals, universities, and/or companies. Input data in BCIs, e.g., electroencephalogram (EEG), contain rich privacy information, and the developed machine learning model is usually proprietary. Data and model transmission among different parties may incur significant privacy threats, and hence privacy protection in BCIs must be considered. Unfortunately, there does not exist any contemporary and comprehensive review on privacy-preserving BCIs. This paper fills this gap, by describing potential privacy threats and protection strategies in BCIs. It also points out several challenges and future research directions in developing privacy-preserving BCIs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-16</td>
<td style='padding: 8px;'>Accurate, Robust and Privacy-Preserving Brain-Computer Interface Decoding</td>
<td style='padding: 6px;'>Xiaoqing Chen, Tianwang Jia, Dongrui Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.11390v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>An electroencephalogram (EEG) based brain-computer interface (BCI) enables direct communication between the brain and external devices. However, EEG-based BCIs face at least three major challenges in real-world applications: data scarcity and individual differences, adversarial vulnerability, and data privacy. While previous studies have addressed one or two of these issues, simultaneous accommodation of all three challenges remains challenging and unexplored. This paper fills this gap, by proposing an Augmented Robustness Ensemble (ARE) algorithm and integrating it into three privacy protection scenarios (centralized source-free transfer, federated source-free transfer, and source data perturbation), achieving simultaneously accurate decoding, adversarial robustness, and privacy protection of EEG-based BCIs. Experiments on three public EEG datasets demonstrated that our proposed approach outperformed over 10 classic and state-of-the-art approaches in both accuracy and robustness in all three privacy-preserving scenarios, even outperforming state-of-the-art transfer learning approaches that do not consider privacy protection at all. This is the first time that three major challenges in EEG-based BCIs can be addressed simultaneously, significantly improving the practicalness of EEG decoding in real-world BCIs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-15</td>
<td style='padding: 8px;'>Imagined Speech State Classification for Robust Brain-Computer Interface</td>
<td style='padding: 6px;'>Byung-Kwan Ko, Jun-Young Kim, Seo-Hyun Lee</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12215v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study examines the effectiveness of traditional machine learning classifiers versus deep learning models for detecting the imagined speech using electroencephalogram data. Specifically, we evaluated conventional machine learning techniques such as CSP-SVM and LDA-SVM classifiers alongside deep learning architectures such as EEGNet, ShallowConvNet, and DeepConvNet. Machine learning classifiers exhibited significantly lower precision and recall, indicating limited feature extraction capabilities and poor generalization between imagined speech and idle states. In contrast, deep learning models, particularly EEGNet, achieved the highest accuracy of 0.7080 and an F1 score of 0.6718, demonstrating their enhanced ability in automatic feature extraction and representation learning, essential for capturing complex neurophysiological patterns. These findings highlight the limitations of conventional machine learning approaches in brain-computer interface (BCI) applications and advocate for adopting deep learning methodologies to achieve more precise and reliable classification of detecting imagined speech. This foundational research contributes to the development of imagined speech-based BCI systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-13</td>
<td style='padding: 8px;'>Active Poisoning: Efficient Backdoor Attacks on Transfer Learning-Based Brain-Computer Interfaces</td>
<td style='padding: 6px;'>X. Jiang, L. Meng, S. Li, D. Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.09933v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Transfer learning (TL) has been widely used in electroencephalogram (EEG)-based brain-computer interfaces (BCIs) for reducing calibration efforts. However, backdoor attacks could be introduced through TL. In such attacks, an attacker embeds a backdoor with a specific pattern into the machine learning model. As a result, the model will misclassify a test sample with the backdoor trigger into a prespecified class while still maintaining good performance on benign samples. Accordingly, this study explores backdoor attacks in the TL of EEG-based BCIs, where source-domain data are poisoned by a backdoor trigger and then used in TL. We propose several active poisoning approaches to select source-domain samples, which are most effective in embedding the backdoor pattern, to improve the attack success rate and efficiency. Experiments on four EEG datasets and three deep learning models demonstrate the effectiveness of the approaches. To our knowledge, this is the first study about backdoor attacks on TL models in EEG-based BCIs. It exposes a serious security risk in BCIs, which should be immediately addressed.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-13</td>
<td style='padding: 8px;'>User Identity Protection in EEG-based Brain-Computer Interfaces</td>
<td style='padding: 6px;'>L. Meng, X. Jiang, J. Huang, W. Li, H. Luo, D. Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.09854v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A brain-computer interface (BCI) establishes a direct communication pathway between the brain and an external device. Electroencephalogram (EEG) is the most popular input signal in BCIs, due to its convenience and low cost. Most research on EEG-based BCIs focuses on the accurate decoding of EEG signals; however, EEG signals also contain rich private information, e.g., user identity, emotion, and so on, which should be protected. This paper first exposes a serious privacy problem in EEG-based BCIs, i.e., the user identity in EEG data can be easily learned so that different sessions of EEG data from the same user can be associated together to more reliably mine private information. To address this issue, we further propose two approaches to convert the original EEG data into identity-unlearnable EEG data, i.e., removing the user identity information while maintaining the good performance on the primary BCI task. Experiments on seven EEG datasets from five different BCI paradigms showed that on average the generated identity-unlearnable EEG data can reduce the user identification accuracy from 70.01\% to at most 21.36\%, greatly facilitating user privacy protection in EEG-based BCIs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-12</td>
<td style='padding: 8px;'>Front-end Replication Dynamic Window (FRDW) for Online Motor Imagery Classification</td>
<td style='padding: 6px;'>X. Chen, J. An, H. Wu, S. Li, B. Liu, D. Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.09015v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Motor imagery (MI) is a classical paradigm in electroencephalogram (EEG) based brain-computer interfaces (BCIs). Online accurate and fast decoding is very important to its successful applications. This paper proposes a simple yet effective front-end replication dynamic window (FRDW) algorithm for this purpose. Dynamic windows enable the classification based on a test EEG trial shorter than those used in training, improving the decision speed; front-end replication fills a short test EEG trial to the length used in training, improving the classification accuracy. Within-subject and cross-subject online MI classification experiments on three public datasets, with three different classifiers and three different data augmentation approaches, demonstrated that FRDW can significantly increase the information transfer rate in MI decoding. Additionally, FR can also be used in training data augmentation. FRDW helped win national champion of the China BCI Competition in 2022.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-12</td>
<td style='padding: 8px;'>Motor Imagery Classification for Asynchronous EEG-Based Brain-Computer Interfaces</td>
<td style='padding: 6px;'>Huanyu Wu, Siyang Li, Dongrui Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.09006v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Motor imagery (MI) based brain-computer interfaces (BCIs) enable the direct control of external devices through the imagined movements of various body parts. Unlike previous systems that used fixed-length EEG trials for MI decoding, asynchronous BCIs aim to detect the user's MI without explicit triggers. They are challenging to implement, because the algorithm needs to first distinguish between resting-states and MI trials, and then classify the MI trials into the correct task, all without any triggers. This paper proposes a sliding window prescreening and classification (SWPC) approach for MI-based asynchronous BCIs, which consists of two modules: a prescreening module to screen MI trials out of the resting-state, and a classification module for MI classification. Both modules are trained with supervised learning followed by self-supervised learning, which refines the feature extractors. Within-subject and cross-subject asynchronous MI classifications on four different EEG datasets validated the effectiveness of SWPC, i.e., it always achieved the highest average classification accuracy, and outperformed the best state-of-the-art baseline on each dataset by about 2%.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-12</td>
<td style='padding: 8px;'>Motor Imagery Teleoperation of a Mobile Robot Using a Low-Cost Brain-Computer Interface for Multi-Day Validation</td>
<td style='padding: 6px;'>Yujin An, Daniel Mitchell, John Lathrop, David Flynn, Soon-Jo Chung</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.08971v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-computer interfaces (BCI) have the potential to provide transformative control in prosthetics, assistive technologies (wheelchairs), robotics, and human-computer interfaces. While Motor Imagery (MI) offers an intuitive approach to BCI control, its practical implementation is often limited by the requirement for expensive devices, extensive training data, and complex algorithms, leading to user fatigue and reduced accessibility. In this paper, we demonstrate that effective MI-BCI control of a mobile robot in real-world settings can be achieved using a fine-tuned Deep Neural Network (DNN) with a sliding window, eliminating the need for complex feature extractions for real-time robot control. The fine-tuning process optimizes the convolutional and attention layers of the DNN to adapt to each user's daily MI data streams, reducing training data by 70% and minimizing user fatigue from extended data collection. Using a low-cost (~$3k), 16-channel, non-invasive, open-source electroencephalogram (EEG) device, four users teleoperated a quadruped robot over three days. The system achieved 78% accuracy on a single-day validation dataset and maintained a 75% validation accuracy over three days without extensive retraining from day-to-day. For real-world robot command classification, we achieved an average of 62% accuracy. By providing empirical evidence that MI-BCI systems can maintain performance over multiple days with reduced training data to DNN and a low-cost EEG device, our work enhances the practicality and accessibility of BCI technology. This advancement makes BCI applications more feasible for real-world scenarios, particularly in controlling robotic systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-10</td>
<td style='padding: 8px;'>Towards Predictive Communication with Brain-Computer Interfaces integrating Large Language Models</td>
<td style='padding: 6px;'>Andrea Caria</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.07355v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This perspective article aims at providing an outline of the state of the art and future developments towards the integration of cutting-edge predictive language models with BCI. A synthetic overview of early and more recent linguistic models, from natural language processing (NLP) models to recent LLM, that to a varying extent improved predictive writing systems, is first provided. Second, a summary of previous BCI implementations integrating language models is presented. The few preliminary studies investigating the possible combination of LLM with BCI spellers to efficiently support fast communication and control are then described. Finally, current challenges and limitations towards the full integration of LLM with BCI systems are discussed. Recent investigations suggest that the combination of LLM with BCI might drastically improve human-computer interaction in patients with motor or language disorders as well as in healthy individuals. In particular, the pretrained autoregressive transformer models, such as GPT, that capitalize from parallelization, learning through pre-training and fine-tuning, promise a substantial improvement of BCI for communication with respect to previous systems incorporating simpler language models. Indeed, among various models, the GPT-2 was shown to represent an excellent candidate for its integration into BCI although testing was only perfomed on simulated conversations and not on real BCI scenarios. Prospectively, the full integration of LLM with advanced BCI systems might lead to a big leap forward towards fast, efficient and user-adaptive neurotechnology.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-13</td>
<td style='padding: 8px;'>Data Integration with Fusion Searchlight: Classifying Brain States from Resting-state fMRI</td>
<td style='padding: 6px;'>Simon Wein, Marco Riebel, Lisa-Marie Brunner, Caroline Nothdurfter, Rainer Rupprecht, Jens V. Schwarzbach</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.10161v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Spontaneous neural activity observed in resting-state fMRI is characterized by complex spatio-temporal dynamics. Different measures related to local and global brain connectivity and fluctuations in low-frequency amplitudes can quantify individual aspects of these neural dynamics. Even though such measures are derived from the same functional signals, they are often evaluated separately, neglecting their interrelations and potentially reducing the analysis sensitivity. In our study, we present a fusion searchlight (FuSL) framework to combine the complementary information contained in different resting-state fMRI metrics and demonstrate how this can improve the decoding of brain states. Moreover, we show how explainable AI allows us to reconstruct the differential impact of each metric on the decoding, which additionally increases spatial specificity of searchlight analysis. In general, this framework can be adapted to combine information derived from different imaging modalities or experimental conditions, offering a versatile and interpretable tool for data fusion in neuroimaging.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-12</td>
<td style='padding: 8px;'>Network Dynamics of Emotional Processing: A Structural Balance Theory Approach</td>
<td style='padding: 6px;'>Sepehr Gourabi, Parinaz Khosravani, Shahrzad Nosrat, Roya Mohammadi, Masoud Lotfalipour</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.09554v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding emotional processing in the human brain requires examining the complex interactions between different brain regions. While previous studies have identified specific regions involved in emotion processing, a holistic network approach may provide deeper insights. We use Structural Balance Theory to investigate the stability and triadic structures of signed brain networks during resting state and emotional processing, specifically in response to fear-related stimuli. We hypothesized that imbalanced triadic interactions would be more prevalent during emotional processing, especially in response to fear-related stimuli, potentially reflecting the brain's adaptation to emotional challenges. By analyzing fMRI data from 138 healthy, right-handed participants, we found that emotional processing was marked by an increase in positive connections and a decrease in negative connections compared to the resting state. Our findings clearly show that balanced triads significantly decreased while imbalanced triads increased, indicating a shift toward instability in the brain's functional network during emotional processing. Additionally, the number of influential hubs was significantly lower during fear processing than in neutral conditions, suggesting a more centralized network and higher levels of network energy. These findings reveal the brain's remarkable adaptive capacity during emotional processing, demonstrating how network stability dynamically shifts through changes in balanced and imbalanced triads, hub tendencies, and energy dynamics. Our research illuminates a complex mechanism by which the brain flexibly reconfigures its functional network in response to emotional stimuli with potential implications for understanding emotional resilience and neurological disorders.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-11</td>
<td style='padding: 8px;'>MHSA: A Multi-scale Hypergraph Network for Mild Cognitive Impairment Detection via Synchronous and Attentive Fusion</td>
<td style='padding: 6px;'>Manman Yuan, Weiming Jia, Xiong Luo, Jiazhen Ye, Peican Zhu, Junlin Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12149v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The precise detection of mild cognitive impairment (MCI) is of significant importance in preventing the deterioration of patients in a timely manner. Although hypergraphs have enhanced performance by learning and analyzing brain networks, they often only depend on vector distances between features at a single scale to infer interactions. In this paper, we deal with a more arduous challenge, hypergraph modelling with synchronization between brain regions, and design a novel framework, i.e., A Multi-scale Hypergraph Network for MCI Detection via Synchronous and Attentive Fusion (MHSA), to tackle this challenge. Specifically, our approach employs the Phase-Locking Value (PLV) to calculate the phase synchronization relationship in the spectrum domain of regions of interest (ROIs) and designs a multi-scale feature fusion mechanism to integrate dynamic connectivity features of functional magnetic resonance imaging (fMRI) from both the temporal and spectrum domains. To evaluate and optimize the direct contribution of each ROI to phase synchronization in the temporal domain, we structure the PLV coefficients dynamically adjust strategy, and the dynamic hypergraph is modelled based on a comprehensive temporal-spectrum fusion matrix. Experiments on the real-world dataset indicate the effectiveness of our strategy. The code is available at https://github.com/Jia-Weiming/MHSA.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-07</td>
<td style='padding: 8px;'>Biological Brain Age Estimation using Sex-Aware Adversarial Variational Autoencoder with Multimodal Neuroimages</td>
<td style='padding: 6px;'>Abd Ur Rehman, Azka Rehman, Muhammad Usman, Abdullah Shahid, Sung-Min Gho, Aleum Lee, Tariq M. Khan, Imran Razzak</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.05632v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain aging involves structural and functional changes and therefore serves as a key biomarker for brain health. Combining structural magnetic resonance imaging (sMRI) and functional magnetic resonance imaging (fMRI) has the potential to improve brain age estimation by leveraging complementary data. However, fMRI data, being noisier than sMRI, complicates multimodal fusion. Traditional fusion methods often introduce more noise than useful information, which can reduce accuracy compared to using sMRI alone. In this paper, we propose a novel multimodal framework for biological brain age estimation, utilizing a sex-aware adversarial variational autoencoder (SA-AVAE). Our framework integrates adversarial and variational learning to effectively disentangle the latent features from both modalities. Specifically, we decompose the latent space into modality-specific codes and shared codes to represent complementary and common information across modalities, respectively. To enhance the disentanglement, we introduce cross-reconstruction and shared-distinct distance ratio loss as regularization terms. Importantly, we incorporate sex information into the learned latent code, enabling the model to capture sex-specific aging patterns for brain age estimation via an integrated regressor module. We evaluate our model using the publicly available OpenBHB dataset, a comprehensive multi-site dataset for brain age estimation. The results from ablation studies and comparisons with state-of-the-art methods demonstrate that our framework outperforms existing approaches and shows significant robustness across various age groups, highlighting its potential for real-time clinical applications in the early detection of neurodegenerative diseases.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-06</td>
<td style='padding: 8px;'>Probing the contents of semantic representations from text, behavior, and brain data using the psychNorms metabase</td>
<td style='padding: 6px;'>Zak Hussain, Rui Mata, Ben R. Newell, Dirk U. Wulff</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.04936v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Semantic representations are integral to natural language processing, psycholinguistics, and artificial intelligence. Although often derived from internet text, recent years have seen a rise in the popularity of behavior-based (e.g., free associations) and brain-based (e.g., fMRI) representations, which promise improvements in our ability to measure and model human representations. We carry out the first systematic evaluation of the similarities and differences between semantic representations derived from text, behavior, and brain data. Using representational similarity analysis, we show that word vectors derived from behavior and brain data encode information that differs from their text-derived cousins. Furthermore, drawing on our psychNorms metabase, alongside an interpretability method that we call representational content analysis, we find that, in particular, behavior representations capture unique variance on certain affective, agentic, and socio-moral dimensions. We thus establish behavior as an important complement to text for capturing human representations and behavior. These results are broadly relevant to research aimed at learning human-aligned semantic representations, including work on evaluating and aligning large language models.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-04</td>
<td style='padding: 8px;'>Detection of Multiple Influential Observations on Model Selection</td>
<td style='padding: 6px;'>Dongliang Zhang, Masoud Asgharian, Martin A. Lindquist</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02945v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Outlying observations are frequently encountered in a wide spectrum of scientific domains, posing significant challenges for the generalizability of statistical models and the reproducibility of downstream analysis. These observations can be identified through influential diagnosis, which refers to the detection of observations that are unduly influential on diverse facets of statistical inference. To date, methods for identifying observations influencing the choice of a stochastically selected submodel have been underdeveloped, especially in the high-dimensional setting where the number of predictors p exceeds the sample size n. Recently we proposed an improved diagnostic measure to handle this setting. However, its distributional properties and approximations have not yet been explored. To address this shortcoming, the notion of exchangeability is revived, and used to determine the exact finite- and large-sample distributions of our assessment metric. This forms the foundation for the introduction of both parametric and non-parametric approaches for its approximation and the establishment of thresholds for diagnosis. The resulting framework is extended to logistic regression models, followed by a simulation study conducted to assess the performance of various detection procedures. Finally the framework is applied to data from an fMRI study of thermal pain, with the goal of identifying outlying subjects that could distort the formulation of statistical models using functional brain activity in predicting physical pain ratings. Both linear and logistic regression models are used to demonstrate the benefits of detection and compare the performances of different detection procedures. In particular, two additional influential observations are identified, which are not discovered by previous studies.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-03</td>
<td style='padding: 8px;'>Hierarchical feature extraction on functional brain networks for autism spectrum disorder identification with resting-state fMRI data</td>
<td style='padding: 6px;'>Yiqian Luo, Qiurong Chen, Fali Li, Liang Yi, Peng Xu, Yangsong Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02424v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Autism spectrum disorder (ASD) is a pervasive developmental disorder of the central nervous system, which occurs most frequently in childhood and is characterized by unusual and repetitive ritualistic behaviors. Currently, diagnostic methods primarily rely on questionnaire surveys and behavioral observation, which may lead to misdiagnoses due to the subjective evaluation and measurement used in these traditional methods. With the advancement in medical imaging, MR imaging-based diagnosis has become an alternative and more objective approach. In this paper, we propose a Hybrid neural Network model for ASD identification, termded ASD-HNet, to hierarchically extract features on the functional brain networks based on resting-state functional magnetic resonance imaging data. This hierarchical method can better extract brain representations, improve the diagnostic accuracy, and help us better locate brain regions related to ASD. Specifically, features are extracted from three scales: local regions of interest (ROIs) scale, community-clustering scale, and the whole-communities scale. For the ROI scale, graph convolution is used to transfer features between ROIs. At the community cluster scale, functional gradients are introduced, the clustering algorithm K-Means is used to automatically cluster ROIs with similar functional gradients into several communities, and features of ROIs belonging to the same community are extracted to characterize these communities. At global information integration scale, we extract global features from community-scale brain networks to characterize the whole brain networks. We validate the effectiveness of our method using the public dataset of Autism Brain Imaging Data Exchange I (ABIDE I), and elucidate the interpretability of the method. Experimental results demonstrate that the proposed ASD-HNet can yield superior performance than compared methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-29</td>
<td style='padding: 8px;'>Dynamic EEG-fMRI mapping: Revealing the relationship between brain connectivity and cognitive state</td>
<td style='padding: 6px;'>Guiran Liu, Binrong Zhu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19922v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study investigated the dynamic connectivity patterns between EEG and fMRI modalities, contributing to our understanding of brain network interactions. By employing a comprehensive approach that integrated static and dynamic analyses of EEG-fMRI data, we were able to uncover distinct connectivity states and characterize their temporal fluctuations. The results revealed modular organization within the intrinsic connectivity networks (ICNs) of the brain, highlighting the significant roles of sensory systems and the default mode network. The use of a sliding window technique allowed us to assess how functional connectivity varies over time, further elucidating the transient nature of brain connectivity. Additionally, our findings align with previous literature, reinforcing the notion that cognitive states can be effectively identified through short-duration data, specifically within the 30-60 second timeframe. The established relationships between connectivity strength and cognitive processes, particularly during different visual states, underscore the relevance of our approach for future research into brain dynamics. Overall, this study not only enhances our understanding of the interplay between EEG and fMRI signals but also paves the way for further exploration into the neural correlates of cognitive functions and their implications in clinical settings. Future research should focus on refining these methodologies and exploring their applications in various cognitive and clinical contexts.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-28</td>
<td style='padding: 8px;'>3D Wasserstein generative adversarial network with dense U-Net based discriminator for preclinical fMRI denoising</td>
<td style='padding: 6px;'>Sima Soltanpour, Arnold Chang, Dan Madularu, Praveen Kulkarni, Craig Ferris, Chris Joslin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19345v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional magnetic resonance imaging (fMRI) is extensively used in clinical and preclinical settings to study brain function, however, fMRI data is inherently noisy due to physiological processes, hardware, and external noise. Denoising is one of the main preprocessing steps in any fMRI analysis pipeline. This process is challenging in preclinical data in comparison to clinical data due to variations in brain geometry, image resolution, and low signal-to-noise ratios. In this paper, we propose a structure-preserved algorithm based on a 3D Wasserstein generative adversarial network with a 3D dense U-net based discriminator called, 3D U-WGAN. We apply a 4D data configuration to effectively denoise temporal and spatial information in analyzing preclinical fMRI data. GAN-based denoising methods often utilize a discriminator to identify significant differences between denoised and noise-free images, focusing on global or local features. To refine the fMRI denoising model, our method employs a 3D dense U-Net discriminator to learn both global and local distinctions. To tackle potential over-smoothing, we introduce an adversarial loss and enhance perceptual similarity by measuring feature space distances. Experiments illustrate that 3D U-WGAN significantly improves image quality in resting-state and task preclinical fMRI data, enhancing signal-to-noise ratio without introducing excessive structural changes in existing methods. The proposed method outperforms state-of-the-art methods when applied to simulated and real data in a fMRI analysis pipeline.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-25</td>
<td style='padding: 8px;'>Energy landscape analysis based on the Ising model: Tutorial review</td>
<td style='padding: 6px;'>Naoki Masuda, Saiful Islam, Si Thu Aung, Takamitsu Watanabe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.16979v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We review a class of energy landscape analysis method that uses the Ising model and takes multivariate time series data as input. The method allows one to capture dynamics of the data as trajectories of a ball from one basin to a different basin to yet another, constrained on the energy landscape specified by the estimated Ising model. While this energy landscape analysis has mostly been applied to functional magnetic resonance imaging (fMRI) data from the brain for historical reasons, there are emerging applications outside fMRI data and neuroscience. To inform such applications in various research fields, this review paper provides a detailed tutorial on each step of the analysis, terminologies, concepts underlying the method, and validation, as well as recent developments of extended and related methods.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-12</td>
<td style='padding: 8px;'>LV-CadeNet: Long View Feature Convolution-Attention Fusion Encoder-Decoder Network for Clinical MEG Spike Detection</td>
<td style='padding: 6px;'>Kuntao Xiao, Xiongfei Wang, Pengfei Teng, Yi Sun, Wanli Yang, Liang Zhang, Hanyang Dong, Guoming Luan, Shurong Sheng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.08896v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>It is widely acknowledged that the epileptic foci can be pinpointed by source localizing interictal epileptic discharges (IEDs) via Magnetoencephalography (MEG). However, manual detection of IEDs, which appear as spikes in MEG data, is extremely labor intensive and requires considerable professional expertise, limiting the broader adoption of MEG technology. Numerous studies have focused on automatic detection of MEG spikes to overcome this challenge, but these efforts often validate their models on synthetic datasets with balanced positive and negative samples. In contrast, clinical MEG data is highly imbalanced, raising doubts on the real-world efficacy of these models. To address this issue, we introduce LV-CadeNet, a Long View feature Convolution-Attention fusion Encoder-Decoder Network, designed for automatic MEG spike detection in real-world clinical scenarios. Beyond addressing the disparity between training data distribution and clinical test data through semi-supervised learning, our approach also mimics human specialists by constructing long view morphological input data. Moreover, we propose an advanced convolution-attention module to extract temporal and spatial features from the input data. LV-CadeNet significantly improves the accuracy of MEG spike detection, boosting it from 42.31\% to 54.88\% on a novel clinical dataset sourced from Sanbo Brain Hospital Capital Medical University. This dataset, characterized by a highly imbalanced distribution of positive and negative samples, accurately represents real-world clinical scenarios.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-06</td>
<td style='padding: 8px;'>Measuring Goal-Directedness</td>
<td style='padding: 6px;'>Matt MacDermott, James Fox, Francesco Belardinelli, Tom Everitt</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.04758v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We define maximum entropy goal-directedness (MEG), a formal measure of goal-directedness in causal models and Markov decision processes, and give algorithms for computing it. Measuring goal-directedness is important, as it is a critical element of many concerns about harm from AI. It is also of philosophical interest, as goal-directedness is a key aspect of agency. MEG is based on an adaptation of the maximum causal entropy framework used in inverse reinforcement learning. It can measure goal-directedness with respect to a known utility function, a hypothesis class of utility functions, or a set of random variables. We prove that MEG satisfies several desiderata and demonstrate our algorithms with small-scale experiments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-29</td>
<td style='padding: 8px;'>Neuroplasticity and Psychedelics: a comprehensive examination of classic and non-classic compounds in pre and clinical models</td>
<td style='padding: 6px;'>Claudio Agnorelli, Meg Spriggs, Kate Godfrey, Gabriela Sawicka, Bettina Bohl, Hannah Douglass, Andrea Fagiolini, Hashemi Parastoo, Robin Carhart-Harris, David Nutt, David Erritzoe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19840v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroplasticity, the ability of the nervous system to adapt throughout an organism's lifespan, offers potential as both a biomarker and treatment target for neuropsychiatric conditions. Psychedelics, a burgeoning category of drugs, are increasingly prominent in psychiatric research, prompting inquiries into their mechanisms of action. Distinguishing themselves from traditional medications, psychedelics demonstrate rapid and enduring therapeutic effects after a single or few administrations, believed to stem from their neuroplasticity-enhancing properties. This review examines how classic psychedelics (e.g., LSD, psilocybin, N,N-DMT) and non-classic psychedelics (e.g., ketamine, MDMA) influence neuroplasticity. Drawing from preclinical and clinical studies, we explore the molecular, structural, and functional changes triggered by these agents. Animal studies suggest psychedelics induce heightened sensitivity of the nervous system to environmental stimuli (meta-plasticity), re-opening developmental windows for long-term structural changes (hyper-plasticity), with implications for mood and behavior. Translating these findings to humans faces challenges due to limitations in current imaging techniques. Nonetheless, promising new directions for human research are emerging, including the employment of novel positron-emission tomography (PET) radioligands, non-invasive brain stimulation methods, and multimodal approaches. By elucidating the interplay between psychedelics and neuroplasticity, this review informs the development of targeted interventions for neuropsychiatric disorders and advances understanding of psychedelics' therapeutic potential.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-29</td>
<td style='padding: 8px;'>On Monitoring Edge-Geodetic Sets of Dynamic Graph</td>
<td style='padding: 6px;'>Zin Mar Myint, Ashish Saxena</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19800v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The concept of a monitoring edge-geodetic set (MEG-set) in a graph $G$, denoted $MEG(G)$, refers to a subset of vertices $MEG(G)\subseteq V(G)$ such that every edge $e$ in $G$ is monitored by some pair of vertices $ u, v \in MEG(G)$, where $e$ lies on all shortest paths between $u$ and $v$. The minimum number of vertices required to form such a set is called the monitoring edge-geodetic number, denoted $meg(G)$. The primary motivation for studying $MEG$-sets in previous works arises from scenarios in which certain edges are removed from $G$. In these cases, the vertices of the $MEG$-set are responsible for detecting these deletions. Such detection is crucial for identifying which edges have been removed from $G$ and need to be repaired. In real life, repairing these edges may be costly, or sometimes it is impossible to repair edges. In this case, the original $MEG$-set may no longer be effective in monitoring the modified graph. This highlights the importance of reassessing and adapting the $MEG$-set after edge deletions. This work investigates the monitoring edge-geodetic properties of graphs, focusing on how the removal of $k$ edges affects the structure of a graph and influences its monitoring capabilities. Specifically, we explore how the monitoring edge-geodetic number $meg(G)$ changes when $k$ edges are removed. The study aims to compare the monitoring properties of the original graph with those of the modified graph and to understand the impact of edge deletions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-14</td>
<td style='padding: 8px;'>Towards Neural Foundation Models for Vision: Aligning EEG, MEG, and fMRI Representations for Decoding, Encoding, and Modality Conversion</td>
<td style='padding: 6px;'>Matteo Ferrante, Tommaso Boccato, Grigorii Rashkov, Nicola Toschi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.09723v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper presents a novel approach towards creating a foundational model for aligning neural data and visual stimuli across multimodal representationsof brain activity by leveraging contrastive learning. We used electroencephalography (EEG), magnetoencephalography (MEG), and functional magnetic resonance imaging (fMRI) data. Our framework's capabilities are demonstrated through three key experiments: decoding visual information from neural data, encoding images into neural representations, and converting between neural modalities. The results highlight the model's ability to accurately capture semantic information across different brain imaging techniques, illustrating its potential in decoding, encoding, and modality conversion tasks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-12</td>
<td style='padding: 8px;'>Search for the X17 particle in $^{7}\mathrm{Li}(\mathrm{p},\mathrm{e}^+ \mathrm{e}^{-}) ^{8}\mathrm{Be}$ processes with the MEG II detector</td>
<td style='padding: 6px;'>The MEG II collaboration, K. Afanaciev, A. M. Baldini, S. Ban, H. Benmansour, G. Boca, P. W. Cattaneo, G. Cavoto, F. Cei, M. Chiappini, A. Corvaglia, G. Dal Maso, A. De Bari, M. De Gerone, L. Ferrari Barusso, M. Francesconi, L. Galli, G. Gallucci, F. Gatti, L. Gerritzen, F. Grancagnolo, E. G. Grandoni, M. Grassi, D. N. Grigoriev, M. Hildebrandt, F. Ignatov, F. Ikeda, T. Iwamoto, S. Karpov, P. -R. Kettle, N. Khomutov, A. Kolesnikov, N. Kravchuk, V. Krylov, N. Kuchinskiy, F. Leonetti, W. Li, V. Malyshev, A. Matsushita, M. Meucci, S. Mihara, W. Molzon, T. Mori, D. Nicol√≤, H. Nishiguchi, A. Ochi, W. Ootani, A. Oya, D. Palo, M. Panareo, A. Papa, V. Pettinacci, A. Popov, F. Renga, S. Ritt, M. Rossella, A. Rozhdestvensky. S. Scarpellini, P. Schwendimann, G. Signorelli, M. Takahashi, Y. Uchiyama, A. Venturini, B. Vitali, C. Voena, K. Yamamoto, R. Yokota, T. Yonemoto</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.07994v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The observation of a resonance structure in the opening angle of the electron-positron pairs in the $^{7}$Li(p,\ee) $^{8}$Be reaction was claimed and interpreted as the production and subsequent decay of a hypothetical particle (X17). Similar excesses, consistent with this particle, were later observed in processes involving $^{4}$He and $^{12}$C nuclei with the same experimental technique. The MEG II apparatus at PSI, designed to search for the $\mu^+ \rightarrow \mathrm{e}^+ \gamma$ decay, can be exploited to investigate the existence of this particle and study its nature. Protons from a Cockroft-Walton accelerator, with an energy up to 1.1 MeV, were delivered on a dedicated Li-based target. The $\gamma$ and the e$^{+}$e$^{-}$ pair emerging from the $^8\mathrm{Be}^*$ transitions were studied with calorimeters and a spectrometer, featuring a broader angular acceptance than previous experiments. We present in this paper the analysis of a four-week data-taking in 2023 with a beam energy of 1080 keV, resulting in the excitation of two different resonances with Q-value \SI{17.6}{\mega\electronvolt} and \SI{18.1}{\mega\electronvolt}. No significant signal was found, and limits at \SI{90}{\percent} C.L. on the branching ratios (relative to the $\gamma$ emission) of the two resonances to X17 were set, $R_{17.6} < 1.8 \times 10^{-6} $ and $R_{18.1} < 1.2 \times 10^{-5} $.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-07</td>
<td style='padding: 8px;'>MEG: Medical Knowledge-Augmented Large Language Models for Question Answering</td>
<td style='padding: 6px;'>Laura Cabello, Carmen Martin-Turrero, Uchenna Akujuobi, Anders S√∏gaard, Carlos Bobed</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.03883v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Question answering is a natural language understanding task that involves reasoning over both explicit context and unstated, relevant domain knowledge. Large language models (LLMs), which underpin most contemporary question answering systems, struggle to induce how concepts relate in specialized domains such as medicine. Existing medical LLMs are also costly to train. In this work, we present MEG, a parameter-efficient approach for medical knowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate graph embeddings into the LLM, enabling it to leverage external knowledge in a cost-effective way. We evaluate our method on four popular medical multiple-choice datasets and show that LLMs greatly benefit from the factual grounding provided by knowledge graph embeddings. MEG attains an average of +10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized models like BioMistral. We also show results based on Llama-3. Finally, we show that MEG's performance remains robust to the choice of graph encoder.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-30</td>
<td style='padding: 8px;'>STIED: A deep learning model for the SpatioTemporal detection of focal Interictal Epileptiform Discharges with MEG</td>
<td style='padding: 6px;'>Raquel Fern√°ndez-Mart√≠n, Alfonso Gij√≥n, Odile Feys, Elodie Juven√©, Alec Aeby, Charline Urbain, Xavier De Ti√®ge, Vincent Wens</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.23386v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Magnetoencephalography (MEG) allows the non-invasive detection of interictal epileptiform discharges (IEDs). Clinical MEG analysis in epileptic patients traditionally relies on the visual identification of IEDs, which is time consuming and partially subjective. Automatic, data-driven detection methods exist but show limited performance. Still, the rise of deep learning (DL)-with its ability to reproduce human-like abilities-could revolutionize clinical MEG practice. Here, we developed and validated STIED, a simple yet powerful supervised DL algorithm combining two convolutional neural networks with temporal (1D time-course) and spatial (2D topography) features of MEG signals inspired from current clinical guidelines. Our DL model enabled both temporal and spatial localization of IEDs in patients suffering from focal epilepsy with frequent and high amplitude spikes (FE group), with high-performance metrics-accuracy, specificity, and sensitivity all exceeding 85%-when learning from spatiotemporal features of IEDs. This performance can be attributed to our handling of input data, which mimics established clinical MEG practice. Reverse engineering further revealed that STIED encodes fine spatiotemporal features of IEDs rather than their mere amplitude. The model trained on the FE group also showed promising results when applied to a separate group of presurgical patients with different types of refractory focal epilepsy, though further work is needed to distinguish IEDs from physiological transients. This study paves the way of incorporating STIED and DL algorithms into the routine clinical MEG evaluation of epilepsy.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-28</td>
<td style='padding: 8px;'>NeuGPT: Unified multi-modal Neural GPT</td>
<td style='padding: 6px;'>Yiqian Yang, Yiqun Duan, Hyejeong Jo, Qiang Zhang, Renjing Xu, Oiwi Parker Jones, Xuming Hu, Chin-teng Lin, Hui Xiong</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.20916v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper introduces NeuGPT, a groundbreaking multi-modal language generation model designed to harmonize the fragmented landscape of neural recording research. Traditionally, studies in the field have been compartmentalized by signal type, with EEG, MEG, ECoG, SEEG, fMRI, and fNIRS data being analyzed in isolation. Recognizing the untapped potential for cross-pollination and the adaptability of neural signals across varying experimental conditions, we set out to develop a unified model capable of interfacing with multiple modalities. Drawing inspiration from the success of pre-trained large models in NLP, computer vision, and speech processing, NeuGPT is architected to process a diverse array of neural recordings and interact with speech and text data. Our model mainly focus on brain-to-text decoding, improving SOTA from 6.94 to 12.92 on BLEU-1 and 6.93 to 13.06 on ROUGE-1F. It can also simulate brain signals, thereby serving as a novel neural interface. Code is available at \href{https://github.com/NeuSpeech/NeuGPT}{NeuSpeech/NeuGPT (https://github.com/NeuSpeech/NeuGPT) .}</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-25</td>
<td style='padding: 8px;'>Resolving Domain Shift For Representations Of Speech In Non-Invasive Brain Recordings</td>
<td style='padding: 6px;'>Jeremiah Ridge, Oiwi Parker Jones</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.19986v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Machine learning techniques have enabled researchers to leverage neuroimaging data to decode speech from brain activity, with some amazing recent successes achieved by applications built using invasive devices. However, research requiring surgical implants has a number of practical limitations. Non-invasive neuroimaging techniques provide an alternative but come with their own set of challenges, the limited scale of individual studies being among them. Without the ability to pool the recordings from different non-invasive studies, data on the order of magnitude needed to leverage deep learning techniques to their full potential remains out of reach. In this work, we focus on non-invasive data collected using magnetoencephalography (MEG). We leverage two different, leading speech decoding models to investigate how an adversarial domain adaptation framework augments their ability to generalize across datasets. We successfully improve the performance of both models when training across multiple datasets. To the best of our knowledge, this study is the first ever application of feature-level, deep learning based harmonization for MEG neuroimaging data. Our analysis additionally offers further evidence of the impact of demographic features on neuroimaging data, demonstrating that participant age strongly affects how machine learning models solve speech decoding tasks using MEG data. Lastly, in the course of this study we produce a new open-source implementation of one of these models to the benefit of the broader scientific community.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-27</td>
<td style='padding: 8px;'>NeuroAI for AI Safety</td>
<td style='padding: 6px;'>Patrick Mineault, Niccol√≤ Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.18526v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-21</td>
<td style='padding: 8px;'>Evaluating Representational Similarity Measures from the Lens of Functional Correspondence</td>
<td style='padding: 6px;'>Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.14633v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-25</td>
<td style='padding: 8px;'>A prescriptive theory for brain-like inference</td>
<td style='padding: 6px;'>Hadi Vafaii, Dekel Galor, Jacob L. Yates</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.19315v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The Evidence Lower Bound (ELBO) is a widely used objective for training deep generative models, such as Variational Autoencoders (VAEs). In the neuroscience literature, an identical objective is known as the variational free energy, hinting at a potential unified framework for brain function and machine learning. Despite its utility in interpreting generative models, including diffusion models, ELBO maximization is often seen as too broad to offer prescriptive guidance for specific architectures in neuroscience or machine learning. In this work, we show that maximizing ELBO under Poisson assumptions for general sequence data leads to a spiking neural network that performs Bayesian posterior inference through its membrane potential dynamics. The resulting model, the iterative Poisson VAE (iP-VAE), has a closer connection to biological neurons than previous brain-inspired predictive coding models based on Gaussian assumptions. Compared to amortized and iterative VAEs, iP-VAElearns sparser representations and exhibits superior generalization to out-of-distribution samples. These findings suggest that optimizing ELBO, combined with Poisson assumptions, provides a solid foundation for developing prescriptive theories in NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-09</td>
<td style='padding: 8px;'>Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models</td>
<td style='padding: 6px;'>Emily Cheng, Richard J. Antonello</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.05771v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first "composition" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-07-22</td>
<td style='padding: 8px;'>Predictive Coding Networks and Inference Learning: Tutorial and Survey</td>
<td style='padding: 6px;'>Bj√∂rn van Zwol, Ro Jefferson, Egon L. van den Broek</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2407.04117v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent years have witnessed a growing call for renewed emphasis on neuroscience-inspired approaches in artificial intelligence research, under the banner of NeuroAI. A prime example of this is predictive coding networks (PCNs), based on the neuroscientific framework of predictive coding. This framework views the brain as a hierarchical Bayesian inference model that minimizes prediction errors through feedback connections. Unlike traditional neural networks trained with backpropagation (BP), PCNs utilize inference learning (IL), a more biologically plausible algorithm that explains patterns of neural activity that BP cannot. Historically, IL has been more computationally intensive, but recent advancements have demonstrated that it can achieve higher efficiency than BP with sufficient parallelization. Furthermore, PCNs can be mathematically considered a superset of traditional feedforward neural networks (FNNs), significantly extending the range of trainable architectures. As inherently probabilistic (graphical) latent variable models, PCNs provide a versatile framework for both supervised learning and unsupervised (generative) modeling that goes beyond traditional artificial neural networks. This work provides a comprehensive review and detailed formal specification of PCNs, particularly situating them within the context of modern ML methods. Additionally, we introduce a Python library (PRECO) for practical implementation. This positions PC as a promising framework for future ML innovations.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-10-29</td>
<td style='padding: 8px;'>Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis</td>
<td style='padding: 6px;'>Mitchell Ostrow, Adam Eisen, Leo Kozachkov, Ila Fiete</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2306.10168v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-05-25</td>
<td style='padding: 8px;'>Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture</td>
<td style='padding: 6px;'>Galen Pogoncheff, Jacob Granley, Michael Beyeler</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2305.11275v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-09</td>
<td style='padding: 8px;'>A Deep Probabilistic Spatiotemporal Framework for Dynamic Graph Representation Learning with Application to Brain Disorder Identification</td>
<td style='padding: 6px;'>Sin-Yee Yap, Junn Yong Loo, Chee-Ming Ting, Fuad Noman, Raphael C. -W. Phan, Adeel Razi, David L. Dowe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2302.07243v4' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent applications of pattern recognition techniques on brain connectome classification using functional connectivity (FC) are shifting towards acknowledging the non-Euclidean topology and dynamic aspects of brain connectivity across time. In this paper, a deep spatiotemporal variational Bayes (DSVB) framework is proposed to learn time-varying topological structures in dynamic FC networks for identifying autism spectrum disorder (ASD) in human participants. The framework incorporates a spatial-aware recurrent neural network with an attention-based message passing scheme to capture rich spatiotemporal patterns across dynamic FC networks. To overcome model overfitting on limited training datasets, an adversarial training strategy is introduced to learn graph embedding models that generalize well to unseen brain networks. Evaluation on the ABIDE resting-state functional magnetic resonance imaging dataset shows that our proposed framework substantially outperforms state-of-the-art methods in identifying patients with ASD. Dynamic FC analyses with DSVB-learned embeddings reveal apparent group differences between ASD and healthy controls in brain network connectivity patterns and switching dynamics of brain states. The code is available at https://github.com/Monash-NeuroAI/Deep-Spatiotemporal-Variational-Bayes.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-03-11</td>
<td style='padding: 8px;'>Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks</td>
<td style='padding: 6px;'>Feng-Lei Fan, Yingxin Li, Hanchuan Peng, Tieyong Zeng, Fei Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2301.09245v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Throughout history, the development of artificial intelligence, particularly artificial neural networks, has been open to and constantly inspired by the increasingly deepened understanding of the brain, such as the inspiration of neocognitron, which is the pioneering work of convolutional neural networks. Per the motives of the emerging field: NeuroAI, a great amount of neuroscience knowledge can help catalyze the next generation of AI by endowing a network with more powerful capabilities. As we know, the human brain has numerous morphologically and functionally different neurons, while artificial neural networks are almost exclusively built on a single neuron type. In the human brain, neuronal diversity is an enabling factor for all kinds of biological intelligent behaviors. Since an artificial network is a miniature of the human brain, introducing neuronal diversity should be valuable in terms of addressing those essential problems of artificial networks such as efficiency, interpretability, and memory. In this Primer, we first discuss the preliminaries of biological neuronal diversity and the characteristics of information transmission and processing in a biological neuron. Then, we review studies of designing new neurons for artificial networks. Next, we discuss what gains can neuronal diversity bring into artificial networks and exemplary applications in several important fields. Lastly, we discuss the challenges and future directions of neuronal diversity to explore the potential of NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2022-12-08</td>
<td style='padding: 8px;'>A Rubric for Human-like Agents and NeuroAI</td>
<td style='padding: 6px;'>Ida Momennejad</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2212.04401v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Researchers across cognitive, neuro-, and computer sciences increasingly reference human-like artificial intelligence and neuroAI. However, the scope and use of the terms are often inconsistent. Contributed research ranges widely from mimicking behaviour, to testing machine learning methods as neurally plausible hypotheses at the cellular or functional levels, or solving engineering problems. However, it cannot be assumed nor expected that progress on one of these three goals will automatically translate to progress in others. Here a simple rubric is proposed to clarify the scope of individual contributions, grounded in their commitments to human-like behaviour, neural plausibility, or benchmark/engineering goals. This is clarified using examples of weak and strong neuroAI and human-like agents, and discussing the generative, corroborate, and corrective ways in which the three dimensions interact with one another. The author maintains that future progress in artificial intelligence will need strong interactions across the disciplines, with iterative feedback loops and meticulous validity tests, leading to both known and yet-unknown advances that may span decades to come.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>S2S2: Semantic Stacking for Robust Semantic Segmentation in Medical Imaging</td>
<td style='padding: 6px;'>Yimu Pan, Sitao Zhang, Alison D. Gernand, Jeffery A. Goldstein, James Z. Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.13156v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Robustness and generalizability in medical image segmentation are often hindered by scarcity and limited diversity of training data, which stands in contrast to the variability encountered during inference. While conventional strategies -- such as domain-specific augmentation, specialized architectures, and tailored training procedures -- can alleviate these issues, they depend on the availability and reliability of domain knowledge. When such knowledge is unavailable, misleading, or improperly applied, performance may deteriorate. In response, we introduce a novel, domain-agnostic, add-on, and data-driven strategy inspired by image stacking in image denoising. Termed ``semantic stacking,'' our method estimates a denoised semantic representation that complements the conventional segmentation loss during training. This method does not depend on domain-specific assumptions, making it broadly applicable across diverse image modalities, model architectures, and augmentation techniques. Through extensive experiments, we validate the superiority of our approach in improving segmentation performance under diverse conditions. Code is available at https://github.com/ymp5078/Semantic-Stacking.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>Continuous Patient Monitoring with AI: Real-Time Analysis of Video in Hospital Care Settings</td>
<td style='padding: 6px;'>Paolo Gabriel, Peter Rehani, Tyler Troy, Tiffany Wyatt, Michael Choma, Narinder Singh</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.13152v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study introduces an AI-driven platform for continuous and passive patient monitoring in hospital settings, developed by LookDeep Health. Leveraging advanced computer vision, the platform provides real-time insights into patient behavior and interactions through video analysis, securely storing inference results in the cloud for retrospective evaluation. The dataset, compiled in collaboration with 11 hospital partners, encompasses over 300 high-risk fall patients and over 1,000 days of inference, enabling applications such as fall detection and safety monitoring for vulnerable patient populations. To foster innovation and reproducibility, an anonymized subset of this dataset is publicly available. The AI system detects key components in hospital rooms, including individual presence and role, furniture location, motion magnitude, and boundary crossings. Performance evaluation demonstrates strong accuracy in object detection (macro F1-score = 0.92) and patient-role classification (F1-score = 0.98), as well as reliable trend analysis for the "patient alone" metric (mean logistic regression accuracy = 0.82 \pm 0.15). These capabilities enable automated detection of patient isolation, wandering, or unsupervised movement-key indicators for fall risk and other adverse events. This work establishes benchmarks for validating AI-driven patient monitoring systems, highlighting the platform's potential to enhance patient safety and care by providing continuous, data-driven insights into patient behavior and interactions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>Active Reinforcement Learning Strategies for Offline Policy Improvement</td>
<td style='padding: 6px;'>Ambedkar Dukkipati, Ranga Shaarad Ayyagari, Bodhisattwa Dasgupta, Parag Dutta, Prabhas Reddy Onteru</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.13106v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Learning agents that excel at sequential decision-making tasks must continuously resolve the problem of exploration and exploitation for optimal learning. However, such interactions with the environment online might be prohibitively expensive and may involve some constraints, such as a limited budget for agent-environment interactions and restricted exploration in certain regions of the state space. Examples include selecting candidates for medical trials and training agents in complex navigation environments. This problem necessitates the study of active reinforcement learning strategies that collect minimal additional experience trajectories by reusing existing offline data previously collected by some unknown behavior policy. In this work, we propose a representation-aware uncertainty-based active trajectory collection method that intelligently decides interaction strategies that consider the distribution of the existing offline data. With extensive experimentation, we demonstrate that our proposed method reduces additional online interaction with the environment by up to 75% over competitive baselines across various continuous control environments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>3D MedDiffusion: A 3D Medical Diffusion Model for Controllable and High-quality Medical Image Generation</td>
<td style='padding: 6px;'>Haoshen Wang, Zhentao Liu, Kaicong Sun, Xiaodong Wang, Dinggang Shen, Zhiming Cui</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.13059v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The generation of medical images presents significant challenges due to their high-resolution and three-dimensional nature. Existing methods often yield suboptimal performance in generating high-quality 3D medical images, and there is currently no universal generative framework for medical imaging. In this paper, we introduce the 3D Medical Diffusion (3D MedDiffusion) model for controllable, high-quality 3D medical image generation. 3D MedDiffusion incorporates a novel, highly efficient Patch-Volume Autoencoder that compresses medical images into latent space through patch-wise encoding and recovers back into image space through volume-wise decoding. Additionally, we design a new noise estimator to capture both local details and global structure information during diffusion denoising process. 3D MedDiffusion can generate fine-detailed, high-resolution images (up to 512x512x512) and effectively adapt to various downstream tasks as it is trained on large-scale datasets covering CT and MRI modalities and different anatomical regions (from head to leg). Experimental results demonstrate that 3D MedDiffusion surpasses state-of-the-art methods in generative quality and exhibits strong generalizability across tasks such as sparse-view CT reconstruction, fast MRI reconstruction, and data augmentation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>Design, fabrication and initial test of a novel 3D-Trench sensor utilizing 8-inch CMOS compatible technology</td>
<td style='padding: 6px;'>Manwen Liu, Huimin Ji, Wenzheng Cheng, Le Zhang, Zheng Li, Bo Tang, Peng Zhang, Wenjuan Xiong, Trevor Vickey, E. Giulio Villani, Zhihua Li, Dengfeng Zhang, Jun Luo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.13016v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The 3D silicon sensor has demonstrated excellent performances (signal collection, detection efficiency, power consumption, etc.) comparable or even better with respect to the traditional planar sensor of the ATLAS Detector at the Large Hadron Collider (LHC), especially after the high irradiation fluence, mainly due to the shorter drift length of the generated carriers. These characteristics have made it the most attractive technology for the detection and track reconstruction of charged particles for the High Energy Physics (HEP). In addition, its application is also being explored in astronomy, microdosimetry and medical imaging. This paper will present the design and fabrication of a novel 3D-Trench sensor which features an enclosed deep trench surrounding the central columnar cathode. This novel sensor has been fabricated on the 8-inch COMS pilot line at the Institute of Microelectronics of the Chinese Academy of Sciences (IMECAS) where ultra-narrow etch width of 0.5 {\mu}m and the ultra-high depth-to-width ratio (aspect ratio) (>70) have been achieved. Its preliminary simulation and characterization results including electrostatic potential, electric field, Current-Voltage (IV), Capacitance-Voltage (CV), Charge Collection Efficiency (CCE) and Timing Performance before irradiation will be presented in this paper.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>Future Aspects in Human Action Recognition: Exploring Emerging Techniques and Ethical Influences</td>
<td style='padding: 6px;'>Antonios Gasteratos, Stavros N. Moutsis, Konstantinos A. Tsintotas, Yiannis Aloimonos</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12990v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Visual-based human action recognition can be found in various application fields, e.g., surveillance systems, sports analytics, medical assistive technologies, or human-robot interaction frameworks, and it concerns the identification and classification of individuals' activities within a video. Since actions typically occur over a sequence of consecutive images, it is particularly challenging due to the inclusion of temporal analysis, which introduces an extra layer of complexity. However, although multiple approaches try to handle temporal analysis, there are still difficulties because of their computational cost and lack of adaptability. Therefore, different types of vision data, containing transition information between consecutive images, provided by next-generation hardware sensors will guide the robotics community in tackling the problem of human action recognition. On the other hand, while there is a plethora of still-image datasets, that researchers can adopt to train new artificial intelligence models, videos representing human activities are of limited capabilities, e.g., small and unbalanced datasets or selected without control from multiple sources. To this end, generating new and realistic synthetic videos is possible since labeling is performed throughout the data creation process, while reinforcement learning techniques can permit the avoidance of considerable dataset dependence. At the same time, human factors' involvement raises ethical issues for the research community, as doubts and concerns about new technologies already exist.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>The IBEX Imaging Knowledge-Base: A Community Resource Enabling Adoption and Development of Immunofluoresence Imaging Methods</td>
<td style='padding: 6px;'>Ziv Yaniv, Ifeanyichukwu U. Anidi, Leanne Arakkal, Armando J. Arroyo-Mej√≠as, Rebecca T. Beuschel, Katy B√∂rner, Colin J. Chu, Beatrice Clark, Menna R. Clatworthy, Jake Colautti, Fabian Coscia, Joshua Croteau, Saven Denha, Rose Dever, Walderez O. Dutra, Sonja Fritzsche, Spencer Fullam, Michael Y. Gerner, Anita Gola, Kenneth J. Gollob, Jonathan M. Hernandez, Jyh Liang Hor, Hiroshi Ichise, Zhixin Jing, Danny Jonigk, Evelyn Kandov, Wolfgang Kastenm√ºller, Joshua F. E. Koenig, Aanandita Kothurkar, Rosa K. Kortekaas, Alexandra Y. Kreins, Ian T. Lamborn, Yuri Lin, Katia Luciano Pereira Morais, Aleksandra Lunich, Jean C. S. Luz, Ryan B. MacDonald, Chen Makranz, Vivien I. Maltez, John E. McDonough, Ryan V. Moriarty, Juan M. Ocampo-Godinez, Vitoria M. Olyntho, Annette Oxenius, Kartika Padhan, Kirsten Remmert, Nathan Richoz, Edward C. Schrom, Wanjing Shang, Lihong Shi, Rochelle M. Shih, Emily Speranza, Salome Stierli, Sarah A. Teichmann, Tibor Z. Veres, Megan Vierhout, Brianna T. Wachter, Margaret Williams, Nathan Zangger, Ronald N. Germain, Andrea J. Radtke</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12965v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The iterative bleaching extends multiplexity (IBEX) Knowledge-Base is a central portal for researchers adopting IBEX and related 2D and 3D immunofluorescence imaging methods. The design of the Knowledge-Base is modeled after efforts in the open-source software community and includes three facets: a development platform (GitHub), static website, and service for data archiving. The Knowledge-Base facilitates the practice of open science throughout the research life cycle by providing validation data for recommended and non-recommended reagents, e.g., primary and secondary antibodies. In addition to reporting negative data, the Knowledge-Base empowers method adoption and evolution by providing a venue for sharing protocols, videos, datasets, software, and publications. A dedicated discussion forum fosters a sense of community among researchers while addressing questions not covered in published manuscripts. Together, scientists from around the world are advancing scientific discovery at a faster pace, reducing wasted time and effort, and instilling greater confidence in the resulting data.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>4DRGS: 4D Radiative Gaussian Splatting for Efficient 3D Vessel Reconstruction from Sparse-View Dynamic DSA Images</td>
<td style='padding: 6px;'>Zhentao Liu, Ruyi Zha, Huangxuan Zhao, Hongdong Li, Zhiming Cui</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12919v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Reconstructing 3D vessel structures from sparse-view dynamic digital subtraction angiography (DSA) images enables accurate medical assessment while reducing radiation exposure. Existing methods often produce suboptimal results or require excessive computation time. In this work, we propose 4D radiative Gaussian splatting (4DRGS) to achieve high-quality reconstruction efficiently. In detail, we represent the vessels with 4D radiative Gaussian kernels. Each kernel has time-invariant geometry parameters, including position, rotation, and scale, to model static vessel structures. The time-dependent central attenuation of each kernel is predicted from a compact neural network to capture the temporal varying response of contrast agent flow. We splat these Gaussian kernels to synthesize DSA images via X-ray rasterization and optimize the model with real captured ones. The final 3D vessel volume is voxelized from the well-trained kernels. Moreover, we introduce accumulated attenuation pruning and bounded scaling activation to improve reconstruction quality. Extensive experiments on real-world patient data demonstrate that 4DRGS achieves impressive results in 5 minutes training, which is 32x faster than the state-of-the-art method. This underscores the potential of 4DRGS for real-world clinics.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>PT: A Plain Transformer is Good Hospital Readmission Predictor</td>
<td style='padding: 6px;'>Zhenyi Fan, Jiaqi Li, Dongyu Luo, Yuqi Yuan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12909v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Hospital readmission prediction is critical for clinical decision support, aiming to identify patients at risk of returning within 30 days post-discharge. High readmission rates often indicate inadequate treatment or post-discharge care, making effective prediction models essential for optimizing resources and improving patient outcomes. We propose PT, a Transformer-based model that integrates Electronic Health Records (EHR), medical images, and clinical notes to predict 30-day all-cause hospital readmissions. PT extracts features from raw data and uses specialized Transformer blocks tailored to the data's complexity. Enhanced with Random Forest for EHR feature selection and test-time ensemble techniques, PT achieves superior accuracy, scalability, and robustness. It performs well even when temporal information is missing. Our main contributions are: (1)Simplicity: A powerful and efficient baseline model outperforming existing ones in prediction accuracy; (2)Scalability: Flexible handling of various features from different modalities, achieving high performance with just clinical notes or EHR data; (3)Robustness: Strong predictive performance even with missing or unclear temporal data.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-17</td>
<td style='padding: 8px;'>Scatter correction for photon-counting detector based CBCT imaging</td>
<td style='padding: 6px;'>Xin Zhang, Ting Su, Jiongtao Zhu, Hairong Zheng, Dong Liang, Yongshuai Ge</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.12862v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Objective: The aim of this study is to validate the effectiveness of an energy-modulated scatter correction method in suppressing scatter in photon-counting detector (PCD)-based cone beam CT (CBCT) imaging. Approach: The scatter correction method, named e-Grid, which was initially applied to dual-layer flat-panel detector (DLFPD)-based CBCT imaging, was tested for its performance in PCD-CBCT imaging. Benchtop PCD-CBCT imaging experiments were conducted to verify the effectiveness of the e-Grid method. Additionally, quantitative metrics were measured from these experimental results. Main results: It was found that the use of the e-Grid method could significantly eliminate cupping artifacts caused by Compton scatter in PCD-CBCT imaging. Meanwhile, its effectiveness was observed in both low- and high-energy images, as well as for objects of varying sizes. Quantitative results showed that the e-Grid method could reduce scatter artifacts by at least 71% in low-energy images and 75% in high-energy images. Significance: It was demonstrated that the scatter correction method originally applied to DLFPD-based CBCT could also perform well in PCD-CBCT, showing that the e-Grid method has great potential for application in other spectral CBCT imaging systems.</td>
</tr>
</tbody>
</table>

