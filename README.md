<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2025-10-17</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>T3former: Temporal Graph Classification with Topological Machine Learning</td>
<td style='padding: 6px;'>Md. Joshem Uddin, Soham Changani, Baris Coskunuzer</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13789v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Temporal graph classification plays a critical role in applications such as cybersecurity, brain connectivity analysis, social dynamics, and traffic monitoring. Despite its significance, this problem remains underexplored compared to temporal link prediction or node forecasting. Existing methods often rely on snapshot-based or recurrent architectures that either lose fine-grained temporal information or struggle with long-range dependencies. Moreover, local message-passing approaches suffer from oversmoothing and oversquashing, limiting their ability to capture complex temporal structures.   We introduce T3former, a novel Topological Temporal Transformer that leverages sliding-window topological and spectral descriptors as first-class tokens, integrated via a specialized Descriptor-Attention mechanism. This design preserves temporal fidelity, enhances robustness, and enables principled cross-modal fusion without rigid discretization. T3former achieves state-of-the-art performance across multiple benchmarks, including dynamic social networks, brain functional connectivity datasets, and traffic networks. It also offers theoretical guarantees of stability under temporal and structural perturbations. Our results highlight the power of combining topological and spectral insights for advancing the frontier of temporal graph learning.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Scaling Vision Transformers for Functional MRI with Flat Maps</td>
<td style='padding: 6px;'>Connor Lane, Daniel Z. Kaplan, Tanishq Mathew Abraham, Paul S. Scotti</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13768v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A key question for adapting modern deep learning architectures to functional MRI (fMRI) is how to represent the data for model input. To bridge the modality gap between fMRI and natural images, we transform the 4D volumetric fMRI data into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K hours of fMRI flat map videos from the Human Connectome Project using the spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI modeling performance improves with dataset size according to a strict power scaling law. Downstream classification benchmarks show that our model learns rich representations supporting both fine-grained state decoding across subjects, as well as subject-specific trait decoding across changes in brain state. This work is part of an ongoing open science project to build foundation models for fMRI data. Our code and datasets are available at https://github.com/MedARC-AI/fmri-fm.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Circle of Willis Centerline Graphs: A Dataset and Baseline Algorithm</td>
<td style='padding: 6px;'>Fabio Musio, Norman Juchler, Kaiyuan Yang, Suprosanna Shit, Chinmay Prabhakar, Bjoern Menze, Sven Hirsch</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13720v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The Circle of Willis (CoW) is a critical network of arteries in the brain, often implicated in cerebrovascular pathologies. Voxel-level segmentation is an important first step toward an automated CoW assessment, but a full quantitative analysis requires centerline representations. However, conventional skeletonization techniques often struggle to extract reliable centerlines due to the CoW's complex geometry, and publicly available centerline datasets remain scarce. To address these challenges, we used a thinning-based skeletonization algorithm to extract and curate centerline graphs and morphometric features from the TopCoW dataset, which includes 200 stroke patients, each imaged with MRA and CTA. The curated graphs were used to develop a baseline algorithm for centerline and feature extraction, combining U-Net-based skeletonization with A* graph connection. Performance was evaluated on a held-out test set, focusing on anatomical accuracy and feature robustness. Further, we used the extracted features to predict the frequency of fetal PCA variants, confirm theoretical bifurcation optimality relations, and detect subtle modality differences. The baseline algorithm consistently reconstructed graph topology with high accuracy (F1 = 1), and the average Euclidean node distance between reference and predicted graphs was below one voxel. Features such as segment radius, length, and bifurcation ratios showed strong robustness, with median relative errors below 5% and Pearson correlations above 0.95. Our results demonstrate the utility of learning-based skeletonization combined with graph connection for anatomically plausible centerline extraction. We emphasize the importance of going beyond simple voxel-based measures by evaluating anatomical accuracy and feature robustness. The dataset and baseline algorithm have been released to support further method development and clinical research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Jacobian-Based Interpretation of Nonlinear Neural Encoding Model</td>
<td style='padding: 6px;'>Xiaohui Gao, Haoran Yang, Yue Cheng, Mengfei Zuo, Yiheng Liu, Peiyang Li, Xintao Hu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13688v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In recent years, the alignment between artificial neural network (ANN) embeddings and blood oxygenation level dependent (BOLD) responses in functional magnetic resonance imaging (fMRI) via neural encoding models has significantly advanced research on neural representation mechanisms and interpretability in the brain. However, these approaches remain limited in characterizing the brain's inherently nonlinear response properties. To address this, we propose the Jacobian-based Nonlinearity Evaluation (JNE), an interpretability metric for nonlinear neural encoding models. JNE quantifies nonlinearity by statistically measuring the dispersion of local linear mappings (Jacobians) from model representations to predicted BOLD responses, thereby approximating the nonlinearity of BOLD signals. Centered on proposing JNE as a novel interpretability metric, we validated its effectiveness through controlled simulation experiments on various activation functions and network architectures, and further verified it on real fMRI data, demonstrating a hierarchical progression of nonlinear characteristics from primary to higher-order visual cortices, consistent with established cortical organization. We further extended JNE with Sample-Specificity (JNE-SS), revealing stimulus-selective nonlinear response patterns in functionally specialized brain regions. As the first interpretability metric for quantifying nonlinear responses, JNE provides new insights into brain information processing. Code available at https://github.com/Gaitxh/JNE.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>EEGChaT: A Transformer-Based Modular Channel Selector for SEEG Analysis</td>
<td style='padding: 6px;'>Chen Wang, Yansen Wang, Dongqi Han, Zilong Wang, Dongsheng Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13592v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Analyzing stereoelectroencephalography (SEEG) signals is critical for brain-computer interface (BCI) applications and neuroscience research, yet poses significant challenges due to the large number of input channels and their heterogeneous relevance. Traditional channel selection methods struggle to scale or provide meaningful interpretability for SEEG data. In this work, we propose EEGChaT, a novel Transformer-based channel selection module designed to automatically identify the most task-relevant channels in SEEG recordings. EEGChaT introduces Channel Aggregation Tokens (CATs) to aggregate information across channels, and leverages an improved Attention Rollout technique to compute interpretable, quantitative channel importance scores. We evaluate EEGChaT on the DuIN dataset, demonstrating that integrating EEGChaT with existing classification models consistently improves decoding accuracy, achieving up to 17\% absolute gains. Furthermore, the channel weights produced by EEGChaT show substantial overlap with manually selected channels, supporting the interpretability of the approach. Our results suggest that EEGChaT is an effective and generalizable solution for channel selection in high-dimensional SEEG analysis, offering both enhanced performance and insights into neural signal relevance.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>ArtNet: Hierarchical Clustering-Based Artificial Netlist Generator for ML and DTCO Application</td>
<td style='padding: 6px;'>Andrew B. Kahng. Seokhyeong Kang, Seonghyeon Park, Dooseok Yoon</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13582v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In advanced nodes, optimization of power, performance and area (PPA) has become highly complex and challenging. Machine learning (ML) and design-technology co-optimization (DTCO) provide promising mitigations, but face limitations due to a lack of diverse training data as well as long design flow turnaround times (TAT). We propose ArtNet, a novel artificial netlist generator designed to tackle these issues. Unlike previous methods, ArtNet replicates key topological characteristics, enhancing ML model generalization and supporting broader design space exploration for DTCO. By producing realistic artificial datasets that moreclosely match given target parameters, ArtNet enables more efficient PPAoptimization and exploration of flows and design enablements. In the context of CNN-based DRV prediction, ArtNet's data augmentationimproves F1 score by 0.16 compared to using only the original (real) dataset. In the DTCO context, ArtNet-generated mini-brains achieve a PPA match up to 97.94%, demonstrating close alignment with design metrics of targeted full-scale block designs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Steerable Conditional Diffusion for Domain Adaptation in PET Image Reconstruction</td>
<td style='padding: 6px;'>George Webber, Alexander Hammers, Andrew P. King, Andrew J. Reader</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13441v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Diffusion models have recently enabled state-of-the-art reconstruction of positron emission tomography (PET) images while requiring only image training data. However, domain shift remains a key concern for clinical adoption: priors trained on images from one anatomy, acquisition protocol or pathology may produce artefacts on out-of-distribution data. We propose integrating steerable conditional diffusion (SCD) with our previously-introduced likelihood-scheduled diffusion (PET-LiSch) framework to improve the alignment of the diffusion model's prior to the target subject. At reconstruction time, for each diffusion step, we use low-rank adaptation (LoRA) to align the diffusion model prior with the target domain on the fly. Experiments on realistic synthetic 2D brain phantoms demonstrate that our approach suppresses hallucinated artefacts under domain shift, i.e. when our diffusion model is trained on perturbed images and tested on normal anatomy, our approach suppresses the hallucinated structure, outperforming both OSEM and diffusion model baselines qualitatively and quantitatively. These results provide a proof-of-concept that steerable priors can mitigate domain shift in diffusion-based PET reconstruction and motivate future evaluation on real data.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Working Memory Functional Connectivity Analysis for Dementia Classification using EEG</td>
<td style='padding: 6px;'>Shivani Ranjan, Anant Jain, Robin Badal, Amit Kumar, Harshal Shende, Deepak Joshi, Pramod Yadav, Lalan Kumar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13399v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Background: Dementia, particularly Alzheimer's Disease (AD), is a progressive neurodegenerative disorder marked by cognitive decline. Early detection, especially at the Mild Cognitive Impairment (MCI) stage, is essential for timely intervention. Working Memory (WM) impairment is a key early indicator of neurodegeneration, affecting higher cognitive processes. Electroencephalography (EEG), with its high temporal resolution, offers a cost-effective method to assess brain dynamics. This study investigates WM-related EEG functional connectivity (FC) to identify brain network alterations across dementia stages. Methods: EEG signals were recorded from 24 participants (8 AD, 8 MCI, and 8 healthy controls) during WM tasks, including encoding, recall, and retrieval stages. Data preprocessing involved noise reduction and feature extraction using Spherical and Head Harmonic Decomposition (SHD, HHD). FC was quantified using Cross-Plot Transition Entropy (CPTE) and Phase Lag Index (PLI). Network metrics such as Degree and Eigenvector Centrality were analyzed using Support Vector Machine, Random Forest, and XGBoost classifiers. Results: The CPTE-based connectivity metrics outperformed the traditional PLI approach in differentiating dementia stages, attaining a peak classification accuracy of 97.53% during the retrieval phase with the Random Forest model. A connectivity threshold of 0.5 was optimal for network discrimination. SHD and HHD features also demonstrated strong discriminative potential. AD subjects exhibited higher synchronization patterns during WM tasks than healthy controls. Conclusions: The integration of WM tasks with EEG-based FC analysis provides a robust framework for dementia classification. The proposed CPTE-based approach offers a robust, scalable, non-invasive, and effective diagnostic tool for early detection and monitoring of neurodegenerative diseases.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Target Controllability Score</td>
<td style='padding: 6px;'>Kazuhiro Sato</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13354v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We introduce the target controllability score (TCS), a concept for evaluating node importance under actuator constraints and designated target objectives, formulated within a virtual system setting. The TCS consists of the target volumetric controllability score (VCS) and the target average energy controllability score (AECS), each defined as an optimal solution to a convex optimization problem associated with the output controllability Gramian. We establish the existence and uniqueness (for almost all time horizons) and develop a projected gradient method for their computation. To enable scalability, we construct a target-only reduced virtual system and derive non-asymptotic bounds showing that weak cross-coupling and a low or negative logarithmic norm of the system matrix yield accurate approximations of target VCS/AECS, particularly over short or moderate time horizons. Experiments on human brain networks reveal a clear trade-off: at short horizons, both target VCS and target AECS are well approximated by their reduced formulations, while at long horizons, target AECS remains robust but target VCS deteriorates.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain</td>
<td style='padding: 6px;'>Jingmin An, Yilong Song, Ruolin Yang, Nai Ding, Lingxi Lu, Yuxuan Wang, Wei Wang, Chu Zhuang, Qian Wang, Fang Fang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13255v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large Language Models (LLMs) demonstrate human-level or even superior language abilities, effectively modeling syntactic structures, yet the specific computational modules responsible remain unclear. A key question is whether LLM behavioral capabilities stem from mechanisms akin to those in the human brain. To address these questions, we introduce the Hierarchical Frequency Tagging Probe (HFTP), a tool that utilizes frequency-domain analysis to identify neuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP) neurons) and cortical regions (via intracranial recordings) encoding syntactic structures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama 2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human brain relies on distinct cortical regions for different syntactic levels. Representational similarity analysis reveals a stronger alignment between LLM representations and the left hemisphere of the brain (dominant in language processing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows greater brain similarity than Gemma, while Llama 3.1 shows less alignment with the brain compared to Llama 2. These findings offer new insights into the interpretability of LLM behavioral improvements, raising questions about whether these advancements are driven by human-like or non-human-like mechanisms, and establish HFTP as a valuable tool bridging computational linguistics and cognitive neuroscience. This project is available at https://github.com/LilTiger/HFTP.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>DistilCLIP-EEG: Enhancing Epileptic Seizure Detection Through Multi-modal Learning and Knowledge Distillation</td>
<td style='padding: 6px;'>Zexin Wang, Lin Shi, Haoyu Wu, Junru Luo, Xiangzeng Kong, Jun Qi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13497v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Epilepsy is a prevalent neurological disorder marked by sudden, brief episodes of excessive neuronal activity caused by abnormal electrical discharges, which may lead to some mental disorders. Most existing deep learning methods for epilepsy detection rely solely on unimodal EEG signals, neglecting the potential benefits of multimodal information. To address this, we propose a novel multimodal model, DistilCLIP-EEG, based on the CLIP framework, which integrates both EEG signals and text descriptions to capture comprehensive features of epileptic seizures. The model involves an EEG encoder based on the Conformer architecture as a text encoder, the proposed Learnable BERT (BERT-LP) as prompt learning within the encoders. Both operate in a shared latent space for effective cross-modal representation learning. To enhance efficiency and adaptability, we introduce a knowledge distillation method where the trained DistilCLIP-EEG serves as a teacher to guide a more compact student model to reduce training complexity and time. On the TUSZ, AUBMC, and CHB-MIT datasets, both the teacher and student models achieved accuracy rates exceeding 97%. Across all datasets, the F1-scores were consistently above 0.94, demonstrating the robustness and reliability of the proposed framework. Moreover, the student model's parameter count and model size are approximately 58.1% of those of the teacher model, significantly reducing model complexity and storage requirements while maintaining high performance. These results highlight the potential of our proposed model for EEG-based epilepsy detection and establish a solid foundation for deploying lightweight models in resource-constrained settings.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Working Memory Functional Connectivity Analysis for Dementia Classification using EEG</td>
<td style='padding: 6px;'>Shivani Ranjan, Anant Jain, Robin Badal, Amit Kumar, Harshal Shende, Deepak Joshi, Pramod Yadav, Lalan Kumar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13399v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Background: Dementia, particularly Alzheimer's Disease (AD), is a progressive neurodegenerative disorder marked by cognitive decline. Early detection, especially at the Mild Cognitive Impairment (MCI) stage, is essential for timely intervention. Working Memory (WM) impairment is a key early indicator of neurodegeneration, affecting higher cognitive processes. Electroencephalography (EEG), with its high temporal resolution, offers a cost-effective method to assess brain dynamics. This study investigates WM-related EEG functional connectivity (FC) to identify brain network alterations across dementia stages. Methods: EEG signals were recorded from 24 participants (8 AD, 8 MCI, and 8 healthy controls) during WM tasks, including encoding, recall, and retrieval stages. Data preprocessing involved noise reduction and feature extraction using Spherical and Head Harmonic Decomposition (SHD, HHD). FC was quantified using Cross-Plot Transition Entropy (CPTE) and Phase Lag Index (PLI). Network metrics such as Degree and Eigenvector Centrality were analyzed using Support Vector Machine, Random Forest, and XGBoost classifiers. Results: The CPTE-based connectivity metrics outperformed the traditional PLI approach in differentiating dementia stages, attaining a peak classification accuracy of 97.53% during the retrieval phase with the Random Forest model. A connectivity threshold of 0.5 was optimal for network discrimination. SHD and HHD features also demonstrated strong discriminative potential. AD subjects exhibited higher synchronization patterns during WM tasks than healthy controls. Conclusions: The integration of WM tasks with EEG-based FC analysis provides a robust framework for dementia classification. The proposed CPTE-based approach offers a robust, scalable, non-invasive, and effective diagnostic tool for early detection and monitoring of neurodegenerative diseases.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models</td>
<td style='padding: 6px;'>Konstantinos Barmpas, Na Lee, Alexandros Koliousis, Yannis Panagakis, Dimitrios A. Adamos, Nikolaos Laskaris, Stefanos Zafeiriou</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13068v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) captures neural activity across multiple temporal and spectral scales, yielding signals that are rich but complex for representation learning. Recently, EEG foundation models trained to predict masked signal-tokens have shown promise for learning generalizable representations. However, their performance is hindered by their signal tokenization modules. Existing neural tokenizers fail to preserve high-frequency dynamics, limiting their ability to reconstruct EEG signals with high fidelity. We introduce NeuroRVQ, a scalable Large Brainwave Model (LBM) centered on a codebook-based tokenizer. Our tokenizer integrates: (i) multi-scale feature extraction modules that capture the full frequency neural spectrum; (ii) hierarchical residual vector quantization (RVQ) codebooks for high-resolution encoding; and, (iii) an EEG signal phase- and amplitude-aware loss function for efficient training. This design enables efficient EEG compression while supporting accurate reconstruction across all frequency bands, leading to robust generative masked modeling. Our empirical results demonstrate that NeuroRVQ achieves lower reconstruction error and outperforms existing LBMs on a variety of downstream tasks. More broadly, NeuroRVQ tokenizer establishes a strong prior for codebook-based general-purpose brainwave models, enabling advances in neural decoding, generative modeling and multimodal biosignal integration.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-14</td>
<td style='padding: 8px;'>Deep Learning-Based Visual Fatigue Detection Using Eye Gaze Patterns in VR</td>
<td style='padding: 6px;'>Numan Zafar, Johnathan Locke, Shafique Ahmad Chaudhry</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.12994v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Prolonged exposure to virtual reality (VR) systems leads to visual fatigue, impairs user comfort, performance, and safety, particularly in high-stakes or long-duration applications. Existing fatigue detection approaches rely on subjective questionnaires or intrusive physiological signals, such as EEG, heart rate, or eye-blink count, which limit their scalability and real-time applicability. This paper introduces a deep learning-based study for detecting visual fatigue using continuous eye-gaze trajectories recorded in VR. We use the GazeBaseVR dataset comprising binocular eye-tracking data from 407 participants across five immersive tasks, extract cyclopean eye-gaze angles, and evaluate six deep classifiers. Our results demonstrate that EKYT achieves up to 94% accuracy, particularly in tasks demanding high visual attention, such as video viewing and text reading. We further analyze gaze variance and subjective fatigue measures, indicating significant behavioral differences between fatigued and non-fatigued conditions. These findings establish eye-gaze dynamics as a reliable and nonintrusive modality for continuous fatigue detection in immersive VR, offering practical implications for adaptive human-computer interactions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-14</td>
<td style='padding: 8px;'>Effective Connectivity-Based Unsupervised Channel Selection Method for EEG</td>
<td style='padding: 6px;'>Neda Abdollahpour, N. Sertac Artan, Ian Daly, Mohammadreza Yazdchi, Zahra Baharlouei</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.12910v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Analyzing neural data such as Electroencephalography (EEG) data often involves dealing with high-dimensional datasets, where not all channels provide equally meaningful informa- tion. Selecting the most relevant channels is crucial for improving computational efficiency and ensuring robust insights into neural dynamics. This study introduces the Importance of Channels based on Effective Connectivity (ICEC) criterion for quantifying effective connectivity (EC) in each channel. Effective connectivity refers to the causal influence one neural region exerts over another, providing insights into the directional flow of information. Using this criterion, we propose an unsupervised channel selection method that accounts for the intensity of interactions among channels. To evaluate the proposed channel selection method, we applied it to three well-known EEG datasets across four categories. The assessment involved calculating the ICEC criterion using five effective connectivity metrics: partial directed coherence (PDC), generalized PDC (GPDC), renormalized PDC (RPDC), directed transfer function (DTF), and direct DTF (dDTF). To focus on the effect of channel selection, we employed the Common Spatial Pattern (CSP) algorithm for feature extraction and a Support Vector Machine (SVM) for classification across all participants. Results were compared with other CSP-based methods. The evaluation included comparing participant- specific accuracies with and without the proposed method across five effective connectivity metrics. The results showed consistent performance improvements and a significant reduction in the number of selected electrodes for all participants. Compared to state-of-the-art methods, our approach achieved the highest accuracies: 82% (13 out of 22 channels), 86.01% (29 out of 59 channels), and 87.56% (48 out of 118 channels) across three datasets.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-14</td>
<td style='padding: 8px;'>HEAR: An EEG Foundation Model with Heterogeneous Electrode Adaptive Representation</td>
<td style='padding: 6px;'>Zhige Chen, Chengxuan Qin, Wenlong You, Rui Liu, Congying Chu, Rui Yang, Kay Chen Tan, Jibin Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.12515v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) is an essential technique for neuroscience research and brain-computer interface (BCI) applications. Recently, large-scale EEG foundation models have been developed, exhibiting robust generalization capabilities across diverse tasks and subjects. However, the heterogeneity of EEG devices not only hinders the widespread adoption of these models but also poses significant challenges to their further scaling and development. In this paper, we introduce HEAR, the first EEG foundation model explicitly designed to support heterogeneous EEG devices, accommodating varying electrode layouts and electrode counts. HEAR employs a learnable, coordinate-based spatial embedding to map electrodes with diverse layouts and varying counts into a unified representational space. This unified spatial representation is then processed by a novel spatially-guided transformer, which effectively captures spatiotemporal dependencies across electrodes. To support the development of HEAR, we construct a large-scale EEG dataset comprising 8,782 hours of data collected from over 150 distinct electrode layouts with up to 1,132 electrodes. Experimental results demonstrate that HEAR substantially outperforms existing EEG foundation models in supporting heterogeneous EEG devices and generalizing across diverse cognitive tasks and subjects.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-14</td>
<td style='padding: 8px;'>TFGA-Net: Temporal-Frequency Graph Attention Network for Brain-Controlled Speaker Extraction</td>
<td style='padding: 6px;'>Youhao Si, Yuan Liao, Qiushi Han, Yuhang Yang, Rui Dai, Liya Huang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.12275v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The rapid development of auditory attention decoding (AAD) based on electroencephalography (EEG) signals offers the possibility EEG-driven target speaker extraction. However, how to effectively utilize the target-speaker common information between EEG and speech remains an unresolved problem. In this paper, we propose a model for brain-controlled speaker extraction, which utilizes the EEG recorded from the listener to extract the target speech. In order to effectively extract information from EEG signals, we derive multi-scale time--frequency features and further incorporate cortical topological structures that are selectively engaged during the task. Moreover, to effectively exploit the non-Euclidean structure of EEG signals and capture their global features, the graph convolutional networks and self-attention mechanism are used in the EEG encoder. In addition, to make full use of the fused EEG and speech feature and preserve global context and capture speech rhythm and prosody, we introduce MossFormer2 which combines MossFormer and RNN-Free Recurrent as separator. Experimental results on both the public Cocktail Party and KUL dataset in this paper show that our TFGA-Net model significantly outper-forms the state-of-the-art method in certain objective evaluation metrics. The source code is available at: https://github.com/LaoDa-X/TFGA-NET.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-13</td>
<td style='padding: 8px;'>Variational Mixture of Graph Neural Experts for Alzheimer's Disease Biomarker Recognition in EEG Brain Networks</td>
<td style='padding: 6px;'>Jun-En Ding, Anna Zilverstand, Shihao Yang, Albert Chih-Chieh Yang, Feng Liu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.11917v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Dementia disorders such as Alzheimer's disease (AD) and frontotemporal dementia (FTD) exhibit overlapping electrophysiological signatures in EEG that challenge accurate diagnosis. Existing EEG-based methods are limited by full-band frequency analysis that hinders precise differentiation of dementia subtypes and severity stages. We propose a variational mixture of graph neural experts (VMoGE) that integrates frequency-specific biomarker identification with structured variational inference for enhanced dementia diagnosis and staging. VMoGE employs a multi-granularity transformer to extract multi-scale temporal patterns across four frequency bands, followed by a variational graph convolutional encoder using Gaussian Markov Random Field priors. Through structured variational inference and adaptive gating, VMoGE links neural specialization to physiologically meaningful EEG frequency bands. Evaluated on two diverse datasets for both subtype classification and severity staging, VMoGE achieves superior performance with AUC improvements of +4% to +10% over state-of-the-art methods. Moreover, VMoGE provides interpretable insights through expert weights that correlate with clinical indicators and spatial patterns aligned with neuropathological signatures, facilitating EEG biomarker discovery for comprehensive dementia diagnosis and monitoring.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-12</td>
<td style='padding: 8px;'>The Cost of Simplicity: How Reducing EEG Electrodes Affects Source Localization and BCI Accuracy</td>
<td style='padding: 6px;'>Eva Guttmann-Flury, Yanyan Wei, Shan Zhao, Jian Zhao, Mohamad Sawan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.10770v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electrode density optimization in electroencephalography (EEG)-based Brain-Computer Interfaces (BCIs) requires balancing practical usability against signal fidelity, particularly for source localization. Reducing electrodes enhances portability but its effects on neural source reconstruction quality and source connectivity - treated as proxies to BCI performance - remain understudied. We address this gap through systematic evaluation of 62-, 32-, and 16-channel configurations using a fixed, fully automated processing pipeline applied to the well-characterized P300 potential. This approach's rationale is to minimize variability and bias inherent to EEG analysis by leveraging the P300's stimulus-locked reproducibility and pipeline standardization. Analyzing 63 sessions (31 subjects) from the Eye-BCI dataset with rigorous artifact correction and channel validation, we demonstrate: (1) Progressive degradation in source reconstruction quality with sparser configurations, including obscured deep neural generators and spatiotemporal distortions; (2) A novel sqrt(Re) scaling law linking electrode reduction ratio (Re) to localization accuracy - a previously unquantified relationship to the best of our knowledge; (3) While reduced configurations preserve basic P300 topography and may suffice for communicative BCIs, higher-density channels are essential for reliable deep source reconstruction. Overall, this study establishes a first step towards quantitative benchmarks for electrode selection, with critical implications for clinical BCIs requiring anatomical precision in applications like neurodegenerative disease monitoring, where compromised spatial resolution could mask pathological signatures. Most importantly, the sqrt(Re) scaling law may provide the first principled method to determine the minimal electrode density required based on acceptable error margins or expected effect sizes.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-12</td>
<td style='padding: 8px;'>Does Re-referencing Matter? Large Laplacian Filter Optimizes Single-Trial P300 BCI Performance</td>
<td style='padding: 6px;'>Eva Guttmann-Flury, Jian Zhao, Mohamad Sawan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.10733v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) provides a non-invasive window into brain activity, enabling Brain-Computer Interfaces (BCIs) for communication and control. However, their performance is limited by signal fidelity issues, among which the choice of re-referencing strategy is a pervasive but often overlooked preprocessing bias. Addressing controversies about its necessity and optimal choice, we adopted a quantified approach to evaluate four strategies - no re-referencing, Common Average Reference (CAR), small Laplacian, and large Laplacian - using 62-channels EEG (31 subjects, 2,520 trials). To our knowledge, this is the first study systematically quantifying their impact on single-trial P300 classification accuracy. Our controlled pipeline isolated re-referencing effects for source-space reconstruction (eLORETA with Phase Lag Index) and anatomically constrained classification. The large Laplacian resolves distributed P3b networks while maintaining P3a specificity, achieving the best P300 peak classification accuracy (81.57% hybrid method; 75.97% majority regions of interest). Performance follows a consistent and statistically significant hierarchy: large Laplacian > CAR > no re-reference > small Laplacian, providing a foundation for unified methodological evaluation.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>EEGChaT: A Transformer-Based Modular Channel Selector for SEEG Analysis</td>
<td style='padding: 6px;'>Chen Wang, Yansen Wang, Dongqi Han, Zilong Wang, Dongsheng Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13592v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Analyzing stereoelectroencephalography (SEEG) signals is critical for brain-computer interface (BCI) applications and neuroscience research, yet poses significant challenges due to the large number of input channels and their heterogeneous relevance. Traditional channel selection methods struggle to scale or provide meaningful interpretability for SEEG data. In this work, we propose EEGChaT, a novel Transformer-based channel selection module designed to automatically identify the most task-relevant channels in SEEG recordings. EEGChaT introduces Channel Aggregation Tokens (CATs) to aggregate information across channels, and leverages an improved Attention Rollout technique to compute interpretable, quantitative channel importance scores. We evaluate EEGChaT on the DuIN dataset, demonstrating that integrating EEGChaT with existing classification models consistently improves decoding accuracy, achieving up to 17\% absolute gains. Furthermore, the channel weights produced by EEGChaT show substantial overlap with manually selected channels, supporting the interpretability of the approach. Our results suggest that EEGChaT is an effective and generalizable solution for channel selection in high-dimensional SEEG analysis, offering both enhanced performance and insights into neural signal relevance.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-14</td>
<td style='padding: 8px;'>HEAR: An EEG Foundation Model with Heterogeneous Electrode Adaptive Representation</td>
<td style='padding: 6px;'>Zhige Chen, Chengxuan Qin, Wenlong You, Rui Liu, Congying Chu, Rui Yang, Kay Chen Tan, Jibin Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.12515v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) is an essential technique for neuroscience research and brain-computer interface (BCI) applications. Recently, large-scale EEG foundation models have been developed, exhibiting robust generalization capabilities across diverse tasks and subjects. However, the heterogeneity of EEG devices not only hinders the widespread adoption of these models but also poses significant challenges to their further scaling and development. In this paper, we introduce HEAR, the first EEG foundation model explicitly designed to support heterogeneous EEG devices, accommodating varying electrode layouts and electrode counts. HEAR employs a learnable, coordinate-based spatial embedding to map electrodes with diverse layouts and varying counts into a unified representational space. This unified spatial representation is then processed by a novel spatially-guided transformer, which effectively captures spatiotemporal dependencies across electrodes. To support the development of HEAR, we construct a large-scale EEG dataset comprising 8,782 hours of data collected from over 150 distinct electrode layouts with up to 1,132 electrodes. Experimental results demonstrate that HEAR substantially outperforms existing EEG foundation models in supporting heterogeneous EEG devices and generalizing across diverse cognitive tasks and subjects.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-12</td>
<td style='padding: 8px;'>The Cost of Simplicity: How Reducing EEG Electrodes Affects Source Localization and BCI Accuracy</td>
<td style='padding: 6px;'>Eva Guttmann-Flury, Yanyan Wei, Shan Zhao, Jian Zhao, Mohamad Sawan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.10770v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electrode density optimization in electroencephalography (EEG)-based Brain-Computer Interfaces (BCIs) requires balancing practical usability against signal fidelity, particularly for source localization. Reducing electrodes enhances portability but its effects on neural source reconstruction quality and source connectivity - treated as proxies to BCI performance - remain understudied. We address this gap through systematic evaluation of 62-, 32-, and 16-channel configurations using a fixed, fully automated processing pipeline applied to the well-characterized P300 potential. This approach's rationale is to minimize variability and bias inherent to EEG analysis by leveraging the P300's stimulus-locked reproducibility and pipeline standardization. Analyzing 63 sessions (31 subjects) from the Eye-BCI dataset with rigorous artifact correction and channel validation, we demonstrate: (1) Progressive degradation in source reconstruction quality with sparser configurations, including obscured deep neural generators and spatiotemporal distortions; (2) A novel sqrt(Re) scaling law linking electrode reduction ratio (Re) to localization accuracy - a previously unquantified relationship to the best of our knowledge; (3) While reduced configurations preserve basic P300 topography and may suffice for communicative BCIs, higher-density channels are essential for reliable deep source reconstruction. Overall, this study establishes a first step towards quantitative benchmarks for electrode selection, with critical implications for clinical BCIs requiring anatomical precision in applications like neurodegenerative disease monitoring, where compromised spatial resolution could mask pathological signatures. Most importantly, the sqrt(Re) scaling law may provide the first principled method to determine the minimal electrode density required based on acceptable error margins or expected effect sizes.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-12</td>
<td style='padding: 8px;'>Does Re-referencing Matter? Large Laplacian Filter Optimizes Single-Trial P300 BCI Performance</td>
<td style='padding: 6px;'>Eva Guttmann-Flury, Jian Zhao, Mohamad Sawan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.10733v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) provides a non-invasive window into brain activity, enabling Brain-Computer Interfaces (BCIs) for communication and control. However, their performance is limited by signal fidelity issues, among which the choice of re-referencing strategy is a pervasive but often overlooked preprocessing bias. Addressing controversies about its necessity and optimal choice, we adopted a quantified approach to evaluate four strategies - no re-referencing, Common Average Reference (CAR), small Laplacian, and large Laplacian - using 62-channels EEG (31 subjects, 2,520 trials). To our knowledge, this is the first study systematically quantifying their impact on single-trial P300 classification accuracy. Our controlled pipeline isolated re-referencing effects for source-space reconstruction (eLORETA with Phase Lag Index) and anatomically constrained classification. The large Laplacian resolves distributed P3b networks while maintaining P3a specificity, achieving the best P300 peak classification accuracy (81.57% hybrid method; 75.97% majority regions of interest). Performance follows a consistent and statistically significant hierarchy: large Laplacian > CAR > no re-reference > small Laplacian, providing a foundation for unified methodological evaluation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-12</td>
<td style='padding: 8px;'>FusionGen: Feature Fusion-Based Few-Shot EEG Data Generation</td>
<td style='padding: 6px;'>Yuheng Chen, Dingkun Liu, Xinyao Yang, Xinping Xu, Baicheng Chen, Dongrui Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.10604v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-computer interfaces (BCIs) provide potential for applications ranging from medical rehabilitation to cognitive state assessment by establishing direct communication pathways between the brain and external devices via electroencephalography (EEG). However, EEG-based BCIs are severely constrained by data scarcity and significant inter-subject variability, which hinder the generalization and applicability of EEG decoding models in practical settings. To address these challenges, we propose FusionGen, a novel EEG data generation framework based on disentangled representation learning and feature fusion. By integrating features across trials through a feature matching fusion module and combining them with a lightweight feature extraction and reconstruction pipeline, FusionGen ensures both data diversity and trainability under limited data constraints. Extensive experiments on multiple publicly available EEG datasets demonstrate that FusionGen significantly outperforms existing augmentation techniques, yielding notable improvements in classification accuracy.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-14</td>
<td style='padding: 8px;'>BrainForm: a Serious Game for BCI Training and Data Collection</td>
<td style='padding: 6px;'>Michele Romani, Devis Zanoni, Elisabetta Farella, Luca Turchet</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.10169v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>$\textit{BrainForm}$ is a gamified Brain-Computer Interface (BCI) training system designed for scalable data collection using consumer hardware and a minimal setup. We investigated (1) how users develop BCI control skills across repeated sessions and (2) perceptual and performance effects of two visual stimulation textures. Game Experience Questionnaire (GEQ) scores for Flow, Positive Affect, Competence and Challenge were strongly positive, indicating sustained engagement. A within-subject study with multiple runs, two task complexities, and post-session questionnaires revealed no significant performance differences between textures but increased ocular irritation over time. Online metrics$\unicode{x2013}$Task Accuracy, Task Time, and Information Transfer Rate$\unicode{x2013}$improved across sessions, confirming learning effects for symbol spelling, even under pressure conditions. Our results highlight the potential of $\textit{BrainForm}$ as a scalable, user-friendly BCI research tool and offer guidance for sustained engagement and reduced training fatigue.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-11</td>
<td style='padding: 8px;'>Bidirectional Time-Frequency Pyramid Network for Enhanced Robust EEG Classification</td>
<td style='padding: 6px;'>Jiahui Hong, Siqing Li, Muqing Jian, Luming Yang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.10004v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Existing EEG recognition models suffer from poor cross-paradigm generalization due to dataset-specific constraints and individual variability. To overcome these limitations, we propose BITE (Bidirectional Time-Freq Pyramid Network), an end-to-end unified architecture featuring robust multistream synergy, pyramid time-frequency attention (PTFA), and bidirectional adaptive convolutions. The framework uniquely integrates: 1) Aligned time-frequency streams maintaining temporal synchronization with STFT for bidirectional modeling, 2) PTFA-based multi-scale feature enhancement amplifying critical neural patterns, 3) BiTCN with learnable fusion capturing forward/backward neural dynamics. Demonstrating enhanced robustness, BITE achieves state-of-the-art performance across four divergent paradigms (BCICIV-2A/2B, HGD, SD-SSVEP), excelling in both within-subject accuracy and cross-subject generalization. As a unified architecture, it combines robust performance across both MI and SSVEP tasks with exceptional computational efficiency. Our work validates that paradigm-aligned spectral-temporal processing is essential for reliable BCI systems. Just as its name suggests, BITE "takes a bite out of EEG." The source code is available at https://github.com/cindy-hong/BiteEEG.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-10</td>
<td style='padding: 8px;'>Investigating the Impact of Rational Dilated Wavelet Transform on Motor Imagery EEG Decoding with Deep Learning Models</td>
<td style='padding: 6px;'>Marco Siino, Giuseppe Bonomo, Rosario Sorbello, Ilenia Tinnirello</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.09242v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The present study investigates the impact of the Rational Discrete Wavelet Transform (RDWT), used as a plug-in preprocessing step for motor imagery electroencephalographic (EEG) decoding prior to applying deep learning classifiers. A systematic paired evaluation (with/without RDWT) is conducted on four state-of-the-art deep learning architectures: EEGNet, ShallowConvNet, MBEEG\_SENet, and EEGTCNet. This evaluation was carried out across three benchmark datasets: High Gamma, BCI-IV-2a, and BCI-IV-2b. The performance of the RDWT is reported with subject-wise averages using accuracy and Cohen's kappa, complemented by subject-level analyses to identify when RDWT is beneficial. On BCI-IV-2a, RDWT yields clear average gains for EEGTCNet (+4.44 percentage points, pp; kappa +0.059) and MBEEG\_SENet (+2.23 pp; +0.030), with smaller improvements for EEGNet (+2.08 pp; +0.027) and ShallowConvNet (+0.58 pp; +0.008). On BCI-IV-2b, the enhancements observed are modest yet consistent for EEGNet (+0.21 pp; +0.044) and EEGTCNet (+0.28 pp; +0.077). On HGD, average effects are modest to positive, with the most significant gain observed for MBEEG\_SENet (+1.65 pp; +0.022), followed by EEGNet (+0.76 pp; +0.010) and EEGTCNet (+0.54 pp; +0.008). Inspection of the subject material reveals significant enhancements in challenging recordings (e.g., non-stationary sessions), indicating that RDWT can mitigate localized noise and enhance rhythm-specific information. In conclusion, RDWT is shown to be a low-overhead, architecture-aware preprocessing technique that can yield tangible gains in accuracy and agreement for deep model families and challenging subjects.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>Optimizing BCI Rehabilitation Protocols for Stroke: Exploring Task Design and Training Duration</td>
<td style='padding: 6px;'>Aniana Cruz, Marko Kuzmanoski, Gabriel Pires</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08082v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Stroke is a leading cause of long-term disability and the second most common cause of death worldwide. Although acute treatments have advanced, recovery remains challenging and limited. Brain-computer interfaces (BCIs) have emerged as a promising tool for post-stroke rehabilitation by promoting neuroplasticity. However, clinical outcomes remain variable, and optimal protocols have yet to be established. This study explores strategies to optimize BCI-based rehabilitation by comparing motor imagery of affected hand movement versus rest, instead of the conventional left-versus-right motor imagery. This alternative aims to simplify the task and address the weak contralateral activation commonly observed in stroke patients. Two datasets, one from healthy individuals and one from stroke patients, were used to evaluate the proposed approach. The results showed improved performance using both FBCSP and EEGNet. Additionally, we investigated the impact of session duration and found that shorter training sessions produced better BCI performance than longer sessions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-01</td>
<td style='padding: 8px;'>NeuroTTT: Bridging Pretraining-Downstream Task Misalignment in EEG Foundation Models via Test-Time Training</td>
<td style='padding: 6px;'>Suli Wang, Yangshen Deng, Zhenghua Bao, Xinyu Zhan, Yiqun Duan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.26301v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large-scale foundation models for EEG signals offer a promising path to generalizable brain-computer interface (BCI) applications, but they often suffer from misalignment between pretraining objectives and downstream tasks, as well as significant cross-subject distribution shifts. This paper addresses these challenges by introducing a two-stage alignment strategy that bridges the gap between generic pretraining and specific EEG decoding tasks. First, we propose NeuroTTT: a domain-specific self-supervised fine-tuning paradigm that augments the foundation model with task-relevant self-supervised objectives, aligning latent representations to important spectral, spatial, and temporal EEG features without requiring additional labeled data. Second, we incorporate test-time training (TTT) at inference, we perform (i) self-supervised test-time training on individual unlabeled test samples and (ii) prediction entropy minimization (Tent), which updates only normalization statistics to continually calibrate the model to each new input on the fly. Our approach, which, to our knowledge, is the first to unify domain-tuned self-supervision with test-time training in large-scale EEG foundation models, yields substantially improved robustness and accuracy across diverse BCI tasks (imagined speech, stress detection, motor imagery). Using CBraMod and LaBraM as backbones, our method pushes their performance to a markedly higher level. Results on three diverse tasks demonstrate that the proposed alignment strategy achieves state-of-the-art performance, outperforming conventional fine-tuning and adaptation methods. Our code is available at https://github.com/wsl2000/NeuroTTT.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Scaling Vision Transformers for Functional MRI with Flat Maps</td>
<td style='padding: 6px;'>Connor Lane, Daniel Z. Kaplan, Tanishq Mathew Abraham, Paul S. Scotti</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13768v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A key question for adapting modern deep learning architectures to functional MRI (fMRI) is how to represent the data for model input. To bridge the modality gap between fMRI and natural images, we transform the 4D volumetric fMRI data into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K hours of fMRI flat map videos from the Human Connectome Project using the spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI modeling performance improves with dataset size according to a strict power scaling law. Downstream classification benchmarks show that our model learns rich representations supporting both fine-grained state decoding across subjects, as well as subject-specific trait decoding across changes in brain state. This work is part of an ongoing open science project to build foundation models for fMRI data. Our code and datasets are available at https://github.com/MedARC-AI/fmri-fm.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Jacobian-Based Interpretation of Nonlinear Neural Encoding Model</td>
<td style='padding: 6px;'>Xiaohui Gao, Haoran Yang, Yue Cheng, Mengfei Zuo, Yiheng Liu, Peiyang Li, Xintao Hu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13688v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In recent years, the alignment between artificial neural network (ANN) embeddings and blood oxygenation level dependent (BOLD) responses in functional magnetic resonance imaging (fMRI) via neural encoding models has significantly advanced research on neural representation mechanisms and interpretability in the brain. However, these approaches remain limited in characterizing the brain's inherently nonlinear response properties. To address this, we propose the Jacobian-based Nonlinearity Evaluation (JNE), an interpretability metric for nonlinear neural encoding models. JNE quantifies nonlinearity by statistically measuring the dispersion of local linear mappings (Jacobians) from model representations to predicted BOLD responses, thereby approximating the nonlinearity of BOLD signals. Centered on proposing JNE as a novel interpretability metric, we validated its effectiveness through controlled simulation experiments on various activation functions and network architectures, and further verified it on real fMRI data, demonstrating a hierarchical progression of nonlinear characteristics from primary to higher-order visual cortices, consistent with established cortical organization. We further extended JNE with Sample-Specificity (JNE-SS), revealing stimulus-selective nonlinear response patterns in functionally specialized brain regions. As the first interpretability metric for quantifying nonlinear responses, JNE provides new insights into brain information processing. Code available at https://github.com/Gaitxh/JNE.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-12</td>
<td style='padding: 8px;'>A compressed code for memory discrimination</td>
<td style='padding: 6px;'>Dale Zhou, Sharon Mina Noh, Nora C Harhen, Nidhi V Banavar, C. Brock Kirwan, Michael A Yassa, Aaron M Bornstein</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.10791v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The ability to discriminate similar visual stimuli is an important index of memory function. This ability is widely thought to be supported by expanding the dimensionality of relevant neural codes, such that neural representations for similar stimuli are maximally distinct, or ``separated.'' An alternative hypothesis is that discrimination is supported by lossy compression of visual inputs, efficiently coding sensory information by discarding seemingly irrelevant details. A benefit of compression, relative to expansion, is that it allows individuals to retain fewer essential dimensions underlying stimulus variation -- a process linked to higher-order visual processing -- without hindering discrimination. Under this hypothesis, pattern separation is facilitated when more information from similar stimuli can be discarded, rather than preserved. We test the compression versus expansion hypotheses by predicting performance on the canonical mnemonic similarity task. We train neural networks to compress perceptual and semantic factors of stimuli, measuring lossiness using the mathematical framework underlying compression. Consistent with the compression hypothesis, and not the expansion hypothesis, greater lossiness predicts the ease and performance of lure discrimination, especially in deeper convolutional network layers that predict higher-order visual brain activity. We then confirm these predictions across two image sets, four behavioral datasets, and alternative lossiness metrics. Finally, using task fMRI, we identify signatures of lossy compression -- neural dimensionality reduction and information loss -- in higher-order visual regions V4 and IT and hippocampal DG/CA3 and CA1 linked to lure discrimination. These results suggest lossy compression supports mnemonic discrimination by discarding redundant and overlapping information.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-10</td>
<td style='padding: 8px;'>Estimating Brain Activity with High Spatial and Temporal Resolution using a Naturalistic MEG-fMRI Encoding Model</td>
<td style='padding: 6px;'>Beige Jerry Jin, Leila Wehbe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.09415v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Current non-invasive neuroimaging techniques trade off between spatial resolution and temporal resolution. While magnetoencephalography (MEG) can capture rapid neural dynamics and functional magnetic resonance imaging (fMRI) can spatially localize brain activity, a unified picture that preserves both high resolutions remains an unsolved challenge with existing source localization or MEG-fMRI fusion methods, especially for single-trial naturalistic data. We collected whole-head MEG when subjects listened passively to more than seven hours of narrative stories, using the same stimuli in an open fMRI dataset (LeBel et al., 2023). We developed a transformer-based encoding model that combines the MEG and fMRI from these two naturalistic speech comprehension experiments to estimate latent cortical source responses with high spatiotemporal resolution. Our model is trained to predict MEG and fMRI from multiple subjects simultaneously, with a latent layer that represents our estimates of reconstructed cortical sources. Our model predicts MEG better than the common standard of single-modality encoding models, and it also yields source estimates with higher spatial and temporal fidelity than classic minimum-norm solutions in simulation experiments. We validated the estimated latent sources by showing its strong generalizability across unseen subjects and modalities. Estimated activity in our source space predict electrocorticography (ECoG) better than an ECoG-trained encoding model in an entirely new dataset. By integrating the power of large naturalistic experiments, MEG, fMRI, and encoding models, we propose a practical route towards millisecond-and-millimeter brain mapping.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-07</td>
<td style='padding: 8px;'>Beyond Grid-Locked Voxels: Neural Response Functions for Continuous Brain Encoding</td>
<td style='padding: 6px;'>Haomiao Chen, Keith W Jamison, Mert R. Sabuncu, Amy Kuceyeski</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.07342v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neural encoding models aim to predict fMRI-measured brain responses to natural images. fMRI data is acquired as a 3D volume of voxels, where each voxel has a defined spatial location in the brain. However, conventional encoding models often flatten this volume into a 1D vector and treat voxel responses as independent outputs. This removes spatial context, discards anatomical information, and ties each model to a subject-specific voxel grid. We introduce the Neural Response Function (NRF), a framework that models fMRI activity as a continuous function over anatomical space rather than a flat vector of voxels. NRF represents brain activity as a continuous implicit function: given an image and a spatial coordinate (x, y, z) in standardized MNI space, the model predicts the response at that location. This formulation decouples predictions from the training grid, supports querying at arbitrary spatial resolutions, and enables resolution-agnostic analyses. By grounding the model in anatomical space, NRF exploits two key properties of brain responses: (1) local smoothness -- neighboring voxels exhibit similar response patterns; modeling responses continuously captures these correlations and improves data efficiency, and (2) cross-subject alignment -- MNI coordinates unify data across individuals, allowing a model pretrained on one subject to be fine-tuned on new subjects. In experiments, NRF outperformed baseline models in both intrasubject encoding and cross-subject adaptation, achieving high performance while reducing the data size needed by orders of magnitude. To our knowledge, NRF is the first anatomically aware encoding model to move beyond flattened voxels, learning a continuous mapping from images to brain responses in 3D space.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-06</td>
<td style='padding: 8px;'>Dynamic Functional Connectivity Features for Brain State Classification: Insights from the Human Connectome Project</td>
<td style='padding: 6px;'>Valeriya Kirova, Dzerassa Kadieva, Daniil Vlasenko, Isak B. Blank, Fedor Ratnikov</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.05325v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We analyze functional magnetic resonance imaging (fMRI) data from the Human Connectome Project (HCP) to match brain activities during a range of cognitive tasks. Our findings demonstrate that even basic linear machine learning models can effectively classify brain states and achieve state-of-the-art accuracy, particularly for tasks related to motor functions and language processing. Feature importance ranking allows to identify distinct sets of brain regions whose activation patterns are uniquely associated with specific cognitive functions. These discriminative features provide strong support for the hypothesis of functional specialization across cortical and subcortical areas of the human brain.   Additionally, we investigate the temporal dynamics of the identified brain regions, demonstrating that the time-dependent structure of fMRI signals are essential for shaping functional connectivity between regions: uncorrelated areas are least important for classification. This temporal perspective provides deeper insights into the formation and modulation of brain neural networks involved in cognitive processing.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-10</td>
<td style='padding: 8px;'>Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing</td>
<td style='padding: 6px;'>Xuanhua Yin, Runkai Zhao, Weidong Cai</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.04670v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic Framework for Multimodal fMRI Response Encoding), an agnostic interface that standardizes time-aligned post-fusion tokens from varied encoders, and MIND, a plug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating. Trained end-to-end for whole-brain prediction, AFIRE decouples the decoder from upstream fusion, while MIND combines token-dependent Top-K sparse routing with a subject prior to personalize expert usage without sacrificing generality. Experiments across multiple multimodal backbones and subjects show consistent improvements over strong baselines, enhanced cross-subject generalization, and interpretable expert patterns that correlate with content type. The framework offers a simple attachment point for new encoders and datasets, enabling robust, plug-and-improve performance for naturalistic neuroimaging studies.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-05</td>
<td style='padding: 8px;'>Adapting HFMCA to Graph Data: Self-Supervised Learning for Generalizable fMRI Representations</td>
<td style='padding: 6px;'>Jakub Frac, Alexander Schmatz, Qiang Li, Guido Van Wingen, Shujian Yu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.05177v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional magnetic resonance imaging (fMRI) analysis faces significant challenges due to limited dataset sizes and domain variability between studies. Traditional self-supervised learning methods inspired by computer vision often rely on positive and negative sample pairs, which can be problematic for neuroimaging data where defining appropriate contrasts is non-trivial. We propose adapting a recently developed Hierarchical Functional Maximal Correlation Algorithm (HFMCA) to graph-structured fMRI data, providing a theoretically grounded approach that measures statistical dependence via density ratio decomposition in a reproducing kernel Hilbert space (RKHS),and applies HFMCA-based pretraining to learn robust and generalizable representations. Evaluations across five neuroimaging datasets demonstrate that our adapted method produces competitive embeddings for various classification tasks and enables effective knowledge transfer to unseen datasets. Codebase and supplementary material can be found here: https://github.com/fr30/mri-eigenencoder</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-03</td>
<td style='padding: 8px;'>Neural Correlates of Language Models Are Specific to Human Language</td>
<td style='padding: 6px;'>Iigo Parra</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.03156v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Previous work has shown correlations between the hidden states of large language models and fMRI brain responses, on language tasks. These correlations have been taken as evidence of the representational similarity of these models and brain states. This study tests whether these previous results are robust to several possible concerns. Specifically this study shows: (i) that the previous results are still found after dimensionality reduction, and thus are not attributable to the curse of dimensionality; (ii) that previous results are confirmed when using new measures of similarity; (iii) that correlations between brain representations and those from models are specific to models trained on human language; and (iv) that the results are dependent on the presence of positional encoding in the models. These results confirm and strengthen the results of previous research and contribute to the debate on the biological plausibility and interpretability of state-of-the-art large language models.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-03</td>
<td style='padding: 8px;'>BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck for Functional Brain Biomarkers in Schizophrenia</td>
<td style='padding: 6px;'>Tianzheng Hu, Qiang Li, Shu Liu, Vince D. Calhoun, Guido van Wingen, Shujian Yu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.03004v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The development of diagnostic models is gaining traction in the field of psychiatric disorders. Recently, machine learning classifiers based on resting-state functional magnetic resonance imaging (rs-fMRI) have been developed to identify brain biomarkers that differentiate psychiatric disorders from healthy controls. However, conventional machine learning-based diagnostic models often depend on extensive feature engineering, which introduces bias through manual intervention. While deep learning models are expected to operate without manual involvement, their lack of interpretability poses significant challenges in obtaining explainable and reliable brain biomarkers to support diagnostic decisions, ultimately limiting their clinical applicability. In this study, we introduce an end-to-end innovative graph neural network framework named BrainIB++, which applies the information bottleneck (IB) principle to identify the most informative data-driven brain regions as subgraphs during model training for interpretation. We evaluate the performance of our model against nine established brain network classification methods across three multi-cohort schizophrenia datasets. It consistently demonstrates superior diagnostic accuracy and exhibits generalizability to unseen data. Furthermore, the subgraphs identified by our model also correspond with established clinical biomarkers in schizophrenia, particularly emphasizing abnormalities in the visual, sensorimotor, and higher cognition brain functional network. This alignment enhances the model's interpretability and underscores its relevance for real-world diagnostic applications.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-10</td>
<td style='padding: 8px;'>Estimating Brain Activity with High Spatial and Temporal Resolution using a Naturalistic MEG-fMRI Encoding Model</td>
<td style='padding: 6px;'>Beige Jerry Jin, Leila Wehbe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.09415v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Current non-invasive neuroimaging techniques trade off between spatial resolution and temporal resolution. While magnetoencephalography (MEG) can capture rapid neural dynamics and functional magnetic resonance imaging (fMRI) can spatially localize brain activity, a unified picture that preserves both high resolutions remains an unsolved challenge with existing source localization or MEG-fMRI fusion methods, especially for single-trial naturalistic data. We collected whole-head MEG when subjects listened passively to more than seven hours of narrative stories, using the same stimuli in an open fMRI dataset (LeBel et al., 2023). We developed a transformer-based encoding model that combines the MEG and fMRI from these two naturalistic speech comprehension experiments to estimate latent cortical source responses with high spatiotemporal resolution. Our model is trained to predict MEG and fMRI from multiple subjects simultaneously, with a latent layer that represents our estimates of reconstructed cortical sources. Our model predicts MEG better than the common standard of single-modality encoding models, and it also yields source estimates with higher spatial and temporal fidelity than classic minimum-norm solutions in simulation experiments. We validated the estimated latent sources by showing its strong generalizability across unseen subjects and modalities. Estimated activity in our source space predict electrocorticography (ECoG) better than an ECoG-trained encoding model in an entirely new dataset. By integrating the power of large naturalistic experiments, MEG, fMRI, and encoding models, we propose a practical route towards millisecond-and-millimeter brain mapping.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution</td>
<td style='padding: 6px;'>Terry Yue Zhuo, Xiaolong Jin, Hange Liu, Juyong Jiang, Tianyang Liu, Chen Gong, Bhupesh Bishnoi, Vaisakhi Mishra, Marek Suppa, Noah Ziems, Saiteja Utpala, Ming Xu, Guangyu Song, Kaixin Li, Yuhan Cao, Bo Liu, Zheng Liu, Sabina Abdurakhmanova, Wenhao Yu, Mengzhao Jia, Jihan Yao, Kenneth Hamilton, Kumar Shridhar, Minh Chien Vu, Dingmin Wang, Jiawei Liu, Zijian Wang, Qian Liu, Binyuan Hui, Meg Risdal, Ahsen Khaliq, Atin Sood, Zhenchang Xing, Wasi Uddin Ahmad, John Grundy, David Lo, Banghua Zhu, Xiaoning Du, Torsten Scholak, Leandro von Werra</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08697v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time evaluation from human perspectives to assess the quality of model responses. In the coding domain, manually examining the quality of LLM-generated content is extremely challenging, as it requires understanding long chunks of raw code and deliberately simulating code execution. To this end, we introduce BigCodeArena, an open human evaluation platform for code generation backed by a comprehensive and on-the-fly execution environment. Built on top of Chatbot Arena, BigCodeArena enables the execution of LLM-generated code and allows humans to interact with the execution process and outcomes. We collected over 14,000 raw code-centric conversation sessions across 10 widely used LLMs, spanning 10 languages and 8 types of execution environments. Among these conversations, we identified more than 4,700 multi-turn samples with pairwise human preferences. Further analysis uncovers underexplored preferences of LLMs in fine-grained domains characterized by tasks, languages, and frameworks. To systematically examine code understanding and generation capabilities of frontier LLMs, we curated two benchmarks based on the collected data, namely BigCodeReward and AutoCodeArena. For BigCodeReward, we post-processed the 4,700 conversations and evaluated the consistency between reward models and human preferences. The evaluation shows that most LLMs have superior performance in judging coding preferences when the execution results are available. Inspired by these findings, we propose AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding quality of LLMs without human involvement. We find that proprietary LLMs like GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation performance among recent emerging models.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>Expanding the Landscape of Exotic Muon Decays</td>
<td style='padding: 6px;'>Admir Greljo, Ajdin Palavri, Mirsad Tunja, Jure Zupan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08674v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We chart new-physics models that produce exotic, high-multiplicity muon decays featuring prompt or displaced $e^+e^-$ pairs and/or photons, with or without missing energy, such as $\mu \to 5e$, $\mu \to 7e$, etc. Starting from an effective-field-theory perspective, we estimate the reach on the ultraviolet scale and identify conditions under which lower-multiplicity modes are suppressed or occur at comparable rates. We then construct explicit realizations in minimal dark-sector models with light, feebly interacting particles, such as flavor-protected scalars, dark photons, inelastic dark matter, and axion-like particles. The predicted novel signatures can be probed at MEG II and Mu3e, as well as during calibration runs of COMET and Mu2e. A future discovery would provide valuable insights into short-distance dynamics and the mechanism of lepton-flavor symmetry breaking.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-07</td>
<td style='padding: 8px;'>The Gamma-ray Luminosity Function of Flat-Spectrum Radio Quasars</td>
<td style='padding: 6px;'>Garima Rajguru, Lea Marcotulli, Marco Ajello, Mattia Di Mauro, Meg Urry</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.05515v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We have utilized the largest sample of $\gamma$-ray selected Fermi flat-spectrum radio quasars (FSRQs) ever used (519 sources) to construct the luminosity function and its evolution through the cosmic history. In addition to spanning large redshift ($0<z\lesssim 4$) and luminosity ranges ($2.9\times10^{43}$ erg s$^{-1}$ - $7.3\times10^{48}$ erg s$^{-1}$), this sample also has a robust calculation of the detection efficiency associated with its observation, making its selection effects and biases well understood. We confirm that the local luminosity function is best explained by a double power law. The evolution of the luminosity function of FSRQs follows a luminosity-dependent density evolution. FSRQs experience positive evolution with their space density growing with increasing redshift up to a maximum redshift, after which the numbers decrease. This peak in redshift occurs at larger redshifts for higher luminosity sources and at lower redshifts for lower luminosity sources. We find an unexpected similarity between the luminosity function of FSRQs and that of BL Lacertae objects at intermediate luminosity. This could be a sign of a strong genetic link between the two blazar sub-classes or that BL Lac samples are contaminated by large amounts of FSRQs with their jets nearly perfectly aligned with our line of sight.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-18</td>
<td style='padding: 8px;'>UMind: A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding</td>
<td style='padding: 6px;'>Chengjian Xu, Yonghao Song, Zelin Liao, Haochuan Zhang, Qiong Wang, Qingqing Zheng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.14772v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Decoding visual information from time-resolved brain recordings, such as EEG and MEG, plays a pivotal role in real-time brain-computer interfaces. However, existing approaches primarily focus on direct brain-image feature alignment and are limited to single-task frameworks or task-specific models. In this paper, we propose a Unified MultItask Network for zero-shot M/EEG visual Decoding (referred to UMind), including visual stimulus retrieval, classification, and reconstruction, where multiple tasks mutually enhance each other. Our method learns robust neural-visual and semantic representations through multimodal alignment with both image and text modalities. The integration of both coarse and fine-grained texts enhances the extraction of these neural representations, enabling more detailed semantic and visual decoding. These representations then serve as dual conditional inputs to a pre-trained diffusion model, guiding visual reconstruction from both visual and semantic perspectives. Extensive evaluations on MEG and EEG datasets demonstrate the effectiveness, robustness, and biological plausibility of our approach in capturing spatiotemporal neural dynamics. Our approach sets a multitask pipeline for brain visual decoding, highlighting the synergy of semantic information in visual feature extraction.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Quantum-like representation of neuronal networks' activity: modeling "mental entanglement"</td>
<td style='padding: 6px;'>Andrei Khrennikov, Makiko Yamada</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.16253v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Quantum-like modeling (QLM) - quantum theory applications outside of physics - are intensively developed with applications in biology, cognition, psychology, and decision-making. For cognition, QLM should be distinguished from quantum reductionist models in the spirit of Hameroff and Penrose and well as Umezawa and Vitiello. QLM is not concerned with just quantum physical processes in the brain but also QL information processing by macroscopic neuronal structures. Although QLM of cognition and decision-making has seen some success, it suffers from a knowledge gap that exists between oscillatory neuronal network functioning in the brain and QL behavioral patterns. Recently, steps toward closing this gap have been taken using the generalized probability theory and prequantum classical statistical field theory (PCSFT) - a random field model beyond the complex Hilbert space formalism. PCSFT is used to move from the classical ``oscillatory cognition'' of the neuronal networks to QLM for decision.making. In this study, we addressed the most difficult problem within this construction: QLM for entanglement generation by classical networks, i.e., mental entanglement. We started with the observational approach to entanglement based on operator algebras describing local observables and bringing into being the tensor product structure in the space of QL states. Moreover, we applied the standard states entanglement approach: entanglement generation by spatially separated networks in the brain. Finally, we discussed possible future experiments on mental entanglement detection using the EEG/MEG technique.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Multi-Source Neural Activity Indices and Spatial Filters for EEG/MEG Inverse Problem: An Extension to MNE-Python</td>
<td style='padding: 6px;'>Julia Jurkowska, Joanna Dreszer, Monika Lewandowska, Krzysztof Topa, Tomasz Piotrowski</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.14118v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurate EEG/MEG source localization is essential for understanding brain function, yet remains challenging because the inverse problem is inherently ill-posed. In spatial filtering (beamforming) approaches, single-source LCMV spatial filters, though widely used, suffer from source cancellation when sources are correlated - a common experimental scenario. Multi-source frameworks, such as the multi-source minimum-variance pseudo-unbiased reduced-rank (MV-PURE) method, offer improved reconstruction and robust neural activity indices, yet their adoption has been limited by incomplete theory and lack of accessible implementations. In this paper, we present a rigorous derivation of multi-source neural activity indices and spatial filters, establishing a complete analytical framework with automated parameter selection. The resulting compact algebraic forms enable straightforward implementation. To facilitate adoption, we provide a full implementation extending MNE-Python, along with an accompanying tutorial, and demonstrate its utility on EEG experimental data, highlighting the practical advantages of multi-source spatial filtering for source localization and reconstruction.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-23</td>
<td style='padding: 8px;'>MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning</td>
<td style='padding: 6px;'>Jiarui Chen, Yikeng Chen, Yingshuang Zou, Ye Huang, Peng Wang, Yuan Liu, Yujing Sun, Wenping Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.07021v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis technique, but its high memory consumption severely limits its applicability on edge devices. A growing number of 3DGS compression methods have been proposed to make 3DGS more efficient, yet most only focus on storage compression and fail to address the critical bottleneck of rendering memory. To address this problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that tackles this challenge by jointly optimizing two key factors: the total primitive number and the parameters per primitive, achieving unprecedented memory compression. Specifically, we replace the memory-intensive spherical harmonics with lightweight, arbitrarily oriented spherical Gaussian lobes as our color representations. More importantly, we propose a unified soft pruning framework that models primitive-number and lobe-number pruning as a single constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a 50% static VRAM reduction and a 40% rendering VRAM reduction compared to existing methods, while maintaining comparable rendering quality. Project page: https://megs-2.github.io/</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-15</td>
<td style='padding: 8px;'>A fast machine learning tool to predict the composition of astronomical ices from infrared absorption spectra</td>
<td style='padding: 6px;'>Andrs Megas, Izaskun Jimnez-Serra, Franois Dulieu, Julie Vitorino, Beln Mat, David Ciudad, Will R. M. Rocha, Marcos Martnez Jimnez, Jacobo Aguirre</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.04331v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Current observations taken by James Webb Space Telescope (JWST) allow us to observe the absorption features of icy mantles that cover interstellar dust grains, which are mainly composed of $\mathrm{H_2O}$, $\mathrm{CO}$, and $\mathrm{CO_2}$, along with other minor species. Thanks to its sensitivity and spectral resolution, JWST has the potential to observe ice features towards hundreds of sources at different stages along the process of star formation. However, identifying the spectral features of the different species and quantifying the ice composition is not trivial and requires complex spectroscopic analysis. We present Automatic Ice Composition Estimator (AICE), a new tool based on artificial neural networks. Based on the infrared (IR) ice absorption spectrum between 2.5 and 10 microns, AICE predicts the ice fractional composition in terms of $\mathrm{H_2O}$, $\mathrm{CO}$, $\mathrm{CO_2}$, $\mathrm{CH_3OH}$, $\mathrm{NH_3}$, and $\mathrm{CH_4}$. To train the model, we used hundreds of laboratory experiments of ice mixtures from different databases, which were reprocessed with baseline subtraction and normalisation. Once trained, AICE takes less than one second on a conventional computer to predict the ice composition associated with the observed IR absorption spectrum, with typical errors of $\sim$3 $\%$ in the species fraction. We tested its performance on two spectra reported towards the NIR38 and J110621 background stars observed within the JWST Ice Age program, demonstrating a good agreement with previous estimations of the ice composition. The fast and accurate performance of AICE enables the systematic analysis of hundreds of different ice spectra with a modest time investment. In addition, this model can be enhanced and re-trained with more laboratory data, improving the precision of the predictions and expanding the list of predicted species.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Towards a 384-channel magnetoencephalography system based on optically pumped magnetometers</td>
<td style='padding: 6px;'>Holly Schofield, Ryan M. Hill, Lukas Rier, Ewan Kennett, Gonzalo Reina Rivero, Joseph Gibson, Ashley Tyler, Zoe Tanner, Frank Worcester, Tyler Hayward, James Osborne, Cody Doyle, Vishal Shah, Elena Boto, Niall Holmes, Matthew J. Brookes</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03107v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Magnetoencephalography using optically pumped magnetometers (OPM-MEG) is gaining significant traction as a neuroimaging tool, with the potential for improved performance and practicality compared to conventional instrumentation. However, OPM-MEG has so far lagged conventional-MEG in terms of the number of independent measurements of magnetic field that can be made across the scalp (i.e. the number of channels). This is important since increasing channel count offers improvements in sensitivity, spatial resolution and coverage. Unfortunately, increasing channel count also poses significant technical and practical challenges. Here, we describe a new OPM-MEG array which exploits triaxial sensors and integrated miniaturised electronic control units to measure MEG data from up to 384 channels. We also introduce a high-speed calibration method to ensure the that the fields measured by this array are high fidelity. The system was validated using a phantom, with results showing that dipole localisation accuracy is better than 1 mm, and correlation between the measured magnetic fields and a dipole model is >0.998. We then demonstrate utility in human MEG acquisition: via measurement of visual gamma oscillations we demonstrate the improvements in sensitivity that are afforded by high channel density, and via a moviewatching paradigm we quantify improvements in spatial resolution. In sum, we show the first OPM-MEG system with a channel count larger than that of typical conventional MEG devices. This represents a significant step on the path towards OPMs becoming the sensor of choice for MEG measurement.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>A Computational Perspective on NeuroAI and Synthetic Biological Intelligence</td>
<td style='padding: 6px;'>Dhruvik Patel, Md Sayed Tanveer, Jesus Gonzalez-Ferrer, Alon Loeffler, Brett J. Kagan, Mohammed A. Mostajo-Radji, Ge Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.23896v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>NeuroAI is an emerging field at the intersection of neuroscience and artificial intelligence, where insights from brain function guide the design of intelligent systems. A central area within this field is synthetic biological intelligence (SBI), which combines the adaptive learning properties of biological neural networks with engineered hardware and software. SBI systems provide a platform for modeling neural computation, developing biohybrid architectures, and enabling new forms of embodied intelligence. In this review, we organize the NeuroAI landscape into three interacting domains: hardware, software, and wetware. We outline computational frameworks that integrate biological and non-biological systems and highlight recent advances in organoid intelligence, neuromorphic computing, and neuro-symbolic learning. These developments collectively point toward a new class of systems that compute through interactions between living neural tissue and digital algorithms.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-09</td>
<td style='padding: 8px;'>Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers</td>
<td style='padding: 6px;'>Thomas Klein, Sascha Meyen, Wieland Brendel, Felix A. Wichmann, Kristof Meding</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.06645v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Benchmarking models is a key factor for the rapid progress in machine learning (ML) research. Thus, further progress depends on improving benchmarking metrics. A standard metric to measure the behavioral alignment between ML models and human observers is error consistency (EC). EC allows for more fine-grained comparisons of behavior than other metrics such as e.g. accuracy, and has been used in the influential Brain-Score benchmark to rank different DNNs by their behavioral consistency with humans. Previously, EC values have been reported without confidence intervals. However, empirically measured EC values are typically noisy -- thus, without confidence intervals, valid benchmarking conclusions are problematic. Here we improve on standard EC in two ways: First, we show how to obtain confidence intervals for EC using a bootstrapping technique, allowing us to derive significance tests for EC. Second, we propose a new computational model relating the EC between two classifiers to the implicit probability that one of them copies responses from the other. This view of EC allows us to give practical guidance to scientists regarding the number of trials required for sufficiently powerful, conclusive experiments. Finally, we use our methodology to revisit popular NeuroAI-results. We find that while the general trend of behavioral differences between humans and machines holds up to scrutiny, many reported differences between deep vision models are statistically insignificant. Our methodology enables researchers to design adequately powered experiments that can reliably detect behavioral differences between models, providing a foundation for more rigorous benchmarking of behavioral alignment.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-02</td>
<td style='padding: 8px;'>What Neuroscience Can Teach AI About Learning in Continuously Changing Environments</td>
<td style='padding: 6px;'>Daniel Durstewitz, Bruno Averbeck, Georgia Koppe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.02103v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-06-12</td>
<td style='padding: 8px;'>NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models</td>
<td style='padding: 6px;'>Luca Ghafourpour, Valentin Duruisseaux, Bahareh Tolooshams, Philip H. Wong, Costas A. Anastassiou, Anima Anandkumar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2506.04536v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Characterizing the diverse computational properties of human neurons via multimodal electrophysiological, transcriptomic, and morphological data provides the foundation for constructing and validating bio-realistic neuron models that can advance our understanding of fundamental mechanisms underlying brain function. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. To capture variability, ensembles of deterministic models are often used, but are difficult to scale as model generation requires repeating computationally expensive optimization for each neuron. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on data generated from biophysically realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE is the first scaled-up deep learning framework validated on real experimental data, enabling efficient generation of synthetic neurons that exhibit trial-to-trial variability and achieve a $4200\times$ speedup over numerical solvers. To this end, NOBLE captures fundamental neural properties, opening the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-21</td>
<td style='padding: 8px;'>SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation</td>
<td style='padding: 6px;'>Jiayue Liu, Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Kuo Yang, Xu Wang, Yang Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.16080v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-02-22</td>
<td style='padding: 8px;'>Brain-Model Evaluations Need the NeuroAI Turing Test</td>
<td style='padding: 6px;'>Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2502.16238v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-04</td>
<td style='padding: 8px;'>Asynchronous Hebbian/anti-Hebbian networks</td>
<td style='padding: 6px;'>Henrique Reis Aguiar, Matthias H. Hennig</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02402v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-03</td>
<td style='padding: 8px;'>NeuroAI for AI Safety</td>
<td style='padding: 6px;'>Patrick Mineault, Niccol Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.18526v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-14</td>
<td style='padding: 8px;'>Evaluating Representational Similarity Measures from the Lens of Functional Correspondence</td>
<td style='padding: 6px;'>Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.14633v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-09</td>
<td style='padding: 8px;'>Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models</td>
<td style='padding: 6px;'>Emily Cheng, Richard J. Antonello</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.05771v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first "composition" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge</td>
<td style='padding: 6px;'>Mikolaj Walczak, Uttej Kallakuri, Edward Humes, Xiaomin Lin, Tinoosh Mohsenin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13760v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Vision Transformers (ViTs) have demonstrated strong capabilities in interpreting complex medical imaging data. However, their significant computational and memory demands pose challenges for deployment in real-time, resource-constrained mobile and wearable devices used in clinical environments. We introduce, BiTMedViT, a new class of Edge ViTs serving as medical AI assistants that perform structured analysis of medical images directly on the edge. BiTMedViT utilizes ternary- quantized linear layers tailored for medical imaging and com- bines a training procedure with multi-query attention, preserving stability under ternary weights with low-precision activations. Furthermore, BiTMedViT employs task-aware distillation from a high-capacity teacher to recover accuracy lost due to extreme quantization. Lastly, we also present a pipeline that maps the ternarized ViTs to a custom CUDA kernel for efficient memory bandwidth utilization and latency reduction on the Jetson Orin Nano. Finally, BiTMedViT achieves 86% diagnostic accuracy (89% SOTA) on MedMNIST across 12 datasets, while reducing model size by 43x, memory traffic by 39x, and enabling 16.8 ms inference at an energy efficiency up to 41x that of SOTA models at 183.62 GOPs/J on the Orin Nano. Our results demonstrate a practical and scientifically grounded route for extreme-precision medical imaging ViTs deployable on the edge, narrowing the gap between algorithmic advances and deployable clinical tools.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Generating healthy counterfactuals with denoising diffusion bridge models</td>
<td style='padding: 6px;'>Ana Lawry Aguila, Peirong Liu, Marina Crespo Aguirre, Juan Eugenio Iglesias</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13684v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Generating healthy counterfactuals from pathological images holds significant promise in medical imaging, e.g., in anomaly detection or for application of analysis tools that are designed for healthy scans. These counterfactuals should represent what a patient's scan would plausibly look like in the absence of pathology, preserving individual anatomical characteristics while modifying only the pathological regions. Denoising diffusion probabilistic models (DDPMs) have become popular methods for generating healthy counterfactuals of pathology data. Typically, this involves training on solely healthy data with the assumption that a partial denoising process will be unable to model disease regions and will instead reconstruct a closely matched healthy counterpart. More recent methods have incorporated synthetic pathological images to better guide the diffusion process. However, it remains challenging to guide the generative process in a way that effectively balances the removal of anomalies with the retention of subject-specific features. To solve this problem, we propose a novel application of denoising diffusion bridge models (DDBMs) - which, unlike DDPMs, condition the diffusion process not only on the initial point (i.e., the healthy image), but also on the final point (i.e., a corresponding synthetically generated pathological image). Treating the pathological image as a structurally informative prior enables us to generate counterfactuals that closely match the patient's anatomy while selectively removing pathology. The results show that our DDBM outperforms previously proposed diffusion models and fully supervised approaches at segmentation and anomaly detection tasks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Spherical Radiomics - A Novel Approach to Glioblastoma Radiogenomic Analysis of Heterogeneity</td>
<td style='padding: 6px;'>Haotian Feng, Ke Sheng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13658v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We develop and validate a novel spherical radiomics framework for predicting key molecular biomarkers using multiparametric MRI. Conventional Cartesian radiomics extract tumor features on orthogonal grids, which do not fully capture the tumor's radial growth patterns and can be insensitive to evolving molecular signatures. In this study, we analyzed GBM radiomic features on concentric 2D shells, which were then mapped onto 2D planes for radiomics analysis. Radiomic features were extracted using PyRadiomics from four different regions in GBM. Feature selection was performed using ANOVA F-statistics. Classification was conducted with multiple machine-learning models. Model interpretability was evaluated through SHAP analysis, clustering analysis, feature significance profiling, and comparison between radiomic patterns and underlying biological processes. Spherical radiomics consistently outperformed conventional 2D and 3D Cartesian radiomics across all prediction tasks. The best framework reached an AUC of 0.85 for MGMT, 0.80 for EGFR, 0.80 for PTEN, and 0.83 for survival prediction. GLCM-derived features were identified as the most informative predictors. Radial transition analysis using the Mann-Whitney U-test demonstrates that transition slopes between T1-weighted contrast-enhancing and T2/FLAIR hyperintense lesion regions, as well as between T2 intense lesion and a 2 cm peritumoral expansion region, are significantly associated with biomarker status. Furthermore, the observed radiomic changes along the radial direction closely reflected known biological characteristics. Radiomic features extracted on the spherical surfaces at varying radial distances to the GBM tumor centroid are better correlated with important tumor molecular markers and patient survival than the conventional Cartesian analysis.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Challenges, Advances, and Evaluation Metrics in Medical Image Enhancement: A Systematic Literature Review</td>
<td style='padding: 6px;'>Chun Wai Chin, Haniza Yazid, Hoi Leong Lee</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13638v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Medical image enhancement is crucial for improving the quality and interpretability of diagnostic images, ultimately supporting early detection, accurate diagnosis, and effective treatment planning. Despite advancements in imaging technologies such as X-ray, CT, MRI, and ultrasound, medical images often suffer from challenges like noise, artifacts, and low contrast, which limit their diagnostic potential. Addressing these challenges requires robust preprocessing, denoising algorithms, and advanced enhancement methods, with deep learning techniques playing an increasingly significant role. This systematic literature review, following the PRISMA approach, investigates the key challenges, recent advancements, and evaluation metrics in medical image enhancement. By analyzing findings from 39 peer-reviewed studies, this review provides insights into the effectiveness of various enhancement methods across different imaging modalities and the importance of evaluation metrics in assessing their impact. Key issues like low contrast and noise are identified as the most frequent, with MRI and multi-modal imaging receiving the most attention, while specialized modalities such as histopathology, endoscopy, and bone scintigraphy remain underexplored. Out of the 39 studies, 29 utilize conventional mathematical methods, 9 focus on deep learning techniques, and 1 explores a hybrid approach. In terms of image quality assessment, 18 studies employ both reference-based and non-reference-based metrics, 9 rely solely on reference-based metrics, and 12 use only non-reference-based metrics, with a total of 65 IQA metrics introduced, predominantly non-reference-based. This review highlights current limitations, research gaps, and potential future directions for advancing medical image enhancement.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German Tumor Diagnoses</td>
<td style='padding: 6px;'>Stefan Lenz, Lakisha Ortiz Rosario, Georg Vollmar, Arsenij Ustjanzew, Fatma Alickovic, Thomas Kindler, Torsten Panholzer</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13624v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential for structured cancer documentation in Germany. Smaller open-weight LLMs are appealing for privacy-preserving automation but often struggle with coding accuracy in German-language contexts. This study investigates whether instruction-based fine-tuning on public datasets improves the coding accuracy of open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded diagnoses from the local tumor documentation system as test data. In a systematic data quality assessment, the upper limit for ICD-10 coding performance was estimated at 60-79% for exact and 81-94% for partial (three-character codes only) derivation. As training data, over 500,000 question-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS catalogues. Eight open-weight models from the Qwen, Llama, and Mistral families (7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to 41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3 topography coding also improved but started and remained considerably lower with an exact accuracy of 22-40% and a partial accuracy of 56-67% after fine-tuning. Malformed code outputs dropped to 0% for all models. Tumor-diagnosis recognition reached 99%. Accuracy correlated positively with model size, but gaps between small and large models narrowed after fine-tuning. The reasoning mode in Qwen3 generally yielded a lower performance than fine-tuning and was over 100 times slower. Our findings highlight the potential of leveraging public catalogues to build instruction datasets that improve LLMs in medical documentation tasks. The complete training dataset and the best-performing checkpoints of the fine-tuned models are available from https://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>An efficient approach with theoretical guarantees to simultaneously reconstruct activity and attenuation sinogram for TOF-PET</td>
<td style='padding: 6px;'>Liyang Hu, Chong Chen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13562v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In positron emission tomography (PET), it is indispensable to perform attenuation correction in order to obtain the quantitatively accurate activity map (tracer distribution) in the body. Generally, this is carried out based on the estimated attenuation map obtained from computed tomography or magnetic resonance imaging. However, except for errors in the attenuation correction factors obtained, the additional scan not only brings in new radiation doses and/or increases the scanning time but also leads to severe misalignment induced by various motions during and between the two sequential scans. To address these issues, based on maximum likelihood estimation, we propose a new mathematical model for simultaneously reconstructing the activity and attenuation sinogram from the time-of-flight (TOF)-PET emission data only. Particularly, we make full use of the exclusively exponential form for the attenuation correction factors, and consider the constraint of a total amount of the activity in some mask region in the proposed model. Furthermore, we prove its well-posedness, including the existence, uniqueness and stability of the solution. We propose an alternating update algorithm to solve the model, and also analyze its convergence. Finally, numerical experiments with various TOF-PET emission data demonstrate that the proposed method is of numerical convergence and robust to noise, and outperforms some state-of-the-art methods in terms of accuracy and efficiency, and has the capability of autonomous attenuation correction.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>ProtoTopic: Prototypical Network for Few-Shot Medical Topic Modeling</td>
<td style='padding: 6px;'>Martin Licht, Sara Ketabi, Farzad Khalvati</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13542v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Topic modeling is a useful tool for analyzing large corpora of written documents, particularly academic papers. Despite a wide variety of proposed topic modeling techniques, these techniques do not perform well when applied to medical texts. This can be due to the low number of documents available for some topics in the healthcare domain. In this paper, we propose ProtoTopic, a prototypical network-based topic model used for topic generation for a set of medical paper abstracts. Prototypical networks are efficient, explainable models that make predictions by computing distances between input datapoints and a set of prototype representations, making them particularly effective in low-data or few-shot learning scenarios. With ProtoTopic, we demonstrate improved topic coherence and diversity compared to two topic modeling baselines used in the literature, demonstrating the ability of our model to generate medically relevant topics even with limited data.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts</td>
<td style='padding: 6px;'>Shujun Xia, Haokun Lin, Yichen Wu, Yinan Zhou, Zixuan Li, Zhongwei Wan, Xingrun Xing, Yefeng Zheng, Xiang Li, Caifeng Shan, Zhenan Sun, Quanzheng Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13500v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>LLMs hold great promise for healthcare applications, but the rapid evolution of medical knowledge and errors in training data often cause them to generate outdated or inaccurate information, limiting their applicability in high-stakes clinical practice. Model editing has emerged as a potential remedy without full retraining. While parameter-based editing often compromises locality and is thus ill-suited for the medical domain, retrieval-based editing offers a more viable alternative. However, it still faces two critical challenges: (1) representation overlap within the medical knowledge space often causes inaccurate retrieval and reduces editing accuracy; (2) existing methods are restricted to single-sample edits, while batch-editing remains largely unexplored despite its importance for real-world medical applications. To address these challenges, we first construct MedVersa, \hk{an enhanced benchmark with broader coverage of medical subjects, designed to evaluate both single and batch edits under strict locality constraints}. We then propose MedREK, a retrieval-based editing framework that integrates a shared query-key module for precise matching with an attention-based prompt encoder for informative guidance. Experimental results on various medical benchmarks demonstrate that our MedREK achieves superior performance across different core metrics and provides the first validated solution for batch-editing in medical LLMs. Our code and dataset are available at https://github.com/mylittleriver/MedREK.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>A Quantitative Holographic Agglutination Assay for Immunoglobulin A</td>
<td style='padding: 6px;'>Rushna Quddus, Kent Kirshenbaum, David G. Grier</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13480v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study introduces a Holographic Agglutination Assay for quantifying levels of the immunoglobulin protein IgA in biological samples. This is the first example of a label-free and bead-free assay that quantifies protein agglutinates by direct detection using Total Holographic Characterization. A proof-of-concept assay for human serum immunoglobulins is demonstrated using Jacalin, the galactose-specific plant lectin, to induce selective agglutination.   By analyzing the size, refractive index, and number of particles in an assay sample, we obtain a reproducible and quantitative measurement of galactosylated immunoglobulins in a given sample. The assay is calibrated for a physiologically relevant reference interval of IgA concentrations in a 10x diluted emulated biological sample from low (80 mg/dL, 5 {\mu}M) to high (320 mg/dL, 20 {\mu}M) levels. The assay clearly distinguishes samples containing IgA from samples containing IgG.   More broadly, this study introduces a platform for creating lectin-mediated Holographic Agglutination Assays to monitor levels of immunoglobulins in biological samples. The ability to quantify immunoglobulin levels efficiently in clinical samples is likely to be valuable for diagnostics and will provide a basis for assaying other proteins that can be induced to agglutinate.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-15</td>
<td style='padding: 8px;'>Steerable Conditional Diffusion for Domain Adaptation in PET Image Reconstruction</td>
<td style='padding: 6px;'>George Webber, Alexander Hammers, Andrew P. King, Andrew J. Reader</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.13441v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Diffusion models have recently enabled state-of-the-art reconstruction of positron emission tomography (PET) images while requiring only image training data. However, domain shift remains a key concern for clinical adoption: priors trained on images from one anatomy, acquisition protocol or pathology may produce artefacts on out-of-distribution data. We propose integrating steerable conditional diffusion (SCD) with our previously-introduced likelihood-scheduled diffusion (PET-LiSch) framework to improve the alignment of the diffusion model's prior to the target subject. At reconstruction time, for each diffusion step, we use low-rank adaptation (LoRA) to align the diffusion model prior with the target domain on the fly. Experiments on realistic synthetic 2D brain phantoms demonstrate that our approach suppresses hallucinated artefacts under domain shift, i.e. when our diffusion model is trained on perturbed images and tested on normal anatomy, our approach suppresses the hallucinated structure, outperforming both OSEM and diffusion model baselines qualitatively and quantitatively. These results provide a proof-of-concept that steerable priors can mitigate domain shift in diffusion-based PET reconstruction and motivate future evaluation on real data.</td>
</tr>
</tbody>
</table>

