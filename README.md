<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2026-02-03</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-01</td>
<td style='padding: 8px;'>Sparse Reward Subsystem in Large Language Models</td>
<td style='padding: 6px;'>Guowei Xu, Mert Yuksekgonul, James Zou</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00986v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments, we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-01</td>
<td style='padding: 8px;'>Hybrid Topological and Deep Feature Fusion for Accurate MRI-Based Alzheimer's Disease Severity Classification</td>
<td style='padding: 6px;'>Faisal Ahmed</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00956v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Early and accurate diagnosis of Alzheimer's disease (AD) remains a critical challenge in neuroimaging-based clinical decision support systems. In this work, we propose a novel hybrid deep learning framework that integrates Topological Data Analysis (TDA) with a DenseNet121 backbone for four-class Alzheimer's disease classification using structural MRI data from the OASIS dataset. TDA is employed to capture complementary topological characteristics of brain structures that are often overlooked by conventional neural networks, while DenseNet121 efficiently learns hierarchical spatial features from MRI slices. The extracted deep and topological features are fused to enhance class separability across the four AD stages.   Extensive experiments conducted on the OASIS-1 Kaggle MRI dataset demonstrate that the proposed TDA+DenseNet121 model significantly outperforms existing state-of-the-art approaches. The model achieves an accuracy of 99.93% and an AUC of 100%, surpassing recently published CNN-based, transfer learning, ensemble, and multi-scale architectures. These results confirm the effectiveness of incorporating topological insights into deep learning pipelines and highlight the potential of the proposed framework as a robust and highly accurate tool for automated Alzheimer's disease diagnosis.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-31</td>
<td style='padding: 8px;'>Multi-Compartment Volume Conductor with Complete Electrode Model: Simulated Stereo-EEG Source Localization using Brainstorm-Zeffiro Plugin</td>
<td style='padding: 6px;'>Fernando Galaz Prieto, Takfarinas Medani, Chinmay Chinara, Richard M. Leahy, Sampsa Pursiainen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00684v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study introduces a novel integration of the Brainstorm (BST) software and the Zeffiro Interface (ZI) to enable whole-head, multi-compartment volume conductor modeling for electroencephalography (EEG) source imaging, with a particular focus on stereotactic EEG applications. We present the BST-2-ZI plugin, a MATLAB-based tool that facilitates seamless transfer of tissue segmentations and anatomical atlases from BST into ZI for finite element (FE) mesh generation as well as forward and inverse modeling. The generated FE meshes support variable spatial resolution and implement the complete electrode model (CEM), allowing for precise modeling of both invasive depth electrodes and non-invasive scalp electrodes. Using the ICBM152 template and synthetic source simulation, we demonstrate the end-to-end pipeline from MRI data to lead field (LF) computation and source localization in a stereotactic EEG (stereo-EEG) setting. Our numerical experiments highlight the capability of the pipeline to accurately model multi-compartment head geometry and conductivity with a stereotactic CEM-based electrode configuration. Our preliminary source localization results show how a synthetic stereo-EEG probe corresponding to a bidirectional deep brain stimulation (DBS) probe with four omnidirectional contacts can, in principle, be coupled with scalp electrodes to improve source localization in its vicinity.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-31</td>
<td style='padding: 8px;'>Three-Way Emotion Classification of EEG-based Signals using Machine Learning</td>
<td style='padding: 6px;'>Ashna Purwar, Gaurav Simkar, Madhumita, Sachin Kadam</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00670v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) is a widely used technique for measuring brain activity. EEG-based signals can reveal a persons emotional state, as they directly reflect activity in different brain regions. Emotion-aware systems and EEG-based emotion recognition are a growing research area. This paper presents how machine learning (ML) models categorize a limited dataset of EEG signals into three different classes, namely Negative, Neutral, or Positive. It also presents the complete workflow, including data preprocessing and comparison of ML models. To understand which ML classification model works best for this kind of problem, we train and test the following three commonly used models: logistic regression (LR), support vector machine (SVM), and random forest (RF). The performance of each is evaluated with respect to accuracy and F1-score. The results indicate that ML models can be effectively utilized for three-way emotion classification of EEG signals. Among the three ML models trained on the available dataset, the RF model gave the best results. Its higher accuracy and F1-score suggest that it is able to capture the emotional patterns more accurately and effectively than the other two models. The RF model also outperformed the existing state-of-the-art classification models in terms of the accuracy parameter.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-31</td>
<td style='padding: 8px;'>Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation</td>
<td style='padding: 6px;'>Marina Crespo Aguirre, Jonathan Williams-Ramirez, Dina Zemlyanker, Xiaoling Hu, Lucas J. Deden-Binder, Rogeny Herisse, Mark Montine, Theresa R. Connors, Christopher Mount, Christine L. MacDonald, C. Dirk Keene, Caitlin S. Latimer, Derek H. Oakley, Bradley T. Hyman, Ana Lawry Aguila, Juan Eugenio Iglesias</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00669v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhibit coarse, overly smooth reconstructions of structures, especially under high anisotropy (i.e., reconstructions from thick slabs). Here, we introduce a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions of dissection photographs. By training on domain-randomized synthetic data, we ensure that our method generalizes across dissection protocols and remains robust to large slab thicknesses. The imputed volumes yield improved automated segmentations, achieving higher Dice scores, particularly in cortical and white matter regions. Validation on surface reconstruction and atlas registration tasks demonstrates more accurate cortical surfaces and MRI registration. By enhancing the resolution and anatomical fidelity of photograph-based reconstructions, our approach strengthens the bridge between neuropathology and neuroimaging. Our method is publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/mri_3d_photo_recon</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-31</td>
<td style='padding: 8px;'>Uncovering Latent Communication Patterns in Brain Networks via Adaptive Flow Routing</td>
<td style='padding: 6px;'>Tianhao Huang, Guanghui Min, Zhenyu Lei, Aiying Zhang, Chen Chen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00561v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Unraveling how macroscopic cognitive phenotypes emerge from microscopic neuronal connectivity remains one of the core pursuits of neuroscience. To this end, researchers typically leverage multi-modal information from structural connectivity (SC) and functional connectivity (FC) to complete downstream tasks. Recent methodologies explore the intricate coupling mechanisms between SC and FC, attempting to fuse their representations at the regional level. However, lacking fundamental neuroscientific insight, these approaches fail to uncover the latent interactions between neural regions underlying these connectomes, and thus cannot explain why SC and FC exhibit dynamic states of both coupling and heterogeneity. In this paper, we formulate multi-modal fusion through the lens of neural communication dynamics and propose the Adaptive Flow Routing Network (AFR-Net), a physics-informed framework that models how structural constraints (SC) give rise to functional communication patterns (FC), enabling interpretable discovery of critical neural pathways. Extensive experiments demonstrate that AFR-Net significantly outperforms state-of-the-art baselines. The code is available at https://anonymous.4open.science/r/DIAL-F0D1.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-30</td>
<td style='padding: 8px;'>Robust automatic brain vessel segmentation in 3D CTA scans using dynamic 4D-CTA data</td>
<td style='padding: 6px;'>Alberto Mario Ceballos-Arroyo, Shrikanth M. Yadav, Chu-Hsuan Lin, Jisoo Kim, Geoffrey S. Young, Huaizu Jiang, Lei Qin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00391v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In this study, we develop a novel methodology for annotating the brain vasculature using dynamic 4D-CTA head scans. By using multiple time points from dynamic CTA acquisitions, we subtract bone and soft tissue to enhance the visualization of arteries and veins, reducing the effort required to obtain manual annotations of brain vessels. We then train deep learning models on our ground truth annotations by using the same segmentation for multiple phases from the dynamic 4D-CTA collection, effectively enlarging our dataset by 4 to 5 times and inducing robustness to contrast phases. In total, our dataset comprises 110 training images from 25 patients and 165 test images from 14 patients. In comparison with two similarly-sized datasets for CTA-based brain vessel segmentation, a nnUNet model trained on our dataset can achieve significantly better segmentations across all vascular regions, with an average mDC of 0.846 for arteries and 0.957 for veins in the TopBrain dataset. Furthermore, metrics such as average directed Hausdorff distance (adHD) and topology sensitivity (tSens) reflected similar trends: using our dataset resulted in low error margins (aDHD of 0.304 mm for arteries and 0.078 for veins) and high sensitivity (tSens of 0.877 for arteries and 0.974 for veins), indicating excellent accuracy in capturing vessel morphology. Our code and model weights are available online: https://github.com/alceballosa/robust-vessel-segmentation</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-30</td>
<td style='padding: 8px;'>Benchmarking Vanilla GAN, DCGAN, and WGAN Architectures for MRI Reconstruction: A Quantitative Analysis</td>
<td style='padding: 6px;'>Humaira Mehwish, Hina Shakir, Muneeba Rashid, Asarim Aamir, Reema Qaiser Khan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00221v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Magnetic Resonance Imaging (MRI) is a crucial imaging modality for viewing internal body structures. This research work analyses the performance of popular GAN models for accurate and precise MRI reconstruction by enhancing image quality and improving diagnostic accuracy. Three GAN architectures considered in this study are Vanilla GAN, Deep Convolutional GAN (DCGAN), and Wasserstein GAN (WGAN). They were trained and evaluated using knee, brain, and cardiac MRI datasets to assess their generalizability across body regions. While the Vanilla GAN operates on the fundamentals of the adversarial network setup, DCGAN advances image synthesis by securing the convolutional layers, giving a superior appearance to the prevalent spatial features. Training instability is resolved in WGAN through the Wasserstein distance to minimize an unstable regime, therefore, ensuring stable convergence and high-quality images. The GAN models were trained and tested using 1000 MR images of an anonymized knee, 805 images of Heart, 90 images of Brain MRI dataset. The Structural Similarity Index (SSIM) for Vanilla GAN is 0.84, DCGAN is 0.97, and WGAN is 0.99. The Peak Signal to Noise Ratio (PSNR) for Vanilla GAN is 26, DCGAN is 49.3, and WGAN is 43.5. The results were further statistically validated. This study shows that DCGAN and WGAN-based frameworks are promising in MR image reconstruction because of good image quality and superior accuracy. With the first cross-organ benchmark of baseline GANs under a common preprocessing pipeline, this work provides a reproducible benchmark for future hybrid GANs and clinical MRI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-30</td>
<td style='padding: 8px;'>Scale-Cascaded Diffusion Models for Super-Resolution in Medical Imaging</td>
<td style='padding: 6px;'>Darshan Thaker, Mahmoud Mostapha, Radu Miron, Shihan Qiu, Mariappan Nadar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.23201v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Diffusion models have been increasingly used as strong generative priors for solving inverse problems such as super-resolution in medical imaging. However, these approaches typically utilize a diffusion prior trained at a single scale, ignoring the hierarchical scale structure of image data. In this work, we propose to decompose images into Laplacian pyramid scales and train separate diffusion priors for each frequency band. We then develop an algorithm to perform super-resolution that utilizes these priors to progressively refine reconstructions across different scales. Evaluated on brain, knee, and prostate MRI data, our approach both improves perceptual quality over baselines and reduces inference time through smaller coarse-scale networks. Our framework unifies multiscale reconstruction and diffusion priors for medical image super-resolution.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-30</td>
<td style='padding: 8px;'>Omni-fMRI: A Universal Atlas-Free fMRI Foundation Model</td>
<td style='padding: 6px;'>Mo Wang, Wenhao Ye, Junfeng Xia, Junxiang Zhang, Xuanye Pan, Minghao Xu, Haotian Deng, Hongkai Wen, Quanying Liu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.23090v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Self-supervised fMRI foundation models have shown promising transfer performance, yet most rely on predefined region-level parcellations that discard fine-grained voxel information and introduce atlas-dependent biases. We propose Omni-fMRI, an atlas-free foundation model that operates directly on voxel-level signals. To enable scalable pretraining on 49,497 fMRI sessions across nine datasets, Omni-fMRI introduces a dynamic patching mechanism that substantially reduces computational cost while preserving informative spatial structure. To support reproducibility and fair comparison, we establish a comprehensive benchmark suite spanning 11 datasets and a diverse set of resting-state and task-based fMRI tasks. Experimental results demonstrate that Omni-fMRI consistently outperforms existing foundation models, providing a scalable and reproducible framework for atlas-free brain representation learning. Code and logs are available.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-31</td>
<td style='padding: 8px;'>Multi-Compartment Volume Conductor with Complete Electrode Model: Simulated Stereo-EEG Source Localization using Brainstorm-Zeffiro Plugin</td>
<td style='padding: 6px;'>Fernando Galaz Prieto, Takfarinas Medani, Chinmay Chinara, Richard M. Leahy, Sampsa Pursiainen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00684v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study introduces a novel integration of the Brainstorm (BST) software and the Zeffiro Interface (ZI) to enable whole-head, multi-compartment volume conductor modeling for electroencephalography (EEG) source imaging, with a particular focus on stereotactic EEG applications. We present the BST-2-ZI plugin, a MATLAB-based tool that facilitates seamless transfer of tissue segmentations and anatomical atlases from BST into ZI for finite element (FE) mesh generation as well as forward and inverse modeling. The generated FE meshes support variable spatial resolution and implement the complete electrode model (CEM), allowing for precise modeling of both invasive depth electrodes and non-invasive scalp electrodes. Using the ICBM152 template and synthetic source simulation, we demonstrate the end-to-end pipeline from MRI data to lead field (LF) computation and source localization in a stereotactic EEG (stereo-EEG) setting. Our numerical experiments highlight the capability of the pipeline to accurately model multi-compartment head geometry and conductivity with a stereotactic CEM-based electrode configuration. Our preliminary source localization results show how a synthetic stereo-EEG probe corresponding to a bidirectional deep brain stimulation (DBS) probe with four omnidirectional contacts can, in principle, be coupled with scalp electrodes to improve source localization in its vicinity.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-31</td>
<td style='padding: 8px;'>Three-Way Emotion Classification of EEG-based Signals using Machine Learning</td>
<td style='padding: 6px;'>Ashna Purwar, Gaurav Simkar, Madhumita, Sachin Kadam</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00670v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) is a widely used technique for measuring brain activity. EEG-based signals can reveal a persons emotional state, as they directly reflect activity in different brain regions. Emotion-aware systems and EEG-based emotion recognition are a growing research area. This paper presents how machine learning (ML) models categorize a limited dataset of EEG signals into three different classes, namely Negative, Neutral, or Positive. It also presents the complete workflow, including data preprocessing and comparison of ML models. To understand which ML classification model works best for this kind of problem, we train and test the following three commonly used models: logistic regression (LR), support vector machine (SVM), and random forest (RF). The performance of each is evaluated with respect to accuracy and F1-score. The results indicate that ML models can be effectively utilized for three-way emotion classification of EEG signals. Among the three ML models trained on the available dataset, the RF model gave the best results. Its higher accuracy and F1-score suggest that it is able to capture the emotional patterns more accurately and effectively than the other two models. The RF model also outperformed the existing state-of-the-art classification models in terms of the accuracy parameter.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-31</td>
<td style='padding: 8px;'>Physiology as Language: Translating Respiration to Sleep EEG</td>
<td style='padding: 6px;'>Kaiwen Zha, Chao Li, Hao He, Peng Cao, Tianhong Li, Ali Mirzazadeh, Ellen Zhang, Jong Woo Lee, Yoon Kim, Dina Katabi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00526v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper introduces a novel cross-physiology translation task: synthesizing sleep electroencephalography (EEG) from respiration signals. To address the significant complexity gap between the two modalities, we propose a waveform-conditional generative framework that preserves fine-grained respiratory dynamics while constraining the EEG target space through discrete tokenization. Trained on over 28,000 individuals, our model achieves a 7% Mean Absolute Error in EEG spectrogram reconstruction. Beyond reconstruction, the synthesized EEG supports downstream tasks with performance comparable to ground truth EEG on age estimation (MAE 5.0 vs. 5.1 years), sex detection (AUROC 0.81 vs. 0.82), and sleep staging (Accuracy 0.84 vs. 0.88), significantly outperforming baselines trained directly on breathing. Finally, we demonstrate that the framework generalizes to contactless sensing by synthesizing EEG from wireless radio-frequency reflections, highlighting the feasibility of remote, non-contact neurological assessment during sleep.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-30</td>
<td style='padding: 8px;'>Computationally efficient segmentation for non-stationary time series with oscillatory patterns</td>
<td style='padding: 6px;'>Nicolas Bianco, Lorenzo Cappello</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.22999v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We propose a novel approach for change-point detection and parameter learning in multivariate non-stationary time series exhibiting oscillatory behaviour. We approximate the process through a piecewise function defined by a sum of sinusoidal functions with unknown frequencies and amplitudes plus noise. The inference for this model is non-trivial. However, discretising the parameter space allows us to recast this complex estimation problem into a more tractable linear model, where the covariates are Fourier basis functions. Then, any change-point detection algorithms for segmentation can be used. The advantage of our proposal is that it bypasses the need for trans-dimensional Markov chain Monte Carlo algorithms used by state-of-the-art methods. Through simulations, we demonstrate that our method is significantly faster than existing approaches while maintaining comparable numerical accuracy. We also provide high probability bounds on the change-point localization error. We apply our methodology to climate and EEG sleep data.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-29</td>
<td style='padding: 8px;'>Brain-Informed Speech Separation for Cochlear Implants</td>
<td style='padding: 6px;'>Tom Gajecki, Jonas Althoff, Waldo Nogueira</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.22260v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We propose a brain-informed speech separation method for cochlear implants (CIs) that uses electroencephalography (EEG)-derived attention cues to guide enhancement toward the attended speaker. An attention-guided network fuses audio mixtures with EEG features through a lightweight fusion layer, producing attended-source electrodograms for CI stimulation while resolving the label-permutation ambiguity of audio-only separators. Robustness to degraded attention cues is improved with a mixed curriculum that varies cue quality during training, yielding stable gains even when EEG-speech correlation is moderate. In multi-talker conditions, the model achieves higher signal-to-interference ratio improvements than an audio-only electrodogram baseline while remaining slightly smaller (167k vs. 171k parameters). With 2 ms algorithmic latency and comparable cost, the approach highlights the promise of coupling auditory and neural cues for cognitively adaptive CI processing.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-29</td>
<td style='padding: 8px;'>Cognitive Load Estimation Using Brain Foundation Models and Interpretability for BCIs</td>
<td style='padding: 6px;'>Deeksha M. Shama, Dimitra Emmanouilidou, Ivan J. Tashev</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.21965v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurately monitoring cognitive load in real time is critical for Brain-Computer Interfaces (BCIs) that adapt to user engagement and support personalized learning. Electroencephalography (EEG) offers a non-invasive, cost-effective modality for capturing neural activity, though traditional methods often struggle with cross-subject variability and task-specific preprocessing. We propose leveraging Brain Foundation Models (BFMs), large pre-trained neural networks, to extract generalizable EEG features for cognitive load estimation. We adapt BFMs for long-term EEG monitoring and show that fine-tuning a small subset of layers yields improved accuracy over the state-of-the-art. Despite their scale, BFMs allow for real-time inference with a longer context window. To address often-overlooked interpretability challenges, we apply Partition SHAP (SHapley Additive exPlanations) to quantify feature importance. Our findings reveal consistent emphasis on prefrontal regions linked to cognitive control, while longitudinal trends suggest learning progression. These results position BFMs as efficient and interpretable tools for continuous cognitive load monitoring in real-world BCIs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-29</td>
<td style='padding: 8px;'>Synthesizing Epileptic Seizures: Gaussian Processes for EEG Generation</td>
<td style='padding: 6px;'>Nina Moutonnet, Joshua Corneck, Felipe Tobar, Danilo Mandic</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.21752v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Reliable seizure detection from electroencephalography (EEG) time series is a high-priority clinical goal, yet the acquisition cost and scarcity of labeled EEG data limit the performance of machine learning methods. This challenge is exacerbated by the long-range, high-dimensional, and non-stationary nature of epileptic EEG recordings, which makes realistic data generation particularly difficult. In this work, we revisit Gaussian processes as a principled and interpretable foundation for modeling EEG dynamics, and propose a novel hierarchical framework, \textit{GP-EEG}, for generating synthetic epileptic EEG recordings. At its core, our approach decomposes EEG signals into temporal segments modeled via Gaussian process regression, and integrates a domain-adaptation variational autoencoder. We validate the proposed method on two real-world, open-source epileptic EEG datasets. The synthetic EEG recordings generated by our model match real-world epileptic EEG both quantitatively and qualitatively, and can be used to augment training sets.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-29</td>
<td style='padding: 8px;'>Neural Signals Generate Clinical Notes in the Wild</td>
<td style='padding: 6px;'>Jathurshan Pradeepkumar, Zheng Chen, Jimeng Sun</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.22197v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\%$--$95\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL].</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-29</td>
<td style='padding: 8px;'>A Unified SPD Token Transformer Framework for EEG Classification: Systematic Comparison of Geometric Embeddings</td>
<td style='padding: 6px;'>Chi-Sheng Chen, En-Jui Kuo, Guan-Ying Chen, Xinyu Zhang, Fan Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.21521v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Spatial covariance matrices of EEG signals are Symmetric Positive Definite (SPD) and lie on a Riemannian manifold, yet the theoretical connection between embedding geometry and optimization dynamics remains unexplored. We provide a formal analysis linking embedding choice to gradient conditioning and numerical stability for SPD manifolds, establishing three theoretical results: (1) BWSPD's $\sqrtκ$ gradient conditioning (vs $κ$ for Log-Euclidean) via Daleckii-Kreĭn matrices provides better gradient conditioning on high-dimensional inputs ($d \geq 22$), with this advantage reducing on low-dimensional inputs ($d \leq 8$) where eigendecomposition overhead dominates; (2) Embedding-Space Batch Normalization (BN-Embed) approximates Riemannian normalization up to $O(\varepsilon^2)$ error, yielding $+26\%$ accuracy on 56-channel ERP data but negligible effect on 8-channel SSVEP data, matching the channel-count-dependent prediction; (3) bi-Lipschitz bounds prove BWSPD tokens preserve manifold distances with distortion governed solely by the condition ratio $κ$. We validate these predictions via a unified Transformer framework comparing BWSPD, Log-Euclidean, and Euclidean embeddings within identical architecture across 1,500+ runs on three EEG paradigms (motor imagery, ERP, SSVEP; 36 subjects). Our Log-Euclidean Transformer achieves state-of-the-art performance on all datasets, substantially outperforming classical Riemannian classifiers and recent SPD baselines, while BWSPD offers competitive accuracy with similar training time.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-29</td>
<td style='padding: 8px;'>HADUA: Hierarchical Attention and Dynamic Uniform Alignment for Robust Cross-Subject Emotion Recognition</td>
<td style='padding: 6px;'>Jiahao Tang, Youjun Li, Yangxuan Zheng, Xiangting Fan, Siyuan Lu, Nuo Zhang, Zi-Gang Huang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.21488v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Robust cross-subject emotion recognition from multimodal physiological signals remains a challenging problem, primarily due to modality heterogeneity and inter-subject distribution shift. To tackle these challenges, we propose a novel adaptive learning framework named Hierarchical Attention and Dynamic Uniform Alignment (HADUA). Our approach unifies the learning of multimodal representations with domain adaptation. First, we design a hierarchical attention module that explicitly models intra-modal temporal dynamics and inter-modal semantic interactions (e.g., between electroencephalogram(EEG) and eye movement(EM)), yielding discriminative and semantically coherent fused features. Second, to overcome the noise inherent in pseudo-labels during adaptation, we introduce a confidence-aware Gaussian weighting scheme that smooths the supervision from target-domain samples by down-weighting uncertain instances. Third, a uniform alignment loss is employed to regularize the distribution of pseudo-labels across classes, thereby mitigating imbalance and stabilizing conditional distribution matching. Extensive experiments on multiple cross-subject emotion recognition benchmarks show that HADUA consistently surpasses existing state-of-the-art methods in both accuracy and robustness, validating its effectiveness in handling modality gaps, noisy pseudo-labels, and class imbalance. Taken together, these contributions offer a practical and generalizable solution for building robust cross-subject affective computing systems.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-29</td>
<td style='padding: 8px;'>Cognitive Load Estimation Using Brain Foundation Models and Interpretability for BCIs</td>
<td style='padding: 6px;'>Deeksha M. Shama, Dimitra Emmanouilidou, Ivan J. Tashev</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.21965v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurately monitoring cognitive load in real time is critical for Brain-Computer Interfaces (BCIs) that adapt to user engagement and support personalized learning. Electroencephalography (EEG) offers a non-invasive, cost-effective modality for capturing neural activity, though traditional methods often struggle with cross-subject variability and task-specific preprocessing. We propose leveraging Brain Foundation Models (BFMs), large pre-trained neural networks, to extract generalizable EEG features for cognitive load estimation. We adapt BFMs for long-term EEG monitoring and show that fine-tuning a small subset of layers yields improved accuracy over the state-of-the-art. Despite their scale, BFMs allow for real-time inference with a longer context window. To address often-overlooked interpretability challenges, we apply Partition SHAP (SHapley Additive exPlanations) to quantify feature importance. Our findings reveal consistent emphasis on prefrontal regions linked to cognitive control, while longitudinal trends suggest learning progression. These results position BFMs as efficient and interpretable tools for continuous cognitive load monitoring in real-world BCIs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-29</td>
<td style='padding: 8px;'>Rethinking Self-Training Based Cross-Subject Domain Adaptation for SSVEP Classification</td>
<td style='padding: 6px;'>Weiguang Wang, Yong Liu, Yingjie Gao, Guangyuan Xu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.21203v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Steady-state visually evoked potentials (SSVEP)-based brain-computer interfaces (BCIs) are widely used due to their high signal-to-noise ratio and user-friendliness. Accurate decoding of SSVEP signals is crucial for interpreting user intentions in BCI applications. However, signal variability across subjects and the costly user-specific annotation limit recognition performance. Therefore, we propose a novel cross-subject domain adaptation method built upon the self-training paradigm. Specifically, a Filter-Bank Euclidean Alignment (FBEA) strategy is designed to exploit frequency information from SSVEP filter banks. Then, we propose a Cross-Subject Self-Training (CSST) framework consisting of two stages: Pre-Training with Adversarial Learning (PTAL), which aligns the source and target distributions, and Dual-Ensemble Self-Training (DEST), which refines pseudo-label quality. Moreover, we introduce a Time-Frequency Augmented Contrastive Learning (TFA-CL) module to enhance feature discriminability across multiple augmented views. Extensive experiments on the Benchmark and BETA datasets demonstrate that our approach achieves state-of-the-art performance across varying signal lengths, highlighting its superiority.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-28</td>
<td style='padding: 8px;'>Assembling the Mind's Mosaic: Towards EEG Semantic Intent Decoding</td>
<td style='padding: 6px;'>Jiahe Li, Junru Chen, Fanqi Shen, Jialan Yang, Jada Li, Zhizhang Yuan, Baowen Cheng, Meng Li, Yang Yang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.20447v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Enabling natural communication through brain-computer interfaces (BCIs) remains one of the most profound challenges in neuroscience and neurotechnology. While existing frameworks offer partial solutions, they are constrained by oversimplified semantic representations and a lack of interpretability. To overcome these limitations, we introduce Semantic Intent Decoding (SID), a novel framework that translates neural activity into natural language by modeling meaning as a flexible set of compositional semantic units. SID is built on three core principles: semantic compositionality, continuity and expandability of semantic space, and fidelity in reconstruction. We present BrainMosaic, a deep learning architecture implementing SID. BrainMosaic decodes multiple semantic units from EEG/SEEG signals using set matching and then reconstructs coherent sentences through semantic-guided reconstruction. This approach moves beyond traditional pipelines that rely on fixed-class classification or unconstrained generation, enabling a more interpretable and expressive communication paradigm. Extensive experiments on multilingual EEG and clinical SEEG datasets demonstrate that SID and BrainMosaic offer substantial advantages over existing frameworks, paving the way for natural and effective BCI-mediated communication.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-27</td>
<td style='padding: 8px;'>Cross-Session Decoding of Neural Spiking Data via Task-Conditioned Latent Alignment</td>
<td style='padding: 6px;'>Canyang Zhao, Bolin Peng, J. Patrick Mayo, Ce Ju, Bing Liu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.19963v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Cross-session nonstationarity in neural activity recorded by implanted electrodes is a major challenge for invasive Brain-computer interfaces (BCIs), as decoders trained on data from one session often fail to generalize to subsequent sessions. This issue is further exacerbated in practice, as retraining or adapting decoders becomes particularly challenging when only limited data are available from a new session. To address this challenge, we propose a Task-Conditioned Latent Alignment framework (TCLA) for cross-session neural decoding. Building upon an autoencoder architecture, TCLA first learns a low-dimensional representation of neural dynamics from a source session with sufficient data. For target sessions with limited data, TCLA then aligns target latent representations to the source in a task-conditioned manner, enabling effective transfer of learned neural dynamics. We evaluate TCLA on the macaque motor and oculomotor center-out dataset. Compared to baseline methods trained solely on target-session data, TCLA consistently improves decoding performance across datasets and decoding settings, with gains in the coefficient of determination of up to 0.386 for y coordinate velocity decoding in a motor dataset. These results suggest that TCLA provides an effective strategy for transferring knowledge from source to target sessions, enabling more robust neural decoding under conditions with limited data.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-27</td>
<td style='padding: 8px;'>A Personalized and Adaptable User Interface for a Speech and Cursor Brain-Computer Interface</td>
<td style='padding: 6px;'>Hamza Peracha, Carrina Iacobacci, Tyler Singer-Clark, Leigh R. Hochberg, Sergey D. Stavisky, David M. Brandman, Nicholas S. Card</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.19269v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Communication and computer interaction are important for autonomy in modern life. Unfortunately, these capabilities can be limited or inaccessible for the millions of people living with paralysis. While implantable brain-computer interfaces (BCIs) show promise for restoring these capabilities, little has been explored on designing BCI user interfaces (UIs) for sustained daily use. Here, we present a personalized UI for an intracortical BCI system that enables users with severe paralysis to communicate and interact with their computers independently. Through a 22-month longitudinal deployment with one participant, we used iterative co-design to develop a system for everyday at-home use and documented how it evolved to meet changing needs. Our findings highlight how personalization and adaptability enabled independence in daily life and provide design implications for developing future BCI assistive technologies.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-25</td>
<td style='padding: 8px;'>EEG Foundation Models: Progresses, Benchmarking, and Open Problems</td>
<td style='padding: 6px;'>Dingkun Liu, Yuheng Chen, Zhu Chen, Zhenyao Cui, Yaozhi Wen, Jiayu An, Jingwei Luo, Dongrui Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.17883v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-24</td>
<td style='padding: 8px;'>BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation</td>
<td style='padding: 6px;'>Yuhan Xie, Jinhan Liu, Xiaoyong Ni, Fei Tan, Icare Sakr, Thibault Collin, Shiqi Sun, Alejandro Rodriguez Guajardo, Demon Fanny, Charles-francois Vincent Latchoumane, Henri Lorach, Jocelyne Bloch, Gregoire Courtine, Mahsa Shoaran</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.17625v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce BrainDistill, a novel implantable motor decoding pipeline that integrates an implantable neural decoder (IND) with a task-specific knowledge distillation (TSKD) framework. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. Across multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-18</td>
<td style='padding: 8px;'>HCFT: Hierarchical Convolutional Fusion Transformer for EEG Decoding</td>
<td style='padding: 6px;'>Haodong Zhang, Jiapeng Zhu, Yitong Chen, Hongqi Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.12279v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) decoding requires models that can effectively extract and integrate complex temporal, spectral, and spatial features from multichannel signals. To address this challenge, we propose a lightweight and generalizable decoding framework named Hierarchical Convolutional Fusion Transformer (HCFT), which combines dual-branch convolutional encoders and hierarchical Transformer blocks for multi-scale EEG representation learning. Specifically, the model first captures local temporal and spatiotemporal dynamics through time-domain and time-space convolutional branches, and then aligns these features via a cross-attention mechanism that enables interaction between branches at each stage. Subsequently, a hierarchical Transformer fusion structure is employed to encode global dependencies across all feature stages, while a customized Dynamic Tanh normalization module is introduced to replace traditional Layer Normalization in order to enhance training stability and reduce redundancy. Extensive experiments are conducted on two representative benchmark datasets, BCI Competition IV-2b and CHB-MIT, covering both event-related cross-subject classification and continuous seizure prediction tasks. Results show that HCFT achieves 80.83% average accuracy and a Cohen's kappa of 0.6165 on BCI IV-2b, as well as 99.10% sensitivity, 0.0236 false positives per hour, and 98.82% specificity on CHB-MIT, consistently outperforming over ten state-of-the-art baseline methods. Ablation studies confirm that each core component of the proposed framework contributes significantly to the overall decoding performance, demonstrating HCFT's effectiveness in capturing EEG dynamics and its potential for real-world BCI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-12</td>
<td style='padding: 8px;'>Backpropagation-Free Test-Time Adaptation for Lightweight EEG-Based Brain-Computer Interfaces</td>
<td style='padding: 6px;'>Siyang Li, Jiayi Ouyang, Zhenyao Cui, Ziwei Wang, Tianwang Jia, Feng Wan, Dongrui Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.07556v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalogram (EEG)-based brain-computer interfaces (BCIs) face significant deployment challenges due to inter-subject variability, signal non-stationarity, and computational constraints. While test-time adaptation (TTA) mitigates distribution shifts under online data streams without per-use calibration sessions, existing TTA approaches heavily rely on explicitly defined loss objectives that require backpropagation for updating model parameters, which incurs computational overhead, privacy risks, and sensitivity to noisy data streams. This paper proposes Backpropagation-Free Transformations (BFT), a TTA approach for EEG decoding that eliminates such issues. BFT applies multiple sample-wise transformations of knowledge-guided augmentations or approximate Bayesian inference to each test trial, generating multiple prediction scores for a single test sample. A learning-to-rank module enhances the weighting of these predictions, enabling robust aggregation for uncertainty suppression during inference under theoretical justifications. Extensive experiments on five EEG datasets of motor imagery classification and driver drowsiness regression tasks demonstrate the effectiveness, versatility, robustness, and efficiency of BFT. This research enables lightweight plug-and-play BCIs on resource-constrained devices, broadening the real-world deployment of decoding algorithms for EEG-based BCI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-09</td>
<td style='padding: 8px;'>Bidirectional Channel-selective Semantic Interaction for Semi-Supervised Medical Segmentation</td>
<td style='padding: 6px;'>Kaiwen Huang, Yizhe Zhang, Yi Zhou, Tianyang Xu, Tao Zhou</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.05855v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Semi-supervised medical image segmentation is an effective method for addressing scenarios with limited labeled data. Existing methods mainly rely on frameworks such as mean teacher and dual-stream consistency learning. These approaches often face issues like error accumulation and model structural complexity, while also neglecting the interaction between labeled and unlabeled data streams. To overcome these challenges, we propose a Bidirectional Channel-selective Semantic Interaction~(BCSI) framework for semi-supervised medical image segmentation. First, we propose a Semantic-Spatial Perturbation~(SSP) mechanism, which disturbs the data using two strong augmentation operations and leverages unsupervised learning with pseudo-labels from weak augmentations. Additionally, we employ consistency on the predictions from the two strong augmentations to further improve model stability and robustness. Second, to reduce noise during the interaction between labeled and unlabeled data, we propose a Channel-selective Router~(CR) component, which dynamically selects the most relevant channels for information exchange. This mechanism ensures that only highly relevant features are activated, minimizing unnecessary interference. Finally, the Bidirectional Channel-wise Interaction~(BCI) strategy is employed to supplement additional semantic information and enhance the representation of important channels. Experimental results on multiple benchmarking 3D medical datasets demonstrate that the proposed method outperforms existing semi-supervised approaches.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-30</td>
<td style='padding: 8px;'>Omni-fMRI: A Universal Atlas-Free fMRI Foundation Model</td>
<td style='padding: 6px;'>Mo Wang, Wenhao Ye, Junfeng Xia, Junxiang Zhang, Xuanye Pan, Minghao Xu, Haotian Deng, Hongkai Wen, Quanying Liu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.23090v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Self-supervised fMRI foundation models have shown promising transfer performance, yet most rely on predefined region-level parcellations that discard fine-grained voxel information and introduce atlas-dependent biases. We propose Omni-fMRI, an atlas-free foundation model that operates directly on voxel-level signals. To enable scalable pretraining on 49,497 fMRI sessions across nine datasets, Omni-fMRI introduces a dynamic patching mechanism that substantially reduces computational cost while preserving informative spatial structure. To support reproducibility and fair comparison, we establish a comprehensive benchmark suite spanning 11 datasets and a diverse set of resting-state and task-based fMRI tasks. Experimental results demonstrate that Omni-fMRI consistently outperforms existing foundation models, providing a scalable and reproducible framework for atlas-free brain representation learning. Code and logs are available.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-25</td>
<td style='padding: 8px;'>SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction</td>
<td style='padding: 6px;'>Lan Yang, Minghan Yang, Ke Li, Honggang Zhang, Kaiyue Pang, Yi-Zhe Song</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.17857v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-23</td>
<td style='padding: 8px;'>Quantum Sensing MRI for Noninvasive Detection of Neuronal Electrical Activity in Human Brains</td>
<td style='padding: 6px;'>Yongxian Qian, Ying-Chia Lin, Seyedehsara Hejazi, Kamri Clarke, Kennedy Watson, Xingye Chen, Nahbila-Malikha Kumbella, Justin Quimbo, Abena Dinizulu, Simon Henin, Yulin Ge, Arjun Masurkar, Anli Liu, Yvonne W. Lui, Fernando E. Boada</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.16423v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuronal electrical activity underlies human cognition, yet its direct, noninvasive measurement in the living human brain remains a fundamental challenge. Existing neuroimaging techniques, including EEG, MEG, and fMRI, are limited by trade-offs in sensitivity and spatial or temporal resolution. Here we propose quantum sensing MRI (qsMRI), a noninvasive approach that enables direct detection of neuronal firing-induced magnetic fields using a clinical MRI system. qsMRI exploits endogenous proton (1H) nuclear spins in water molecules as intrinsic quantum sensors and decodes time-resolved phase information from free induction decay (FID) signals to infer neuronal magnetic fields. We validate qsMRI through simulations, phantom experiments, and human studies at rest and during motor tasks, and provide open experimental procedures to facilitate independent validation. We further present a case study demonstrating potential applications to neurological disorders. qsMRI represents a first-in-human application of quantum sensing on a clinical MRI platform, establishes a non-BOLD functional imaging modality, and enables interrogation of neuronal firing dynamics in both cortical and deep brain regions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-22</td>
<td style='padding: 8px;'>Experience with Single Domain Generalization in Real World Medical Imaging Deployments</td>
<td style='padding: 6px;'>Ayan Banerjee, Komandoor Srivathsan, Sandeep K. S. Gupta</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.16359v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A desirable property of any deployed artificial intelligence is generalization across domains, i.e. data generation distribution under a specific acquisition condition. In medical imagining applications the most coveted property for effective deployment is Single Domain Generalization (SDG), which addresses the challenge of training a model on a single domain to ensure it generalizes well to unseen target domains. In multi-center studies, differences in scanners and imaging protocols introduce domain shifts that exacerbate variability in rare class characteristics. This paper presents our experience on SDG in real life deployment for two exemplary medical imaging case studies on seizure onset zone detection using fMRI data, and stress electrocardiogram based coronary artery detection. Utilizing the commonly used application of diabetic retinopathy, we first demonstrate that state-of-the-art SDG techniques fail to achieve generalized performance across data domains. We then develop a generic expert knowledge integrated deep learning technique DL+EKE and instantiate it for the DR application and show that DL+EKE outperforms SOTA SDG methods on DR. We then deploy instances of DL+EKE technique on the two real world examples of stress ECG and resting state (rs)-fMRI and discuss issues faced with SDG techniques.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-22</td>
<td style='padding: 8px;'>Matrix-Response Generalized Linear Mixed Model with Applications to Longitudinal Brain Images</td>
<td style='padding: 6px;'>Zhentao Yu, Jiaqi Ding, Guorong Wu, Quefeng Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.16340v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Longitudinal brain imaging data facilitate the monitoring of structural and functional alterations in individual brains across time, offering essential understanding of dynamic neurobiological mechanisms. Such data improve sensitivity for detecting early biomarkers of disease progression and enhance the evaluation of intervention effects. While recent matrix-response regression models can relate static brain networks to external predictors, there remain few statistical methods for longitudinal brain networks, especially those derived from high-dimensional imaging data. We introduce a matrix-response generalized linear mixed model that accommodates longitudinal brain networks and identifies edges whose connectivity is influenced by external predictors. An efficient Monte Carlo Expectation-Maximization algorithm is developed for parameter estimation. Extensive simulations demonstrate effective identification of covariate-related network components and accurate parameter estimation. We further demonstrate the usage of the proposed method through applications to diffusion tensor imaging (DTI) and functional MRI (fMRI) datasets.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-21</td>
<td style='padding: 8px;'>The Pictorial Cortex: Zero-Shot Cross-Subject fMRI-to-Image Reconstruction via Compositional Latent Modeling</td>
<td style='padding: 6px;'>Jingyang Huo, Yikai Wang, Yanwei Fu, Jianfeng Feng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.15071v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Decoding visual experiences from human brain activity remains a central challenge at the intersection of neuroscience, neuroimaging, and artificial intelligence. A critical obstacle is the inherent variability of cortical responses: neural activity elicited by the same visual stimulus differs across individuals and trials due to anatomical, functional, cognitive, and experimental factors, making fMRI-to-image reconstruction non-injective. In this paper, we tackle a challenging yet practically meaningful problem: zero-shot cross-subject fMRI-to-image reconstruction, where the visual experience of a previously unseen individual must be reconstructed without subject-specific training. To enable principled evaluation, we present a unified cortical-surface dataset -- UniCortex-fMRI, assembled from multiple visual-stimulus fMRI datasets to provide broad coverage of subjects and stimuli. Our UniCortex-fMRI is particularly processed by standardized data formats to make it possible to explore this possibility in the zero-shot scenario of cross-subject fMRI-to-image reconstruction. To tackle the modeling challenge, we propose PictorialCortex, which models fMRI activity using a compositional latent formulation that structures stimulus-driven representations under subject-, dataset-, and trial-related variability. PictorialCortex operates in a universal cortical latent space and implements this formulation through a latent factorization-composition module, reinforced by paired factorization and re-factorizing consistency regularization. During inference, surrogate latents synthesized under multiple seen-subject conditions are aggregated to guide diffusion-based image synthesis for unseen subjects. Extensive experiments show that PictorialCortex improves zero-shot cross-subject visual reconstruction, highlighting the benefits of compositional latent modeling and multi-dataset training.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-20</td>
<td style='padding: 8px;'>A Dual-Head Transformer-State-Space Architecture for Neurocircuit Mechanism Decomposition from fMRI</td>
<td style='padding: 6px;'>Cole Korponay</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.15344v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Precision psychiatry aspires to elucidate brain-based biomarkers of psychopathology to bolster disease risk assessment and treatment development. To this end, functional magnetic resonance imaging (fMRI) has helped triangulate brain circuits whose functional features are correlated with or even predictive of forms of psychopathology. Yet, fMRI biomarkers to date remain largely descriptive identifiers of where, rather than how, neurobiology is aberrant, limiting their utility for guiding treatment. We present a method for decomposing fMRI-based functional connectivity (FC) into constituent biomechanisms - output drive, input responsivity, modulator gating - with clearer alignment to differentiable therapeutic interventions. Neurocircuit mechanism decomposition (NMD) integrates (i) a graph-constrained, lag-aware transformer to estimate directed, pathway-specific routing distributions and drive signals, with (ii) a measurement-aware state-space model (SSM) that models hemodynamic convolution and recovers intrinsic latent dynamics. This dual-head architecture yields interpretable circuit parameters that may provide a more direct bridge from fMRI to treatment strategy selection. We instantiate the model in an anatomically and electrophysiologically well-defined circuit: the cortico-basal ganglia-thalamo-cortical loop.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-19</td>
<td style='padding: 8px;'>Multifaceted neural representation of words in naturalistic language</td>
<td style='padding: 6px;'>Xuan Yang, Chuanji Gao, Cheng Xiao, Nicholas Riccardi, Rutvik H. Desai</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.13297v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding how the brain represents the multifaceted properties of words in context is essential for explaining the neural architecture of human language. Here, we combine large-scale psycholinguistic modeling with naturalistic fMRI to uncover the latent structure of word properties and their neural representations during narrative comprehension. By analyzing 106 psycholinguistic variables across 13,850 English words, we identified eight interpretable latent dimensions spanning lexical usage, word form, phonology orthography mapping, sublexical regularity, and semantic organization. These factors robustly predicted behavioral performance across lexical decision, naming, recognition, and semantic judgment tasks, demonstrating their cognitive relevance. Parcel-based and multivariate fMRI analyses of narrative listening revealed that these latent dimensions are encoded in overlapping yet functionally differentiated cortical systems. Multidimensional scaling and hierarchical clustering analyses further identified four interacting subsystems supporting sensorimotor grounding, controlled semantic retrieval, resolution of lexical competition, and contextual episodic integration. Together, these findings provide a unified neurocognitive framework linking fundamental lexical psycholinguistic dimensions to distributed cortical systems engaged during naturalistic language comprehension.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-14</td>
<td style='padding: 8px;'>LARGE: A Locally Adaptive Regularization Approach for Estimating Gaussian Graphical Models</td>
<td style='padding: 6px;'>Ha Nguyen, Sumanta Basu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.09686v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The graphical Lasso (GLASSO) is a widely used algorithm for learning high-dimensional undirected Gaussian graphical models (GGM). Given i.i.d. observations from a multivariate normal distribution, GLASSO estimates the precision matrix by maximizing the log-likelihood with an \ell_1-penalty on the off-diagonal entries. However, selecting an optimal regularization parameter λin this unsupervised setting remains a significant challenge. A well-known issue is that existing methods, such as out-of-sample likelihood maximization, select a single global λand do not account for heterogeneity in variable scaling or partial variances. Standardizing the data to unit variances, although a common workaround, has been shown to negatively affect graph recovery. Addressing the problem of nodewise adaptive tuning in graph estimation is crucial for applications like computational neuroscience, where brain networks are constructed from highly heterogeneous, region-specific fMRI data.   In this work, we develop Locally Adaptive Regularization for Graph Estimation (LARGE), an approach to adaptively learn nodewise tuning parameters to improve graph estimation and selection. In each block coordinate descent step of GLASSO, we augment the nodewise Lasso regression to jointly estimate the regression coefficients and error variance, which in turn guides the adaptive learning of nodewise penalties. In simulations, LARGE consistently outperforms benchmark methods in graph recovery, demonstrates greater stability across replications, and achieves the best estimation accuracy in the most difficult simulation settings. We demonstrate the practical utility of our method by estimating brain functional connectivity from a real fMRI data set.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-14</td>
<td style='padding: 8px;'>Sparse covariate-driven factorization of high-dimensional brain connectivity with application to site effect correction</td>
<td style='padding: 6px;'>Rongqian Zhang, Elena Tuzhilina, Jun Young Park</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.09525v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large-scale neuroimaging studies often collect data from multiple scanners across different sites, where variations in scanners, scanning procedures, and other conditions across sites can introduce artificial site effects. These effects may bias brain connectivity measures, such as functional connectivity (FC), which quantify functional network organization derived from functional magnetic resonance imaging (fMRI). How to leverage high-dimensional network structures to effectively mitigate site effects has yet to be addressed. In this paper, we propose SLACC (Sparse LAtent Covariate-driven Connectome) factorization, a multivariate method that explicitly parameterizes covariate effects in latent subject scores corresponding to sparse rank-1 latent patterns derived from brain connectivity. The proposed method identifies localized site-driven variability within and across brain networks, enabling targeted correction. We develop a penalized Expectation-Maximization (EM) algorithm for parameter estimation, incorporating the Bayesian Information Criterion (BIC) to guide optimization. Extensive simulations validate SLACC's robustness in recovering the true parameters and underlying connectivity patterns. Applied to the Autism Brain Imaging Data Exchange (ABIDE) dataset, SLACC demonstrates its ability to reduce site effects. The R package to implement our method is publicly available.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-29</td>
<td style='padding: 8px;'>Scaling Next-Brain-Token Prediction for MEG</td>
<td style='padding: 6px;'>Richard Csaky</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.20138v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We present a large autoregressive model for source-space MEG that scales next-token prediction to long context across datasets and scanners: handling a corpus of over 500 hours and thousands of sessions across the three largest MEG datasets. A modified SEANet-style vector-quantizer reduces multichannel MEG into a flattened token stream on which we train a Qwen2.5-VL backbone from scratch to predict the next brain token and to recursively generate minutes of MEG from up to a minute of context. To evaluate long-horizon generation, we introduce task-matched tests: (i) on-manifold stability via generated-only drift compared to the time-resolved distribution of real sliding windows, and (ii) conditional specificity via correct context versus prompt-swap controls using a neurophysiologically grounded metric set. We train on CamCAN and Omega and run all analyses on held-out MOUS, establishing cross-dataset generalization. Across metrics, generations remain relatively stable over long rollouts and are closer to the correct continuation than swapped controls. Code available at: https://github.com/ricsinaruto/brain-gen.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-26</td>
<td style='padding: 8px;'>MEGnifying Emotion: Sentiment Analysis from Annotated Brain Data</td>
<td style='padding: 6px;'>Brian Liu, Oiwi Parker Jones</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.18792v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Decoding emotion from brain activity could unlock a deeper understanding of the human experience. While a number of existing datasets align brain data with speech and with speech transcripts, no datasets have annotated brain data with sentiment. To bridge this gap, we explore the use of pre-trained Text-to-Sentiment models to annotate non invasive brain recordings, acquired using magnetoencephalography (MEG), while participants listened to audiobooks. Having annotated the text, we employ force-alignment of the text and audio to align our sentiment labels with the brain recordings. It is straightforward then to train Brainto-Sentiment models on these data. Experimental results show an improvement in balanced accuracy for Brain-to-Sentiment compared to baseline, supporting the proposed approach as a proof-of-concept for leveraging existing MEG datasets and learning to decode sentiment directly from the brain.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-26</td>
<td style='padding: 8px;'>Human Cardiac Measurements with Diamond Magnetometers</td>
<td style='padding: 6px;'>Muhib Omar, Magnus Benke, Shaowen Zhang, Jixing Zhang, Michael Kuebler, Pouya Sharbati, Ara Rahimpour, Arno Gueck, Maryna Kapitonova, Devyani Kadam, Carlos Rene Izquierdo Geiser, Jens Haller, Arno Trautmann, Katharina Jag-Lauber, Robert Roelver, Thanh-Duc Nguyen, Leonardo Gizzi, Michelle Schweizer, Mena Abdelsayed, Ingo Wickenbrock, Andrew M. Edmonds, Matthew Markham, Peter A. Koss, Oliver Schnell, Ulrich G. Hofmann, Tonio Ball, Juergen Beck, Dmitry Budker, Joerg Wrachtrup, Arne Wickenbrock</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.18843v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We demonstrate direct, non-invasive and non-contact detection of human cardiac magnetic signals using quantum sensors based on nitrogen-vacancy (NV) centers in diamond. Three configurations were employed recording magnetocardiography (MCG) signals in various shielded and unshielded environments. The signals were averaged over a few hundreds up to several thousands of heart beats to detect the MCG traces. The compact room-temperature NV sensors exhibit sensitivities of 6-26 pT/Hz^(1/2) with active sensing volumes below 0.5 mm^3, defining the performance level of the demonstrated MCG measurements. While the present signals are obtained by averaging, this performance already indicates a clear path toward single-shot MCG sensing. To move beyond shielded environments toward practical clinical use, strong noise suppression is required. To this end, we implement NV-based gradiometry and achieve efficient common-mode noise rejection, enabled by the intrinsically small sensing volume of NV sensors. Together, these multi-platform results obtained across diverse magnetic environments provide a solid foundation for translating quantum sensors into human medical diagnostics such as MCG and magnetoencephalography (MEG).</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-23</td>
<td style='padding: 8px;'>Quantum Sensing MRI for Noninvasive Detection of Neuronal Electrical Activity in Human Brains</td>
<td style='padding: 6px;'>Yongxian Qian, Ying-Chia Lin, Seyedehsara Hejazi, Kamri Clarke, Kennedy Watson, Xingye Chen, Nahbila-Malikha Kumbella, Justin Quimbo, Abena Dinizulu, Simon Henin, Yulin Ge, Arjun Masurkar, Anli Liu, Yvonne W. Lui, Fernando E. Boada</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.16423v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuronal electrical activity underlies human cognition, yet its direct, noninvasive measurement in the living human brain remains a fundamental challenge. Existing neuroimaging techniques, including EEG, MEG, and fMRI, are limited by trade-offs in sensitivity and spatial or temporal resolution. Here we propose quantum sensing MRI (qsMRI), a noninvasive approach that enables direct detection of neuronal firing-induced magnetic fields using a clinical MRI system. qsMRI exploits endogenous proton (1H) nuclear spins in water molecules as intrinsic quantum sensors and decodes time-resolved phase information from free induction decay (FID) signals to infer neuronal magnetic fields. We validate qsMRI through simulations, phantom experiments, and human studies at rest and during motor tasks, and provide open experimental procedures to facilitate independent validation. We further present a case study demonstrating potential applications to neurological disorders. qsMRI represents a first-in-human application of quantum sensing on a clinical MRI platform, establishes a non-BOLD functional imaging modality, and enables interrogation of neuronal firing dynamics in both cortical and deep brain regions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-22</td>
<td style='padding: 8px;'>Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech</td>
<td style='padding: 6px;'>Soufiane Jhilal, Stéphanie Martin, Anne-Lise Giraud</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.15909v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-09</td>
<td style='padding: 8px;'>Cedalion Tutorial: A Python-based framework for comprehensive analysis of multimodal fNIRS & DOT from the lab to the everyday world</td>
<td style='padding: 6px;'>E. Middell, L. Carlton, S. Moradi, T. Codina, T. Fischer, J. Cutler, S. Kelley, J. Behrendt, T. Dissanayake, N. Harmening, M. A. Yücel, D. A. Boas, A. von Lühmann</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.05923v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional near-infrared spectroscopy (fNIRS) and diffuse optical tomography (DOT) are rapidly evolving toward wearable, multimodal, and data-driven, AI-supported neuroimaging in the everyday world. However, current analytical tools are fragmented across platforms, limiting reproducibility, interoperability, and integration with modern machine learning (ML) workflows. Cedalion is a Python-based open-source framework designed to unify advanced model-based and data-driven analysis of multimodal fNIRS and DOT data within a reproducible, extensible, and community-driven environment. Cedalion integrates forward modelling, photogrammetric optode co-registration, signal processing, GLM Analysis, DOT image reconstruction, and ML-based data-driven methods within a single standardized architecture based on the Python ecosystem. It adheres to SNIRF and BIDS standards, supports cloud-executable Jupyter notebooks, and provides containerized workflows for scalable, fully reproducible analysis pipelines that can be provided alongside original research publications. Cedalion connects established optical-neuroimaging pipelines with ML frameworks such as scikit-learn and PyTorch, enabling seamless multimodal fusion with EEG, MEG, and physiological data. It implements validated algorithms for signal-quality assessment, motion correction, GLM modelling, and DOT reconstruction, complemented by modules for simulation, data augmentation, and multimodal physiology analysis. Automated documentation links each method to its source publication, and continuous-integration testing ensures robustness. This tutorial paper provides seven fully executable notebooks that demonstrate core features. Cedalion offers an open, transparent, and community extensible foundation that supports reproducible, scalable, cloud- and ML-ready fNIRS/DOT workflows for laboratory-based and real-world neuroimaging.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-02</td>
<td style='padding: 8px;'>Nematic-fluctuation-mediated superconductivity in CuxTiSe2</td>
<td style='padding: 6px;'>Xingyu Lv, Yang Fu, Shangjie Tian, Ying Ma, Shouguo Wang, Cedomir Petrovic, Xiao Zhang, Hechang Lei</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.00723v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The interplay among electronic nematicity, charge density wave, and superconductivity in correlated electronic systems has induced extensive research interest. Here, we discover the existence of nematic fluctuations in TiSe2 single crystal and investigate its evolution with Cu intercalation. It is observed that the elastoresistivity coefficient mEg exhibits a divergent temperature dependence following a Curie-Weiss law at high temperature. Upon Cu intercalation, the characteristic temperature T* of nematic fluctuation is progressively suppressed and becomes near zero when the superconductivity is optimized. Further intercalation of Cu leads to the sign change of T* and the suppression of superconductivity. These results strongly indicate that nematic phase transition may play a vital role in enhancing superconductivity in CuxTiSe2. Therefore, CuxTiSe2 provides a unique material platform to explore the nematic-fluctuation-mediated superconductivity.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-12-22</td>
<td style='padding: 8px;'>Transformer-Based Approach to Enhance Positron Tracking Performance in MEG II</td>
<td style='padding: 6px;'>Lapo Dispoto, Fedor Ignatov, Atsushi Oya, Yusuke Uchiyama, Antoine Venturini</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2512.19482v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We developed a Transformer-based pattern recognition method for positron track reconstruction in the MEG II experiment. The model acts as a classifier to remove pileup hits in the MEG II drift chamber, which operates under a high pileup occupancy of 35 - 50 %. The trained model significantly improved hit purity, leading to enhancements in tracking efficiency and resolution by 15 % and 5 %, respectively, at a muon stopping rate of $5\times 10^7 μ$/sec. This improvement translates into an approximately 10 % increase in the sensitivity of the $μ\to eγ$ branching ratio measurement.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-12-22</td>
<td style='padding: 8px;'>Brain-Grounded Axes for Reading and Steering LLM States</td>
<td style='padding: 6px;'>Sandro Andric</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2512.19399v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-12-19</td>
<td style='padding: 8px;'>MEGState: Phoneme Decoding from Magnetoencephalography Signals</td>
<td style='padding: 6px;'>Shuntaro Suzuki, Chia-Chun Dan Hsu, Yu Tsao, Komei Sugiura</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2512.17978v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Decoding linguistically meaningful representations from non-invasive neural recordings remains a central challenge in neural speech decoding. Among available neuroimaging modalities, magnetoencephalography (MEG) provides a safe and repeatable means of mapping speech-related cortical dynamics, yet its low signal-to-noise ratio and high temporal dimensionality continue to hinder robust decoding. In this work, we introduce MEGState, a novel architecture for phoneme decoding from MEG signals that captures fine-grained cortical responses evoked by auditory stimuli. Extensive experiments on the LibriBrain dataset demonstrate that MEGState consistently surpasses baseline model across multiple evaluation metrics. These findings highlight the potential of MEG-based phoneme decoding as a scalable pathway toward non-invasive brain-computer interfaces for speech.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-27</td>
<td style='padding: 8px;'>NeuroAI and Beyond</td>
<td style='padding: 6px;'>Jean-Marc Fellous, Gert Cauwenberghs, Cornelia Fermüller, Yulia Sandamisrkaya, Terrence Sejnowski</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2601.19955v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-24</td>
<td style='padding: 8px;'>When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics</td>
<td style='padding: 6px;'>Yiven, Zhu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.19548v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, "brain-based" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies "true" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-25</td>
<td style='padding: 8px;'>Dopamine-driven synaptic credit assignment in neural networks</td>
<td style='padding: 6px;'>Saranraj Nambusubramaniyan, Shervin Safavi, Raja Guru, Andreas Knoblauch</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.22178v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Solving the synaptic Credit Assignment Problem(CAP) is central to learning in both biological and artificial neural systems. Finding an optimal solution for synaptic CAP means setting the synaptic weights that assign credit to each neuron for influencing the final output and behavior of neural networks or animals. Gradient-based methods solve this problem in artificial neural networks using back-propagation, however, not in the most efficient way. For instance, back-propagation requires a chain of top-down gradient computations. This leads to an expensive optimization process in terms of computing power and memory linked with well-known weight transport and update locking problems. To address these shortcomings, we take a NeuroAI approach and draw inspiration from neural Reinforcement Learning to develop a derivative-free optimizer for training neural networks, Dopamine. Dopamine is developed for Weight Perturbation (WP) learning that exploits stochastic updating of weights towards optima. It achieves this by minimizing the regret, a form of Reward Prediction Error (RPE) between the expected outcome from the perturbed model and the actual outcome from the unperturbed model. We use this RPE to adjust the learning rate in the network (i.e., creating an adaptive learning rate strategy, similar to the role of dopamine in the brain). We tested the Dopamine optimizer for training multi-layered perceptrons for XOR tasks, and recurrent neural networks for chaotic time series forecasting. Dopamine-trained models demonstrate accelerated convergence and outperform standard WP, and give comparable performance to gradient-based algorithms, while consuming significantly less computation and memory. Overall, the Dopamine optimizer not only finds robust solutions and comparable performance to the state-of-the-art Machine Learning optimizers but is also neurobiologically more plausible.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>A Computational Perspective on NeuroAI and Synthetic Biological Intelligence</td>
<td style='padding: 6px;'>Dhruvik Patel, Md Sayed Tanveer, Jesus Gonzalez-Ferrer, Alon Loeffler, Brett J. Kagan, Mohammed A. Mostajo-Radji, Ge Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.23896v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>NeuroAI is an emerging field at the intersection of neuroscience and artificial intelligence, where insights from brain function guide the design of intelligent systems. A central area within this field is synthetic biological intelligence (SBI), which combines the adaptive learning properties of biological neural networks with engineered hardware and software. SBI systems provide a platform for modeling neural computation, developing biohybrid architectures, and enabling new forms of embodied intelligence. In this review, we organize the NeuroAI landscape into three interacting domains: hardware, software, and wetware. We outline computational frameworks that integrate biological and non-biological systems and highlight recent advances in organoid intelligence, neuromorphic computing, and neuro-symbolic learning. These developments collectively point toward a new class of systems that compute through interactions between living neural tissue and digital algorithms.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-07</td>
<td style='padding: 8px;'>Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers</td>
<td style='padding: 6px;'>Thomas Klein, Sascha Meyen, Wieland Brendel, Felix A. Wichmann, Kristof Meding</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.06645v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Benchmarking models is a key factor for the rapid progress in machine learning (ML) research. Thus, further progress depends on improving benchmarking metrics. A standard metric to measure the behavioral alignment between ML models and human observers is error consistency (EC). EC allows for more fine-grained comparisons of behavior than other metrics such as accuracy, and has been used in the influential Brain-Score benchmark to rank different DNNs by their behavioral consistency with humans. Previously, EC values have been reported without confidence intervals. However, empirically measured EC values are typically noisy -- thus, without confidence intervals, valid benchmarking conclusions are problematic. Here we improve on standard EC in two ways: First, we show how to obtain confidence intervals for EC using a bootstrapping technique, allowing us to derive significance tests for EC. Second, we propose a new computational model relating the EC between two classifiers to the implicit probability that one of them copies responses from the other. This view of EC allows us to give practical guidance to scientists regarding the number of trials required for sufficiently powerful, conclusive experiments. Finally, we use our methodology to revisit popular NeuroAI-results. We find that while the general trend of behavioral differences between humans and machines holds up to scrutiny, many reported differences between deep vision models are statistically insignificant. Our methodology enables researchers to design adequately powered experiments that can reliably detect behavioral differences between models, providing a foundation for more rigorous benchmarking of behavioral alignment.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-02</td>
<td style='padding: 8px;'>What Neuroscience Can Teach AI About Learning in Continuously Changing Environments</td>
<td style='padding: 6px;'>Daniel Durstewitz, Bruno Averbeck, Georgia Koppe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.02103v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-27</td>
<td style='padding: 8px;'>NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models</td>
<td style='padding: 6px;'>Luca Ghafourpour, Valentin Duruisseaux, Bahareh Tolooshams, Philip H. Wong, Costas A. Anastassiou, Anima Anandkumar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2506.04536v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Characterizing the cellular properties of neurons is fundamental to understanding their function in the brain. In this quest, the generation of bio-realistic models is central towards integrating multimodal cellular data sets and establishing causal relationships. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. The deterministic formalism of bio-realistic models currently precludes accounting for the natural variability observed experimentally. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on synthetic data generated from bio-realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE enables the efficient generation of synthetic neurons that closely resemble experimental data and exhibit trial-to-trial variability, offering a $4200\times$ speedup over the numerical solver. NOBLE is the first scaled-up deep learning framework that validates its generalization with real experimental data. To this end, NOBLE captures fundamental neural properties in a unique and emergent manner that opens the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-27</td>
<td style='padding: 8px;'>Decoding Cortical Microcircuits: A Generative Model for Latent Space Exploration and Controlled Synthesis</td>
<td style='padding: 6px;'>Xingyu Liu, Yubin Li, Guozhang Chen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2506.11062v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A central idea in understanding brains and building artificial intelligence is that structure determines function. Yet, how the brain's complex structure arises from a limited set of genetic instructions remains a key question. The ultra high-dimensional detail of neural connections vastly exceeds the information storage capacity of genes, suggesting a compact, low-dimensional blueprint must guide brain development. Our motivation is to uncover this blueprint. We introduce a generative model, to learn this underlying representation from detailed connectivity maps of mouse cortical microcircuits. Our model successfully captures the essential structural information of these circuits in a compressed latent space. We found that specific, interpretable directions within this space directly relate to understandable network properties. Building on this, we demonstrate a novel method to controllably generate new, synthetic microcircuits with desired structural features by navigating this latent space. This work offers a new way to investigate the design principles of neural circuits and explore how structure gives rise to function, potentially informing the development of more advanced artificial neural networks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-21</td>
<td style='padding: 8px;'>SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation</td>
<td style='padding: 6px;'>Jiayue Liu, Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Kuo Yang, Xu Wang, Yang Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.16080v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-02-22</td>
<td style='padding: 8px;'>Brain-Model Evaluations Need the NeuroAI Turing Test</td>
<td style='padding: 6px;'>Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2502.16238v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-01</td>
<td style='padding: 8px;'>VAMOS-OCTA: Vessel-Aware Multi-Axis Orthogonal Supervision for Inpainting Motion-Corrupted OCT Angiography Volumes</td>
<td style='padding: 6px;'>Nick DiSanto, Ehsan Khodapanah Aghdam, Han Liu, Jacob Watson, Yuankai K. Tao, Hao Li, Ipek Oguz</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00995v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Handheld Optical Coherence Tomography Angiography (OCTA) enables noninvasive retinal imaging in uncooperative or pediatric subjects, but is highly susceptible to motion artifacts that severely degrade volumetric image quality. Sudden motion during 3D acquisition can lead to unsampled retinal regions across entire B-scans (cross-sectional slices), resulting in blank bands in en face projections. We propose VAMOS-OCTA, a deep learning framework for inpainting motion-corrupted B-scans using vessel-aware multi-axis supervision. We employ a 2.5D U-Net architecture that takes a stack of neighboring B-scans as input to reconstruct a corrupted center B-scan, guided by a novel Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss. This loss combines vessel-weighted intensity reconstruction with axial and lateral projection consistency, encouraging vascular continuity in native B-scans and across orthogonal planes. Unlike prior work that focuses primarily on restoring the en face MIP, VAMOS-OCTA jointly enhances both cross-sectional B-scan sharpness and volumetric projection accuracy, even under severe motion corruptions. We trained our model on both synthetic and real-world corrupted volumes and evaluated its performance using both perceptual quality and pixel-wise accuracy metrics. VAMOS-OCTA consistently outperforms prior methods, producing reconstructions with sharp capillaries, restored vessel continuity, and clean en face projections. These results demonstrate that multi-axis supervision offers a powerful constraint for restoring motion-degraded 3D OCTA data. Our source code is available at https://github.com/MedICL-VU/VAMOS-OCTA.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-01</td>
<td style='padding: 8px;'>Optimal Decision-Making Based on Prediction Sets</td>
<td style='padding: 6px;'>Tao Wang, Edgar Dobriban</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00989v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Prediction sets can wrap around any ML model to cover unknown test outcomes with a guaranteed probability. Yet, it remains unclear how to use them optimally for downstream decision-making. Here, we propose a decision-theoretic framework that seeks to minimize the expected loss (risk) against a worst-case distribution consistent with the prediction set's coverage guarantee. We first characterize the minimax optimal policy for a fixed prediction set, showing that it balances the worst-case loss inside the set with a penalty for potential losses outside the set. Building on this, we derive the optimal prediction set construction that minimizes the resulting robust risk subject to a coverage constraint. Finally, we introduce Risk-Optimal Conformal Prediction (ROCP), a practical algorithm that targets these risk-minimizing sets while maintaining finite-sample distribution-free marginal coverage. Empirical evaluations on medical diagnosis and safety-critical decision-making tasks demonstrate that ROCP reduces critical mistakes compared to baselines, particularly when out-of-set errors are costly.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-02-01</td>
<td style='padding: 8px;'>MedSpeak: A Knowledge Graph-Aided ASR Error Correction Framework for Spoken Medical QA</td>
<td style='padding: 6px;'>Yutong Song, Shiva Shrestha, Chenhan Lyu, Elahe Khatibi, Pengfei Zhang, Honghui Xu, Nikil Dutt, Amir Rahmani</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00981v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Spoken question-answering (SQA) systems relying on automatic speech recognition (ASR) often struggle with accurately recognizing medical terminology. To this end, we propose MedSpeak, a novel knowledge graph-aided ASR error correction framework that refines noisy transcripts and improves downstream answer prediction by leveraging both semantic relationships and phonetic information encoded in a medical knowledge graph, together with the reasoning power of LLMs. Comprehensive experimental results on benchmarks demonstrate that MedSpeak significantly improves the accuracy of medical term recognition and overall medical SQA performance, establishing MedSpeak as a state-of-the-art solution for medical SQA. The code is available at https://github.com/RainieLLM/MedSpeak.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-31</td>
<td style='padding: 8px;'>Efficient Deep Learning for Medical Imaging: Bridging the Gap Between High-Performance AI and Clinical Deployment</td>
<td style='padding: 6px;'>Cuong Manh Nguyen, Truong-Son Hy</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00910v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Deep learning has revolutionized medical image analysis, playing a vital role in modern clinical applications. However, the deployment of large-scale models in real-world clinical settings remains challenging due to high computational costs, latency constraints, and patient data privacy concerns associated with cloud-based processing. To address these bottlenecks, this review provides a comprehensive synthesis of efficient and lightweight deep learning architectures specifically tailored for the medical domain. We categorize the landscape of modern efficient models into three primary streams: Convolutional Neural Networks (CNNs), Lightweight Transformers, and emerging Linear Complexity Models. Furthermore, we examine key model compression strategies (including pruning, quantization, knowledge distillation, and low-rank factorization) and evaluate their efficacy in maintaining diagnostic performance while reducing hardware requirements. By identifying current limitations and discussing the transition toward on-device intelligence, this review serves as a roadmap for researchers and practitioners aiming to bridge the gap between high-performance AI and resource-constrained clinical environments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-31</td>
<td style='padding: 8px;'>Gamma Imagers for Nuclear Security and Nuclear Forensics: Recommendations based on results from a side-by-side intercomparison</td>
<td style='padding: 6px;'>L. E. Sinclair, P. R. B. Saull, A. McCann, A. M. L. MacLeod, N. J. Murtha, A. El-Jaby, G. Jonkmans</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00826v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Nuclear security operations and forensic investigations require the utilization of a suite of instruments ranging from passive gamma spectrometers to high-precision laboratory sample analyzers. Gamma spectroscopy survey is further broken down into wide-area search performed with large-volume scintillator-based mobile survey spectrometers which are integrated with geographic position sensors for mapping and identification of hot zones, and high-precision long-dwell measurements using solid state spectrometers for follow-on characterization to establish isotopic content and ratios. While performing well at detecting the presence, quantity and type of radioactivity, all of these methods have limited ability to determine the location of a source of radioactivity. In recent years, technology advances have resulted in gamma imager devices which can create an image of the distribution of radioactive sources using the gamma emissions which accompany radioactive decay, and overlay this on an optical photograph of the environment. These gamma imaging devices have arisen out of methods developed for medical physics, experimental particle physics, and astrophysics, resulting in a proliferation of different technological approaches. Those responsible for establishing a nuclear security concept of operations, require guidance to choose the proper gamma imager for each of the application spaces in a tiered response. Here the results of an intercomparison of two gamma imagers based on two widely different technologies, semiconductor and scintillator detectors, are presented. The optimal utilization of these imaging technologies in a tiered response is discussed based on the results of the trial. Finally, an outlook on future directions for gamma imaging advances is provided.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-31</td>
<td style='padding: 8px;'>Mobile Exergames: Activity Recognition Based on Smartphone Sensors</td>
<td style='padding: 6px;'>David Craveiro, Hugo Silva</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00809v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Smartphone sensors can be extremely useful in providing information on the activities and behaviors of persons. Human activity recognition is increasingly used for games, medical, or surveillance. In this paper, we propose a proof-of-concept 2D endless game called Duck Catch & Fit, which implements a detailed activity recognition system that uses a smartphone accelerometer, gyroscope, and magnetometer sensors. The system applies feature extraction and learning mechanism to detect human activities like staying, side movements, and fake side movements. In addition, a voice recognition system is combined to recognize the word "fire" and raise the game's complexity. The results show that it is possible to use machine learning techniques to recognize human activity with high recognition levels. Also, the combination of movement-based and voice-based integrations contributes to a more immersive gameplay.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-31</td>
<td style='padding: 8px;'>ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement</td>
<td style='padding: 6px;'>Ziyan Xiao, Yinghao Zhu, Liang Peng, Lequan Yu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00740v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Clinical text improvement is vital for healthcare efficiency but remains difficult due to limited high-quality data and the complex constraints of medical documentation. While Large Language Models (LLMs) show promise, current approaches struggle in small-sample settings: supervised fine-tuning is data-intensive and costly, while retrieval-augmented generation often provides superficial corrections without capturing the reasoning behind revisions. To address these limitations, we propose ExperienceWeaver, a hierarchical framework that shifts the focus from data retrieval to experience learning. Instead of simply recalling past examples, ExperienceWeaver distills noisy, multi-dimensional feedback into structured, actionable knowledge. Specifically, error-specific Tips and high-level Strategies. By injecting this distilled experience into an agentic pipeline, the model learns "how to revise" rather than just "what to revise". Extensive evaluations across four clinical datasets demonstrate that ExperienceWeaver consistently improves performance, surpassing state-of-the-art models such as Gemini-3 Pro in small-sample settings.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-31</td>
<td style='padding: 8px;'>Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries</td>
<td style='padding: 6px;'>Zhengyan Qin, Liyuan Qiu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00739v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the "double surface artifact" caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-31</td>
<td style='padding: 8px;'>Multi-Compartment Volume Conductor with Complete Electrode Model: Simulated Stereo-EEG Source Localization using Brainstorm-Zeffiro Plugin</td>
<td style='padding: 6px;'>Fernando Galaz Prieto, Takfarinas Medani, Chinmay Chinara, Richard M. Leahy, Sampsa Pursiainen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00684v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study introduces a novel integration of the Brainstorm (BST) software and the Zeffiro Interface (ZI) to enable whole-head, multi-compartment volume conductor modeling for electroencephalography (EEG) source imaging, with a particular focus on stereotactic EEG applications. We present the BST-2-ZI plugin, a MATLAB-based tool that facilitates seamless transfer of tissue segmentations and anatomical atlases from BST into ZI for finite element (FE) mesh generation as well as forward and inverse modeling. The generated FE meshes support variable spatial resolution and implement the complete electrode model (CEM), allowing for precise modeling of both invasive depth electrodes and non-invasive scalp electrodes. Using the ICBM152 template and synthetic source simulation, we demonstrate the end-to-end pipeline from MRI data to lead field (LF) computation and source localization in a stereotactic EEG (stereo-EEG) setting. Our numerical experiments highlight the capability of the pipeline to accurately model multi-compartment head geometry and conductivity with a stereotactic CEM-based electrode configuration. Our preliminary source localization results show how a synthetic stereo-EEG probe corresponding to a bidirectional deep brain stimulation (DBS) probe with four omnidirectional contacts can, in principle, be coupled with scalp electrodes to improve source localization in its vicinity.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2026-01-31</td>
<td style='padding: 8px;'>Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation</td>
<td style='padding: 6px;'>Marina Crespo Aguirre, Jonathan Williams-Ramirez, Dina Zemlyanker, Xiaoling Hu, Lucas J. Deden-Binder, Rogeny Herisse, Mark Montine, Theresa R. Connors, Christopher Mount, Christine L. MacDonald, C. Dirk Keene, Caitlin S. Latimer, Derek H. Oakley, Bradley T. Hyman, Ana Lawry Aguila, Juan Eugenio Iglesias</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2602.00669v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhibit coarse, overly smooth reconstructions of structures, especially under high anisotropy (i.e., reconstructions from thick slabs). Here, we introduce a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions of dissection photographs. By training on domain-randomized synthetic data, we ensure that our method generalizes across dissection protocols and remains robust to large slab thicknesses. The imputed volumes yield improved automated segmentations, achieving higher Dice scores, particularly in cortical and white matter regions. Validation on surface reconstruction and atlas registration tasks demonstrates more accurate cortical surfaces and MRI registration. By enhancing the resolution and anatomical fidelity of photograph-based reconstructions, our approach strengthens the bridge between neuropathology and neuroimaging. Our method is publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/mri_3d_photo_recon</td>
</tr>
</tbody>
</table>

