<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2024-12-05</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-03</td>
<td style='padding: 8px;'>An ADHD Diagnostic Interface Based on EEG Spectrograms and Deep Learning Techniques</td>
<td style='padding: 6px;'>Medha Pappula, Syed Muhammad Anwar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02695v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper introduces an innovative approach to Attention-deficit/hyperactivity disorder (ADHD) diagnosis by employing deep learning (DL) techniques on electroencephalography (EEG) signals. This method addresses the limitations of current behavior-based diagnostic methods, which often lead to misdiagnosis and gender bias. By utilizing a publicly available EEG dataset and converting the signals into spectrograms, a Resnet-18 convolutional neural network (CNN) architecture was used to extract features for ADHD classification. The model achieved a high precision, recall, and an overall F1 score of 0.9. Feature extraction highlighted significant brain regions (frontopolar, parietal, and occipital lobes) associated with ADHD. These insights guided the creation of a three-part digital diagnostic system, facilitating cost-effective and accessible ADHD screening, especially in school environments. This system enables earlier and more accurate identification of students at risk for ADHD, providing timely support to enhance their developmental outcomes. This study showcases the potential of integrating EEG analysis with DL to enhance ADHD diagnostics, presenting a viable alternative to traditional methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-03</td>
<td style='padding: 8px;'>Active learning of neural population dynamics using two-photon holographic optogenetics</td>
<td style='padding: 6px;'>Andrew Wagenmaker, Lu Mi, Marton Rozsa, Matthew S. Bull, Karel Svoboda, Kayvon Daie, Matthew D. Golub, Kevin Jamieson</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02529v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent advances in techniques for monitoring and perturbing neural populations have greatly enhanced our ability to study circuits in the brain. In particular, two-photon holographic optogenetics now enables precise photostimulation of experimenter-specified groups of individual neurons, while simultaneous two-photon calcium imaging enables the measurement of ongoing and induced activity across the neural population. Despite the enormous space of potential photostimulation patterns and the time-consuming nature of photostimulation experiments, very little algorithmic work has been done to determine the most effective photostimulation patterns for identifying the neural population dynamics. Here, we develop methods to efficiently select which neurons to stimulate such that the resulting neural responses will best inform a dynamical model of the neural population activity. Using neural population responses to photostimulation in mouse motor cortex, we demonstrate the efficacy of a low-rank linear dynamical systems model, and develop an active learning procedure which takes advantage of low-rank structure to determine informative photostimulation patterns. We demonstrate our approach on both real and synthetic data, obtaining in some cases as much as a two-fold reduction in the amount of data required to reach a given predictive power. Our active stimulation design method is based on a novel active learning procedure for low-rank regression, which may be of independent interest.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-03</td>
<td style='padding: 8px;'>Hierarchical feature extraction on functional brain networks for autism spectrum disorder identification with resting-state fMRI data</td>
<td style='padding: 6px;'>Yiqian Luo, Qiurong Chen, Fali Li, Liang Yi, Peng Xu, Yangsong Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02424v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Autism spectrum disorder (ASD) is a pervasive developmental disorder of the central nervous system, which occurs most frequently in childhood and is characterized by unusual and repetitive ritualistic behaviors. Currently, diagnostic methods primarily rely on questionnaire surveys and behavioral observation, which may lead to misdiagnoses due to the subjective evaluation and measurement used in these traditional methods. With the advancement in medical imaging, MR imaging-based diagnosis has become an alternative and more objective approach. In this paper, we propose a Hybrid neural Network model for ASD identification, termded ASD-HNet, to hierarchically extract features on the functional brain networks based on resting-state functional magnetic resonance imaging data. This hierarchical method can better extract brain representations, improve the diagnostic accuracy, and help us better locate brain regions related to ASD. Specifically, features are extracted from three scales: local regions of interest (ROIs) scale, community-clustering scale, and the whole-communities scale. For the ROI scale, graph convolution is used to transfer features between ROIs. At the community cluster scale, functional gradients are introduced, the clustering algorithm K-Means is used to automatically cluster ROIs with similar functional gradients into several communities, and features of ROIs belonging to the same community are extracted to characterize these communities. At global information integration scale, we extract global features from community-scale brain networks to characterize the whole brain networks. We validate the effectiveness of our method using the public dataset of Autism Brain Imaging Data Exchange I (ABIDE I), and elucidate the interpretability of the method. Experimental results demonstrate that the proposed ASD-HNet can yield superior performance than compared methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-03</td>
<td style='padding: 8px;'>A Simple Channel Compression Method for Brain Signal Decoding on Classification Task</td>
<td style='padding: 6px;'>Changqing Ji, Keisuke Kawasaki, Isao Hasegawa, Takayuki Okatani</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02078v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In the application of brain-computer interface (BCI), while pursuing accurate decoding of brain signals, we also need consider the computational efficiency of BCI devices. ECoG signals are multi-channel temporal signals which is collected using a high-density electrode array at a high sampling frequency. The data between channels has a high similarity or redundancy in the temporal domain. The redundancy of data not only reduces the computational efficiency of the model, but also overwhelms the extraction of effective features, resulting in a decrease in performance. How to efficiently utilize ECoG multi-channel signals is one of the research topics. Effective channel screening or compression can greatly reduce the model size, thereby improving computational efficiency, this would be a good direction to solve the problem. Based on previous work [1], this paper proposes a very simple channel compression method, which uses a learnable matrix to perform matrix multiplication on the original channels, that is, assigning weights to the channels and then linearly add them up. This effectively reduces the number of final channels. In the experiment, we used the vision-based ECoG multi-classification dataset owned by our laboratory to test the proposed channel selection (compression) method. We found that the new method can compress the original 128-channel ECoG signal to 32 channels (of which subject MonJ is compressed to 8 channels), greatly reducing the size of the model. The demand for GPU memory resources during model training is reduced by about 68.57%, 84.33% for each subject respectively; the model training speed also increased up around 3.82, 4.65 times of the original speed for each subject respectively. More importantly, the performance of the model has improved by about 1.10% compared with our previous work, reached the SOTA level of our unique visual based ECoG dataset</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-02</td>
<td style='padding: 8px;'>Topological analysis of brain dynamical signals reveals signatures of seizure susceptibility</td>
<td style='padding: 6px;'>Maxime Lucas, Damien Francois, Cristina Donato, Alexander Skupin, Daniele Proverbio</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.01911v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Epilepsy is known to drastically alter brain dynamics during seizures (ictal periods). However, whether epilepsy may alter brain dynamics during background (non-ictal) periods is less understood. To investigate this, we analyzed the brain activity of epileptic zebrafish as animal models, for two genetic conditions and two fishlines. The recordings were automatically segmented and labeled with machine learning, and then analyzed using Persistent Homology, a method from Topological Data Analysis, which reveals patterns in the topology of brain dynamics in a noise-robust and networkbased manner. We find that ictal and non-ictal periods can be distinguished from the topology of their dynamics, regardless of fishline or genetic condition, which validates our method. Additionally, within a single fishline wild type, we can distinguish the non-ictal periods of seizure-prone and seizure-free individuals. This suggests the presence of topological signatures of the epileptic brain, even during non-ictal periods. In general, our results suggest that Topological Data Analysis can be used as a general quantitative method to screen for dynamical markers of seizure susceptibility also in other species.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-02</td>
<td style='padding: 8px;'>Task learning through stimulation-induced plasticity in neural networks</td>
<td style='padding: 6px;'>Francesco Borra, Simona Cocco, RÃ©mi Monasson</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.01683v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Synaptic plasticity dynamically shapes the connectivity of neural systems and is key to learning processes in the brain. To what extent the mechanisms of plasticity can be exploited to drive a neural network and make it perform some kind of computational task remains unclear. This question, relevant in a bioengineering context, can be formulated as a control problem on a high-dimensional system with strongly constrained and non-linear dynamics. We present a self-contained procedure which, through appropriate spatio-temporal stimulations of the neurons, is able to drive rate-based neural networks with arbitrary initial connectivity towards a desired functional state. We illustrate our approach on two different computational tasks: a non-linear association between multiple input stimulations and activity patterns (representing digit images), and the construction of a continuous attractor encoding a collective variable in a neural population. Our work thus provides a proof of principle for emerging paradigms of in vitro computation based on real neurons.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-02</td>
<td style='padding: 8px;'>Handwriting-based Automated Assessment and Grading of Degree of Handedness: A Pilot Study</td>
<td style='padding: 6px;'>Smriti Bala, Venugopalan Y. Vishnu, Deepak Joshi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.01587v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Hand preference and degree of handedness (DoH) are two different aspects of human behavior which are often confused to be one. DoH is a person's inherent capability of the brain; affected by nature and nurture. In this study, we used dominant and non-dominant handwriting traits to assess DoH for the first time, on 43 subjects of three categories- Unidextrous, Partially Unidextrous, and Ambidextrous. Features extracted from the segmented handwriting signals called strokes were used for DoH quantification. Davies Bouldin Index, Multilayer perceptron, and Convolutional Neural Network (CNN) were used for automated grading of DoH. The outcomes of these methods were compared with the widely used DoH assessment questionnaires from Edinburgh Inventory (EI). The CNN based automated grading outperformed other computational methods with an average classification accuracy of 95.06% under stratified 10-fold cross-validation. The leave-one-subject-out strategy on this CNN resulted in a test individual's DoH score which was converted into a 4-point score. Around 90% of the obtained scores from all the implemented computational methods were found to be in accordance with the EI scores under 95% confidence interval. Automated grading of degree of handedness using handwriting signals can provide more resolution to the Edinburgh Inventory scores. This could be used in multiple applications concerned with neuroscience, rehabilitation, physiology, psychometry, behavioral sciences, and forensics.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-02</td>
<td style='padding: 8px;'>Topological Signal Processing and Learning: Recent Advances and Future Challenges</td>
<td style='padding: 6px;'>Elvin Isufi, Geert Leus, Baltasar Beferull-Lozano, Sergio Barbarossa, Paolo Di Lorenzo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.01576v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Developing methods to process irregularly structured data is crucial in applications like gene-regulatory, brain, power, and socioeconomic networks. Graphs have been the go-to algebraic tool for modeling the structure via nodes and edges capturing their interactions, leading to the establishment of the fields of graph signal processing (GSP) and graph machine learning (GML). Key graph-aware methods include Fourier transform, filtering, sampling, as well as topology identification and spatiotemporal processing. Although versatile, graphs can model only pairwise dependencies in the data. To this end, topological structures such as simplicial and cell complexes have emerged as algebraic representations for more intricate structure modeling in data-driven systems, fueling the rapid development of novel topological-based processing and learning methods. This paper first presents the core principles of topological signal processing through the Hodge theory, a framework instrumental in propelling the field forward thanks to principled connections with GSP-GML. It then outlines advances in topological signal representation, filtering, and sampling, as well as inferring topological structures from data, processing spatiotemporal topological signals, and connections with topological machine learning. The impact of topological signal processing and learning is finally highlighted in applications dealing with flow data over networks, geometric processing, statistical ranking, biology, and semantic communication.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-02</td>
<td style='padding: 8px;'>New Graphs at the braingraph.org Website for Studying the Aging Brain Circuitry</td>
<td style='padding: 6px;'>Balint Varga, Vince Grolmusz</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.01418v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Human braingraphs or connectomes are widely studied in the last decade to understand the structural and functional properties of our brain. In the last several years our research group has computed and deposited thousands of human braingraphs to the braingraph.org site, by applying public structural (diffusion) MRI data from young and healthy subjects. Here we describe a recent addition to the {\tt braingraph.org} site, which contains connectomes from healthy and demented subjects between 42 and 95 years of age, based on the public release of the OASIS-3 dataset. The diffusion MRI data was processed with the Connectome Mapper Toolkit v.3.1. We believe that the new addition to the braingraph.org site will become a useful resource for enlightening the aging circuitry of the human brain in healthy and diseased subjects, including those with Alzheimer's disease in several stages.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-02</td>
<td style='padding: 8px;'>Photon transport through the entire adult human head</td>
<td style='padding: 6px;'>Jack Radford, Vytautas Gradauskas, Kevin J. Mitchell, Samuel Nerenberg, Ilya Starshynov, Daniele Faccio</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.01360v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Optical brain imaging technologies are promising due to their relatively high temporal resolution, portability and cost-effectiveness. However, the highly scattering nature of near-infrared light in human tissue makes it challenging to collect photons emerging from more than 4 cm below the scalp, or with source-detector separation larger than several centimeters. We explore the physical limits of photon transport in the head and show that despite an extreme attenuation of ~10^(18), we can experimentally detect light that is transmitted diametrically through the entire adult human head. Analysis of various photon migration pathways through the head also indicates how the source-detector configuration can be used to isolate photons interacting with deep regions of the brain that are inaccessible with current optical techniques.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-03</td>
<td style='padding: 8px;'>An ADHD Diagnostic Interface Based on EEG Spectrograms and Deep Learning Techniques</td>
<td style='padding: 6px;'>Medha Pappula, Syed Muhammad Anwar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02695v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper introduces an innovative approach to Attention-deficit/hyperactivity disorder (ADHD) diagnosis by employing deep learning (DL) techniques on electroencephalography (EEG) signals. This method addresses the limitations of current behavior-based diagnostic methods, which often lead to misdiagnosis and gender bias. By utilizing a publicly available EEG dataset and converting the signals into spectrograms, a Resnet-18 convolutional neural network (CNN) architecture was used to extract features for ADHD classification. The model achieved a high precision, recall, and an overall F1 score of 0.9. Feature extraction highlighted significant brain regions (frontopolar, parietal, and occipital lobes) associated with ADHD. These insights guided the creation of a three-part digital diagnostic system, facilitating cost-effective and accessible ADHD screening, especially in school environments. This system enables earlier and more accurate identification of students at risk for ADHD, providing timely support to enhance their developmental outcomes. This study showcases the potential of integrating EEG analysis with DL to enhance ADHD diagnostics, presenting a viable alternative to traditional methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-02</td>
<td style='padding: 8px;'>ECG-SleepNet: Deep Learning-Based Comprehensive Sleep Stage Classification Using ECG Signals</td>
<td style='padding: 6px;'>Poorya Aghaomidi, Ge Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.01929v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurate sleep stage classification is essential for understanding sleep disorders and improving overall health. This study proposes a novel three-stage approach for sleep stage classification using ECG signals, offering a more accessible alternative to traditional methods that often rely on complex modalities like EEG. In Stages 1 and 2, we initialize the weights of two networks, which are then integrated in Stage 3 for comprehensive classification. In the first phase, we estimate key features using Feature Imitating Networks (FINs) to achieve higher accuracy and faster convergence. The second phase focuses on identifying the N1 sleep stage through the time-frequency representation of ECG signals. Finally, the third phase integrates models from the previous stages and employs a Kolmogorov-Arnold Network (KAN) to classify five distinct sleep stages. Additionally, data augmentation techniques, particularly SMOTE, are used in enhancing classification capabilities for underrepresented stages like N1. Our results demonstrate significant improvements in the classification performance, with an overall accuracy of 80.79% an overall kappa of 0.73. The model achieves specific accuracies of 86.70% for Wake, 60.36% for N1, 83.89% for N2, 84.85% for N3, and 87.16% for REM. This study emphasizes the importance of weight initialization and data augmentation in optimizing sleep stage classification with ECG signals.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-02</td>
<td style='padding: 8px;'>Linear stimulus reconstruction works on the KU Leuven audiovisual, gaze-controlled auditory attention decoding dataset</td>
<td style='padding: 6px;'>Simon Geirnaert, Iustina Rotaru, Tom Francart, Alexander Bertrand</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.01401v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In a recent paper, we presented the KU Leuven audiovisual, gaze-controlled auditory attention decoding (AV-GC-AAD) dataset, in which we recorded electroencephalography (EEG) signals of participants attending to one out of two competing speakers under various audiovisual conditions. The main goal of this dataset was to disentangle the direction of gaze from the direction of auditory attention, in order to reveal gaze-related shortcuts in existing spatial AAD algorithms that aim to decode the (direction of) auditory attention directly from the EEG. Various methods based on spatial AAD do not achieve significant above-chance performances on our AV-GC-AAD dataset, indicating that previously reported results were mainly driven by eye gaze confounds in existing datasets. Still, these adverse outcomes are often discarded for reasons that are attributed to the limitations of the AV-GC-AAD dataset, such as the limited amount of data to train a working model, too much data heterogeneity due to different audiovisual conditions, or participants allegedly being unable to focus their auditory attention under the complex instructions. In this paper, we present the results of the linear stimulus reconstruction AAD algorithm and show that high AAD accuracy can be obtained within each individual condition and that the model generalizes across conditions, across new subjects, and even across datasets. Therefore, we eliminate any doubts that the inadequacy of the AV-GC-AAD dataset is the primary reason for the (spatial) AAD algorithms failing to achieve above-chance performance when compared to other datasets. Furthermore, this report provides a simple baseline evaluation procedure (including source code) that can serve as the minimal benchmark for all future AAD algorithms evaluated on this dataset.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-02</td>
<td style='padding: 8px;'>Federated Motor Imagery Classification for Privacy-Preserving Brain-Computer Interfaces</td>
<td style='padding: 6px;'>Tianwang Jia, Lubin Meng, Siyang Li, Jiajing Liu, Dongrui Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.01079v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Training an accurate classifier for EEG-based brain-computer interface (BCI) requires EEG data from a large number of users, whereas protecting their data privacy is a critical consideration. Federated learning (FL) is a promising solution to this challenge. This paper proposes Federated classification with local Batch-specific batch normalization and Sharpness-aware minimization (FedBS) for privacy protection in EEG-based motor imagery (MI) classification. FedBS utilizes local batch-specific batch normalization to reduce data discrepancies among different clients, and sharpness-aware minimization optimizer in local training to improve model generalization. Experiments on three public MI datasets using three popular deep learning models demonstrated that FedBS outperformed six state-of-the-art FL approaches. Remarkably, it also outperformed centralized training, which does not consider privacy protection at all. In summary, FedBS protects user EEG data privacy, enabling multiple BCI users to participate in large-scale machine learning model training, which in turn improves the BCI decoding accuracy.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-29</td>
<td style='padding: 8px;'>Dynamic EEG-fMRI mapping: Revealing the relationship between brain connectivity and cognitive state</td>
<td style='padding: 6px;'>Guiran Liu, Binrong Zhu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19922v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study investigated the dynamic connectivity patterns between EEG and fMRI modalities, contributing to our understanding of brain network interactions. By employing a comprehensive approach that integrated static and dynamic analyses of EEG-fMRI data, we were able to uncover distinct connectivity states and characterize their temporal fluctuations. The results revealed modular organization within the intrinsic connectivity networks (ICNs) of the brain, highlighting the significant roles of sensory systems and the default mode network. The use of a sliding window technique allowed us to assess how functional connectivity varies over time, further elucidating the transient nature of brain connectivity. Additionally, our findings align with previous literature, reinforcing the notion that cognitive states can be effectively identified through short-duration data, specifically within the 30-60 second timeframe. The established relationships between connectivity strength and cognitive processes, particularly during different visual states, underscore the relevance of our approach for future research into brain dynamics. Overall, this study not only enhances our understanding of the interplay between EEG and fMRI signals but also paves the way for further exploration into the neural correlates of cognitive functions and their implications in clinical settings. Future research should focus on refining these methodologies and exploring their applications in various cognitive and clinical contexts.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-29</td>
<td style='padding: 8px;'>Graph-Enhanced EEG Foundation Model</td>
<td style='padding: 6px;'>Limin Wang, Toyotaro Suzumura, Hiroki Kanezashi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19507v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) signals provide critical insights for applications in disease diagnosis and healthcare. However, the scarcity of labeled EEG data poses a significant challenge. Foundation models offer a promising solution by leveraging large-scale unlabeled data through pre-training, enabling strong performance across diverse tasks. While both temporal dynamics and inter-channel relationships are vital for understanding EEG signals, existing EEG foundation models primarily focus on the former, overlooking the latter. To address this limitation, we propose a novel foundation model for EEG that integrates both temporal and inter-channel information. Our architecture combines Graph Neural Networks (GNNs), which effectively capture relational structures, with a masked autoencoder to enable efficient pre-training. We evaluated our approach using three downstream tasks and experimented with various GNN architectures. The results demonstrate that our proposed model, particularly when employing the GCN architecture with optimized configurations, consistently outperformed baseline methods across all tasks. These findings suggest that our model serves as a robust foundation model for EEG analysis.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-29</td>
<td style='padding: 8px;'>Knowledge-Data Fusion Based Source-Free Semi-Supervised Domain Adaptation for Seizure Subtype Classification</td>
<td style='padding: 6px;'>Ruimin Peng, Jiayu An, Dongrui Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19502v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalogram (EEG)-based seizure subtype classification enhances clinical diagnosis efficiency. Source-free semi-supervised domain adaptation (SF-SSDA), which transfers a pre-trained model to a new dataset with no source data and limited labeled target data, can be used for privacy-preserving seizure subtype classification. This paper considers two challenges in SF-SSDA for EEG-based seizure subtype classification: 1) How to effectively fuse both raw EEG data and expert knowledge in classifier design? 2) How to align the source and target domain distributions for SF-SSDA? We propose a Knowledge-Data Fusion based SF-SSDA approach, KDF-MutualSHOT, for EEG-based seizure subtype classification. In source model training, KDF uses Jensen-Shannon Divergence to facilitate mutual learning between a feature-driven Decision Tree-based model and a data-driven Transformer-based model. To adapt KDF to a new target dataset, an SF-SSDA algorithm, MutualSHOT, is developed, which features a consistency-based pseudo-label selection strategy. Experiments on the public TUSZ and CHSZ datasets demonstrated that KDF-MutualSHOT outperformed other supervised and source-free domain adaptation approaches in cross-subject seizure subtype classification.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-29</td>
<td style='padding: 8px;'>Protecting Multiple Types of Privacy Simultaneously in EEG-based Brain-Computer Interfaces</td>
<td style='padding: 6px;'>Lubin Meng, Xue Jiang, Tianwang Jia, Dongrui Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19498v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A brain-computer interface (BCI) enables direct communication between the brain and an external device. Electroencephalogram (EEG) is the preferred input signal in non-invasive BCIs, due to its convenience and low cost. EEG-based BCIs have been successfully used in many applications, such as neurological rehabilitation, text input, games, and so on. However, EEG signals inherently carry rich personal information, necessitating privacy protection. This paper demonstrates that multiple types of private information (user identity, gender, and BCI-experience) can be easily inferred from EEG data, imposing a serious privacy threat to BCIs. To address this issue, we design perturbations to convert the original EEG data into privacy-protected EEG data, which conceal the private information while maintaining the primary BCI task performance. Experimental results demonstrated that the privacy-protected EEG data can significantly reduce the classification accuracy of user identity, gender and BCI-experience, but almost do not affect at all the classification accuracy of the primary BCI task, enabling user privacy protection in EEG-based BCIs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-28</td>
<td style='padding: 8px;'>Pre-Training Graph Contrastive Masked Autoencoders are Strong Distillers for EEG</td>
<td style='padding: 6px;'>Xinxu Wei, Kanhao Zhao, Yong Jiao, Nancy B. Carlisle, Hua Xie, Yu Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19230v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Effectively utilizing extensive unlabeled high-density EEG data to improve performance in scenarios with limited labeled low-density EEG data presents a significant challenge. In this paper, we address this by framing it as a graph transfer learning and knowledge distillation problem. We propose a Unified Pre-trained Graph Contrastive Masked Autoencoder Distiller, named EEG-DisGCMAE, to bridge the gap between unlabeled/labeled and high/low-density EEG data. To fully leverage the abundant unlabeled EEG data, we introduce a novel unified graph self-supervised pre-training paradigm, which seamlessly integrates Graph Contrastive Pre-training and Graph Masked Autoencoder Pre-training. This approach synergistically combines contrastive and generative pre-training techniques by reconstructing contrastive samples and contrasting the reconstructions. For knowledge distillation from high-density to low-density EEG data, we propose a Graph Topology Distillation loss function, allowing a lightweight student model trained on low-density data to learn from a teacher model trained on high-density data, effectively handling missing electrodes through contrastive distillation. To integrate transfer learning and distillation, we jointly pre-train the teacher and student models by contrasting their queries and keys during pre-training, enabling robust distillers for downstream tasks. We demonstrate the effectiveness of our method on four classification tasks across two clinical EEG datasets with abundant unlabeled data and limited labeled data. The experimental results show that our approach significantly outperforms contemporary methods in both efficiency and accuracy.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-28</td>
<td style='padding: 8px;'>Decoding Imagined Movement in People with Multiple Sclerosis for Brain-Computer Interface Translation</td>
<td style='padding: 6px;'>John S. Russo, Thomas A. Shiels, Chin-Hsuan Sophie Lin, Sam E. John, David B. Grayden</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.18916v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Multiple Sclerosis (MS) is a heterogeneous autoimmune-mediated disorder affecting the central nervous system, commonly manifesting as fatigue and progressive limb impairment. This can significantly impact quality of life due to weakness or paralysis in the upper and lower limbs. A Brain-Computer Interface (BCI) aims to restore quality of life through control of an external device, such as a wheelchair. However, the limited BCI research in people with MS is insufficient. The current study aims to expand on the current MS-BCI literature by highlighting the feasibility of decoding MS imagined movement. We collected electroencephalography (EEG) data from eight participants with various symptoms of MS and ten neurotypical control participants. Participants made imagined movements of the hands and feet as directed by a go no-go protocol. Binary regularised linear discriminant analysis was used to classify imagined movement at individual time-frequency points. The frequency bands which provided the maximal accuracy, and the associated latency, were compared. In all MS participants, the classification algorithm achieved above 70% accuracy in at least one imagined movement vs. rest classification and most movement vs. movement classifications. There was no significant difference between classification of limbs with weakness or paralysis to neurotypical controls. Both the MS and control groups possessed decodable information within the alpha (7-13 Hz) and beta (16-30 Hz) bands at similar latency. This study is the first to demonstrate the feasibility of decoding imagined movements in people with MS. As an alternative to the P300 response, motor imagery-based control of a BCI may also be combined with existing motor imagery therapy to supplement MS rehabilitation. These promising results merit further long term BCI studies to investigate the effect of MS progression on classification performance.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-03</td>
<td style='padding: 8px;'>A Simple Channel Compression Method for Brain Signal Decoding on Classification Task</td>
<td style='padding: 6px;'>Changqing Ji, Keisuke Kawasaki, Isao Hasegawa, Takayuki Okatani</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02078v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In the application of brain-computer interface (BCI), while pursuing accurate decoding of brain signals, we also need consider the computational efficiency of BCI devices. ECoG signals are multi-channel temporal signals which is collected using a high-density electrode array at a high sampling frequency. The data between channels has a high similarity or redundancy in the temporal domain. The redundancy of data not only reduces the computational efficiency of the model, but also overwhelms the extraction of effective features, resulting in a decrease in performance. How to efficiently utilize ECoG multi-channel signals is one of the research topics. Effective channel screening or compression can greatly reduce the model size, thereby improving computational efficiency, this would be a good direction to solve the problem. Based on previous work [1], this paper proposes a very simple channel compression method, which uses a learnable matrix to perform matrix multiplication on the original channels, that is, assigning weights to the channels and then linearly add them up. This effectively reduces the number of final channels. In the experiment, we used the vision-based ECoG multi-classification dataset owned by our laboratory to test the proposed channel selection (compression) method. We found that the new method can compress the original 128-channel ECoG signal to 32 channels (of which subject MonJ is compressed to 8 channels), greatly reducing the size of the model. The demand for GPU memory resources during model training is reduced by about 68.57%, 84.33% for each subject respectively; the model training speed also increased up around 3.82, 4.65 times of the original speed for each subject respectively. More importantly, the performance of the model has improved by about 1.10% compared with our previous work, reached the SOTA level of our unique visual based ECoG dataset</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-02</td>
<td style='padding: 8px;'>Federated Motor Imagery Classification for Privacy-Preserving Brain-Computer Interfaces</td>
<td style='padding: 6px;'>Tianwang Jia, Lubin Meng, Siyang Li, Jiajing Liu, Dongrui Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.01079v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Training an accurate classifier for EEG-based brain-computer interface (BCI) requires EEG data from a large number of users, whereas protecting their data privacy is a critical consideration. Federated learning (FL) is a promising solution to this challenge. This paper proposes Federated classification with local Batch-specific batch normalization and Sharpness-aware minimization (FedBS) for privacy protection in EEG-based motor imagery (MI) classification. FedBS utilizes local batch-specific batch normalization to reduce data discrepancies among different clients, and sharpness-aware minimization optimizer in local training to improve model generalization. Experiments on three public MI datasets using three popular deep learning models demonstrated that FedBS outperformed six state-of-the-art FL approaches. Remarkably, it also outperformed centralized training, which does not consider privacy protection at all. In summary, FedBS protects user EEG data privacy, enabling multiple BCI users to participate in large-scale machine learning model training, which in turn improves the BCI decoding accuracy.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-30</td>
<td style='padding: 8px;'>Bi-Band ECoGNet for ECoG Decoding on Classification Task</td>
<td style='padding: 6px;'>Changqing Ji, Keisuke Kawasaki, Isao Hasegwa, Takayuki Okatani</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.00378v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In the application of brain-computer interface (BCI), being able to accurately decode brain signals is a critical task. For the multi-class classification task of brain signal ECoG, how to improve the classification accuracy is one of the current research hotspots. ECoG acquisition uses a high-density electrode array and a high sampling frequency, which makes ECoG data have a certain high similarity and data redundancy in the temporal domain, and also unique spatial pattern in spatial domain. How to effectively extract features is both exciting and challenging. Previous work found that visual-related ECoG can carry visual information via frequency and spatial domain. Based on this finding, we focused on using deep learning to design frequency and spatial feature extraction modules, and proposed a Bi-Band ECoGNet model based on deep learning. The main contributions of this paper are: 1) The Bi-BCWT (Bi-Band Channel-Wise Transform) neural network module is designed to replace the time-consume method MST, this module greatly improves the model calculation and data storage efficiency, and effectively increases the training speed; 2) The Bi-BCWT module can effectively take into account the information both in low-frequency and high-frequency domain, which is more conducive to ECoG multi-classification tasks; 3) ECoG is acquired using 2D electrode array, the newly designed 2D Spatial-Temporal feature encoder can extract the 2D spatial feature better. Experiments have shown that the unique 2D spatial data structure can effectively improve classification accuracy; 3) Compared with previous work, the Bi-Band ECoGNet model is smaller and has higher performance, with an accuracy increase of 1.24%, and the model training speed is increased by 6 times, which is more suitable for BCI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-29</td>
<td style='padding: 8px;'>Protecting Multiple Types of Privacy Simultaneously in EEG-based Brain-Computer Interfaces</td>
<td style='padding: 6px;'>Lubin Meng, Xue Jiang, Tianwang Jia, Dongrui Wu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19498v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A brain-computer interface (BCI) enables direct communication between the brain and an external device. Electroencephalogram (EEG) is the preferred input signal in non-invasive BCIs, due to its convenience and low cost. EEG-based BCIs have been successfully used in many applications, such as neurological rehabilitation, text input, games, and so on. However, EEG signals inherently carry rich personal information, necessitating privacy protection. This paper demonstrates that multiple types of private information (user identity, gender, and BCI-experience) can be easily inferred from EEG data, imposing a serious privacy threat to BCIs. To address this issue, we design perturbations to convert the original EEG data into privacy-protected EEG data, which conceal the private information while maintaining the primary BCI task performance. Experimental results demonstrated that the privacy-protected EEG data can significantly reduce the classification accuracy of user identity, gender and BCI-experience, but almost do not affect at all the classification accuracy of the primary BCI task, enabling user privacy protection in EEG-based BCIs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-28</td>
<td style='padding: 8px;'>Decoding Imagined Movement in People with Multiple Sclerosis for Brain-Computer Interface Translation</td>
<td style='padding: 6px;'>John S. Russo, Thomas A. Shiels, Chin-Hsuan Sophie Lin, Sam E. John, David B. Grayden</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.18916v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Multiple Sclerosis (MS) is a heterogeneous autoimmune-mediated disorder affecting the central nervous system, commonly manifesting as fatigue and progressive limb impairment. This can significantly impact quality of life due to weakness or paralysis in the upper and lower limbs. A Brain-Computer Interface (BCI) aims to restore quality of life through control of an external device, such as a wheelchair. However, the limited BCI research in people with MS is insufficient. The current study aims to expand on the current MS-BCI literature by highlighting the feasibility of decoding MS imagined movement. We collected electroencephalography (EEG) data from eight participants with various symptoms of MS and ten neurotypical control participants. Participants made imagined movements of the hands and feet as directed by a go no-go protocol. Binary regularised linear discriminant analysis was used to classify imagined movement at individual time-frequency points. The frequency bands which provided the maximal accuracy, and the associated latency, were compared. In all MS participants, the classification algorithm achieved above 70% accuracy in at least one imagined movement vs. rest classification and most movement vs. movement classifications. There was no significant difference between classification of limbs with weakness or paralysis to neurotypical controls. Both the MS and control groups possessed decodable information within the alpha (7-13 Hz) and beta (16-30 Hz) bands at similar latency. This study is the first to demonstrate the feasibility of decoding imagined movements in people with MS. As an alternative to the P300 response, motor imagery-based control of a BCI may also be combined with existing motor imagery therapy to supplement MS rehabilitation. These promising results merit further long term BCI studies to investigate the effect of MS progression on classification performance.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-28</td>
<td style='padding: 8px;'>ArEEG_Words: Dataset for Envisioned Speech Recognition using EEG for Arabic Words</td>
<td style='padding: 6px;'>Hazem Darwish, Abdalrahman Al Malah, Khloud Al Jallad, Nada Ghneim</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.18888v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-Computer-Interface (BCI) aims to support communication-impaired patients by translating neural signals into speech. A notable research topic in BCI involves Electroencephalography (EEG) signals that measure the electrical activity in the brain. While significant advancements have been made in BCI EEG research, a major limitation still exists: the scarcity of publicly available EEG datasets for non-English languages, such as Arabic. To address this gap, we introduce in this paper ArEEG_Words dataset, a novel EEG dataset recorded from 22 participants with mean age of 22 years (5 female, 17 male) using a 14-channel Emotiv Epoc X device. The participants were asked to be free from any effects on their nervous system, such as coffee, alcohol, cigarettes, and so 8 hours before recording. They were asked to stay calm in a clam room during imagining one of the 16 Arabic Words for 10 seconds. The words include 16 commonly used words such as up, down, left, and right. A total of 352 EEG recordings were collected, then each recording was divided into multiple 250ms signals, resulting in a total of 15,360 EEG signals. To the best of our knowledge, ArEEG_Words data is the first of its kind in Arabic EEG domain. Moreover, it is publicly available for researchers as we hope that will fill the gap in Arabic EEG research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-25</td>
<td style='padding: 8px;'>Explainable MST-ECoGNet Decode Visual Information from ECoG Signal</td>
<td style='padding: 6px;'>Changqing JI</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.16165v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In the application of brain-computer interface (BCI), we not only need to accurately decode brain signals,but also need to consider the explainability of the decoding process, which is related to the reliability of the model. In the process of designing a decoder or processing brain signals, we need to explain the discovered phenomena in physical or physiological way. An explainable model not only makes the signal processing process clearer and improves reliability, but also allows us to better understand brain activities and facilitate further exploration of the brain. In this paper, we systematically analyze the multi-classification dataset of visual brain signals ECoG, using a simple and highly explainable method to explore the ways in which ECoG carry visual information, then based on these findings, we propose a model called MST-ECoGNet that combines traditional mathematics and deep learning. The main contributions of this paper are: 1) found that ECoG time-frequency domain information carries visual information, provides important features for visual classification tasks. The mathematical method of MST (Modified S Transform) can effectively extract temporal-frequency domain information; 2) The spatial domain of ECoG signals also carries visual information, the unique spatial features are also important features for classification tasks; 3) The real and imaginary information in the time-frequency domain are complementary. The effective combination of the two is more helpful for classification tasks than using amplitude information alone; 4) Finally, compared with previous work, our model is smaller and has higher performance: for the object MonJ, the model size is reduced to 10.82% of base model, the accuracy is improved by 6.63%; for the object MonC, the model size is reduced to 8.78%, the accuracy is improved by 16.63%.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-23</td>
<td style='padding: 8px;'>ChatBCI: A P300 Speller BCI Leveraging Large Language Models for Improved Sentence Composition in Realistic Scenarios</td>
<td style='padding: 6px;'>Jiazhen Hong, Weinan Wang, Laleh Najafizadeh</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.15395v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>P300 speller BCIs allow users to compose sentences by selecting target keys on a GUI through the detection of P300 component in their EEG signals following visual stimuli. Most P300 speller BCIs require users to spell words letter by letter, or the first few initial letters, resulting in high keystroke demands that increase time, cognitive load, and fatigue. This highlights the need for more efficient, user-friendly methods for faster sentence composition. In this work, we introduce ChatBCI, a P300 speller BCI that leverages the zero-shot learning capabilities of large language models (LLMs) to suggest words from user-spelled initial letters or predict the subsequent word(s), reducing keystrokes and accelerating sentence composition. ChatBCI retrieves word suggestions through remote queries to the GPT-3.5 API. A new GUI, displaying GPT-3.5 word suggestions as extra keys is designed. SWLDA is used for the P300 classification. Seven subjects completed two online spelling tasks: 1) copy-spelling a self-composed sentence using ChatBCI, and 2) improvising a sentence using ChatBCI's word suggestions. Results demonstrate that in Task 1, on average, ChatBCI outperforms letter-by-letter BCI spellers, reducing time and keystrokes by 62.14% and 53.22%, respectively, and increasing information transfer rate by 198.96%. In Task 2, ChatBCI achieves 80.68% keystroke savings and a record 8.53 characters/min for typing speed. Overall, ChatBCI, by employing remote LLM queries, enhances sentence composition in realistic scenarios, significantly outperforming traditional spellers without requiring local model training or storage. ChatBCI's (multi-) word predictions, combined with its new GUI, pave the way for developing next-generation speller BCIs that are efficient and effective for real-time communication, especially for users with communication and motor disabilities.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-22</td>
<td style='padding: 8px;'>Brain-Computer Interfaces for Emotional Regulation in Patients with Various Disorders</td>
<td style='padding: 6px;'>Vedant Mehta</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.14666v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neurological and Physiological Disorders that impact emotional regulation each have their own unique characteristics which are important to understand in order to create a generalized solution to all of them. The purpose of this experiment is to explore the potential applications of EEG-based Brain-Computer Interfaces (BCIs) in enhancing emotional regulation for individuals with neurological and physiological disorders. The research focuses on the development of a novel neural network algorithm for understanding EEG data, with a particular emphasis on recognizing and regulating emotional states. The procedure involves the collection of EEG-based emotion data from open-Neuro. Using novel data modification techniques, information from the dataset can be altered to create a dataset that has neural patterns of patients with disorders whilst showing emotional change. The data analysis reveals promising results, as the algorithm is able to successfully classify emotional states with a high degree of accuracy. This suggests that EEG-based BCIs have the potential to be a valuable tool in aiding individuals with a range of neurological and physiological disorders in recognizing and regulating their emotions. To improve upon this work, data collection on patients with neurological disorders should be done to improve overall sample diversity.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-19</td>
<td style='padding: 8px;'>Enhanced Cross-Dataset Electroencephalogram-based Emotion Recognition using Unsupervised Domain Adaptation</td>
<td style='padding: 6px;'>Md Niaz Imtiaz, Naimul Khan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.12852v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Emotion recognition has significant potential in healthcare and affect-sensitive systems such as brain-computer interfaces (BCIs). However, challenges such as the high cost of labeled data and variability in electroencephalogram (EEG) signals across individuals limit the applicability of EEG-based emotion recognition models across domains. These challenges are exacerbated in cross-dataset scenarios due to differences in subject demographics, recording devices, and presented stimuli. To address these issues, we propose a novel approach to improve cross-domain EEG-based emotion classification. Our method, Gradual Proximity-guided Target Data Selection (GPTDS), incrementally selects reliable target domain samples for training. By evaluating their proximity to source clusters and the models confidence in predicting them, GPTDS minimizes negative transfer caused by noisy and diverse samples. Additionally, we introduce Prediction Confidence-aware Test-Time Augmentation (PC-TTA), a cost-effective augmentation technique. Unlike traditional TTA methods, which are computationally intensive, PC-TTA activates only when model confidence is low, improving inference performance while drastically reducing computational costs. Experiments on the DEAP and SEED datasets validate the effectiveness of our approach. When trained on DEAP and tested on SEED, our model achieves 67.44% accuracy, a 7.09% improvement over the baseline. Conversely, training on SEED and testing on DEAP yields 59.68% accuracy, a 6.07% improvement. Furthermore, PC-TTA reduces computational time by a factor of 15 compared to traditional TTA methods. Our method excels in detecting both positive and negative emotions, demonstrating its practical utility in healthcare applications. Code available at: https://github.com/RyersonMultimediaLab/EmotionRecognitionUDA</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-03</td>
<td style='padding: 8px;'>Hierarchical feature extraction on functional brain networks for autism spectrum disorder identification with resting-state fMRI data</td>
<td style='padding: 6px;'>Yiqian Luo, Qiurong Chen, Fali Li, Liang Yi, Peng Xu, Yangsong Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02424v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Autism spectrum disorder (ASD) is a pervasive developmental disorder of the central nervous system, which occurs most frequently in childhood and is characterized by unusual and repetitive ritualistic behaviors. Currently, diagnostic methods primarily rely on questionnaire surveys and behavioral observation, which may lead to misdiagnoses due to the subjective evaluation and measurement used in these traditional methods. With the advancement in medical imaging, MR imaging-based diagnosis has become an alternative and more objective approach. In this paper, we propose a Hybrid neural Network model for ASD identification, termded ASD-HNet, to hierarchically extract features on the functional brain networks based on resting-state functional magnetic resonance imaging data. This hierarchical method can better extract brain representations, improve the diagnostic accuracy, and help us better locate brain regions related to ASD. Specifically, features are extracted from three scales: local regions of interest (ROIs) scale, community-clustering scale, and the whole-communities scale. For the ROI scale, graph convolution is used to transfer features between ROIs. At the community cluster scale, functional gradients are introduced, the clustering algorithm K-Means is used to automatically cluster ROIs with similar functional gradients into several communities, and features of ROIs belonging to the same community are extracted to characterize these communities. At global information integration scale, we extract global features from community-scale brain networks to characterize the whole brain networks. We validate the effectiveness of our method using the public dataset of Autism Brain Imaging Data Exchange I (ABIDE I), and elucidate the interpretability of the method. Experimental results demonstrate that the proposed ASD-HNet can yield superior performance than compared methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-29</td>
<td style='padding: 8px;'>Dynamic EEG-fMRI mapping: Revealing the relationship between brain connectivity and cognitive state</td>
<td style='padding: 6px;'>Guiran Liu, Binrong Zhu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19922v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study investigated the dynamic connectivity patterns between EEG and fMRI modalities, contributing to our understanding of brain network interactions. By employing a comprehensive approach that integrated static and dynamic analyses of EEG-fMRI data, we were able to uncover distinct connectivity states and characterize their temporal fluctuations. The results revealed modular organization within the intrinsic connectivity networks (ICNs) of the brain, highlighting the significant roles of sensory systems and the default mode network. The use of a sliding window technique allowed us to assess how functional connectivity varies over time, further elucidating the transient nature of brain connectivity. Additionally, our findings align with previous literature, reinforcing the notion that cognitive states can be effectively identified through short-duration data, specifically within the 30-60 second timeframe. The established relationships between connectivity strength and cognitive processes, particularly during different visual states, underscore the relevance of our approach for future research into brain dynamics. Overall, this study not only enhances our understanding of the interplay between EEG and fMRI signals but also paves the way for further exploration into the neural correlates of cognitive functions and their implications in clinical settings. Future research should focus on refining these methodologies and exploring their applications in various cognitive and clinical contexts.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-28</td>
<td style='padding: 8px;'>3D Wasserstein generative adversarial network with dense U-Net based discriminator for preclinical fMRI denoising</td>
<td style='padding: 6px;'>Sima Soltanpour, Arnold Chang, Dan Madularu, Praveen Kulkarni, Craig Ferris, Chris Joslin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19345v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional magnetic resonance imaging (fMRI) is extensively used in clinical and preclinical settings to study brain function, however, fMRI data is inherently noisy due to physiological processes, hardware, and external noise. Denoising is one of the main preprocessing steps in any fMRI analysis pipeline. This process is challenging in preclinical data in comparison to clinical data due to variations in brain geometry, image resolution, and low signal-to-noise ratios. In this paper, we propose a structure-preserved algorithm based on a 3D Wasserstein generative adversarial network with a 3D dense U-net based discriminator called, 3D U-WGAN. We apply a 4D data configuration to effectively denoise temporal and spatial information in analyzing preclinical fMRI data. GAN-based denoising methods often utilize a discriminator to identify significant differences between denoised and noise-free images, focusing on global or local features. To refine the fMRI denoising model, our method employs a 3D dense U-Net discriminator to learn both global and local distinctions. To tackle potential over-smoothing, we introduce an adversarial loss and enhance perceptual similarity by measuring feature space distances. Experiments illustrate that 3D U-WGAN significantly improves image quality in resting-state and task preclinical fMRI data, enhancing signal-to-noise ratio without introducing excessive structural changes in existing methods. The proposed method outperforms state-of-the-art methods when applied to simulated and real data in a fMRI analysis pipeline.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-25</td>
<td style='padding: 8px;'>Energy landscape analysis based on the Ising model: Tutorial review</td>
<td style='padding: 6px;'>Naoki Masuda, Saiful Islam, Si Thu Aung, Takamitsu Watanabe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.16979v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We review a class of energy landscape analysis method that uses the Ising model and takes multivariate time series data as input. The method allows one to capture dynamics of the data as trajectories of a ball from one basin to a different basin to yet another, constrained on the energy landscape specified by the estimated Ising model. While this energy landscape analysis has mostly been applied to functional magnetic resonance imaging (fMRI) data from the brain for historical reasons, there are emerging applications outside fMRI data and neuroscience. To inform such applications in various research fields, this review paper provides a detailed tutorial on each step of the analysis, terminologies, concepts underlying the method, and validation, as well as recent developments of extended and related methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-25</td>
<td style='padding: 8px;'>COBRA: A Continual Learning Approach to Vision-Brain Understanding</td>
<td style='padding: 6px;'>Xuan-Bac Nguyen, Arabinda Kumar Choudhary, Pawan Sinha, Xin Li, Khoa Luu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.17475v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Vision-Brain Understanding (VBU) aims to extract visual information perceived by humans from brain activity recorded through functional Magnetic Resonance Imaging (fMRI). Despite notable advancements in recent years, existing studies in VBU continue to face the challenge of catastrophic forgetting, where models lose knowledge from prior subjects as they adapt to new ones. Addressing continual learning in this field is, therefore, essential. This paper introduces a novel framework called Continual Learning for Vision-Brain (COBRA) to address continual learning in VBU. Our approach includes three novel modules: a Subject Commonality (SC) module, a Prompt-based Subject Specific (PSS) module, and a transformer-based module for fMRI, denoted as MRIFormer module. The SC module captures shared vision-brain patterns across subjects, preserving this knowledge as the model encounters new subjects, thereby reducing the impact of catastrophic forgetting. On the other hand, the PSS module learns unique vision-brain patterns specific to each subject. Finally, the MRIFormer module contains a transformer encoder and decoder that learns the fMRI features for VBU from common and specific patterns. In a continual learning setup, COBRA is trained in new PSS and MRIFormer modules for new subjects, leaving the modules of previous subjects unaffected. As a result, COBRA effectively addresses catastrophic forgetting and achieves state-of-the-art performance in both continual learning and vision-brain reconstruction tasks, surpassing previous methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-25</td>
<td style='padding: 8px;'>Stability of Brain Functional Network During Working Memory Using Structural Balance Theory</td>
<td style='padding: 6px;'>Sepehr Gourabi, Masoud Lotfalipour, Reza Khosrowabadi, Reza Jafari</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.16558v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Working memory plays a crucial role in various aspects of human life. Therefore, it has been an area of interest in different research studies, especially neuroscience. The neuroscientists investigating working memory have primarily emphasized the brain's functional modularity. At the same time, a holistic perspective is still required to investigate the brain as an integrated and unified system. We hypothesized that the brain should shift towards a more stable state during working memory than the resting state. Therefore, based on the Structural Balance Theory (SBT), we aimed to address this process. To achieve this, we examined triadic associations in signed fMRI networks in healthy individuals using the N-back as the working memory task. We demonstrated that the number of balanced triads increased during the working memory task compared to the resting state, while the opposite is true for imbalanced triads. The increase of balanced triads forced the network to a more stable state with a lower balance energy level. The increase of balanced triads was crucially related to changes in anti-synchrony to synchronous activities between the Temporal Cortex, the Prefrontal Cortex, and the Parietal Cortex, which are known to be involved in various aspects of working memory, during the working memory process. We hope these findings pave the way to a better understanding the working memory process.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-23</td>
<td style='padding: 8px;'>Inducing Human-like Biases in Moral Reasoning Language Models</td>
<td style='padding: 6px;'>Artem Karpov, Seong Hah Cho, Austin Meek, Raymond Koopmanschap, Lucy Farnik, Bogdan-Ionut Cirstea</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.15386v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In this work, we study the alignment (BrainScore) of large language models (LLMs) fine-tuned for moral reasoning on behavioral data and/or brain data of humans performing the same task. We also explore if fine-tuning several LLMs on the fMRI data of humans performing moral reasoning can improve the BrainScore. We fine-tune several LLMs (BERT, RoBERTa, DeBERTa) on moral reasoning behavioral data from the ETHICS benchmark [Hendrycks et al., 2020], on the moral reasoning fMRI data from Koster-Hale et al. [2013], or on both. We study both the accuracy on the ETHICS benchmark and the BrainScores between model activations and fMRI data. While larger models generally performed better on both metrics, BrainScores did not significantly improve after fine-tuning.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-20</td>
<td style='padding: 8px;'>Quantum-Brain: Quantum-Inspired Neural Network Approach to Vision-Brain Understanding</td>
<td style='padding: 6px;'>Hoang-Quan Nguyen, Xuan-Bac Nguyen, Hugh Churchill, Arabinda Kumar Choudhary, Pawan Sinha, Samee U. Khan, Khoa Luu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.13378v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Vision-brain understanding aims to extract semantic information about brain signals from human perceptions. Existing deep learning methods for vision-brain understanding are usually introduced in a traditional learning paradigm missing the ability to learn the connectivities between brain regions. Meanwhile, the quantum computing theory offers a new paradigm for designing deep learning models. Motivated by the connectivities in the brain signals and the entanglement properties in quantum computing, we propose a novel Quantum-Brain approach, a quantum-inspired neural network, to tackle the vision-brain understanding problem. To compute the connectivity between areas in brain signals, we introduce a new Quantum-Inspired Voxel-Controlling module to learn the impact of a brain voxel on others represented in the Hilbert space. To effectively learn connectivity, a novel Phase-Shifting module is presented to calibrate the value of the brain signals. Finally, we introduce a new Measurement-like Projection module to present the connectivity information from the Hilbert space into the feature space. The proposed approach can learn to find the connectivities between fMRI voxels and enhance the semantic information obtained from human perceptions. Our experimental results on the Natural Scene Dataset benchmarks illustrate the effectiveness of the proposed method with Top-1 accuracies of 95.1% and 95.6% on image and brain retrieval tasks and an Inception score of 95.3% on fMRI-to-image reconstruction task. Our proposed quantum-inspired network brings a potential paradigm to solving the vision-brain problems via the quantum computing theory.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-19</td>
<td style='padding: 8px;'>Enhancing Blind Source Separation with Dissociative Principal Component Analysis</td>
<td style='padding: 6px;'>Muhammad Usman Khalid</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.12321v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Sparse principal component analysis (sPCA) enhances the interpretability of principal components (PCs) by imposing sparsity constraints on loading vectors (LVs). However, when used as a precursor to independent component analysis (ICA) for blind source separation (BSS), sPCA may underperform due to its focus on simplicity, potentially disregarding some statistical information essential for effective ICA. To overcome this limitation, a sophisticated approach is proposed that preserves the interpretability advantages of sPCA while significantly enhancing its source extraction capabilities. This consists of two tailored algorithms, dissociative PCA (DPCA1 and DPCA2), which employ adaptive and firm thresholding alongside gradient and coordinate descent approaches to optimize the proposed model dynamically. These algorithms integrate left and right singular vectors from singular value decomposition (SVD) through dissociation matrices (DMs) that replace traditional singular values, thus capturing latent interdependencies effectively to model complex source relationships. This leads to refined PCs and LVs that more accurately represent the underlying data structure. The proposed approach avoids focusing on individual eigenvectors, instead, it collaboratively combines multiple eigenvectors to disentangle interdependencies within each SVD variate. The superior performance of the proposed DPCA algorithms is demonstrated across four varied imaging applications including functional magnetic resonance imaging (fMRI) source retrieval, foreground-background separation, image reconstruction, and image inpainting. They outperformed traditional methods such as PCA+ICA, PPCA+ICA, SPCA+ICA, PMD, and GPower.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-16</td>
<td style='padding: 8px;'>In silico discovery of representational relationships across visual cortex</td>
<td style='padding: 6px;'>Alessandro T. Gifford, Maya A. JastrzÄbowska, Johannes J. D. Singer, Radoslaw M. Cichy</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.10872v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Human vision is mediated by a complex interconnected network of cortical brain areas jointly representing visual information. While these areas are increasingly understood in isolation, their representational relationships remain elusive. Here we developed relational neural control (RNC), and used it to investigate the representational relationships for univariate and multivariate fMRI responses of early- and mid-level visual areas. RNC generated and explored in silico fMRI responses for large amounts of images, discovering controlling images that align or disentangle responses across areas, thus indicating their shared or unique representational content. A large portion of representational content was shared across areas, unique representational content increased with cortical distance, and we isolated the visual features determining these effects. Closing the empirical cycle, we validated the in silico discoveries on in vivo fMRI responses from independent subjects. Together, this reveals how visual areas jointly represent the world as an interconnected network.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-29</td>
<td style='padding: 8px;'>Neuroplasticity and Psychedelics: a comprehensive examination of classic and non-classic compounds in pre and clinical models</td>
<td style='padding: 6px;'>Claudio Agnorelli, Meg Spriggs, Kate Godfrey, Gabriela Sawicka, Bettina Bohl, Hannah Douglass, Andrea Fagiolini, Hashemi Parastoo, Robin Carhart-Harris, David Nutt, David Erritzoe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19840v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroplasticity, the ability of the nervous system to adapt throughout an organism's lifespan, offers potential as both a biomarker and treatment target for neuropsychiatric conditions. Psychedelics, a burgeoning category of drugs, are increasingly prominent in psychiatric research, prompting inquiries into their mechanisms of action. Distinguishing themselves from traditional medications, psychedelics demonstrate rapid and enduring therapeutic effects after a single or few administrations, believed to stem from their neuroplasticity-enhancing properties. This review examines how classic psychedelics (e.g., LSD, psilocybin, N,N-DMT) and non-classic psychedelics (e.g., ketamine, MDMA) influence neuroplasticity. Drawing from preclinical and clinical studies, we explore the molecular, structural, and functional changes triggered by these agents. Animal studies suggest psychedelics induce heightened sensitivity of the nervous system to environmental stimuli (meta-plasticity), re-opening developmental windows for long-term structural changes (hyper-plasticity), with implications for mood and behavior. Translating these findings to humans faces challenges due to limitations in current imaging techniques. Nonetheless, promising new directions for human research are emerging, including the employment of novel positron-emission tomography (PET) radioligands, non-invasive brain stimulation methods, and multimodal approaches. By elucidating the interplay between psychedelics and neuroplasticity, this review informs the development of targeted interventions for neuropsychiatric disorders and advances understanding of psychedelics' therapeutic potential.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-29</td>
<td style='padding: 8px;'>On Monitoring Edge-Geodetic Sets of Dynamic Graph</td>
<td style='padding: 6px;'>Zin Mar Myint, Ashish Saxena</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.19800v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The concept of a monitoring edge-geodetic set (MEG-set) in a graph $G$, denoted $MEG(G)$, refers to a subset of vertices $MEG(G)\subseteq V(G)$ such that every edge $e$ in $G$ is monitored by some pair of vertices $ u, v \in MEG(G)$, where $e$ lies on all shortest paths between $u$ and $v$. The minimum number of vertices required to form such a set is called the monitoring edge-geodetic number, denoted $meg(G)$. The primary motivation for studying $MEG$-sets in previous works arises from scenarios in which certain edges are removed from $G$. In these cases, the vertices of the $MEG$-set are responsible for detecting these deletions. Such detection is crucial for identifying which edges have been removed from $G$ and need to be repaired. In real life, repairing these edges may be costly, or sometimes it is impossible to repair edges. In this case, the original $MEG$-set may no longer be effective in monitoring the modified graph. This highlights the importance of reassessing and adapting the $MEG$-set after edge deletions. This work investigates the monitoring edge-geodetic properties of graphs, focusing on how the removal of $k$ edges affects the structure of a graph and influences its monitoring capabilities. Specifically, we explore how the monitoring edge-geodetic number $meg(G)$ changes when $k$ edges are removed. The study aims to compare the monitoring properties of the original graph with those of the modified graph and to understand the impact of edge deletions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-14</td>
<td style='padding: 8px;'>Towards Neural Foundation Models for Vision: Aligning EEG, MEG, and fMRI Representations for Decoding, Encoding, and Modality Conversion</td>
<td style='padding: 6px;'>Matteo Ferrante, Tommaso Boccato, Grigorii Rashkov, Nicola Toschi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.09723v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper presents a novel approach towards creating a foundational model for aligning neural data and visual stimuli across multimodal representationsof brain activity by leveraging contrastive learning. We used electroencephalography (EEG), magnetoencephalography (MEG), and functional magnetic resonance imaging (fMRI) data. Our framework's capabilities are demonstrated through three key experiments: decoding visual information from neural data, encoding images into neural representations, and converting between neural modalities. The results highlight the model's ability to accurately capture semantic information across different brain imaging techniques, illustrating its potential in decoding, encoding, and modality conversion tasks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-12</td>
<td style='padding: 8px;'>Search for the X17 particle in $^{7}\mathrm{Li}(\mathrm{p},\mathrm{e}^+ \mathrm{e}^{-}) ^{8}\mathrm{Be}$ processes with the MEG II detector</td>
<td style='padding: 6px;'>The MEG II collaboration, K. Afanaciev, A. M. Baldini, S. Ban, H. Benmansour, G. Boca, P. W. Cattaneo, G. Cavoto, F. Cei, M. Chiappini, A. Corvaglia, G. Dal Maso, A. De Bari, M. De Gerone, L. Ferrari Barusso, M. Francesconi, L. Galli, G. Gallucci, F. Gatti, L. Gerritzen, F. Grancagnolo, E. G. Grandoni, M. Grassi, D. N. Grigoriev, M. Hildebrandt, F. Ignatov, F. Ikeda, T. Iwamoto, S. Karpov, P. -R. Kettle, N. Khomutov, A. Kolesnikov, N. Kravchuk, V. Krylov, N. Kuchinskiy, F. Leonetti, W. Li, V. Malyshev, A. Matsushita, M. Meucci, S. Mihara, W. Molzon, T. Mori, D. NicolÃ², H. Nishiguchi, A. Ochi, W. Ootani, A. Oya, D. Palo, M. Panareo, A. Papa, V. Pettinacci, A. Popov, F. Renga, S. Ritt, M. Rossella, A. Rozhdestvensky. S. Scarpellini, P. Schwendimann, G. Signorelli, M. Takahashi, Y. Uchiyama, A. Venturini, B. Vitali, C. Voena, K. Yamamoto, R. Yokota, T. Yonemoto</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.07994v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The observation of a resonance structure in the opening angle of the electron-positron pairs in the $^{7}$Li(p,\ee) $^{8}$Be reaction was claimed and interpreted as the production and subsequent decay of a hypothetical particle (X17). Similar excesses, consistent with this particle, were later observed in processes involving $^{4}$He and $^{12}$C nuclei with the same experimental technique. The MEG II apparatus at PSI, designed to search for the $\mu^+ \rightarrow \mathrm{e}^+ \gamma$ decay, can be exploited to investigate the existence of this particle and study its nature. Protons from a Cockroft-Walton accelerator, with an energy up to 1.1 MeV, were delivered on a dedicated Li-based target. The $\gamma$ and the e$^{+}$e$^{-}$ pair emerging from the $^8\mathrm{Be}^*$ transitions were studied with calorimeters and a spectrometer, featuring a broader angular acceptance than previous experiments. We present in this paper the analysis of a four-week data-taking in 2023 with a beam energy of 1080 keV, resulting in the excitation of two different resonances with Q-value \SI{17.6}{\mega\electronvolt} and \SI{18.1}{\mega\electronvolt}. No significant signal was found, and limits at \SI{90}{\percent} C.L. on the branching ratios (relative to the $\gamma$ emission) of the two resonances to X17 were set, $R_{17.6} < 1.8 \times 10^{-6} $ and $R_{18.1} < 1.2 \times 10^{-5} $.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-07</td>
<td style='padding: 8px;'>MEG: Medical Knowledge-Augmented Large Language Models for Question Answering</td>
<td style='padding: 6px;'>Laura Cabello, Carmen Martin-Turrero, Uchenna Akujuobi, Anders SÃ¸gaard, Carlos Bobed</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.03883v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Question answering is a natural language understanding task that involves reasoning over both explicit context and unstated, relevant domain knowledge. Large language models (LLMs), which underpin most contemporary question answering systems, struggle to induce how concepts relate in specialized domains such as medicine. Existing medical LLMs are also costly to train. In this work, we present MEG, a parameter-efficient approach for medical knowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate graph embeddings into the LLM, enabling it to leverage external knowledge in a cost-effective way. We evaluate our method on four popular medical multiple-choice datasets and show that LLMs greatly benefit from the factual grounding provided by knowledge graph embeddings. MEG attains an average of +10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized models like BioMistral. We also show results based on Llama-3. Finally, we show that MEG's performance remains robust to the choice of graph encoder.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-30</td>
<td style='padding: 8px;'>STIED: A deep learning model for the SpatioTemporal detection of focal Interictal Epileptiform Discharges with MEG</td>
<td style='padding: 6px;'>Raquel FernÃ¡ndez-MartÃ­n, Alfonso GijÃ³n, Odile Feys, Elodie JuvenÃ©, Alec Aeby, Charline Urbain, Xavier De TiÃ¨ge, Vincent Wens</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.23386v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Magnetoencephalography (MEG) allows the non-invasive detection of interictal epileptiform discharges (IEDs). Clinical MEG analysis in epileptic patients traditionally relies on the visual identification of IEDs, which is time consuming and partially subjective. Automatic, data-driven detection methods exist but show limited performance. Still, the rise of deep learning (DL)-with its ability to reproduce human-like abilities-could revolutionize clinical MEG practice. Here, we developed and validated STIED, a simple yet powerful supervised DL algorithm combining two convolutional neural networks with temporal (1D time-course) and spatial (2D topography) features of MEG signals inspired from current clinical guidelines. Our DL model enabled both temporal and spatial localization of IEDs in patients suffering from focal epilepsy with frequent and high amplitude spikes (FE group), with high-performance metrics-accuracy, specificity, and sensitivity all exceeding 85%-when learning from spatiotemporal features of IEDs. This performance can be attributed to our handling of input data, which mimics established clinical MEG practice. Reverse engineering further revealed that STIED encodes fine spatiotemporal features of IEDs rather than their mere amplitude. The model trained on the FE group also showed promising results when applied to a separate group of presurgical patients with different types of refractory focal epilepsy, though further work is needed to distinguish IEDs from physiological transients. This study paves the way of incorporating STIED and DL algorithms into the routine clinical MEG evaluation of epilepsy.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-28</td>
<td style='padding: 8px;'>NeuGPT: Unified multi-modal Neural GPT</td>
<td style='padding: 6px;'>Yiqian Yang, Yiqun Duan, Hyejeong Jo, Qiang Zhang, Renjing Xu, Oiwi Parker Jones, Xuming Hu, Chin-teng Lin, Hui Xiong</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.20916v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper introduces NeuGPT, a groundbreaking multi-modal language generation model designed to harmonize the fragmented landscape of neural recording research. Traditionally, studies in the field have been compartmentalized by signal type, with EEG, MEG, ECoG, SEEG, fMRI, and fNIRS data being analyzed in isolation. Recognizing the untapped potential for cross-pollination and the adaptability of neural signals across varying experimental conditions, we set out to develop a unified model capable of interfacing with multiple modalities. Drawing inspiration from the success of pre-trained large models in NLP, computer vision, and speech processing, NeuGPT is architected to process a diverse array of neural recordings and interact with speech and text data. Our model mainly focus on brain-to-text decoding, improving SOTA from 6.94 to 12.92 on BLEU-1 and 6.93 to 13.06 on ROUGE-1F. It can also simulate brain signals, thereby serving as a novel neural interface. Code is available at \href{https://github.com/NeuSpeech/NeuGPT}{NeuSpeech/NeuGPT (https://github.com/NeuSpeech/NeuGPT) .}</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-25</td>
<td style='padding: 8px;'>Resolving Domain Shift For Representations Of Speech In Non-Invasive Brain Recordings</td>
<td style='padding: 6px;'>Jeremiah Ridge, Oiwi Parker Jones</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.19986v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Machine learning techniques have enabled researchers to leverage neuroimaging data to decode speech from brain activity, with some amazing recent successes achieved by applications built using invasive devices. However, research requiring surgical implants has a number of practical limitations. Non-invasive neuroimaging techniques provide an alternative but come with their own set of challenges, the limited scale of individual studies being among them. Without the ability to pool the recordings from different non-invasive studies, data on the order of magnitude needed to leverage deep learning techniques to their full potential remains out of reach. In this work, we focus on non-invasive data collected using magnetoencephalography (MEG). We leverage two different, leading speech decoding models to investigate how an adversarial domain adaptation framework augments their ability to generalize across datasets. We successfully improve the performance of both models when training across multiple datasets. To the best of our knowledge, this study is the first ever application of feature-level, deep learning based harmonization for MEG neuroimaging data. Our analysis additionally offers further evidence of the impact of demographic features on neuroimaging data, demonstrating that participant age strongly affects how machine learning models solve speech decoding tasks using MEG data. Lastly, in the course of this study we produce a new open-source implementation of one of these models to the benefit of the broader scientific community.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-20</td>
<td style='padding: 8px;'>Non-invasive Neural Decoding in Source Reconstructed Brain Space</td>
<td style='padding: 6px;'>Yonatan Gideoni, Ryan Charles Timms, Oiwi Parker Jones</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.19838v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Non-invasive brainwave decoding is usually done using Magneto/Electroencephalography (MEG/EEG) sensor measurements as inputs. This makes combining datasets and building models with inductive biases difficult as most datasets use different scanners and the sensor arrays have a nonintuitive spatial structure. In contrast, fMRI scans are acquired directly in brain space, a voxel grid with a typical structured input representation. By using established techniques to reconstruct the sensors' sources' neural activity it is possible to decode from voxels for MEG data as well. We show that this enables spatial inductive biases, spatial data augmentations, better interpretability, zero-shot generalisation between datasets, and data harmonisation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-19</td>
<td style='padding: 8px;'>BrainECHO: Semantic Brain Signal Decoding through Vector-Quantized Spectrogram Reconstruction for Whisper-Enhanced Text Generation</td>
<td style='padding: 6px;'>Jilong Li, Zhenxi Song, Jiaqi Wang, Min Zhang, Zhiguo Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.14971v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent advances in decoding language from brain signals (EEG and MEG) have been significantly driven by pre-trained language models, leading to remarkable progress on publicly available non-invasive EEG/MEG datasets. However, previous works predominantly utilize teacher forcing during text generation, leading to significant performance drops without its use. A fundamental issue is the inability to establish a unified feature space correlating textual data with the corresponding evoked brain signals. Although some recent studies attempt to mitigate this gap using an audio-text pre-trained model, Whisper, which is favored for its signal input modality, they still largely overlook the inherent differences between audio signals and brain signals in directly applying Whisper to decode brain signals. To address these limitations, we propose a new multi-stage strategy for semantic brain signal decoding via vEctor-quantized speCtrogram reconstruction for WHisper-enhanced text generatiOn, termed BrainECHO. Specifically, BrainECHO successively conducts: 1) Discrete autoencoding of the audio spectrogram; 2) Brain-audio latent space alignment; and 3) Semantic text generation via Whisper finetuning. Through this autoencoding--alignment--finetuning process, BrainECHO outperforms state-of-the-art methods under the same data split settings on two widely accepted resources: the EEG dataset (Brennan) and the MEG dataset (GWilliams). The innovation of BrainECHO, coupled with its robustness and superiority at the sentence, session, and subject-independent levels across public datasets, underscores its significance for language-based brain-computer interfaces.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-27</td>
<td style='padding: 8px;'>NeuroAI for AI Safety</td>
<td style='padding: 6px;'>Patrick Mineault, NiccolÃ² Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.18526v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-21</td>
<td style='padding: 8px;'>Evaluating Representational Similarity Measures from the Lens of Functional Correspondence</td>
<td style='padding: 6px;'>Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.14633v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-25</td>
<td style='padding: 8px;'>A prescriptive theory for brain-like inference</td>
<td style='padding: 6px;'>Hadi Vafaii, Dekel Galor, Jacob L. Yates</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.19315v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The Evidence Lower Bound (ELBO) is a widely used objective for training deep generative models, such as Variational Autoencoders (VAEs). In the neuroscience literature, an identical objective is known as the variational free energy, hinting at a potential unified framework for brain function and machine learning. Despite its utility in interpreting generative models, including diffusion models, ELBO maximization is often seen as too broad to offer prescriptive guidance for specific architectures in neuroscience or machine learning. In this work, we show that maximizing ELBO under Poisson assumptions for general sequence data leads to a spiking neural network that performs Bayesian posterior inference through its membrane potential dynamics. The resulting model, the iterative Poisson VAE (iP-VAE), has a closer connection to biological neurons than previous brain-inspired predictive coding models based on Gaussian assumptions. Compared to amortized and iterative VAEs, iP-VAElearns sparser representations and exhibits superior generalization to out-of-distribution samples. These findings suggest that optimizing ELBO, combined with Poisson assumptions, provides a solid foundation for developing prescriptive theories in NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-09</td>
<td style='padding: 8px;'>Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models</td>
<td style='padding: 6px;'>Emily Cheng, Richard J. Antonello</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.05771v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first "composition" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-07-22</td>
<td style='padding: 8px;'>Predictive Coding Networks and Inference Learning: Tutorial and Survey</td>
<td style='padding: 6px;'>BjÃ¶rn van Zwol, Ro Jefferson, Egon L. van den Broek</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2407.04117v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent years have witnessed a growing call for renewed emphasis on neuroscience-inspired approaches in artificial intelligence research, under the banner of NeuroAI. A prime example of this is predictive coding networks (PCNs), based on the neuroscientific framework of predictive coding. This framework views the brain as a hierarchical Bayesian inference model that minimizes prediction errors through feedback connections. Unlike traditional neural networks trained with backpropagation (BP), PCNs utilize inference learning (IL), a more biologically plausible algorithm that explains patterns of neural activity that BP cannot. Historically, IL has been more computationally intensive, but recent advancements have demonstrated that it can achieve higher efficiency than BP with sufficient parallelization. Furthermore, PCNs can be mathematically considered a superset of traditional feedforward neural networks (FNNs), significantly extending the range of trainable architectures. As inherently probabilistic (graphical) latent variable models, PCNs provide a versatile framework for both supervised learning and unsupervised (generative) modeling that goes beyond traditional artificial neural networks. This work provides a comprehensive review and detailed formal specification of PCNs, particularly situating them within the context of modern ML methods. Additionally, we introduce a Python library (PRECO) for practical implementation. This positions PC as a promising framework for future ML innovations.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-10-29</td>
<td style='padding: 8px;'>Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis</td>
<td style='padding: 6px;'>Mitchell Ostrow, Adam Eisen, Leo Kozachkov, Ila Fiete</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2306.10168v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-05-25</td>
<td style='padding: 8px;'>Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture</td>
<td style='padding: 6px;'>Galen Pogoncheff, Jacob Granley, Michael Beyeler</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2305.11275v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-09</td>
<td style='padding: 8px;'>A Deep Probabilistic Spatiotemporal Framework for Dynamic Graph Representation Learning with Application to Brain Disorder Identification</td>
<td style='padding: 6px;'>Sin-Yee Yap, Junn Yong Loo, Chee-Ming Ting, Fuad Noman, Raphael C. -W. Phan, Adeel Razi, David L. Dowe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2302.07243v4' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent applications of pattern recognition techniques on brain connectome classification using functional connectivity (FC) are shifting towards acknowledging the non-Euclidean topology and dynamic aspects of brain connectivity across time. In this paper, a deep spatiotemporal variational Bayes (DSVB) framework is proposed to learn time-varying topological structures in dynamic FC networks for identifying autism spectrum disorder (ASD) in human participants. The framework incorporates a spatial-aware recurrent neural network with an attention-based message passing scheme to capture rich spatiotemporal patterns across dynamic FC networks. To overcome model overfitting on limited training datasets, an adversarial training strategy is introduced to learn graph embedding models that generalize well to unseen brain networks. Evaluation on the ABIDE resting-state functional magnetic resonance imaging dataset shows that our proposed framework substantially outperforms state-of-the-art methods in identifying patients with ASD. Dynamic FC analyses with DSVB-learned embeddings reveal apparent group differences between ASD and healthy controls in brain network connectivity patterns and switching dynamics of brain states. The code is available at https://github.com/Monash-NeuroAI/Deep-Spatiotemporal-Variational-Bayes.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-03-11</td>
<td style='padding: 8px;'>Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks</td>
<td style='padding: 6px;'>Feng-Lei Fan, Yingxin Li, Hanchuan Peng, Tieyong Zeng, Fei Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2301.09245v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Throughout history, the development of artificial intelligence, particularly artificial neural networks, has been open to and constantly inspired by the increasingly deepened understanding of the brain, such as the inspiration of neocognitron, which is the pioneering work of convolutional neural networks. Per the motives of the emerging field: NeuroAI, a great amount of neuroscience knowledge can help catalyze the next generation of AI by endowing a network with more powerful capabilities. As we know, the human brain has numerous morphologically and functionally different neurons, while artificial neural networks are almost exclusively built on a single neuron type. In the human brain, neuronal diversity is an enabling factor for all kinds of biological intelligent behaviors. Since an artificial network is a miniature of the human brain, introducing neuronal diversity should be valuable in terms of addressing those essential problems of artificial networks such as efficiency, interpretability, and memory. In this Primer, we first discuss the preliminaries of biological neuronal diversity and the characteristics of information transmission and processing in a biological neuron. Then, we review studies of designing new neurons for artificial networks. Next, we discuss what gains can neuronal diversity bring into artificial networks and exemplary applications in several important fields. Lastly, we discuss the challenges and future directions of neuronal diversity to explore the potential of NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2022-12-08</td>
<td style='padding: 8px;'>A Rubric for Human-like Agents and NeuroAI</td>
<td style='padding: 6px;'>Ida Momennejad</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2212.04401v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Researchers across cognitive, neuro-, and computer sciences increasingly reference human-like artificial intelligence and neuroAI. However, the scope and use of the terms are often inconsistent. Contributed research ranges widely from mimicking behaviour, to testing machine learning methods as neurally plausible hypotheses at the cellular or functional levels, or solving engineering problems. However, it cannot be assumed nor expected that progress on one of these three goals will automatically translate to progress in others. Here a simple rubric is proposed to clarify the scope of individual contributions, grounded in their commitments to human-like behaviour, neural plausibility, or benchmark/engineering goals. This is clarified using examples of weak and strong neuroAI and human-like agents, and discussing the generative, corroborate, and corrective ways in which the three dimensions interact with one another. The author maintains that future progress in artificial intelligence will need strong interactions across the disciplines, with iterative feedback loops and meticulous validity tests, leading to both known and yet-unknown advances that may span decades to come.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-03</td>
<td style='padding: 8px;'>Medical Multimodal Foundation Models in Clinical Diagnosis and Treatment: Applications, Challenges, and Future Directions</td>
<td style='padding: 6px;'>Kai Sun, Siyan Xue, Fuchun Sun, Haoran Sun, Yu Luo, Ling Wang, Siyuan Wang, Na Guo, Lei Liu, Tian Zhao, Xinzhou Wang, Lei Yang, Shuo Jin, Jun Yan, Jiahong Dong</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02621v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent advancements in deep learning have significantly revolutionized the field of clinical diagnosis and treatment, offering novel approaches to improve diagnostic precision and treatment efficacy across diverse clinical domains, thus driving the pursuit of precision medicine. The growing availability of multi-organ and multimodal datasets has accelerated the development of large-scale Medical Multimodal Foundation Models (MMFMs). These models, known for their strong generalization capabilities and rich representational power, are increasingly being adapted to address a wide range of clinical tasks, from early diagnosis to personalized treatment strategies. This review offers a comprehensive analysis of recent developments in MMFMs, focusing on three key aspects: datasets, model architectures, and clinical applications. We also explore the challenges and opportunities in optimizing multimodal representations and discuss how these advancements are shaping the future of healthcare by enabling improved patient outcomes and more efficient clinical workflows.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-03</td>
<td style='padding: 8px;'>Hierarchical feature extraction on functional brain networks for autism spectrum disorder identification with resting-state fMRI data</td>
<td style='padding: 6px;'>Yiqian Luo, Qiurong Chen, Fali Li, Liang Yi, Peng Xu, Yangsong Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02424v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Autism spectrum disorder (ASD) is a pervasive developmental disorder of the central nervous system, which occurs most frequently in childhood and is characterized by unusual and repetitive ritualistic behaviors. Currently, diagnostic methods primarily rely on questionnaire surveys and behavioral observation, which may lead to misdiagnoses due to the subjective evaluation and measurement used in these traditional methods. With the advancement in medical imaging, MR imaging-based diagnosis has become an alternative and more objective approach. In this paper, we propose a Hybrid neural Network model for ASD identification, termded ASD-HNet, to hierarchically extract features on the functional brain networks based on resting-state functional magnetic resonance imaging data. This hierarchical method can better extract brain representations, improve the diagnostic accuracy, and help us better locate brain regions related to ASD. Specifically, features are extracted from three scales: local regions of interest (ROIs) scale, community-clustering scale, and the whole-communities scale. For the ROI scale, graph convolution is used to transfer features between ROIs. At the community cluster scale, functional gradients are introduced, the clustering algorithm K-Means is used to automatically cluster ROIs with similar functional gradients into several communities, and features of ROIs belonging to the same community are extracted to characterize these communities. At global information integration scale, we extract global features from community-scale brain networks to characterize the whole brain networks. We validate the effectiveness of our method using the public dataset of Autism Brain Imaging Data Exchange I (ABIDE I), and elucidate the interpretability of the method. Experimental results demonstrate that the proposed ASD-HNet can yield superior performance than compared methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-03</td>
<td style='padding: 8px;'>High-Quality Passive Acoustic Mapping with the Cross-Correlated Angular Spectrum Method</td>
<td style='padding: 6px;'>Yi Zeng, Hui Zhu, Jinwei Li, Jianfeng Li, Fei Li, Shukuan Lu, Xiran Cai</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02413v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While passive acoustic mapping (PAM) has been advanced for monitoring acoustic cavitation activity in focused ultrasound (FUS) therapy, achieving both real-time and high-quality imaging capabilities is still challenging. The angular spectrum (AS) method presents the most efficient algorithm for PAM, but it suffers from artifacts and low resolution due to the diffraction pattern of the imaging array. Data-adaptive beamformers suppress artifacts well, but their overwhelming computational complexity, more than two orders of magnitude higher than the classical time exposure acoustic (TEA) method, hinders their application in real-time. In this work, we introduce the cross-correlated AS method to address the challenge. This method is based on cross-correlating the AS back-propagated wave fields, in the frequency domain, measured by different apodized sub-apertures of the transducer array to provide the normalized correlation coefficient (NCC) matrix for artifacts suppression. We observed that the spatial pattern of NCC matrix is variable which can be utilized by the triple apodization with cross-correlation (TAX) with AS scheme, namely the AS-TAX method, for optimal artifacts suppression outcomes. Both the phantom and mouse tumor experiments showed that: 1) the AS-TAX method has comparable image quality as the data-adaptive beamformers, reducing the energy spread area by 34.8-66.6% and improving image signal-to-noise ratio by 10.7-14.5 dB compared to TEA; 2) it reduces the computational complexity by two orders of magnitude compared to TEA allowing millisecond-level image reconstruction speed with a parallel implementation; 3) it can well map microbubble cavitation activity of different status (stable or inertial). The AS-TAX method represents a real-time approach to monitor cavitation-based FUS therapy with high image quality.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-03</td>
<td style='padding: 8px;'>PETAT -- An ASIC for Simple and Efficient Readout of Large PET Scanners</td>
<td style='padding: 6px;'>Peter Fischer, Michael Ritzert, Thomas Kerschenbauer</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02394v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Modern PET scanners based on scintillating crystals use solid state photo detectors for light readout. The small area of these devices is beneficial for spatial resolution, but also leads to a large number of electronic channels to be read out, mostly by application specific integrated circuits (ASICs) containing amplification, noise reduction, hit finding, time stamping and amplitude measurement. Although each ASIC provides up to $\approx 64$ channels, a large number of chips is required with the need for auxiliary electronic components like voltage regulators or FPGAs for control and data readout. The FPGAs in turn often require multiple supply voltages and configuration infrastructure, so that PCBs get complicated, cumbersome and power-hungry, in addition to the significant power requirement of the front-end ASICs. We address this issue in the latest generation of our PETA readout ASIC for SiPMs by a simplified control scheme and, in particular, by a hierarchical serial data readout which does not require any additional FPGA. In addition, it provides a time-sorted stream of hit data, allowing early on-detector data reduction and hit pre-processing like the removal of hits with no coincident partner. The simplicity of this readout facilitates a supply scheme where power/ground of multiple ASICs are connected in series instead of the standard parallel connection. This 'serial-powering' approach can reduce supply current (while increasing overall supply voltage) so that voltage drop issues in the supply are alleviated.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-03</td>
<td style='padding: 8px;'>U-Net in Medical Image Segmentation: A Review of Its Applications Across Modalities</td>
<td style='padding: 6px;'>Fnu Neha, Deepshikha Bhati, Deepak Kumar Shukla, Sonavi Makarand Dalvi, Nikolaos Mantzou, Safa Shubbar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02242v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Medical imaging is essential in healthcare to provide key insights into patient anatomy and pathology, aiding in diagnosis and treatment. Non-invasive techniques such as X-ray, Magnetic Resonance Imaging (MRI), Computed Tomography (CT), and Ultrasound (US), capture detailed images of organs, tissues, and abnormalities. Effective analysis of these images requires precise segmentation to delineate regions of interest (ROI), such as organs or lesions. Traditional segmentation methods, relying on manual feature-extraction, are labor-intensive and vary across experts. Recent advancements in Artificial Intelligence (AI) and Deep Learning (DL), particularly convolutional models such as U-Net and its variants (U-Net++ and U-Net 3+), have transformed medical image segmentation (MIS) by automating the process and enhancing accuracy. These models enable efficient, precise pixel-wise classification across various imaging modalities, overcoming the limitations of manual segmentation. This review explores various medical imaging techniques, examines the U-Net architectures and their adaptations, and discusses their application across different modalities. It also identifies common challenges in MIS and proposes potential solutions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-03</td>
<td style='padding: 8px;'>Recovering implicit physics model under real-world constraints</td>
<td style='padding: 6px;'>Ayan Banerjee, Sandeep K. S. Gupta</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02215v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recovering a physics-driven model, i.e. a governing set of equations of the underlying dynamical systems, from the real-world data has been of recent interest. Most existing methods either operate on simulation data with unrealistically high sampling rates or require explicit measurements of all system variables, which is not amenable in real-world deployments. Moreover, they assume the timestamps of external perturbations to the physical system are known a priori, without uncertainty, implicitly discounting any sensor time-synchronization or human reporting errors. In this paper, we propose a novel liquid time constant neural network (LTC-NN) based architecture to recover underlying model of physical dynamics from real-world data. The automatic differentiation property of LTC-NN nodes overcomes problems associated with low sampling rates, the input dependent time constant in the forward pass of the hidden layer of LTC-NN nodes creates a massive search space of implicit physical dynamics, the physics model solver based data reconstruction loss guides the search for the correct set of implicit dynamics, and the use of the dropout regularization in the dense layer ensures extraction of the sparsest model. Further, to account for the perturbation timing error, we utilize dense layer nodes to search through input shifts that results in the lowest reconstruction loss. Experiments on four benchmark dynamical systems, three with simulation data and one with the real-world data show that the LTC-NN architecture is more accurate in recovering implicit physics model coefficients than the state-of-the-art sparse model recovery approaches. We also introduce four additional case studies (total eight) on real-life medical examples in simulation and with real-world clinical data to show effectiveness of our approach in recovering underlying model in practice.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-03</td>
<td style='padding: 8px;'>Anatomically-Grounded Fact Checking of Automated Chest X-ray Reports</td>
<td style='padding: 6px;'>R. Mahmood, K. C. L. Wong, D. M. Reyes, N. D'Souza, L. Shi, J. Wu, P. Kaviani, M. Kalra, G. Wang, P. Yan, T. Syeda-Mahmood</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02177v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>With the emergence of large-scale vision-language models, realistic radiology reports may be generated using only medical images as input guided by simple prompts. However, their practical utility has been limited due to the factual errors in their description of findings. In this paper, we propose a novel model for explainable fact-checking that identifies errors in findings and their locations indicated through the reports. Specifically, we analyze the types of errors made by automated reporting methods and derive a new synthetic dataset of images paired with real and fake descriptions of findings and their locations from a ground truth dataset. A new multi-label cross-modal contrastive regression network is then trained on this datsaset. We evaluate the resulting fact-checking model and its utility in correcting reports generated by several SOTA automated reporting tools on a variety of benchmark datasets with results pointing to over 40\% improvement in report quality through such error detection and correction.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-02</td>
<td style='padding: 8px;'>INSIGHT: Explainable Weakly-Supervised Medical Image Analysis</td>
<td style='padding: 6px;'>Wenbo Zhang, Junyu Chen, Christopher Kanan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.02012v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Due to their large sizes, volumetric scans and whole-slide pathology images (WSIs) are often processed by extracting embeddings from local regions and then an aggregator makes predictions from this set. However, current methods require post-hoc visualization techniques (e.g., Grad-CAM) and often fail to localize small yet clinically crucial details. To address these limitations, we introduce INSIGHT, a novel weakly-supervised aggregator that integrates heatmap generation as an inductive bias. Starting from pre-trained feature maps, INSIGHT employs a detection module with small convolutional kernels to capture fine details and a context module with a broader receptive field to suppress local false positives. The resulting internal heatmap highlights diagnostically relevant regions. On CT and WSI benchmarks, INSIGHT achieves state-of-the-art classification results and high weakly-labeled semantic segmentation performance. Project website and code are available at: https://zhangdylan83.github.io/ewsmia/</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-02</td>
<td style='padding: 8px;'>Learning a Filtered Backprojection Reconstruction Method for Photoacoustic Computed Tomography with Hemispherical Measurement Geometries</td>
<td style='padding: 6px;'>Panpan Chen, Seonyeong Park, Refik Mert Cam, Hsuan-Kai Huang, Alexander A. Oraevsky, Umberto Villa, Mark A. Anastasio</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.01971v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In certain three-dimensional (3D) applications of photoacoustic computed tomography (PACT), including \textit{in vivo} breast imaging, hemispherical measurement apertures that enclose the object within their convex hull are employed for data acquisition. Data acquired with such measurement geometries are referred to as \textit{half-scan} data, as only half of a complete spherical measurement aperture is employed. Although previous studies have demonstrated that half-scan data can uniquely and stably reconstruct the sought-after object, no closed-form reconstruction formula for use with half-scan data has been reported. To address this, a semi-analytic reconstruction method in the form of filtered backprojection (FBP), referred to as the half-scan FBP method, is developed in this work. Because the explicit form of the filtering operation in the half-scan FBP method is not currently known, a learning-based method is proposed to approximate it. The proposed method is systematically investigated by use of virtual imaging studies of 3D breast PACT that employ ensembles of numerical breast phantoms and a physics-based model of the data acquisition process. The method is subsequently applied to experimental data acquired in an \textit{in vivo} breast PACT study. The results confirm that the half-scan FBP method can accurately reconstruct 3D images from half-scan data. Importantly, because the sought-after inverse mapping is well-posed, the reconstruction method remains accurate even when applied to data that differ considerably from those employed to learn the filtering operation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-02</td>
<td style='padding: 8px;'>Deep Guess acceleration for explainable image reconstruction in sparse-view CT</td>
<td style='padding: 6px;'>Elena Loli Piccolomini, Davide Evangelista, Elena Morotti</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.01703v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Sparse-view Computed Tomography (CT) is an emerging protocol designed to reduce X-ray dose radiation in medical imaging. Traditional Filtered Back Projection algorithm reconstructions suffer from severe artifacts due to sparse data. In contrast, Model-Based Iterative Reconstruction (MBIR) algorithms, though better at mitigating noise through regularization, are too computationally costly for clinical use. This paper introduces a novel technique, denoted as the Deep Guess acceleration scheme, using a trained neural network both to quicken the regularized MBIR and to enhance the reconstruction accuracy. We integrate state-of-the-art deep learning tools to initialize a clever starting guess for a proximal algorithm solving a non-convex model and thus computing an interpretable solution image in a few iterations. Experimental results on real CT images demonstrate the Deep Guess effectiveness in (very) sparse tomographic protocols, where it overcomes its mere variational counterpart and many data-driven approaches at the state of the art. We also consider a ground truth-free implementation and test the robustness of the proposed framework to noise.</td>
</tr>
</tbody>
</table>

