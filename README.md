<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2025-11-21</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis</td>
<td style='padding: 6px;'>Yves Pauli, Jan-Bernard Marsman, Finn Rabe, Victoria Edkins, Roya Hüppi, Silvia Ciampelli, Akhil Ratan Misra, Nils Lang, Wolfram Hinzen, Iris Sommer, Philipp Homan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15512v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>3D printed waveguides for optogenetics applications: design optimization and optical characterization</td>
<td style='padding: 6px;'>Giorgio Scordo, Kostas Kanellopulos, Surangrat Thongkorn, Samuel Tavares da Silva Maraschin, Kambiz Ghaseminasab, Evgeniy Shkondin, Deepshika Arasu, Stephan Sylvest Keller, Arto Rainer Heiskanen, Marta Perez Pereira, Jenny Emnéus</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15420v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Optogenetics has emerged as a powerful tool for disease modeling, enabling precise control of cellular activities through light stimulation and providing a valuable insights into disease mechanisms and therapeutic possibilities. Innovative materials and technologies such as micro-LEDs, optical fibers and micro/nano probes have been developed to allow precise spatial and temporal control of light delivery to target cells. Recent advances in 3D printing have further enhanced optogenetic applications by enabling the fabrication of implantable, customizable, and miniaturized light stimulation systems with high spatial resolution. In this study, we introduce a novel concept of a 3D printed light delivery system for brain organoid stimulation exploring the capabilities of projection microstereolithography (P$μ$SL). We characterized the optical properties of the high-resolution acrylate-based 3D print resin, i.e., refractive index and extinction coefficient, to evaluate if the light transmission efficiency might limit the performance of the optogenetic stimulation systems. Finite element method simulations were employed to optimize the 3D printed design. An optogenetic setup was developed for optimal light delivery, and initial tests with optogenetically modified cells showed light-induced dopamine release with a stimulation efficiency of 2.8\%, confirming the 3D printed waveguide functionality and guiding future optimization. Our results demonstrate that this light stimulation tool offers strong potential for advancing customizable optogenetic applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG</td>
<td style='padding: 6px;'>Kunyu Zhang, Mingxuan Wang, Xiangjie Shi, Haoxing Xu, Chao Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15393v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>Detection of spiking motifs of arbitrary length in neural activity using bounded synaptic delays</td>
<td style='padding: 6px;'>Thomas Kronland-Martinet, Stéphane Viollet, Laurent U Perrinet</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15296v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In the context of spiking neural networks, temporal coding of signals is increasingly preferred over the rate coding hypothesis due to its advantages in processing speed and energy efficiency. In temporal coding, synaptic delays are crucial for processing signals with precise spike timings, known as spiking motifs. Synaptic delays are however bounded in the brain and can thus be shorter than the duration of a motif. This prevents the use of motif recognition methods that consist of setting heterogeneous delays to synchronize the input spikes on a single output neuron acting as a coincidence detector. To address this issue, we developed a method to detect motifs of arbitrary length using a sequence of output neurons connected to input neurons by bounded synaptic delays. Each output neuron is associated with a sub-motif of bounded duration. A motif is recognized if all sub-motifs are sequentially detected by the output neurons. We simulated this network using leaky integrate-and-fire neurons and tested it on the Spiking Heidelberg Digits (SHD) database, that is, on audio data converted to spikes via a cochlear model, as well as on random simultaneous motifs. The results demonstrate that the network can effectively recognize motifs of arbitrary length extracted from the SHD database. Our method features a correct detection rate of about 60% in presence of ten simultaneous motifs from the SHD dataset and up to 80% for five motifs, showing the robustness of the network to noise. Results on random overlapping patterns show that the recognition of a single motif overlapping with other motifs is most effective for a large number of input neurons and sparser motifs. Our method provides a foundation for more general models for the storage and retrieval of neural information of arbitrary temporal lengths.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>Efficient Transformer-Integrated Deep Neural Architectures for Robust EEG Decoding of Complex Visual Imagery</td>
<td style='padding: 6px;'>Byoung-Hee Kwon</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15218v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study introduces a pioneering approach in brain-computer interface (BCI) technology, featuring our novel concept of complex visual imagery for non-invasive electroencephalography (EEG)-based communication. Complex visual imagery, as proposed in our work, involves the user engaging in the mental visualization of complex upper limb movements. This innovative approach significantly enhances the BCI system, facilitating the extension of its applications to more sophisticated tasks such as EEG-based robotic arm control. By leveraging this advanced form of visual imagery, our study opens new horizons for intricate and intuitive mind-controlled interfaces. We developed an advanced deep learning architecture that integrates functional connectivity metrics with a convolutional neural network-image transformer. This framework is adept at decoding subtle user intentions, addressing the spatial variability in complex visual tasks, and effectively translating these into precise commands for robotic arm control. Our comprehensive offline and pseudo-online evaluations demonstrate the framework's efficacy in real-time applications, including the nuanced control of robotic arms. The robustness of our approach is further validated through leave-one-subject-out cross-validation, marking a significant step towards versatile, subject-independent BCI applications. This research highlights the transformative impact of advanced visual imagery and deep learning in enhancing the usability and adaptability of BCI systems, particularly in robotic arm manipulation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI</td>
<td style='padding: 6px;'>Wasif Jalal, Md Nafiu Rahman, M. Sohel Rahman</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15188v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurate brain age estimation from structural MRI is a valuable biomarker for studying aging and neurodegeneration. Traditional regression and CNN-based methods face limitations such as manual feature engineering, limited receptive fields, and overfitting on heterogeneous data. Pure transformer models, while effective, require large datasets and high computational cost. We propose Brain ResNet over trained Vision Transformer (BrainRotViT), a hybrid architecture that combines the global context modeling of vision transformers (ViT) with the local refinement of residual CNNs. A ViT encoder is first trained on an auxiliary age and sex classification task to learn slice-level features. The frozen encoder is then applied to all sagittal slices to generate a 2D matrix of embedding vectors, which is fed into a residual CNN regressor that incorporates subject sex at the final fully-connected layer to estimate continuous brain age. Our method achieves an MAE of 3.34 years (Pearson $r=0.98$, Spearman $ρ=0.97$, $R^2=0.95$) on validation across 11 MRI datasets encompassing more than 130 acquisition sites, outperforming baseline and state-of-the-art models. It also generalizes well across 4 independent cohorts with MAEs between 3.77 and 5.04 years. Analyses on the brain age gap (the difference between the predicted age and actual age) show that aging patterns are associated with Alzheimer's disease, cognitive impairment, and autism spectrum disorder. Model attention maps highlight aging-associated regions of the brain, notably the cerebellar vermis, precentral and postcentral gyri, temporal lobes, and medial superior frontal gyrus. Our results demonstrate that this method provides an efficient, interpretable, and generalizable framework for brain-age prediction, bridging the gap between CNN- and transformer-based approaches while opening new avenues for aging and neurodegeneration research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging</td>
<td style='padding: 6px;'>Meihua Zhou, Xinyu Tong, Jiarui Zhao, Min Cheng, Li Yang, Lei Tian, Nan Wan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15151v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>High-dimensional neuroimaging analyses for clinical diagnosis are often constrained by compromises in spatiotemporal fidelity and by the limited adaptability of large-scale, general-purpose models. To address these challenges, we introduce Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an end-to-end framework centered on data-driven spatiotemporal encoding (DaSE). We leverage Approximate Rank Pooling (ARP) to efficiently encode three-dimensional volumetric brain data into information-rich, two-dimensional dynamic representations, and then employ a dynamic curriculum learning strategy, guided by a Dynamic Group Mechanism (DGM), to progressively train the decoder, refining feature extraction from global anatomical structures to fine pathological details. Evaluated across six publicly available datasets, including Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction, DCL-SE consistently outperforms existing methods in accuracy, robustness, and interpretability. These findings underscore the critical importance of compact, task-specific architectures in the era of large-scale pretrained networks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems</td>
<td style='padding: 6px;'>Hyo-Jeong Jang, Hye-Bin Shin, Kang Yin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15138v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-18</td>
<td style='padding: 8px;'>Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis</td>
<td style='padding: 6px;'>Pranay Kumar Peddi, Dhrubajyoti Ghosh</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.14922v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-18</td>
<td style='padding: 8px;'>Teaching signal synchronization in deep neural networks with prospective neurons</td>
<td style='padding: 6px;'>Nicoas Zucchet, Qianqian Feng, Axel Laborieux, Friedemann Zenke, Walter Senn, João Sacramento</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.14917v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Working memory requires the brain to maintain information from the recent past to guide ongoing behavior. Neurons can contribute to this capacity by slowly integrating their inputs over time, creating persistent activity that outlasts the original stimulus. However, when these slowly integrating neurons are organized hierarchically, they introduce cumulative delays that create a fundamental challenge for learning: teaching signals that indicate whether behavior was correct or incorrect arrive out-of-sync with the neural activity they are meant to instruct. Here, we demonstrate that neurons enhanced with an adaptive current can compensate for these delays by responding to external stimuli prospectively -- effectively predicting future inputs to synchronize with them. First, we show that such prospective neurons enable teaching signal synchronization across a range of learning algorithms that propagate error signals through hierarchical networks. Second, we demonstrate that this successfully guides learning in slowly integrating neurons, enabling the formation and retrieval of memories over extended timescales. We support our findings with a mathematical analysis of the prospective coding mechanism and learning experiments on motor control tasks. Together, our results reveal how neural adaptation could solve a critical timing problem and enable efficient learning in dynamic environments.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG</td>
<td style='padding: 6px;'>Kunyu Zhang, Mingxuan Wang, Xiangjie Shi, Haoxing Xu, Chao Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15393v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>Efficient Transformer-Integrated Deep Neural Architectures for Robust EEG Decoding of Complex Visual Imagery</td>
<td style='padding: 6px;'>Byoung-Hee Kwon</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15218v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study introduces a pioneering approach in brain-computer interface (BCI) technology, featuring our novel concept of complex visual imagery for non-invasive electroencephalography (EEG)-based communication. Complex visual imagery, as proposed in our work, involves the user engaging in the mental visualization of complex upper limb movements. This innovative approach significantly enhances the BCI system, facilitating the extension of its applications to more sophisticated tasks such as EEG-based robotic arm control. By leveraging this advanced form of visual imagery, our study opens new horizons for intricate and intuitive mind-controlled interfaces. We developed an advanced deep learning architecture that integrates functional connectivity metrics with a convolutional neural network-image transformer. This framework is adept at decoding subtle user intentions, addressing the spatial variability in complex visual tasks, and effectively translating these into precise commands for robotic arm control. Our comprehensive offline and pseudo-online evaluations demonstrate the framework's efficacy in real-time applications, including the nuanced control of robotic arms. The robustness of our approach is further validated through leave-one-subject-out cross-validation, marking a significant step towards versatile, subject-independent BCI applications. This research highlights the transformative impact of advanced visual imagery and deep learning in enhancing the usability and adaptability of BCI systems, particularly in robotic arm manipulation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems</td>
<td style='padding: 6px;'>Hyo-Jeong Jang, Hye-Bin Shin, Kang Yin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15138v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>Personalized targeted memory reactivation enhances consolidation of challenging memories via slow wave and spindle dynamics</td>
<td style='padding: 6px;'>Gi-Hwan Shin, Young-Seok Kweon, Seungwon Oh, Seong-Whan Lee</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15013v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Sleep is crucial for memory consolidation, underpinning effective learning. Targeted memory reactivation (TMR) can strengthen neural representations by re-engaging learning circuits during sleep. However, TMR protocols overlook individual differences in learning capacity and memory trace strength, limiting efficacy for difficult-to-recall memories. Here, we present a personalized TMR protocol that adjusts stimulation frequency based on individual retrieval performance and task difficulty during a word-pair memory task. In an experiment comparing personalized TMR, TMR, and control groups, the personalized protocol significantly reduced memory decay and improved error correction under challenging recall. Electroencephalogram (EEG) analyses revealed enhanced synchronization of slow waves and spindles, with a significant positive correlation between behavioral and EEG features for challenging memories. Multivariate classification identified distinct neural signatures linked to the personalized approach, highlighting its ability to target memory-specific circuits. These findings provide novel insights into sleep-dependent memory consolidation and support personalized TMR interventions to optimize learning outcomes.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>A Quantitative Framework for Assessing Sleep Quality from EEG Time Series in Complex Dynamic Systems</td>
<td style='padding: 6px;'>Gi-Hwan Shin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15012v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Modern lifestyles contribute to insufficient sleep, impairing cognitive function and weakening the immune system. Sleep quality (SQ) is vital for physiological and mental health, making its understanding and accurate assessment critical. However, its multifaceted nature, shaped by neurological and environmental factors, makes precise quantification challenging. Here, we address this challenge by utilizing electroencephalography (EEG) for phase-amplitude coupling (PAC) analysis to elucidate the neurological basis of SQ, examining both states of sleep and wakefulness, including resting state (RS) and working memory. Our results revealed distinct patterns in beta power and delta connectivity in sleep and RS, together with the reaction time of working memory. A notable finding was the pronounced delta-beta PAC, a feature markedly stronger in individuals with good SQ. We further observed that SQ was positively correlated with increased delta-beta PAC. Leveraging these insights, we applied machine learning models to classify SQ at an individual level, demonstrating that the delta-beta PAC outperformed other EEG characteristics. These findings establish delta-beta PAC as a robust electrophysiological marker to quantify SQ and elucidate its neurological determinants.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-18</td>
<td style='padding: 8px;'>Nonlinear Coherence for Vector Time Series: Defining Region-to-Region Functional Brain Connectivity</td>
<td style='padding: 6px;'>Paolo Victor Redondo, Raphaël Huser, Hernando Ombao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.14417v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Alterations in functional brain connectivity characterize neurodegenerative disorders such as Alzheimer's disease (AD) and frontotemporal dementia (FTD). As a non-invasive and cost-effective technique, electroencephalography (EEG) is gaining increasing attention for its potential to identify reliable biomarkers for early detection and differential diagnosis of AD and FTD. Considering the behavioral similarities of signals from adjacent EEG channels, we propose a new spectral dependence measure, the nonlinear vector coherence (NVC), to capture beyond-linear interactions between oscillations of two multivariate time series observed from distinct brain regions. This addresses the limitations of conventional channel-to-channel approaches and defines a more natural region-to-region connectivity framework in the frequency domain. As a result, the NVC measure offers a new approach to investigate dependence between brain regions, which then enables to identify altered functional connectivity dynamics associated with AD and FTD. We further introduce a rank-based inference procedure that enables fast and distribution-free estimation of the proposed measure, as well as a fully nonparametric test for spectral independence. The empirical performance of our proposed inference methodology is demonstrated through extensive numerical experiments. An application to resting-state EEG data reveals that our novel NVC measure uncovers distinct and diagnostically meaningful connectivity patterns which effectively discriminate healthy individuals from those with AD and FTD.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-18</td>
<td style='padding: 8px;'>MindCross: Fast New Subject Adaptation with Limited Data for Cross-subject Video Reconstruction from Brain Signals</td>
<td style='padding: 6px;'>Xuan-Hao Liu, Yan-Kai Liu, Tianyi Zhou, Bao-Liang Lu, Wei-Long Zheng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.14196v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Reconstructing video from brain signals is an important brain decoding task. Existing brain decoding frameworks are primarily built on a subject-dependent paradigm, which requires large amounts of brain data for each subject. However, the expensive cost of collecting brain-video data causes severe data scarcity. Although some cross-subject methods being introduced, they often overfocus with subject-invariant information while neglecting subject-specific information, resulting in slow fine-tune-based adaptation strategy. To achieve fast and data-efficient new subject adaptation, we propose MindCross, a novel cross-subject framework. MindCross's N specific encoders and one shared encoder are designed to extract subject-specific and subject-invariant information, respectively. Additionally, a Top-K collaboration module is adopted to enhance new subject decoding with the knowledge learned from previous subjects' encoders. Extensive experiments on fMRI/EEG-to-video benchmarks demonstrate MindCross's efficacy and efficiency of cross-subject decoding and new subject adaptation using only one model.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>A region-specific brain dysfunction underlies cognitive impairment in long COVID brain fog</td>
<td style='padding: 6px;'>Jinhao Yang, Shaojiong Zhou, Zhibin Wang, Jiahua Xu, Jia Chen, Zhouqian Yin, Tao Wei, Chaofan Geng, Xiaoduo Liu, Xiang Li, Xiaoyu Zhou, Kun Li, Ruolei Gu, Raymond Dolan, Yi Tang, Yunzhe Liu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.14188v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Long COVID "brain fog" is a common and debilitating subjective syndrome often associated with persistent cognitive impairment after COVID-19 infection. Here we identify a specific regional brain dysfunction that mediates this cognitive impairment and provide evidence that targeted neuromodulation improves this deficit. In 120 patients with long COVID brain fog, we found an aberrant perceptual processing pattern. Patients with more severe brain fog committed significantly more false alarms (impulsive responses to non-signals) despite preserved overall accuracy. Both high-density (128-channel) EEG and structural MRI analyses provided converging evidence of a right inferior insula deficit, characterized by a blunted neural monitoring signal and cortical atrophy. We confirmed this deficit in a separate 796-participant UK Biobank longitudinal COVID re-imaging cohort, where COVID-19 survivors also showed selective impairment on a perceptual processing task and corresponding longitudinal atrophy of the right inferior insula compared with healthy controls. Finally, in a proof-of-principle randomized, sham-controlled trial (n = 40), a non-invasive, excitatory theta-burst ultrasound stimulation protocol targeting the right inferior insula rescued the perceptual deficit by reducing false alarms. These findings provide evidence of a causal role for right inferior insula dysfunction in long COVID-related perceptual impairment and show that modulation of this region can rescue the deficit, establishing it as a novel therapeutic target for long COVID cognitive impairment.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-18</td>
<td style='padding: 8px;'>A Patient-Independent Neonatal Seizure Prediction Model Using Reduced Montage EEG and ECG</td>
<td style='padding: 6px;'>Sithmini Ranasingha, Agasthi Haputhanthri, Hansa Marasinghe, Nima Wickramasinghe, Kithmin Wickremasinghe, Jithangi Wanigasinghe, Chamira U. S. Edussooriya, Joshua P. Kulasingham</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.14110v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neonates are highly susceptible to seizures, often leading to short or long-term neurological impairments. However, clinical manifestations of neonatal seizures are subtle and often lead to misdiagnoses. This increases the risk of prolonged, untreated seizure activity and subsequent brain injury. Continuous video electroencephalogram (cEEG) monitoring is the gold standard for seizure detection. However, this is an expensive evaluation that requires expertise and time. In this study, we propose a convolutional neural network-based model for early prediction of neonatal seizures by distinguishing between interictal and preictal states of the EEG. Our model is patient-independent, enabling generalization across multiple subjects, and utilizes mel-frequency cepstral coefficient matrices extracted from multichannel EEG and electrocardiogram (ECG) signals as input features. Trained and validated on the Helsinki neonatal EEG dataset with 10-fold cross-validation, the proposed model achieved an average accuracy of 97.52%, sensitivity of 98.31%, specificity of 96.39%, and F1-score of 97.95%, enabling accurate seizure prediction up to 30 minutes before onset. The inclusion of ECG alongside EEG improved the F1-score by 1.42%, while the incorporation of an attention mechanism yielded an additional 0.5% improvement. To enhance transparency, we incorporated SHapley Additive exPlanations (SHAP) as an explainable artificial intelligence method to interpret the model and provided localization of seizure focus using scalp plots. The overall results demonstrate the model's potential for minimally supervised deployment in neonatal intensive care units, enabling timely and reliable prediction of neonatal seizures, while demonstrating strong generalization capability across unseen subjects through transfer learning.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-17</td>
<td style='padding: 8px;'>A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition</td>
<td style='padding: 6px;'>Nilay Kumar, Priyansh Bhandari, G. Maragatham</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.13954v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Human emotions are difficult to convey through words and are often abstracted in the process; however, electroencephalogram (EEG) signals can offer a more direct lens into emotional brain activity. Recent studies show that deep learning models can process these signals to perform emotion recognition with high accuracy. However, many existing approaches overlook the dynamic interplay between distinct brain regions, which can be crucial to understanding how emotions unfold and evolve over time, potentially aiding in more accurate emotion recognition. To address this, we propose RBTransformer, a Transformer-based neural network architecture that models inter-cortical neural dynamics of the brain in latent space to better capture structured neural interactions for effective EEG-based emotion recognition. First, the EEG signals are converted into Band Differential Entropy (BDE) tokens, which are then passed through Electrode Identity embeddings to retain spatial provenance. These tokens are processed through successive inter-cortical multi-head attention blocks that construct an electrode x electrode attention matrix, allowing the model to learn the inter-cortical neural dependencies. The resulting features are then passed through a classification head to obtain the final prediction. We conducted extensive experiments, specifically under subject-dependent settings, on the SEED, DEAP, and DREAMER datasets, over all three dimensions, Valence, Arousal, and Dominance (for DEAP and DREAMER), under both binary and multi-class classification settings. The results demonstrate that the proposed RBTransformer outperforms all previous state-of-the-art methods across all three datasets, over all three dimensions under both classification settings. The source code is available at: https://github.com/nnilayy/RBTransformer.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>Efficient Transformer-Integrated Deep Neural Architectures for Robust EEG Decoding of Complex Visual Imagery</td>
<td style='padding: 6px;'>Byoung-Hee Kwon</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15218v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study introduces a pioneering approach in brain-computer interface (BCI) technology, featuring our novel concept of complex visual imagery for non-invasive electroencephalography (EEG)-based communication. Complex visual imagery, as proposed in our work, involves the user engaging in the mental visualization of complex upper limb movements. This innovative approach significantly enhances the BCI system, facilitating the extension of its applications to more sophisticated tasks such as EEG-based robotic arm control. By leveraging this advanced form of visual imagery, our study opens new horizons for intricate and intuitive mind-controlled interfaces. We developed an advanced deep learning architecture that integrates functional connectivity metrics with a convolutional neural network-image transformer. This framework is adept at decoding subtle user intentions, addressing the spatial variability in complex visual tasks, and effectively translating these into precise commands for robotic arm control. Our comprehensive offline and pseudo-online evaluations demonstrate the framework's efficacy in real-time applications, including the nuanced control of robotic arms. The robustness of our approach is further validated through leave-one-subject-out cross-validation, marking a significant step towards versatile, subject-independent BCI applications. This research highlights the transformative impact of advanced visual imagery and deep learning in enhancing the usability and adaptability of BCI systems, particularly in robotic arm manipulation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems</td>
<td style='padding: 6px;'>Hyo-Jeong Jang, Hye-Bin Shin, Kang Yin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15138v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-17</td>
<td style='padding: 8px;'>Learning Time-Scale Invariant Population-Level Neural Representations</td>
<td style='padding: 6px;'>Eshani Patel, Yisong Yue, Geeling Chau</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.13022v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>General-purpose foundation models for neural time series can help accelerate neuroscientific discoveries and enable applications such as brain computer interfaces (BCIs). A key component in scaling these models is population-level representation learning, which leverages information across channels to capture spatial as well as temporal structure. Population-level approaches have recently shown that such representations can be both efficient to learn on top of pretrained temporal encoders and produce useful representations for decoding a variety of downstream tasks. However, these models remain sensitive to mismatches in preprocessing, particularly on time-scales, between pretraining and downstream settings. We systematically examine how time-scale mismatches affects generalization and find that existing representations lack invariance. To address this, we introduce Time-scale Augmented Pretraining (TSAP), which consistently improves robustness to different time-scales across decoding tasks and builds invariance in the representation space. These results highlight handling preprocessing diversity as a key step toward building generalizable neural foundation models.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-17</td>
<td style='padding: 8px;'>Guidewire-driven deployment of high density ECoG arrays for large area brain-computer interface</td>
<td style='padding: 6px;'>Tao Zou, Na Xiao, Ruihong Weng, Yifan Guo, Danny Tat Ming Chan, Gilberto Ka Kit Leung, Paddy Kwok Leung Chan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.12907v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electrocorticographic brain computer interfaces are powerful emergent technologies for advancing basic neuroscience research and targeted clinical interventions. However, existing devices require trade-offs between coverage area, electrode density, surgical invasiveness and complication risk: limitations that fail to meet the demands of next-generation BCI. Here, we report a guidewire-driven deployable ECoG BCI device that can be epidurally implanted using minimally invasive procedures. Our ultra-flexible but strong thin-film electrode array, which packs 256 electrodes into 4 cm2, can be folded, pulled through millimetre-sized skull holes, and unfurled seamlessly onto the brain dura mater. When deployed on the canine brain, it captures abundant high-quality auditory neural signals with distinct features of hearing that can be used to classify sound types with over 80% accuracy using various standard machine learning models. Our device is biocompatible for chronic monitoring, easy and fast to deploy and importantly, resolves the key trade-offs limiting current BCI technologies.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-17</td>
<td style='padding: 8px;'>Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback</td>
<td style='padding: 6px;'>Julia Santaniello, Matthew Russell, Benson Jiang, Donatello Sassaroli, Robert Jacob, Jivko SInapov</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.12844v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating human feedback into the agent's training process. We introduce a possible framework that employs passive Brain-Computer Interfaces (BCI) to guide agent training from implicit neural signals. We present and release a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: a Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train classifiers to predict levels of agent performance (optimal, sub-optimal, or worst-case) from windows of preprocessed fNIRS feature vectors, achieving an average F1 score of 67% for binary classification and 46% for multi-class models averaged across conditions and domains. We also train regressors to predict the degree of deviation between an agent's chosen action and a set of near-optimal policies, providing a continuous measure of performance. We evaluate cross-subject generalization and demonstrate that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our work demonstrates that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future brain-driven RLHF systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-14</td>
<td style='padding: 8px;'>CAT-Net: A Cross-Attention Tone Network for Cross-Subject EEG-EMG Fusion Tone Decoding</td>
<td style='padding: 6px;'>Yifan Zhuang, Calvin Huang, Zepeng Yu, Yongjie Zou, Jiawei Ju</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.10935v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-computer interface (BCI) speech decoding has emerged as a promising tool for assisting individuals with speech impairments. In this context, the integration of electroencephalography (EEG) and electromyography (EMG) signals offers strong potential for enhancing decoding performance. Mandarin tone classification presents particular challenges, as tonal variations convey distinct meanings even when phonemes remain identical. In this study, we propose a novel cross-subject multimodal BCI decoding framework that fuses EEG and EMG signals to classify four Mandarin tones under both audible and silent speech conditions. Inspired by the cooperative mechanisms of neural and muscular systems in speech production, our neural decoding architecture combines spatial-temporal feature extraction branches with a cross-attention fusion mechanism, enabling informative interaction between modalities. We further incorporate domain-adversarial training to improve cross-subject generalization. We collected 4,800 EEG trials and 4,800 EMG trials from 10 participants using only twenty EEG and five EMG channels, demonstrating the feasibility of minimal-channel decoding. Despite employing lightweight modules, our model outperforms state-of-the-art baselines across all conditions, achieving average classification accuracies of 87.83% for audible speech and 88.08% for silent speech. In cross-subject evaluations, it still maintains strong performance with accuracies of 83.27% and 85.10% for audible and silent speech, respectively. We further conduct ablation studies to validate the effectiveness of each component. Our findings suggest that tone-level decoding with minimal EEG-EMG channels is feasible and potentially generalizable across subjects, contributing to the development of practical BCI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-14</td>
<td style='padding: 8px;'>Motor Imagery Classification Using Feature Fusion of Spatially Weighted Electroencephalography</td>
<td style='padding: 6px;'>Abdullah Al Shiam, Md. Khademul Islam Molla, Abu Saleh Musa Miah, Md. Abdus Samad Kamal</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.13752v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A Brain Computer Interface (BCI) connects the human brain to the outside world, providing a direct communication channel. Electroencephalography (EEG) signals are commonly used in BCIs to reflect cognitive patterns related to motor function activities. However, due to the multichannel nature of EEG signals, explicit information processing is crucial to lessen computational complexity in BCI systems. This study proposes an innovative method based on brain region-specific channel selection and multi-domain feature fusion to improve classification accuracy. The novelty of the proposed approach lies in region-based channel selection, where EEG channels are grouped according to their functional relevance to distinct brain regions. By selecting channels based on specific regions involved in motor imagery (MI) tasks, this technique eliminates irrelevant channels, reducing data dimensionality and improving computational efficiency. This also ensures that the extracted features are more reflective of the brain actual activity related to motor tasks. Three distinct feature extraction methods Common Spatial Pattern (CSP), Fuzzy C-means clustering, and Tangent Space Mapping (TSM), are applied to each group of channels based on their brain region. Each method targets different characteristics of the EEG signal: CSP focuses on spatial patterns, Fuzzy C means identifies clusters within the data, and TSM captures non-linear patterns in the signal. The combined feature vector is used to classify motor imagery tasks (left hand, right hand, and right foot) using Support Vector Machine (SVM). The proposed method was validated on publicly available benchmark EEG datasets (IVA and I) from the BCI competition III and IV. The results show that the approach outperforms existing methods, achieving classification accuracies of 90.77% and 84.50% for datasets IVA and I, respectively.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-12</td>
<td style='padding: 8px;'>Brian Intensify: An Adaptive Machine Learning Framework for Auditory EEG Stimulation and Cognitive Enhancement in FXS</td>
<td style='padding: 6px;'>Zag ElSayed, Grace Westerkamp, Jack Yanchen Liu, Ernest Pedapati</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.09765v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neurodevelopmental disorders such as Fragile X Syndrome (FXS) and Autism Spectrum Disorder (ASD) are characterized by disrupted cortical oscillatory activity, particularly in the alpha and gamma frequency bands. These abnormalities are linked to deficits in attention, sensory processing, and cognitive function. In this work, we present an adaptive machine learning-based brain-computer interface (BCI) system designed to modulate neural oscillations through frequency-specific auditory stimulation to enhance cognitive readiness in individuals with FXS. EEG data were recorded from 38 participants using a 128-channel system under a stimulation paradigm consisting of a 30-second baseline (no stimulus) followed by 60-second auditory entrainment episodes at 7Hz, 9Hz, 11Hz, and 13Hz. A comprehensive analysis of power spectral features (Alpha, Gamma, Delta, Theta, Beta) and cross-frequency coupling metrics (Alpha-Gamma, Alpha-Beta, etc.) was conducted. The results identified Peak Alpha Power, Peak Gamma Power, and Alpha Power per second per channel as the most discriminative biomarkers. The 13Hz stimulation condition consistently elicited a significant increase in Alpha activity and suppression of Gamma activity, aligning with our optimization objective. A supervised machine learning framework was developed to predict EEG responses and dynamically adjust stimulation parameters, enabling real-time, subject-specific adaptation. This work establishes a novel EEG-driven optimization framework for cognitive neuromodulation, providing a foundational model for next-generation AI-integrated BCI systems aimed at personalized neurorehabilitation in FXS and related disorders.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-11</td>
<td style='padding: 8px;'>Intuitive control of supernumerary robotic limbs through a tactile-encoded neural interface</td>
<td style='padding: 6px;'>Tianyu Jia, Xingchen Yang, Ciaran McGeady, Yifeng Li, Jinzhi Lin, Kit San Ho, Feiyu Pan, Linhong Ji, Chong Li, Dario Farina</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.08454v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-computer interfaces (BCIs) promise to extend human movement capabilities by enabling direct neural control of supernumerary effectors, yet integrating augmented commands with multiple degrees of freedom without disrupting natural movement remains a key challenge. Here, we propose a tactile-encoded BCI that leverages sensory afferents through a novel tactile-evoked P300 paradigm, allowing intuitive and reliable decoding of supernumerary motor intentions even when superimposed with voluntary actions. The interface was evaluated in a multi-day experiment comprising of a single motor recognition task to validate baseline BCI performance and a dual task paradigm to assess the potential influence between the BCI and natural human movement. The brain interface achieved real-time and reliable decoding of four supernumerary degrees of freedom, with significant performance improvements after only three days of training. Importantly, after training, performance did not differ significantly between the single- and dual-BCI task conditions, and natural movement remained unimpaired during concurrent supernumerary control. Lastly, the interface was deployed in a movement augmentation task, demonstrating its ability to command two supernumerary robotic arms for functional assistance during bimanual tasks. These results establish a new neural interface paradigm for movement augmentation through stimulation of sensory afferents, expanding motor degrees of freedom without impairing natural movement.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-11</td>
<td style='padding: 8px;'>Toward Practical BCI: A Real-time Wireless Imagined Speech EEG Decoding System</td>
<td style='padding: 6px;'>Ji-Ha Park, Heon-Gyu Kwak, Gi-Hwan Shin, Yoo-In Jeon, Sun-Min Park, Ji-Yeon Hwang, Seong-Whan Lee</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.07936v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-computer interface (BCI) research, while promising, has largely been confined to static and fixed environments, limiting real-world applicability. To move towards practical BCI, we introduce a real-time wireless imagined speech electroencephalogram (EEG) decoding system designed for flexibility and everyday use. Our framework focuses on practicality, demonstrating extensibility beyond wired EEG devices to portable, wireless hardware. A user identification module recognizes the operator and provides a personalized, user-specific service. To achieve seamless, real-time operation, we utilize the lab streaming layer to manage the continuous streaming of live EEG signals to the personalized decoder. This end-to-end pipeline enables a functional real-time application capable of classifying user commands from imagined speech EEG signals, achieving an overall 4-class accuracy of 62.00 % on a wired device and 46.67 % on a portable wireless headset. This paper demonstrates a significant step towards truly practical and accessible BCI technology, establishing a clear direction for future research in robust, practical, and personalized neural interfaces.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-18</td>
<td style='padding: 8px;'>Multi-network Topology Underlying Individual Language Learning Success</td>
<td style='padding: 6px;'>Peilun Song, Shuguang Yang, Xiujuan Geng, Zhenzhong Gan, Suiping Wang, Gangyi Feng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.14453v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Adult language learning varies greatly among individuals. Traditionally associated with frontotemporal language regions, this variability is increasingly seen as stemming from distributed brain networks. However, the role of these networks and their topological organization in explaining these differences remains unclear. We hypothesize that graph-theory-based network analysis of intrinsic multimodal connectivities across multiple networks explains overall and component-specific variations in language learning. We tested this in 101 healthy adults who underwent resting-state fMRI, structural MRI, and diffusion tensor imaging before seven days of six artificial language training tasks. We identified one dominant general learning component shared across tasks and five task-specific ones. Cross-validated predictive models used multimodal multi-network graph-theoretic metrics to predict final learning outcomes (LO) and rates (LR). We significantly predicted the LO and LR of the general component, which were primarily contributed by dorsal attention and frontoparietal networks. Nodal local efficiency was the most consistent predictor, with additional contributions from node clustering coefficient and network centrality for LR, highlighting local robustness, mesoscale network segregation, and global influence in explaining individual differences. Only task-specific word learning LO was predictable, relying on default mode and frontoparietal hubs with high betweenness centrality and efficiency. These findings demonstrate that intrinsic network topologies underlie differences in language learning success, supporting a multiple-systems hypothesis in which attentional-control networks interact with default and subcortical systems to shape learning trajectories. This advances mechanistic understanding and paves the way for personalized language education.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-18</td>
<td style='padding: 8px;'>MindCross: Fast New Subject Adaptation with Limited Data for Cross-subject Video Reconstruction from Brain Signals</td>
<td style='padding: 6px;'>Xuan-Hao Liu, Yan-Kai Liu, Tianyi Zhou, Bao-Liang Lu, Wei-Long Zheng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.14196v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Reconstructing video from brain signals is an important brain decoding task. Existing brain decoding frameworks are primarily built on a subject-dependent paradigm, which requires large amounts of brain data for each subject. However, the expensive cost of collecting brain-video data causes severe data scarcity. Although some cross-subject methods being introduced, they often overfocus with subject-invariant information while neglecting subject-specific information, resulting in slow fine-tune-based adaptation strategy. To achieve fast and data-efficient new subject adaptation, we propose MindCross, a novel cross-subject framework. MindCross's N specific encoders and one shared encoder are designed to extract subject-specific and subject-invariant information, respectively. Additionally, a Top-K collaboration module is adopted to enhance new subject decoding with the knowledge learned from previous subjects' encoders. Extensive experiments on fMRI/EEG-to-video benchmarks demonstrate MindCross's efficacy and efficiency of cross-subject decoding and new subject adaptation using only one model.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-17</td>
<td style='padding: 8px;'>Integrative Model for Interoception and Exteroception: predictive coding, points of modulation, and testable predictions</td>
<td style='padding: 6px;'>Pranjal Balar, Sundeep Kapila</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.13668v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Interoception and exteroception provide continuous feedback about the body and the environment, yet how they are dynamically integrated within a unified predictive coding framework has remained under-specified. This paper develops and empirically validates an integrative predictive coding model that treats interoceptive and exteroceptive inference as parallel hierarchical systems exchanging precision-weighted prediction errors. Within this framework, arbitration between the two streams is governed by relative precision weights (w) and integrated within the anterior insula (AIC) and anterior cingulate cortex (ACC). Computational simulations of the model reproduced biologically plausible dynamics: prediction errors decayed exponentially while arbitration weights self-normalized toward equilibrium (w = 0.5), demonstrating stable convergence and coherent integration. Simulated anxiety and PTSD profiles, characterized respectively by interoceptive and exteroceptive overweighting, yielded rigid, self-sustaining imbalances (w to 1 or w to 0) and slowed recalibration. Empirical application of the arbitration equation to published EEG-fMRI datasets further validated the model. The framework contributes a unifying account of how dysregulated precision weighting may underlie anxiety (overweighted interoception) and PTSD (underweighted interoception). Building on this validation, a proposed experimental paradigm is outlined to test the model's predictions in humans. It examines recalibration across anxiety, neutral, and PTSD groups following targeted interoceptive or exteroceptive therapies. Key predictions include identifiable neural markers of coherence, modulation of heartbeat-evoked potentials by vagal stimulation, and precision-sensitive behavioral signatures in interoceptive-exteroceptive congruency tasks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-16</td>
<td style='padding: 8px;'>SIMBA: Scalable Image Modeling using a Bayesian Approach, A Consistent Framework for Including Spatial Dependencies in fMRI Studies</td>
<td style='padding: 6px;'>Yuan Zhong, Gang Chen, Paul A. Taylor, Jian Kang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.12825v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Bayesian spatial modeling provides a flexible framework for whole-brain fMRI analysis by explicitly incorporating spatial dependencies, overcoming the limitations of traditional massive univariate approaches that lead to information waste. In this work, we introduce SIMBA, a Scalable Image Modeling using a Bayesian Approach, for group-level fMRI analysis, which places Gaussian process (GP) priors on spatially varying functions to capture smooth and interpretable spatial association patterns across the brain volume. To address the significant computational challenges of GP inference in high-dimensional neuroimaging data, we employ a low-rank kernel approximation that enables projection into a reduced-dimensional subspace. This allows for efficient posterior computation without sacrificing spatial resolution, and we have developed efficient algorithms for this implemented in Python that achieve fully Bayesian inference either within minutes using the Gibbs sampler or within seconds using mean-field variational inference (VI). Through extensive simulation studies, we first show that SIMBA outperforms competing methods in estimation accuracy, activation detection sensitivity, and uncertainty quantification, especially in low signal-to-noise settings. We further demonstrate the scalability and interpretability of SIMBA in large-scale task-based fMRI applications, analyzing both volumetric and cortical surface data from the NARPS and ABCD studies.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-16</td>
<td style='padding: 8px;'>Predicting upcoming visual features during eye movements yields scene representations aligned with human visual cortex</td>
<td style='padding: 6px;'>Sushrut Thorat, Adrien Doerig, Alexander Kroner, Carmen Amme, Tim C. Kietzmann</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.12715v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Scenes are complex, yet structured collections of parts, including objects and surfaces, that exhibit spatial and semantic relations to one another. An effective visual system therefore needs unified scene representations that relate scene parts to their location and their co-occurrence. We hypothesize that this structure can be learned self-supervised from natural experience by exploiting the temporal regularities of active vision: each fixation reveals a locally-detailed glimpse that is statistically related to the previous one via co-occurrence and saccade-conditioned spatial regularities. We instantiate this idea with Glimpse Prediction Networks (GPNs) -- recurrent models trained to predict the feature embedding of the next glimpse along human-like scanpaths over natural scenes. GPNs successfully learn co-occurrence structure and, when given relative saccade location vectors, show sensitivity to spatial arrangement. Furthermore, recurrent variants of GPNs were able to integrate information across glimpses into a unified scene representation. Notably, these scene representations align strongly with human fMRI responses during natural-scene viewing across mid/high-level visual cortex. Critically, GPNs outperform architecture- and dataset-matched controls trained with explicit semantic objectives, and match or exceed strong modern vision baselines, leaving little unique variance for those alternatives. These results establish next-glimpse prediction during active vision as a biologically plausible, self-supervised route to brain-aligned scene representations learned from natural visual experience.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-14</td>
<td style='padding: 8px;'>Hi-DREAM: Brain Inspired Hierarchical Diffusion for fMRI Reconstruction via ROI Encoder and visuAl Mapping</td>
<td style='padding: 6px;'>Guowei Zhang, Yun Zhao, Moein Khajehnejad, Adeel Razi, Levin Kuhlmann</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.11437v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Mapping human brain activity to natural images offers a new window into vision and cognition, yet current diffusion-based decoders face a core difficulty: most condition directly on fMRI features without analyzing how visual information is organized across the cortex. This overlooks the brain's hierarchical processing and blurs the roles of early, middle, and late visual areas. We propose Hi-DREAM, a brain-inspired conditional diffusion framework that makes the cortical organization explicit. A region-of-interest (ROI) adapter groups fMRI into early/mid/late streams and converts them into a multi-scale cortical pyramid aligned with the U-Net depth (shallow scales preserve layout and edges; deeper scales emphasize objects and semantics). A lightweight, depth-matched ControlNet injects these scale-specific hints during denoising. The result is an efficient and interpretable decoder in which each signal plays a brain-like role, allowing the model not only to reconstruct images but also to illuminate functional contributions of different visual areas. Experiments on the Natural Scenes Dataset (NSD) show that Hi-DREAM attains state-of-the-art performance on high-level semantic metrics while maintaining competitive low-level fidelity. These findings suggest that structuring conditioning by cortical hierarchy is a powerful alternative to purely data-driven embeddings and provides a useful lens for studying the visual cortex.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-11</td>
<td style='padding: 8px;'>The One Where They Brain-Tune for Social Cognition: Multi-Modal Brain-Tuning on Friends</td>
<td style='padding: 6px;'>Nico Policzer, Cameron Braunstein, Mariya Toneva</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.07988v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent studies on audio models show brain-tuning - fine-tuning models to better predict corresponding fMRI activity - improves brain alignment and increases performance on downstream semantic and audio tasks. We extend this approach to a multimodal audio-video model to enhance social cognition, targeting the Superior Temporal Sulcus (STS), a key region for social processing, while subjects watch Friends. We find significant increases in brain alignment to the STS and an adjacent ROI, as well as improvements to a social cognition task related to the training data - sarcasm detection in sitcoms. In summary, our study extends brain-tuning to the multi-modal domain, demonstrating improvements to a downstream task after tuning to a relevant functional region.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-10</td>
<td style='padding: 8px;'>De-Individualizing fMRI Signals via Mahalanobis Whitening and Bures Geometry</td>
<td style='padding: 6px;'>Aaron Jacobson, Tingting Dan, Martin Styner, Guorong Wu, Shahar Kovalsky, Caroline Moosmueller</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.07313v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional connectivity has been widely investigated to understand brain disease in clinical studies and imaging-based neuroscience, and analyzing changes in functional connectivity has proven to be valuable for understanding and computationally evaluating the effects on brain function caused by diseases or experimental stimuli. By using Mahalanobis data whitening prior to the use of dimensionality reduction algorithms, we are able to distill meaningful information from fMRI signals about subjects and the experimental stimuli used to prompt them. Furthermore, we offer an interpretation of Mahalanobis whitening as a two-stage de-individualization of data which is motivated by similarity as captured by the Bures distance, which is connected to quantum mechanics. These methods have potential to aid discoveries about the mechanisms that link brain function with cognition and behavior and may improve the accuracy and consistency of Alzheimer's diagnosis, especially in the preclinical stage of disease progression.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-08</td>
<td style='padding: 8px;'>Topologically Invariant Permutation Test</td>
<td style='padding: 6px;'>Sixtus Dakurah</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.06153v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional brain networks exhibit topological structures that reflect neural organization; however, statistical comparison of these networks is challenging for several reasons. This paper introduces a topologically invariant permutation test for detecting topological inequivalence. Under topological equivalence, topological features can be permuted separately between groups without distorting individual network structures. The test statistic uses $2$-Wasserstein distances on persistent diagrams, computed in closed form. To reduce variability in brain connectivities while preserving topology, heat kernel expansion on the Hodge Laplacian is applied with bandwidth $t$ controlling diffusion intensity. Theoretical results guarantee variance reduction through optimal Hilbert space projection. Simulations across diverse network topologies show superior performance compared to conventional two-sample tests and alternative metrics. Applied to resting-state fMRI data from the Multimodal Treatment of ADHD study, the method detects significant topological differences between cannabis users and non-users.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-07</td>
<td style='padding: 8px;'>BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction</td>
<td style='padding: 6px;'>Xiongri Shen, Jiaqi Wang, Yi Zhong, Zhenxi Song, Leilei Zhao, Liling Li, Yichen Wei, Lingyan Liang, Shuqiang Wang, Baiying Lei, Demao Deng, Zhiguo Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.05630v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional and structural connectivity (FC/SC) are key multimodal biomarkers for brain analysis, yet their clinical utility is hindered by costly acquisition, complex preprocessing, and frequent missing modalities. Existing foundation models either process single modalities or lack explicit mechanisms for cross-modal and cross-scale consistency. We propose BrainCSD, a hierarchical mixture-of-experts (MoE) foundation model that jointly synthesizes FC/SC biomarkers and supports downstream decoding tasks (diagnosis and prediction). BrainCSD features three neuroanatomically grounded components: (1) a ROI-specific MoE that aligns regional activations from canonical networks (e.g., DMN, FPN) with a global atlas via contrastive consistency; (2) a Encoding-Activation MOE that models dynamic cross-time/gradient dependencies in fMRI/dMRI; and (3) a network-aware refinement MoE that enforces structural priors and symmetry at individual and population levels. Evaluated on the datasets under complete and missing-modality settings, BrainCSD achieves SOTA results: 95.6\% accuracy for MCI vs. CN classification without FC, low synthesis error (FC RMSE: 0.038; SC RMSE: 0.006), brain age prediction (MAE: 4.04 years), and MMSE score estimation (MAE: 1.72 points). Code is available in \href{https://github.com/SXR3015/BrainCSD}{BrainCSD}</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-11</td>
<td style='padding: 8px;'>Hunting for Neutrino Texture Zeros with Muon and Tau Flavor Violation</td>
<td style='padding: 6px;'>Lorenzo Calibbi, Xiyuan Gao, Man Yuan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.08679v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We revisit the minimal type II seesaw mechanism generating the Majorana neutrino mass matrix $M^ν$, under the assumption that two entries of $M^ν$ vanish. Such flavor structures are known as two-zero textures. Processes with charged lepton flavor violation (CLFV), absent in the Standard Model (SM), can have sizable rates in this framework and are directly linked to the flavor structure of $M^ν$. For each allowed two-zero texture, we quantify the predicted correlations among various CLFV observables using current neutrino oscillation data and show that they lead to distinctive patterns of CLFV processes that could be discriminated between at running and upcoming experiments. In addition, together with information from colliders, the sensitivity of these correlations to renormalization group (RG) effects could shed light on the potentially ultra-high scale where new dynamics (e.g. some underlying flavor symmetry) give rise to the two-zero texture. Furthermore, we find that certain zero textures, although not third-generation specific, can suppress $μ\to e$ transitions while allowing the rate of the process $τ\to \barμee$ to be within the future experimental sensitivity, even when the RG evolution is taken into account. The lowest possible cut-off scale of the effective theory, constructed by treating the two-zero flavor structure of $M^ν$ as a CLFV spurion, can therefore reach $5-6$ TeV. Our results provide further motivation for searches for $τ$ CLFV at Belle II, as probes of new physics complementary to MEG II and the upcoming Mu3e, COMET, and Mu2e experiments, as well as for collider searches for doubly charged scalar bosons.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-10</td>
<td style='padding: 8px;'>The Use of O2 in Gas Mixtures for Drift Chambers</td>
<td style='padding: 6px;'>A. M. Baldini, L. Bianco, H. Benmansour, G. Cavoto, F. Cei, M. Chiappini, A. Corvaglia, M. Francesconi, E. Gabbrielli, L. Galli, G. Gallucci, F. Grancagnolo, E. G. Grandoni, M. Grassi, F. Leonetti, D. Nicolo', M. Panareo, D. Pasciuto, A. Papa, F. Renga, S. Scarpellini, A. Venturini, C. Voena</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.07082v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The use of Oxygen in gas mixtures for drift chambers is highly discouraged because Oxygen, being strongly electronegative, is generally believed to lead, even in very small quantities, to extremely reduced drift electron attachment values, thus preventing the detector's operation.The drift chamber of the MEG II experiment at PSI has been operating for several years with a gas mixture that mainly contains He:Isobutane in relative proportions of 90:10% by molar concentration, in addition to 1.5% Isopropanol and 0.5% Oxygen. Oxygen and Isopropanol are essential for the proper functioning of the chamber. The electron attachment in the mixture used has proven negligible for the proper operation of the chamber and agrees well with the Garfield++ simulation after correctly accounting for the three-body attachment simulation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-06</td>
<td style='padding: 8px;'>Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment</td>
<td style='padding: 6px;'>Zehui Feng, Chenqi Zhang, Mingru Wang, Minuo Wei, Shiwei Cheng, Cuntai Guan, Ting Han</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.04078v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI remains a fundamental challenge due to subject variability and the entangled nature of visual features. Existing approaches primarily align neural activity directly with visual embeddings, but visual-only representations often fail to capture latent semantic dimensions, limiting interpretability and deep robustness. To address these limitations, we propose Bratrix, the first end-to-end framework to achieve multimodal Language-Anchored Vision-Brain alignment. Bratrix decouples visual stimuli into hierarchical visual and linguistic semantic components, and projects both visual and brain representations into a shared latent space, enabling the formation of aligned visual-language and brain-language embeddings. To emulate human-like perceptual reliability and handle noisy neural signals, Bratrix incorporates a novel uncertainty perception module that applies uncertainty-aware weighting during alignment. By leveraging learnable language-anchored semantic matrices to enhance cross-modal correlations and employing a two-stage training strategy of single-modality pretraining followed by multimodal fine-tuning, Bratrix-M improves alignment precision. Extensive experiments on EEG, MEG, and fMRI benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and captioning performance compared to state-of-the-art methods, specifically surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-03</td>
<td style='padding: 8px;'>Variational Representational Similarity Analysis (vRSA) for M/EEG</td>
<td style='padding: 6px;'>Alex Lepauvre, Lucia Melloni, Karl Friston, Peter Zeidman</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.01784v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper introduces variational representational similarity analysis RSA (vRSA) for electromagnetic recordings of neural responses (e.g., EEG, MEG, ECoG or LFP). Variational RSA is a Bayesian approach for testing whether the similarity of stimuli or experimental conditions is expressed in univariate or multivariate neural recordings. Extending an approach previously introduced in the context of functional MRI, vRSA decomposes the condition-by-condition data covariance matrix into hypothesised effects and observation noise, thereby casting RSA as a covariance component estimation problem. In this context, peristimulus time may be treated as an experimental factor, enabling one to test for the probability that different experimental effects are expressed in data at different times. Variational Bayesian methods are used for model estimation and model comparison, which confer a number of advantages over classical approaches, including statistically efficient hypothesis testing, quantification of uncertainty using Bayesian credible intervals and computational efficiency. After introducing the theory, we provide a worked example using openly available EEG data. Software functions implementing vRSA for the SPM software package accompany this paper, together with exemplar analysis scripts.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-01</td>
<td style='padding: 8px;'>Smooth Models of Fibered Partially Hyperbolic Systems</td>
<td style='padding: 6px;'>Jonathan DeWitt, Meg Doucette, Oliver Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.00697v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We study fibered partially hyperbolic diffeomorphisms. We show that as long as certain topological obstructions vanish and as long as homological minimum expansion dominates the distortion on the fibers that a fibered partially hyperbolic system can be homotoped to a fibered partially hyperbolic system with a $C^{\infty}$-center fibering. In addition, we study obstructions to the existence of smooth lifts of Anosov diffeomorphisms to bundles. In particular, we give an example of smooth topologically trivial bundle over a torus, where an Anosov diffeomorphism can lift continuously but not smoothly to the bundle.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-29</td>
<td style='padding: 8px;'>Risk-Aware Safety Filters with Poisson Safety Functions and Laplace Guidance Fields</td>
<td style='padding: 6px;'>Gilbert Bahati, Ryan M. Bena, Meg Wilkinson, Pol Mestres, Ryan K. Cosner, Aaron D. Ames</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.25913v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Robotic systems navigating in real-world settings require a semantic understanding of their environment to properly determine safe actions. This work aims to develop the mathematical underpinnings of such a representation -- specifically, the goal is to develop safety filters that are risk-aware. To this end, we take a two step approach: encoding an understanding of the environment via Poisson's equation, and associated risk via Laplace guidance fields. That is, we first solve a Dirichlet problem for Poisson's equation to generate a safety function that encodes system safety as its 0-superlevel set. We then separately solve a Dirichlet problem for Laplace's equation to synthesize a safe \textit{guidance field} that encodes variable levels of caution around obstacles -- by enforcing a tunable flux boundary condition. The safety function and guidance fields are then combined to define a safety constraint and used to synthesize a risk-aware safety filter which, given a semantic understanding of an environment with associated risk levels of environmental features, guarantees safety while prioritizing avoidance of higher risk obstacles. We demonstrate this method in simulation and discuss how \textit{a priori} understandings of obstacle risk can be directly incorporated into the safety filter to generate safe behaviors that are risk-aware.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-27</td>
<td style='padding: 8px;'>Molecular Gas in Major Mergers Hosting Dual and Single AGN at <10 kpc Nuclear Separations</td>
<td style='padding: 6px;'>Makoto A. Johnstone, Ezequiel Treister, Franz E. Bauer, Chin-Shin Chang, Claudia Cicone, Michael J. Koss, Ignacio del Moral-Castro, Francisco Muller-Sanchez, George C. Privon, Claudio Ricci, Nick Scoville, Giacomo Venturi, Loreto Barcos-Muñoz, Lee Armus, Laura Blecha, Caitlin Casey, Julia Comerford, Aaron Evans, Taiki Kawamuro, Anne M. Medling, Hugo Messias, Neil Nagar, Alejandra Rojas, David Sanders, Benny Trakhtenbrot, Vivian U, Meg Urry</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.23742v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We present high-resolution ($\sim$50$-$100 pc) Atacama Large Millimeter Array (ALMA) observations of $^{12}$CO(2-1) or $^{12}$CO(1-0) emission in seven local ($z$ $\lesssim$ 0.05) major mergers -- five of which are dual active galactic nuclei (AGN) systems, and two of which are single AGN systems. We model the molecular gas kinematics through rotating disk profiles using a Bayesian Markov chain Monte Carlo approach. The residuals were then used to isolate non-rotating components of the molecular gas -- the most likely contributor to future SMBH growth. We find that more massive SMBHs have higher surface densities of non-rotating molecular gas within their sphere of influence. This potential molecular gas supply, however, does not correlate with the current accretion efficiency of the SMBHs, suggesting that only a fraction of the observed non-rotating gas is currently reaching the SMBH. Finally, we tentatively find no significant differences in the nuclear molecular gas masses of single AGN and dual AGN hosts, both within the SMBH sphere of influence and within the central kiloparsec. Our results indicate that the probability of occurrence of the dual AGN phenomenon is likely dependent on AGN variability and/or obscuration rather than the availability of molecular gas in the nuclear regions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-24</td>
<td style='padding: 8px;'>Automated interictal epileptic spike detection from simple and noisy annotations in MEG data</td>
<td style='padding: 6px;'>Pauline Mouches, Julien Jung, Armand Demasson, Agnès Guinard, Romain Bouet, Rosalie Marchal, Romain Quentin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.21596v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In drug-resistant epilepsy, presurgical evaluation of epilepsy can be considered. Magnetoencephalography (MEG) has been shown to be an effective exam to inform the localization of the epileptogenic zone through the localization of interictal epileptic spikes. Manual detection of these pathological biomarkers remains a fastidious and error-prone task due to the high dimensionality of MEG recordings, and interrater agreement has been reported to be only moderate. Current automated methods are unsuitable for clinical practice, either requiring extensively annotated data or lacking robustness on non-typical data. In this work, we demonstrate that deep learning models can be used for detecting interictal spikes in MEG recordings, even when only temporal and single-expert annotations are available, which represents real-world clinical practice. We propose two model architectures: a feature-based artificial neural network (ANN) and a convolutional neural network (CNN), trained on a database of 59 patients, and evaluated against a state-of-the-art model to classify short time windows of signal. In addition, we employ an interactive machine learning strategy to iteratively improve our data annotation quality using intermediary model outputs. Both proposed models outperform the state-of-the-art model (F1-scores: CNN=0.46, ANN=0.44) when tested on 10 holdout test patients. The interactive machine learning strategy demonstrates that our models are robust to noisy annotations. Overall, results highlight the robustness of models with simple architectures when analyzing complex and imperfectly annotated data. Our method of interactive machine learning offers great potential for faster data annotation, while our models represent useful and efficient tools for automated interictal spikes detection.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-22</td>
<td style='padding: 8px;'>Dictionary learning methods for brain activity mapping with MEG data</td>
<td style='padding: 6px;'>Daniela Calvetti, Erkki Somersalo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.19702v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A central goal in many brain studies is the identification of those brain regions that are activated during an observation window that may correspond to a motor task, a stimulus, or simply a resting state. While functional MRI is currently the most commonly employed modality for such task, methods based on the electromagnetic activity of the brain are valuable alternatives because of their excellent time resolution and of the fact that the measured signals are directly related to brain activation and not to a secondary effect such as the hemodynamic response. In this work we focus on the MEG modality, investigating the performance of a recently proposed Bayesian dictionary learning (BDL) algorithm for brain region identification. The partitioning of the source space into the 148 regions of interest (ROI) corresponding to parcellation of the Destrieux atlas provides a natural determination of the subdictionaries necessary for the BDL algorithm. We design a simulation protocol where a small randomly selected patch in each ROI is activated, the MEG signal is computed and the inverse problem of active brain region identification is solved using the BDL algorithm. The BDL algorithm consists of two phases, the first one comprising dictionary compression and Bayesian compression error analysis, and the second one performing dictionary coding with a deflated dictionary built on the output of the first phase, both steps relying on Bayesian sparsity promoting computations. For assessing the performance, we give a probabilistic interpretation of the confusion matrix, and consider different impurity measures for a multi-class classifier.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-20</td>
<td style='padding: 8px;'>MEG-GPT: A transformer-based foundation model for magnetoencephalography data</td>
<td style='padding: 6px;'>Rukuang Huang, Sungjun Cho, Chetan Gohil, Oiwi Parker Jones, Mark Woolrich</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.18080v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Modelling the complex spatiotemporal patterns of large-scale brain dynamics is crucial for neuroscience, but traditional methods fail to capture the rich structure in modalities such as magnetoencephalography (MEG). Recent advances in deep learning have enabled significant progress in other domains, such as language and vision, by using foundation models at scale. Here, we introduce MEG-GPT, a transformer based foundation model that uses time-attention and next time-point prediction. To facilitate this, we also introduce a novel data-driven tokeniser for continuous MEG data, which preserves the high temporal resolution of continuous MEG signals without lossy transformations. We trained MEG-GPT on tokenised brain region time-courses extracted from a large-scale MEG dataset (N=612, eyes-closed rest, Cam-CAN data), and show that the learnt model can generate data with realistic spatio-spectral properties, including transient events and population variability. Critically, it performs well in downstream decoding tasks, improving downstream supervised prediction task, showing improved zero-shot generalisation across sessions (improving accuracy from 0.54 to 0.59) and subjects (improving accuracy from 0.41 to 0.49) compared to a baseline methods. Furthermore, we show the model can be efficiently fine-tuned on a smaller labelled dataset to boost performance in cross-subject decoding scenarios. This work establishes a powerful foundation model for electrophysiological data, paving the way for applications in computational neuroscience and neural decoding.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-25</td>
<td style='padding: 8px;'>Dopamine-driven synaptic credit assignment in neural networks</td>
<td style='padding: 6px;'>Saranraj Nambusubramaniyan, Shervin Safavi, Raja Guru, Andreas Knoblauch</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.22178v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Solving the synaptic Credit Assignment Problem(CAP) is central to learning in both biological and artificial neural systems. Finding an optimal solution for synaptic CAP means setting the synaptic weights that assign credit to each neuron for influencing the final output and behavior of neural networks or animals. Gradient-based methods solve this problem in artificial neural networks using back-propagation, however, not in the most efficient way. For instance, back-propagation requires a chain of top-down gradient computations. This leads to an expensive optimization process in terms of computing power and memory linked with well-known weight transport and update locking problems. To address these shortcomings, we take a NeuroAI approach and draw inspiration from neural Reinforcement Learning to develop a derivative-free optimizer for training neural networks, Dopamine. Dopamine is developed for Weight Perturbation (WP) learning that exploits stochastic updating of weights towards optima. It achieves this by minimizing the regret, a form of Reward Prediction Error (RPE) between the expected outcome from the perturbed model and the actual outcome from the unperturbed model. We use this RPE to adjust the learning rate in the network (i.e., creating an adaptive learning rate strategy, similar to the role of dopamine in the brain). We tested the Dopamine optimizer for training multi-layered perceptrons for XOR tasks, and recurrent neural networks for chaotic time series forecasting. Dopamine-trained models demonstrate accelerated convergence and outperform standard WP, and give comparable performance to gradient-based algorithms, while consuming significantly less computation and memory. Overall, the Dopamine optimizer not only finds robust solutions and comparable performance to the state-of-the-art Machine Learning optimizers but is also neurobiologically more plausible.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>A Computational Perspective on NeuroAI and Synthetic Biological Intelligence</td>
<td style='padding: 6px;'>Dhruvik Patel, Md Sayed Tanveer, Jesus Gonzalez-Ferrer, Alon Loeffler, Brett J. Kagan, Mohammed A. Mostajo-Radji, Ge Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.23896v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>NeuroAI is an emerging field at the intersection of neuroscience and artificial intelligence, where insights from brain function guide the design of intelligent systems. A central area within this field is synthetic biological intelligence (SBI), which combines the adaptive learning properties of biological neural networks with engineered hardware and software. SBI systems provide a platform for modeling neural computation, developing biohybrid architectures, and enabling new forms of embodied intelligence. In this review, we organize the NeuroAI landscape into three interacting domains: hardware, software, and wetware. We outline computational frameworks that integrate biological and non-biological systems and highlight recent advances in organoid intelligence, neuromorphic computing, and neuro-symbolic learning. These developments collectively point toward a new class of systems that compute through interactions between living neural tissue and digital algorithms.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-07</td>
<td style='padding: 8px;'>Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers</td>
<td style='padding: 6px;'>Thomas Klein, Sascha Meyen, Wieland Brendel, Felix A. Wichmann, Kristof Meding</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.06645v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Benchmarking models is a key factor for the rapid progress in machine learning (ML) research. Thus, further progress depends on improving benchmarking metrics. A standard metric to measure the behavioral alignment between ML models and human observers is error consistency (EC). EC allows for more fine-grained comparisons of behavior than other metrics such as accuracy, and has been used in the influential Brain-Score benchmark to rank different DNNs by their behavioral consistency with humans. Previously, EC values have been reported without confidence intervals. However, empirically measured EC values are typically noisy -- thus, without confidence intervals, valid benchmarking conclusions are problematic. Here we improve on standard EC in two ways: First, we show how to obtain confidence intervals for EC using a bootstrapping technique, allowing us to derive significance tests for EC. Second, we propose a new computational model relating the EC between two classifiers to the implicit probability that one of them copies responses from the other. This view of EC allows us to give practical guidance to scientists regarding the number of trials required for sufficiently powerful, conclusive experiments. Finally, we use our methodology to revisit popular NeuroAI-results. We find that while the general trend of behavioral differences between humans and machines holds up to scrutiny, many reported differences between deep vision models are statistically insignificant. Our methodology enables researchers to design adequately powered experiments that can reliably detect behavioral differences between models, providing a foundation for more rigorous benchmarking of behavioral alignment.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-02</td>
<td style='padding: 8px;'>What Neuroscience Can Teach AI About Learning in Continuously Changing Environments</td>
<td style='padding: 6px;'>Daniel Durstewitz, Bruno Averbeck, Georgia Koppe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.02103v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-27</td>
<td style='padding: 8px;'>NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models</td>
<td style='padding: 6px;'>Luca Ghafourpour, Valentin Duruisseaux, Bahareh Tolooshams, Philip H. Wong, Costas A. Anastassiou, Anima Anandkumar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2506.04536v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Characterizing the cellular properties of neurons is fundamental to understanding their function in the brain. In this quest, the generation of bio-realistic models is central towards integrating multimodal cellular data sets and establishing causal relationships. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. The deterministic formalism of bio-realistic models currently precludes accounting for the natural variability observed experimentally. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on synthetic data generated from bio-realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE enables the efficient generation of synthetic neurons that closely resemble experimental data and exhibit trial-to-trial variability, offering a $4200\times$ speedup over the numerical solver. NOBLE is the first scaled-up deep learning framework that validates its generalization with real experimental data. To this end, NOBLE captures fundamental neural properties in a unique and emergent manner that opens the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-21</td>
<td style='padding: 8px;'>SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation</td>
<td style='padding: 6px;'>Jiayue Liu, Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Kuo Yang, Xu Wang, Yang Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.16080v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-02-22</td>
<td style='padding: 8px;'>Brain-Model Evaluations Need the NeuroAI Turing Test</td>
<td style='padding: 6px;'>Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2502.16238v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-04</td>
<td style='padding: 8px;'>Asynchronous Hebbian/anti-Hebbian networks</td>
<td style='padding: 6px;'>Henrique Reis Aguiar, Matthias H. Hennig</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02402v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-03</td>
<td style='padding: 8px;'>NeuroAI for AI Safety</td>
<td style='padding: 6px;'>Patrick Mineault, Niccolò Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.18526v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-14</td>
<td style='padding: 8px;'>Evaluating Representational Similarity Measures from the Lens of Functional Correspondence</td>
<td style='padding: 6px;'>Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.14633v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>Multi-Stage Residual-Aware Unsupervised Deep Learning Framework for Consistent Ultrasound Strain Elastography</td>
<td style='padding: 6px;'>Shourov Joarder, Tushar Talukder Showrav, Md. Kamrul Hasan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15640v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Ultrasound Strain Elastography (USE) is a powerful non-invasive imaging technique for assessing tissue mechanical properties, offering crucial diagnostic value across diverse clinical applications. However, its clinical application remains limited by tissue decorrelation noise, scarcity of ground truth, and inconsistent strain estimation under different deformation conditions. Overcoming these barriers, we propose MUSSE-Net, a residual-aware, multi-stage unsupervised sequential deep learning framework designed for robust and consistent strain estimation. At its backbone lies our proposed USSE-Net, an end-to-end multi-stream encoder-decoder architecture that parallelly processes pre- and post-deformation RF sequences to estimate displacement fields and axial strains. The novel architecture incorporates Context-Aware Complementary Feature Fusion (CACFF)-based encoder with Tri-Cross Attention (TCA) bottleneck with a Cross-Attentive Fusion (CAF)-based sequential decoder. To ensure temporal coherence and strain stability across varying deformation levels, this architecture leverages a tailored consistency loss. Finally, with the MUSSE-Net framework, a secondary residual refinement stage further enhances accuracy and suppresses noise. Extensive validation on simulation, in vivo, and private clinical datasets from Bangladesh University of Engineering and Technology (BUET) medical center, demonstrates MUSSE-Net's outperformed existing unsupervised approaches. On MUSSE-Net achieves state-of-the-art performance with a target SNR of 24.54, background SNR of 132.76, CNR of 59.81, and elastographic SNR of 9.73 on simulation data. In particular, on the BUET dataset, MUSSE-Net produces strain maps with enhanced lesion-to-background contrast and significant noise suppression yielding clinically interpretable strain patterns.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>Qualitative and quantitative hard-tissue MRI with portable Halbach scanners</td>
<td style='padding: 6px;'>Jose Borreguero, Luiz G. C. Santos, Lorena Vega Cid, Elisa Castañón, Marina Fernández-García, Pablo Benlloch, Rubén Bosch, Jesús Conejero, Pablo García-Cristóbal, Alba González-Cebrián, Teresa Guallart-Naval, Eduardo Pallás, Laia Porcar, Lucas Swistunow, Jose Miguel Algarín, Fernando Galve, Joseba Alonso</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15617v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Purpose: To demonstrate the feasibility of performing in vivo imaging and quantitative relaxation mapping of soft and hard tissues using a low-cost, portable MRI scanner, and to establish the methodological foundations for zero echo time (ZTE) imaging in systems affected by strong field inhomogeneities. Methods: A complete framework for artifact-free ZTE imaging at low field was developed, including: (i) RF pulse pre/counteremphasis calibration to minimize ring-down and electronics switching time; (ii) an extension of a recent single-point double-shot (SPDS) protocol for simultaneous B0 and B1 mapping; and (iii) a model-based reconstruction incorporating these field maps into the encoding matrix. ZTE imaging and variable flip angle (VFA) T1 mapping were performed on phantoms and in vivo human knees and ankles, and benchmarked against standard RARE and STIR acquisitions. Results: The optimized PETRA sequence produced 3D images of knees and ankles within clinically compatible times (< 15 min), revealing hard tissues such as ligaments, tendons, cartilage, and bone that are invisible in spin-echo sequences. The extended SPDS method enabled accurate field mapping, while the VFA approach provided the first in vivo T1 measurements of hard tissues at B0 < 0.1 T. Conclusions: The proposed framework broadens the range of pulse sequences feasible in portable low-field MRI and demonstrates the potential of ZTE for quantitative and structural imaging of musculoskeletal tissues in affordable Halbach-based systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>MaskMed: Decoupled Mask and Class Prediction for Medical Image Segmentation</td>
<td style='padding: 6px;'>Bin Xie, Gady Agam</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15603v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Medical image segmentation typically adopts a point-wise convolutional segmentation head to predict dense labels, where each output channel is heuristically tied to a specific class. This rigid design limits both feature sharing and semantic generalization. In this work, we propose a unified decoupled segmentation head that separates multi-class prediction into class-agnostic mask prediction and class label prediction using shared object queries. Furthermore, we introduce a Full-Scale Aware Deformable Transformer module that enables low-resolution encoder features to attend across full-resolution encoder features via deformable attention, achieving memory-efficient and spatially aligned full-scale fusion. Our proposed method, named MaskMed, achieves state-of-the-art performance, surpassing nnUNet by +2.0% Dice on AMOS 2022 and +6.9% Dice on BTCV.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery</td>
<td style='padding: 6px;'>Miruna-Alexandra Gafencu, Yordanka Velikova, Nassir Navab, Mohammad Farid Azampour</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15600v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>FunnyNodules: A Customizable Medical Dataset Tailored for Evaluating Explainable AI</td>
<td style='padding: 6px;'>Luisa Gallée, Yiheng Xiong, Meinrad Beer, Michael Götz</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15481v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Densely annotated medical image datasets that capture not only diagnostic labels but also the underlying reasoning behind these diagnoses are scarce. Such reasoning-related annotations are essential for developing and evaluating explainable AI (xAI) models that reason similarly to radiologists: making correct predictions for the right reasons. To address this gap, we introduce FunnyNodules, a fully parameterized synthetic dataset designed for systematic analysis of attribute-based reasoning in medical AI models. The dataset generates abstract, lung nodule-like shapes with controllable visual attributes such as roundness, margin sharpness, and spiculation. Target class is derived from a predefined attribute combination, allowing full control over the decision rule that links attributes to the diagnostic class. We demonstrate how FunnyNodules can be used in model-agnostic evaluations to assess whether models learn correct attribute-target relations, to interpret over- or underperformance in attribute prediction, and to analyze attention alignment with attribute-specific regions of interest. The framework is fully customizable, supporting variations in dataset complexity, target definitions, class balance, and beyond. With complete ground truth information, FunnyNodules provides a versatile foundation for developing, benchmarking, and conducting in-depth analyses of explainable AI methods in medical image analysis.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG</td>
<td style='padding: 6px;'>Kunyu Zhang, Mingxuan Wang, Xiangjie Shi, Haoxing Xu, Chao Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15393v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>Optimal Neumann boundary and distributed control of the Westervelt equation with time-fractional attenuation</td>
<td style='padding: 6px;'>Vanja Nikolić, Belkacem Said-Houari</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15382v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Optimal control of nonlinear acoustic waves is relevant in many medical ultrasound technologies, ranging from cancer therapy to targeted drug delivery, where it can help guide the precise deposition of acoustic energy. In this work, we study Neumann boundary and distributed control problems for tracking a prescribed pressure field governed by the Westervelt equation with time-fractional dissipation. This model captures nonlinear ultrasonic wave propagation in biological media and accounts for the experimentally observed power-law attenuation. We begin by extending the existing well-posedness theory for time-fractional equations to include inhomogeneous Neumann boundary data used as control inputs, which requires constructing an appropriate data extension and regularization. Using these analytical results for the forward problem, we prove the existence of globally optimal controls and analyze the stability of the optimization problem with respect to perturbations in the target pressure field and to vanishing regularization parameters. Finally, we investigate the associated adjoint equation, which has state-dependent coefficients, and use it to derive first-order necessary optimality conditions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>Utilizing subgroup information in random-effects meta-analysis of few studies</td>
<td style='padding: 6px;'>Ao Huang, Christian Röver, Tim Friede</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15366v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Random-effects meta-analyses are widely used for evidence synthesis in medical research. However, conventional methods based on large-sample approximations often exhibit poor performance in case of very few studies (e.g., 2 to 4), which is very common in practice. Existing methods aiming to improve small-sample performance either still suffer from poor estimates of heterogeneity or result in very wide confidence intervals. Motivated by meta-analyses evaluating surrogate outcomes, where units nested within a trial are often exploited when the number of trials is small, we propose an inference approach based on a common-effect estimator synthesizing data from the subgroup-level instead of the study-level. Two DerSimonian-Laird type heterogeneity estimators are derived using the subgroup-level data, and are incorporated into the Henmi-Copas type variance to adequately reflect variance components. We considered t-quantile based intervals to account for small-sample properties and used flexible degrees of freedom to reduce interval lengths. A comprehensive simulation is conducted to study the performance of our methods depending on various magnitudes of subgroup effects as well as subgroup prevalences. Some general recommendations are provided on how to select the subgroups, and methods are illustrated using two example applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>On the phase aberration estimation using common mid-angle correlations</td>
<td style='padding: 6px;'>Naiara Korta Martiartu, Michael Jaeger</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15336v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Phase aberrations, despite degrading ultrasound images, also encode valuable information about the spatial distribution of the speed of sound in tissue. In pulse-echo ultrasound, we can quantify them by exploiting speckle correlations. Among existing strategies, correlations between steered acquisitions that share a common mid-angle have proven particularly effective for inferring the speed of sound. Their phases can be linearly related to the phase aberrations undergone by both the incident and reflected wavefronts. This relationship has so far been demonstrated only through geometric arguments based on point reflectors. Here, we develop a rigorous theoretical formalism that extends this relationship to the speckle regime, completing the previously established linear model and clarifying its underlying assumptions. More importantly, we build on this formalism to analyze correlation-phase fluctuations arising from aberration-induced speckle decorrelation. The analysis reveals that phase variance is governed by the relative loss of coherence, which increases approximately linearly with the square of the correlation phases. Local correlation-phase estimates therefore become increasingly uncertain as their magnitude grows. Experimental measurements in a uniform tissue-mimicking phantom show excellent agreement with the predicted variance. Beyond providing a theoretical basis for advancing speed-of-sound imaging, this formalism establishes the accuracy limit of common-mid-angle correlation phases, offering a benchmark for evaluating more advanced aberration-estimation techniques.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-19</td>
<td style='padding: 8px;'>SkinGPT-R1: Adapter-Only Dual Distillation for Efficient Dermatology Reasoning</td>
<td style='padding: 6px;'>Yuhao Shen, Jiahe Qian, Zhangtianyi Chen, Yuanhao He, Juexiao Zhou</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.15242v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We present SkinGPT-R1, a dermatology focused vision language model that makes diagnostic chain of thought reasoning explicit, step by step, and verifiable. To support skin specific reasoning, we build DermCoT, a corpus of standardized dermatologic chain of thought narratives that combines 10,000 DermEval filtered training cases with 3,000 dermatologist scored certified cases, and we define DermEval as a physician aligned six dimensional evaluator and DermBench as the corresponding benchmark for dermatologic chain of thought quality. On DermBench, across 14 general, reasoning, and medical vision language models, SkinGPT-R1 achieves an average score of 4.031 out of 5 over the six clinician defined dimensions, ranks 1st among all systems, and improves the average score over Vision-R1 by about 41%. On three dermatology classification benchmarks, SkinGPT-R1 delivers stable accuracy gains over Vision-R1 and remains competitive among strong vision language models. Ablation results further show that DermCoT based chain of thought supervision provides substantial improvements over the base model and that adding dermatology aware visual distillation yields consistent additional gains in both narrative quality and recognition.</td>
</tr>
</tbody>
</table>

