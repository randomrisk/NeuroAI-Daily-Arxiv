<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2025-10-12</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models</td>
<td style='padding: 6px;'>Jiayun Luo, Wan-Cyuan Fan, Lyuyang Wang, Xiangteng He, Tanzila Rahman, Purang Abolmaesumi, Leonid Sigal</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08510v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large Vision Language Models (LVLMs) have recently emerged as powerful architectures capable of understanding and reasoning over both visual and textual information. These models typically rely on two key components: a Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual content into a sequence of image tokens and serves as the perceptual front-end -- the eyes of the model. In contrast, the LLM interprets these tokens to perform high-level reasoning, generates responses, and functions as the cognitive core -- the brain of the model. However, it remains unclear which visual tokens contribute most significantly to understanding and reasoning, and how effectively these signals are propagated from ViT to the LLM. While most existing works have focused on identifying attention sinks, low-semantic tokens receiving disproportionately high attention, within the LLM, we shift the focus to the vision encoder by identifying a class of high-norm visual tokens from ViT, referred to as ViT attention sinks -- a problem that has been rarely studied but is indeed very important for LVLMs. Our findings show that these ViT sinks encapsulate high-level semantic concepts from images, allowing the LLM to perform more effective understanding and reasoning. Despite their importance, these sink tokens are often overlooked in existing LVLM architectures. To explore their contribution, we present both qualitative and quantitative analyses of the information embedded in these sink tokens. We also propose both training-free and training-based approaches to better leverage how this information is interpreted by the LLM, and to what extent. By explicitly utilizing these tokens, we demonstrate substantial improvements across a range of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT attention sinks in enhancing visual reasoning.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>AI-Driven Radiology Report Generation for Traumatic Brain Injuries</td>
<td style='padding: 6px;'>Riadh Bouslimi, Houda Trabelsi, Wahiba Ben Abdssalem Karaa, Hana Hedhli</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08498v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Traumatic brain injuries present significant diagnostic challenges in emergency medicine, where the timely interpretation of medical images is crucial for patient outcomes. In this paper, we propose a novel AI-based approach for automatic radiology report generation tailored to cranial trauma cases. Our model integrates an AC-BiFPN with a Transformer architecture to capture and process complex medical imaging data such as CT and MRI scans. The AC-BiFPN extracts multi-scale features, enabling the detection of intricate anomalies like intracranial hemorrhages, while the Transformer generates coherent, contextually relevant diagnostic reports by modeling long-range dependencies. We evaluate the performance of our model on the RSNA Intracranial Hemorrhage Detection dataset, where it outperforms traditional CNN-based models in both diagnostic accuracy and report generation. This solution not only supports radiologists in high-pressure environments but also provides a powerful educational tool for trainee physicians, offering real-time feedback and enhancing their learning experience. Our findings demonstrate the potential of combining advanced feature extraction with transformer-based text generation to improve clinical decision-making in the diagnosis of traumatic brain injuries.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>Spike-frequency and h-current based adaptation are dynamically equivalent in a Wilson-Cowan field model</td>
<td style='padding: 6px;'>Ronja Strömsdörfer, Klaus Obermayer</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08436v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>During slow-wave sleep, the brain produces traveling waves of slow oscillations (SOs; $\leq 2$ Hz), characterized by the propagation of alternating high- and low-activity states. The question of internal mechanisms that modulate traveling waves of SOs is still unanswered although it is established that it is an adaptation mechanism that mediates them. One mechanism investigated is spike-frequency adaptation, a hyperpolarizing feedback current that is activated during periods of high-activity. An alternative mechanism is based on hyperpolarization-activated currents, which are positive feedback currents that are activated in low-activity states. Both adaptation mechanisms were shown to feature SO-like dynamics in neuronal populations, and the inclusion of a spatial domain seems to enhance observable differences in their effects. To investigate this in detail, we examine a spatially extended two-population Wilson-Cowan model with local spatial coupling and the excitatory populations equipped with either one of the two adaptation mechanisms. We describe them with the same dynamical equation and include the inverse mode of action by changing the signs of adaptation strength and gain. We show that the dynamical systems are mathematically equivalent under a compensatory external input, which depends on the adaptation strength, leading to a shift in state space of the otherwise equivalent bifurcation structure. Strong enough adaptation is required to induce traveling waves. Additionally, adaptation modulates the properties of the spatio-temporal activity patterns, such as temporal and spatial frequencies, and the speed of the traveling waves, all of which increase with increasing strength. Though being dynamically equivalent, our results also explain why location-dependent variations in feedback strength cause differences in the propagation of traveling waves between both adaptation mechanisms.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>Effect of modeling subject-specific cortical folds on brain injury risk prediction under blunt impact loading</td>
<td style='padding: 6px;'>Anu Tripathi, Alison Brooks, Traci Snedden, Peter Ferrazzano, Christian Franck, Rika Wright Carlsen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08379v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Purpose: Computational head models are essential tools for studying the risk of mild traumatic brain injury (mTBI) under different activities and across populations. However, different computational models incorporate varied levels of anatomical details, such as cortical folds. In this study, we aim to determine the effect of modeling cortical folds on mTBI risk assessment.   Methods: We compared the gyrencephalic (with cortical folds) and lissencephalic (without cortical folds) FE models of 18 subjects aged 9 - 18 years, under a rotational head acceleration event. A rotational acceleration of 10 krad/s$^2$ and 10 ms duration was simulated about each principal head axis. We analyzed different mTBI injury metrics, including maximum principal strain (MPS95), maximum principal strain rate (MPSR95), and cumulative strain damage measure (CSDM15), for the whole brain as well as for specific regions of interest (ROIs).   Results: Modeling cortical folds consistently predicted higher injury metrics across all individuals and rotational direction, with the bias (mean $\pm$ std. dev.) of $-15.1 \pm 6.5\%$ in MPS95, $-12.9 \pm 5.6\%$ in MPSR95, and $-8.8 \pm 11.09\%$ in CSDM15. We also find that the regions of high strain concentrations vary significantly between the two models, with the DICE metric on peak MPS ranging between $0.07-0.43$ and DICE on CSDM15 ranging between $0.42-0.70$. Modeling cortical folds also affects injury metrics in all ROIs, even the ones that remain geometrically unaltered in the two model types, such as the corpus callosum, cerebellum, and brain stem.   Conclusions: The study finds that modeling cortical folds significantly alters the region of high brain deformations and the mTBI risk under head rotations.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>Beyond Real Data: Synthetic Data through the Lens of Regularization</td>
<td style='padding: 6px;'>Amitis Shidani, Tyler Farghly, Yang Sun, Habib Ganjgahi, George Deligiannidis</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08095v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Synthetic data can improve generalization when real data is scarce, but excessive reliance may introduce distributional mismatches that degrade performance. In this paper, we present a learning-theoretic framework to quantify the trade-off between synthetic and real data. Our approach leverages algorithmic stability to derive generalization error bounds, characterizing the optimal synthetic-to-real data ratio that minimizes expected test error as a function of the Wasserstein distance between the real and synthetic distributions. We motivate our framework in the setting of kernel ridge regression with mixed data, offering a detailed analysis that may be of independent interest. Our theory predicts the existence of an optimal ratio, leading to a U-shaped behavior of test error with respect to the proportion of synthetic data. Empirically, we validate this prediction on CIFAR-10 and a clinical brain MRI dataset. Our theory extends to the important scenario of domain adaptation, showing that carefully blending synthetic target data with limited source data can mitigate domain shift and enhance generalization. We conclude with practical guidance for applying our results to both in-domain and out-of-domain scenarios.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>Optimizing BCI Rehabilitation Protocols for Stroke: Exploring Task Design and Training Duration</td>
<td style='padding: 6px;'>Aniana Cruz, Marko Kuzmanoski, Gabriel Pires</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08082v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Stroke is a leading cause of long-term disability and the second most common cause of death worldwide. Although acute treatments have advanced, recovery remains challenging and limited. Brain-computer interfaces (BCIs) have emerged as a promising tool for post-stroke rehabilitation by promoting neuroplasticity. However, clinical outcomes remain variable, and optimal protocols have yet to be established. This study explores strategies to optimize BCI-based rehabilitation by comparing motor imagery of affected hand movement versus rest, instead of the conventional left-versus-right motor imagery. This alternative aims to simplify the task and address the weak contralateral activation commonly observed in stroke patients. Two datasets, one from healthy individuals and one from stroke patients, were used to evaluate the proposed approach. The results showed improved performance using both FBCSP and EEGNet. Additionally, we investigated the impact of session duration and found that shorter training sessions produced better BCI performance than longer sessions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings for Weakly Supervised Anomaly Detection in Brain MRI Scans</td>
<td style='padding: 6px;'>Bheeshm Sharma, Karthikeyan Jaganathan, Balamurugan Palaniappan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08052v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Weakly Supervised Anomaly detection (WSAD) in brain MRI scans is an important challenge useful to obtain quick and accurate detection of brain anomalies when precise pixel-level anomaly annotations are unavailable and only weak labels (e.g., slice-level) are available. In this work, we propose RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings, a novel two-stage WSAD framework. In the first stage, we introduce a Discriminative Dual Prompt Tuning (DDPT) mechanism that generates high-quality pseudo weak masks based on slice-level labels, serving as coarse localization cues. In the second stage, we propose a segmentation network with a region-aware spatial attention mechanism that relies on fixed location-based random embeddings. This design enables the model to effectively focus on anomalous regions. Our approach achieves state-of-the-art anomaly detection performance, significantly outperforming existing WSAD methods while utilizing less than 8 million parameters. Extensive evaluations on the BraTS20, BraTS21, BraTS23, and MSD datasets demonstrate a substantial performance improvement coupled with a significant reduction in computational complexity. Code is available at: https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>State-dependent brain responsiveness, from local circuits to the whole brain</td>
<td style='padding: 6px;'>A. Destexhe, J Goldman, N. Tort-Colet, A. Roques, J. Fousek, S. Petkoski, V. Jirsa, O. David, M. Jedynak, C. Capone, C. De Luca, G. De Bonis, P. S. Paolucci, E. Mikulan, Pigorini, M Massimini, A. Galluzzi, A. Pazienti, M. Mattia, A. Arena, B. E. Juel, E. Hagen, J. F. Storm, E. Montagni, F. Resta, F. S. Pavone, A. L. Allegra Mascaro, A. Dwarakanath, T. I. Panagiotaropoulos, J. Senk, M. Diesmann, A. Camassa, L. Dalla Porta, A. Manasanch, M. V. Sanchez-Vives</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.07956v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The objective of this paper is to review physiological and computational aspects of the responsiveness of the cerebral cortex to stimulation, and how responsiveness depends on the state of the system. This correspondence between brain state and brain responsiveness (state-dependent responses) is outlined at different scales from the cellular and circuit level, to the mesoscale and macroscale level. At each scale, we review how quantitative methods can be used to characterize network states based on brain responses, such as the Perturbational Complexity Index (PCI). This description will compare data and models, systematically and at multiple scales, with a focus on the mechanisms that explain how brain responses depend on brain states.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>Synergy Between the Strong and the Weak: Spiking Neural Networks are Inherently Self-Distillers</td>
<td style='padding: 6px;'>Yongqi Ding, Lin Zuo, Mengmeng Jing, Kunshan Yang, Pei He, Tonglan Xie</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.07924v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-inspired spiking neural networks (SNNs) promise to be a low-power alternative to computationally intensive artificial neural networks (ANNs), although performance gaps persist. Recent studies have improved the performance of SNNs through knowledge distillation, but rely on large teacher models or introduce additional training overhead. In this paper, we show that SNNs can be naturally deconstructed into multiple submodels for efficient self-distillation. We treat each timestep instance of the SNN as a submodel and evaluate its output confidence, thus efficiently identifying the strong and the weak. Based on this strong and weak relationship, we propose two efficient self-distillation schemes: (1) \textbf{Strong2Weak}: During training, the stronger "teacher" guides the weaker "student", effectively improving overall performance. (2) \textbf{Weak2Strong}: The weak serve as the "teacher", distilling the strong in reverse with underlying dark knowledge, again yielding significant performance gains. For both distillation schemes, we offer flexible implementations such as ensemble, simultaneous, and cascade distillation. Experiments show that our method effectively improves the discriminability and overall performance of the SNN, while its adversarial robustness is also enhanced, benefiting from the stability brought by self-distillation. This ingeniously exploits the temporal properties of SNNs and provides insight into how to efficiently train high-performance SNNs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets and Explainable AI (XAI): A Comparative Analysis</td>
<td style='padding: 6px;'>Ming Jie Ong, Sze Yinn Ung, Sim Kuan Goh, Jimmy Y. Zhong</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.07785v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The current study investigated the use of Explainable Artificial Intelligence (XAI) to improve the accuracy of brain tumor segmentation in MRI images, with the goal of assisting physicians in clinical decision-making. The study focused on applying UNet models for brain tumor segmentation and using the XAI techniques of Gradient-weighted Class Activation Mapping (Grad-CAM) and attention-based visualization to enhance the understanding of these models. Three deep learning models - UNet, Residual UNet (ResUNet), and Attention UNet (AttUNet) - were evaluated to identify the best-performing model. XAI was employed with the aims of clarifying model decisions and increasing physicians' trust in these models. We compared the performance of two UNet variants (ResUNet and AttUNet) with the conventional UNet in segmenting brain tumors from the BraTS2020 public dataset and analyzed model predictions with Grad-CAM and attention-based visualization. Using the latest computer hardware, we trained and validated each model using the Adam optimizer and assessed their performance with respect to: (i) training, validation, and inference times, (ii) segmentation similarity coefficients and loss functions, and (iii) classification performance. Notably, during the final testing phase, ResUNet outperformed the other models with respect to Dice and Jaccard similarity scores, as well as accuracy, recall, and F1 scores. Grad-CAM provided visuospatial insights into the tumor subregions each UNet model focused on while attention-based visualization provided valuable insights into the working mechanisms of AttUNet's attention modules. These results demonstrated ResUNet as the best-performing model and we conclude by recommending its use for automated brain tumor segmentation in future clinical assessments. Our source code and checkpoint are available at https://github.com/ethanong98/MultiModel-XAI-Brats2020</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>Mitigating Subject Dependency in EEG Decoding with Subject-Specific Low-Rank Adapters</td>
<td style='padding: 6px;'>Timon Klein, Piotr Minakowski, Sebastian Sager</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08059v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Subject-specific distribution shifts represent an important obstacle to the development of foundation models for EEG decoding. To address this, we propose Subject-Conditioned Layer,, an adaptive layer designed as a drop-in replacement for standard linear or convolutional layers in any neural network architecture. Our layer captures subject-specific variability by decomposing its weights into a shared, subject-invariant component and a lightweight, low-rank correction unique to each subject. This explicit separation of general knowledge from personalized adaptation allows existing models to become robust to subject shifts. Empirically, models equipped with our layer outperform both a shared-weight-only model (subject-agnostic model) and the average of individually trained subject-specific models. Consequently, the Subject-Conditioned Layer, offers a practical and scalable path towards building effective cross-subject foundation models for EEG.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>A Systematic Evaluation of Self-Supervised Learning for Label-Efficient Sleep Staging with Wearable EEG</td>
<td style='padding: 6px;'>Emilio Estevan, María Sierra-Torralba, Eduardo López-Larraz, Luis Montesano</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.07960v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Wearable EEG devices have emerged as a promising alternative to polysomnography (PSG). As affordable and scalable solutions, their widespread adoption results in the collection of massive volumes of unlabeled data that cannot be analyzed by clinicians at scale. Meanwhile, the recent success of deep learning for sleep scoring has relied on large annotated datasets. Self-supervised learning (SSL) offers an opportunity to bridge this gap, leveraging unlabeled signals to address label scarcity and reduce annotation effort. In this paper, we present the first systematic evaluation of SSL for sleep staging using wearable EEG. We investigate a range of well-established SSL methods and evaluate them on two sleep databases acquired with the Ikon Sleep wearable EEG headband: BOAS, a high-quality benchmark containing PSG and wearable EEG recordings with consensus labels, and HOGAR, a large collection of home-based, self-recorded, and unlabeled recordings. Three evaluation scenarios are defined to study label efficiency, representation quality, and cross-dataset generalization. Results show that SSL consistently improves classification performance by up to 10% over supervised baselines, with gains particularly evident when labeled data is scarce. SSL achieves clinical-grade accuracy above 80% leveraging only 5% to 10% of labeled data, while the supervised approach requires twice the labels. Additionally, SSL representations prove robust to variations in population characteristics, recording environments, and signal quality. Our findings demonstrate the potential of SSL to enable label-efficient sleep staging with wearable EEG, reducing reliance on manual annotations and advancing the development of affordable sleep monitoring systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-08</td>
<td style='padding: 8px;'>EEG Sleep Stage Classification with Continuous Wavelet Transform and Deep Learning</td>
<td style='padding: 6px;'>Mehdi Zekriyapanah Gashti, Ghasem Farjamnia</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.07524v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurate classification of sleep stages is crucial for the diagnosis and management of sleep disorders. Conventional approaches for sleep scoring rely on manual annotation or features extracted from EEG signals in the time or frequency domain. This study proposes a novel framework for automated sleep stage scoring using time-frequency analysis based on the wavelet transform. The Sleep-EDF Expanded Database (sleep-cassette recordings) was used for evaluation. The continuous wavelet transform (CWT) generated time-frequency maps that capture both transient and oscillatory patterns across frequency bands relevant to sleep staging. Experimental results demonstrate that the proposed wavelet-based representation, combined with ensemble learning, achieves an overall accuracy of 88.37 percent and a macro-averaged F1 score of 73.15, outperforming conventional machine learning methods and exhibiting comparable or superior performance to recent deep learning approaches. These findings highlight the potential of wavelet analysis for robust, interpretable, and clinically applicable sleep stage classification.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-08</td>
<td style='padding: 8px;'>Autoencoding Coordinate Sequences from Psychophysiologic Signals</td>
<td style='padding: 6px;'>Timothy L. Hutcheson, Anil K. Raj</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.07415v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We present a method for converting 24 channels of psychophysiologic time series data collected from individual participants via electroencephalogram (EEG), electrocardiogram (ECG), electrodermal activity (EDA), respiration rate (RR) into trackable three dimensional (3D) coordinates sufficient to estimate participation in specific task and cognitive states.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-07</td>
<td style='padding: 8px;'>Retrieving the structure of probabilistic sequences from EEG data during the goalkeeper game</td>
<td style='padding: 6px;'>P. R. Cabral-Passos, P. S. Azevedo, V. H. Moraes, B. L. Ramalho, A. Duarte, C. D. Vargas</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.06344v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This work draws on the conjecture that fingerprints of stochastic event sequences can be retrieved from electroencephalographic data (EEG) recorded during a behavioral task. To test this, we used the Goalkeeper Game (game.numec.prp.usp.br). Acting as a goalkeeper, the participant predicted each kick in a probabilistic sequence while EEG activity was recorded. At each trial, driven by a context tree, the kicker chose one of three options: left, center, or right. The goalkeeper then predicted the next kick by pressing a button. Tree estimation was performed by applying the Context Algorithm to EEG segments locked to the button press (-300 to 0 ms). We calculated the distance between the penalty taker's tree and the trees retrieved per participant and electrode. This metric was then correlated with the goalkeeper's success rates. We observed a clear reduction in the overall distance distribution over time for a subset of electrodes, indicating that EEG dependencies become more congruent with the penalty taker's tree as the goalkeeper learns the sequence. This distance is inversely proportional to the goalkeepers' success rates, indicating a clear relationship between performance and the neural signatures associated with the sequence structure.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-07</td>
<td style='padding: 8px;'>Efficient Coherence Inference Using the Demodulated Band Transform and a Generalized Linear Model</td>
<td style='padding: 6px;'>Md Rakibul Mowla, Sukhbinder Kumar, Ariane E. Rhone, Brian J. Dlouhy, Christopher K. Kovach</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.05559v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Statistical significance testing of neural coherence is essential for distinguishing genuine cross-signal coupling from spurious correlations. A widely accepted approach uses surrogate-based inference, where null distributions are generated via time-shift or phase-randomization procedures. While effective, these methods are computationally expensive and yield discrete p-values that can be unstable near decision thresholds, limiting scalability to large EEG/iEEG datasets. We introduce and validate a parametric alternative based on a generalized linear model (GLM) applied to complex-valued time--frequency coefficients (e.g., from DBT or STFT), using a likelihood-ratio test. Using real respiration belt traces as a driver and simulated neural signals contaminated with broadband Gaussian noise, we perform dense sweeps of ground-truth coherence and compare GLM-based inference against time-shift/phase-randomized surrogate testing under matched conditions. GLM achieved comparable or superior sensitivity while producing continuous, stable p-values and a substantial computational advantage. At 80% detection power, GLM detects at C=0.25, whereas surrogate testing requires C=0.49, corresponding to an approximately 6--7 dB SNR improvement. Runtime benchmarking showed GLM to be nearly 200x faster than surrogate approaches. These results establish GLM-based inference on complex time--frequency coefficients as a robust, scalable alternative to surrogate testing, enabling efficient analysis of large EEG/iEEG datasets across channels, frequencies, and participants.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-07</td>
<td style='padding: 8px;'>EEG-Based Acute Pain Classification: Machine Learning Model Comparison and Real-Time Clinical Feasibility</td>
<td style='padding: 6px;'>Aavid Mathrawala, Dhruv Kurup, Josie Lau</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.05511v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Current pain assessment within hospitals often relies on self-reporting or non-specific EKG vital signs. This system leaves critically ill, sedated, and cognitively impaired patients vulnerable to undertreated pain and opioid overuse. Electroencephalography (EEG) offers a noninvasive method of measuring brain activity. This technology could potentially be applied as an assistive tool to highlight nociceptive processing in order to mitigate this issue. In this study, we compared machine learning models for classifying high-pain versus low/no-pain EEG epochs using data from fifty-two healthy adults exposed to laser-evoked pain at three intensities (low, medium, high). Each four-second epoch was transformed into a 537-feature vector spanning spectral power, band ratios, Hjorth parameters, entropy measures, coherence, wavelet energies, and peak-frequency metrics. Nine traditional machine learning models were evaluated with leave-one-participant-out cross-validation. A support vector machine with radial basis function kernel achieved the best offline performance with 88.9% accuracy and sub-millisecond inference time (1.02 ms). Our Feature importance analysis was consistent with current canonical pain physiology, showing contralateral alpha suppression, midline theta/alpha enhancement, and frontal gamma bursts. The real-time XGBoost model maintained an end-to-end latency of about 4 ms and 94.2% accuracy, demonstrating that an EEG-based pain monitor is technically feasible within a clinical setting and provides a pathway towards clinical validation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-06</td>
<td style='padding: 8px;'>CLAd-VR: Cognitive Load-based Adaptive Training for Machining Tasks in Virtual Reality</td>
<td style='padding: 6px;'>Bhavya Matam, Adamay Mann, Kachina Studer, Christian Gabbianelli, Sonia Castelo, John Liu, Claudio Silva, Dishita Turakhia</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.05249v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>With the growing need to effectively support workforce upskilling in the manufacturing sector, virtual reality is gaining popularity as a scalable training solution. However, most current systems are designed as static, step-by-step tutorials and do not adapt to a learner's needs or cognitive load, which is a critical factor in learning and longterm retention. We address this limitation with CLAd-VR, an adaptive VR training system that integrates realtime EEG-based sensing to measure the learner's cognitive load and adapt instruction accordingly, specifically for domain-specific tasks in manufacturing. The system features a VR training module for a precision drilling task, designed with multimodal instructional elements including animations, text, and video. Our cognitive load sensing pipeline uses a wearable EEG device to capture the trainee's neural activity, which is processed through an LSTM model to classify their cognitive load as low, optimal, or high in real time. Based on these classifications, the system dynamically adjusts task difficulty and delivers adaptive guidance using voice guidance, visual cues, or ghost hand animations. This paper introduces CLAd-VR system's architecture, including the EEG sensing hardware, real-time inference model, and adaptive VR interface.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-06</td>
<td style='padding: 8px;'>What your brain activity says about you: A review of neuropsychiatric disorders identified in resting-state and sleep EEG data</td>
<td style='padding: 6px;'>J. E. M. Scanlon, A. Pelzer, M. Gharleghi, K. C. Fuhrmeister, T. Köllmer, P. Aichroth, R. Göder, C. Hansen, K. I. Wolf</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.04984v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalogram monitoring devices and online data repositories hold large amounts of data from individuals participating in research and medical studies without direct reference to personal identifiers. This paper explores what types of personal and health information have been detected and classified within task-free EEG data. Additionally, we investigate key characteristics of the collected resting-state and sleep data, in order to determine the privacy risks involved with openly available EEG data. We used Google Scholar, Web of Science and searched relevant journals to find studies which classified or detected the presence of various disorders and personal information in resting state and sleep EEG. Only English full-text peer-reviewed journal articles or conference papers about classifying the presence of medical disorders between individuals were included. A quality analysis carried out by 3 reviewers determined general paper quality based on specified evaluation criteria. In resting state EEG, various disorders including Autism Spectrum Disorder, Parkinson's disease, and alcohol use disorder have been classified with high classification accuracy, often requiring only 5 mins of data or less. Sleep EEG tends to hold classifiable information about sleep disorders such as sleep apnea, insomnia, and REM sleep disorder, but usually involve longer recordings or data from multiple sleep stages. Many classification methods are still developing but even today, access to a person's EEG can reveal sensitive personal health information. With an increasing ability of machine learning methods to re-identify individuals from their EEG data, this review demonstrates the importance of anonymization, and the development of improved tools for keeping study participants and medical EEG users' privacy safe.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-06</td>
<td style='padding: 8px;'>Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI</td>
<td style='padding: 6px;'>Youngjoon Lee, Seongmin Cho, Yehhyun Jo, Jinu Gong, Hyunjoo Jenny Lee, Joonhyuk Kang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.04622v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The limited data availability due to strict privacy regulations and significant resource demands severely constrains biomedical time-series AI development, which creates a critical gap between data requirements and accessibility. Synthetic data generation presents a promising solution by producing artificial datasets that maintain the statistical properties of real biomedical time-series data without compromising patient confidentiality. We propose a framework for synthetic biomedical time-series data generation based on advanced forecasting models that accurately replicates complex electrophysiological signals such as EEG and EMG with high fidelity. These synthetic datasets preserve essential temporal and spectral properties of real data, which enables robust analysis while effectively addressing data scarcity and privacy challenges. Our evaluations across multiple subjects demonstrate that the generated synthetic data can serve as an effective substitute for real data and also significantly boost AI model performance. The approach maintains critical biomedical features while provides high scalability for various applications and integrates seamlessly into open-source repositories, substantially expanding resources for AI-driven biomedical research.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>Optimizing BCI Rehabilitation Protocols for Stroke: Exploring Task Design and Training Duration</td>
<td style='padding: 6px;'>Aniana Cruz, Marko Kuzmanoski, Gabriel Pires</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08082v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Stroke is a leading cause of long-term disability and the second most common cause of death worldwide. Although acute treatments have advanced, recovery remains challenging and limited. Brain-computer interfaces (BCIs) have emerged as a promising tool for post-stroke rehabilitation by promoting neuroplasticity. However, clinical outcomes remain variable, and optimal protocols have yet to be established. This study explores strategies to optimize BCI-based rehabilitation by comparing motor imagery of affected hand movement versus rest, instead of the conventional left-versus-right motor imagery. This alternative aims to simplify the task and address the weak contralateral activation commonly observed in stroke patients. Two datasets, one from healthy individuals and one from stroke patients, were used to evaluate the proposed approach. The results showed improved performance using both FBCSP and EEGNet. Additionally, we investigated the impact of session duration and found that shorter training sessions produced better BCI performance than longer sessions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-01</td>
<td style='padding: 8px;'>NeuroTTT: Bridging Pretraining-Downstream Task Misalignment in EEG Foundation Models via Test-Time Training</td>
<td style='padding: 6px;'>Suli Wang, Yangshen Deng, Zhenghua Bao, Xinyu Zhan, Yiqun Duan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.26301v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large-scale foundation models for EEG signals offer a promising path to generalizable brain-computer interface (BCI) applications, but they often suffer from misalignment between pretraining objectives and downstream tasks, as well as significant cross-subject distribution shifts. This paper addresses these challenges by introducing a two-stage alignment strategy that bridges the gap between generic pretraining and specific EEG decoding tasks. First, we propose NeuroTTT: a domain-specific self-supervised fine-tuning paradigm that augments the foundation model with task-relevant self-supervised objectives, aligning latent representations to important spectral, spatial, and temporal EEG features without requiring additional labeled data. Second, we incorporate test-time training (TTT) at inference, we perform (i) self-supervised test-time training on individual unlabeled test samples and (ii) prediction entropy minimization (Tent), which updates only normalization statistics to continually calibrate the model to each new input on the fly. Our approach, which, to our knowledge, is the first to unify domain-tuned self-supervision with test-time training in large-scale EEG foundation models, yields substantially improved robustness and accuracy across diverse BCI tasks (imagined speech, stress detection, motor imagery). Using CBraMod and LaBraM as backbones, our method pushes their performance to a markedly higher level. Results on three diverse tasks demonstrate that the proposed alignment strategy achieves state-of-the-art performance, outperforming conventional fine-tuning and adaptation methods. Our code is available at https://github.com/wsl2000/NeuroTTT.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-30</td>
<td style='padding: 8px;'>EEG-based AI-BCI Wheelchair Advancement: Hybrid Deep Learning with Motor Imagery for Brain Computer Interface</td>
<td style='padding: 6px;'>Bipul Thapa, Biplov Paneru, Bishwash Paneru, Khem Narayan Poudyal</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.25667v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper presents an Artificial Intelligence (AI) integrated novel approach to Brain-Computer Interface (BCI)-based wheelchair development, utilizing a motor imagery right-left-hand movement mechanism for control. The system is designed to simulate wheelchair navigation based on motor imagery right and left-hand movements using electroencephalogram (EEG) data. A pre-filtered dataset, obtained from an open-source EEG repository, was segmented into arrays of 19x200 to capture the onset of hand movements. The data was acquired at a sampling frequency of 200Hz. The system integrates a Tkinter-based interface for simulating wheelchair movements, offering users a functional and intuitive control system. We propose a BiLSTM-BiGRU model that shows a superior test accuracy of 92.26% as compared with various machine learning baseline models, including XGBoost, EEGNet, and a transformer-based model. The Bi-LSTM-BiGRU attention-based model achieved a mean accuracy of 90.13% through cross-validation, showcasing the potential of attention mechanisms in BCI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-29</td>
<td style='padding: 8px;'>A Robust Multi-Scale Framework with Test-Time Adaptation for sEEG-Based Speech Decoding</td>
<td style='padding: 6px;'>Suli Wang, Yang-yang Li, Siqi Cai, Haizhou Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.24700v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Decoding speech from stereo-electroencephalography (sEEG) signals has emerged as a promising direction for brain-computer interfaces (BCIs). Its clinical applicability, however, is limited by the inherent non-stationarity of neural signals, which causes domain shifts between training and testing, undermining decoding reliability. To address this challenge, a two-stage framework is proposed for enhanced robustness. First, a multi-scale decomposable mixing (MDM) module is introduced to model the hierarchical temporal dynamics of speech production, learning stable multi-timescale representations from sEEG signals. Second, a source-free online test-time adaptation (TTA) method performs entropy minimization to adapt the model to distribution shifts during inference. Evaluations on the public DU-IN spoken word decoding benchmark show that the approach outperforms state-of-the-art models, particularly in challenging cases. This study demonstrates that combining invariant feature learning with online adaptation is a principled strategy for developing reliable BCI systems. Our code is available at https://github.com/lyyi599/MDM-TENT.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-29</td>
<td style='padding: 8px;'>ELASTIQ: EEG-Language Alignment with Semantic Task Instruction and Querying</td>
<td style='padding: 6px;'>Muyun Jiang, Shuailei Zhang, Zhenjie Yang, Mengjun Wu, Weibang Jiang, Zhiwei Guo, Wei Zhang, Rui Liu, Shangen Zhang, Yong Li, Yi Ding, Cuntai Guan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.24302v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent advances in electroencephalography (EEG) foundation models, which capture transferable EEG representations, have greatly accelerated the development of brain-computer interfaces (BCI). However, existing approaches still struggle to incorporate language instructions as prior constraints for EEG representation learning, limiting their ability to leverage the semantic knowledge inherent in language to unify different labels and tasks. To address this challenge, we present ELASTIQ, a foundation model for EEG-Language Alignment with Semantic Task Instruction and Querying. ELASTIQ integrates task-aware semantic guidance to produce structured and linguistically aligned EEG embeddings, thereby enhancing decoding robustness and transferability. In the pretraining stage, we introduce a joint Spectral-Temporal Reconstruction (STR) module, which combines frequency masking as a global spectral perturbation with two complementary temporal objectives: random masking to capture contextual dependencies and causal masking to model sequential dynamics. In the instruction tuning stage, we propose the Instruction-conditioned Q-Former (IQF), a query-based cross-attention transformer that injects instruction embeddings into EEG tokens and aligns them with textual label embeddings through learnable queries. We evaluate ELASTIQ on 20 datasets spanning motor imagery, emotion recognition, steady-state visual evoked potentials, covert speech, and healthcare tasks. ELASTIQ achieves state-of-the-art performance on 14 of the 20 datasets and obtains the best average results across all five task categories. Importantly, our analyses reveal for the first time that explicit task instructions serve as semantic priors guiding EEG embeddings into coherent and linguistically grounded spaces. The code and pre-trained weights will be released.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-27</td>
<td style='padding: 8px;'>EEG-Based Framework for Reflexive and Perceptual Assessment in CLIS: Preliminary Study in Healthy Volunteers</td>
<td style='padding: 6px;'>Nicoli Leal, Rute Bettencourt, Urbano J. Nunes, Gabriel Pires</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.23524v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Despite the general assumption that completely locked-in state (CLIS) patients remain conscious and aware of their environment, the effectiveness of brain-computer interfaces (BCIs) in facilitating communication has been limited, as reported both in the literature and in our own findings. This limitation is likely attributable to impairments in executive functions, working memory, and vigilance, which appear to hinder the establishment of reliable BCI-based communication. The main goal of this research is to develop a neurophysiological report designed to support the evaluation of the cognitive state of these individuals and determine their ability to interact with BCIs. To achieve this, we designed a set of paradigms to assess CLIS patients at the reflexive and perceptual levels, based on neural responses associated with sensory and perceptual processing, including Mismatch Negativity (MMN), Steady State Auditory Evoked Potential (SSAEP), and Steady State Visual Evoked Potential (SSVEP). Pilot testing with five healthy participants demonstrates the feasibility of generating a neurophysiological report for cognitive assessment at both levels.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-27</td>
<td style='padding: 8px;'>Theoretical framework of passive ME antenna arrays enabling in-vivo monitoring: A pathway to smart implants</td>
<td style='padding: 6px;'>Kalpesh Jaykar, Prasanth Velvaluri, Nian X. Sun, Richard D. James</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.23520v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A new brain-computer interface (BCI) technology, deployed through minimally invasive surgery, is changing the way we think about treating severe neurological conditions. The central idea is to place a device called Stentrode in the brain's vasculature, which enables neuromodulation and helps patients regain the ability to communicate. However, in such devices, the battery and electronics are wired and could introduce damage or implant malfunction. In these cases, a Stentrode integrated with magnetoelectric (ME) antennas could be of great interest. ME antennas offer significant advantages over traditional antennas, leveraging acoustic resonance rather than electromagnetic resonance to achieve a size reduction of up to five orders of magnitude. In addition to their compactness and immunity to ground-plane interference, ME antennas could be adopted for use in vascular implants, such as coronary stents, potentially enabling minimally invasive monitoring and communication. Despite these advantages, a single antenna embedded in the implant may be constrained by the limited volume of magnetostrictive material, which could result in low output gain. To address this gain limitation, we propose using antenna arrays designed to produce constructive interference at a designated far-field point, ideally located outside the patient, to enhance signal transmission and receiving capabilities. We develop a mathematical model to represent the antennas and optimize their spatial arrangement and phase synchronization. Simulations based on this model demonstrate promising high-gain performance at the prescribed far-field location through phase manipulation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-27</td>
<td style='padding: 8px;'>Eye-Tracking and BCI Integration for Assistive Communication in Locked-In Syndrome: Pilot Study with Healthy Participants</td>
<td style='padding: 6px;'>Ana Patrícia Pinto, Rute Bettencourt, Urbano J. Nunes, Gabriel Pires</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.23518v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Patients with Amyotrophic Lateral Sclerosis (ALS) progressively lose voluntary motor control, often leading to a Locked-In State (LIS), or in severe cases, a Completely Locked-in State (CLIS). Eye-tracking (ET) systems are common communication tools in early LIS but become ineffective as oculomotor function declines. EEG-based Brain-Computer Interfaces (BCIs) offer a non-muscular communication alternative, but delayed adoption may reduce performance due to diminished goal-directed thinking. This study presents a preliminary hybrid BCI framework combining ET and BCI to support a gradual transition between modalities. A group of five healthy participants tested a modified P300-based BCI. Gaze and EEG data were processed in real time, and an ET-BCI fusion algorithm was proposed to enhance detection of user intention. Results indicate that combining both modalities may maintain high accuracy and offers insights on how to potentially improve communication continuity for patients transitioning from LIS to CLIS.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-27</td>
<td style='padding: 8px;'>Explicit modelling of subject dependency in BCI decoding</td>
<td style='padding: 6px;'>Michele Romani, Francesco Paissan, Andrea Fossà, Elisabetta Farella</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.23247v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-Computer Interfaces (BCIs) suffer from high inter-subject variability and limited labeled data, often requiring lengthy calibration phases. In this work, we present an end-to-end approach that explicitly models the subject dependency using lightweight convolutional neural networks (CNNs) conditioned on the subject's identity. Our method integrates hyperparameter optimization strategies that prioritize class imbalance and evaluates two conditioning mechanisms to adapt pre-trained models to unseen subjects with minimal calibration data. We benchmark three lightweight architectures on a time-modulated Event-Related Potentials (ERP) classification task, providing interpretable evaluation metrics and explainable visualizations of the learned representations. Results demonstrate improved generalization and data-efficient calibration, highlighting the scalability and practicality of subject-adaptive BCIs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-26</td>
<td style='padding: 8px;'>BrainPro: Towards Large-scale Brain State-aware EEG Representation Learning</td>
<td style='padding: 6px;'>Yi Ding, Muyun Jiang, Weibang Jiang, Shuailei Zhang, Xinliang Zhou, Chenyu Liu, Shanglin Li, Yong Li, Cuntai Guan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.22050v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) is a non-invasive technique for recording brain electrical activity, widely used in brain-computer interface (BCI) and healthcare. Recent EEG foundation models trained on large-scale datasets have shown improved performance and generalizability over traditional decoding methods, yet significant challenges remain. Existing models often fail to explicitly capture channel-to-channel and region-to-region interactions, which are critical sources of information inherently encoded in EEG signals. Due to varying channel configurations across datasets, they either approximate spatial structure with self-attention or restrict training to a limited set of common channels, sacrificing flexibility and effectiveness. Moreover, although EEG datasets reflect diverse brain states such as emotion, motor, and others, current models rarely learn state-aware representations during self-supervised pre-training. To address these gaps, we propose BrainPro, a large EEG model that introduces a retrieval-based spatial learning block to flexibly capture channel- and region-level interactions across varying electrode layouts, and a brain state-decoupling block that enables state-aware representation learning through parallel encoders with decoupling and region-aware reconstruction losses. This design allows BrainPro to adapt seamlessly to diverse tasks and hardware settings. Pre-trained on an extensive EEG corpus, BrainPro achieves state-of-the-art performance and robust generalization across nine public BCI datasets. Our codes and the pre-trained weights will be released.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-07</td>
<td style='padding: 8px;'>Beyond Grid-Locked Voxels: Neural Response Functions for Continuous Brain Encoding</td>
<td style='padding: 6px;'>Haomiao Chen, Keith W Jamison, Mert R. Sabuncu, Amy Kuceyeski</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.07342v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neural encoding models aim to predict fMRI-measured brain responses to natural images. fMRI data is acquired as a 3D volume of voxels, where each voxel has a defined spatial location in the brain. However, conventional encoding models often flatten this volume into a 1D vector and treat voxel responses as independent outputs. This removes spatial context, discards anatomical information, and ties each model to a subject-specific voxel grid. We introduce the Neural Response Function (NRF), a framework that models fMRI activity as a continuous function over anatomical space rather than a flat vector of voxels. NRF represents brain activity as a continuous implicit function: given an image and a spatial coordinate (x, y, z) in standardized MNI space, the model predicts the response at that location. This formulation decouples predictions from the training grid, supports querying at arbitrary spatial resolutions, and enables resolution-agnostic analyses. By grounding the model in anatomical space, NRF exploits two key properties of brain responses: (1) local smoothness -- neighboring voxels exhibit similar response patterns; modeling responses continuously captures these correlations and improves data efficiency, and (2) cross-subject alignment -- MNI coordinates unify data across individuals, allowing a model pretrained on one subject to be fine-tuned on new subjects. In experiments, NRF outperformed baseline models in both intrasubject encoding and cross-subject adaptation, achieving high performance while reducing the data size needed by orders of magnitude. To our knowledge, NRF is the first anatomically aware encoding model to move beyond flattened voxels, learning a continuous mapping from images to brain responses in 3D space.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-06</td>
<td style='padding: 8px;'>Dynamic Functional Connectivity Features for Brain State Classification: Insights from the Human Connectome Project</td>
<td style='padding: 6px;'>Valeriya Kirova, Dzerassa Kadieva, Daniil Vlasenko, Isak B. Blank, Fedor Ratnikov</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.05325v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We analyze functional magnetic resonance imaging (fMRI) data from the Human Connectome Project (HCP) to match brain activities during a range of cognitive tasks. Our findings demonstrate that even basic linear machine learning models can effectively classify brain states and achieve state-of-the-art accuracy, particularly for tasks related to motor functions and language processing. Feature importance ranking allows to identify distinct sets of brain regions whose activation patterns are uniquely associated with specific cognitive functions. These discriminative features provide strong support for the hypothesis of functional specialization across cortical and subcortical areas of the human brain.   Additionally, we investigate the temporal dynamics of the identified brain regions, demonstrating that the time-dependent structure of fMRI signals are essential for shaping functional connectivity between regions: uncorrelated areas are least important for classification. This temporal perspective provides deeper insights into the formation and modulation of brain neural networks involved in cognitive processing.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-06</td>
<td style='padding: 8px;'>Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing</td>
<td style='padding: 6px;'>Xuanhua Yin, Runkai Zhao, Weidong Cai</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.04670v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic Framework for Multimodal fMRI Response Encoding), an agnostic interface that standardizes time-aligned post-fusion tokens from varied encoders, and MIND, a plug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating. Trained end-to-end for whole-brain prediction, AFIRE decouples the decoder from upstream fusion, while MIND combines token-dependent Top-K sparse routing with a subject prior to personalize expert usage without sacrificing generality. Experiments across multiple multimodal backbones and subjects show consistent improvements over strong baselines, enhanced cross-subject generalization, and interpretable expert patterns that correlate with content type. The framework offers a simple attachment point for new encoders and datasets, enabling robust, plug-and-improve performance for naturalistic neuroimaging studies.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-05</td>
<td style='padding: 8px;'>Adapting HFMCA to Graph Data: Self-Supervised Learning for Generalizable fMRI Representations</td>
<td style='padding: 6px;'>Jakub Frac, Alexander Schmatz, Qiang Li, Guido Van Wingen, Shujian Yu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.05177v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional magnetic resonance imaging (fMRI) analysis faces significant challenges due to limited dataset sizes and domain variability between studies. Traditional self-supervised learning methods inspired by computer vision often rely on positive and negative sample pairs, which can be problematic for neuroimaging data where defining appropriate contrasts is non-trivial. We propose adapting a recently developed Hierarchical Functional Maximal Correlation Algorithm (HFMCA) to graph-structured fMRI data, providing a theoretically grounded approach that measures statistical dependence via density ratio decomposition in a reproducing kernel Hilbert space (RKHS),and applies HFMCA-based pretraining to learn robust and generalizable representations. Evaluations across five neuroimaging datasets demonstrate that our adapted method produces competitive embeddings for various classification tasks and enables effective knowledge transfer to unseen datasets. Codebase and supplementary material can be found here: https://github.com/fr30/mri-eigenencoder</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-03</td>
<td style='padding: 8px;'>Neural Correlates of Language Models Are Specific to Human Language</td>
<td style='padding: 6px;'>Iñigo Parra</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.03156v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Previous work has shown correlations between the hidden states of large language models and fMRI brain responses, on language tasks. These correlations have been taken as evidence of the representational similarity of these models and brain states. This study tests whether these previous results are robust to several possible concerns. Specifically this study shows: (i) that the previous results are still found after dimensionality reduction, and thus are not attributable to the curse of dimensionality; (ii) that previous results are confirmed when using new measures of similarity; (iii) that correlations between brain representations and those from models are specific to models trained on human language; and (iv) that the results are dependent on the presence of positional encoding in the models. These results confirm and strengthen the results of previous research and contribute to the debate on the biological plausibility and interpretability of state-of-the-art large language models.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-03</td>
<td style='padding: 8px;'>BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck for Functional Brain Biomarkers in Schizophrenia</td>
<td style='padding: 6px;'>Tianzheng Hu, Qiang Li, Shu Liu, Vince D. Calhoun, Guido van Wingen, Shujian Yu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.03004v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The development of diagnostic models is gaining traction in the field of psychiatric disorders. Recently, machine learning classifiers based on resting-state functional magnetic resonance imaging (rs-fMRI) have been developed to identify brain biomarkers that differentiate psychiatric disorders from healthy controls. However, conventional machine learning-based diagnostic models often depend on extensive feature engineering, which introduces bias through manual intervention. While deep learning models are expected to operate without manual involvement, their lack of interpretability poses significant challenges in obtaining explainable and reliable brain biomarkers to support diagnostic decisions, ultimately limiting their clinical applicability. In this study, we introduce an end-to-end innovative graph neural network framework named BrainIB++, which applies the information bottleneck (IB) principle to identify the most informative data-driven brain regions as subgraphs during model training for interpretation. We evaluate the performance of our model against nine established brain network classification methods across three multi-cohort schizophrenia datasets. It consistently demonstrates superior diagnostic accuracy and exhibits generalizability to unseen data. Furthermore, the subgraphs identified by our model also correspond with established clinical biomarkers in schizophrenia, particularly emphasizing abnormalities in the visual, sensorimotor, and higher cognition brain functional network. This alignment enhances the model's interpretability and underscores its relevance for real-world diagnostic applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-02</td>
<td style='padding: 8px;'>Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks</td>
<td style='padding: 6px;'>Song Wang, Zhenyu Lei, Zhen Tan, Jundong Li, Javier Rasero, Aiying Zhang, Chirag Agarwal</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.03351v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Nearly one in five adolescents currently live with a diagnosed mental or behavioral health condition, such as anxiety, depression, or conduct disorder, underscoring the urgency of developing accurate and interpretable diagnostic tools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a powerful lens into large-scale functional connectivity, where brain regions are modeled as nodes and inter-regional synchrony as edges, offering clinically relevant biomarkers for psychiatric disorders. While prior works use graph neural network (GNN) approaches for disorder prediction, they remain complex black-boxes, limiting their reliability and clinical translation. In this work, we propose CONCEPTNEURO, a concept-based diagnosis framework that leverages large language models (LLMs) and neurobiological domain knowledge to automatically generate, filter, and encode interpretable functional connectivity concepts. Each concept is represented as a structured subgraph linking specific brain regions, which are then passed through a concept classifier. Our design ensures predictions through clinically meaningful connectivity patterns, enabling both interpretability and strong predictive performance. Extensive experiments across multiple psychiatric disorder datasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform their vanilla counterparts, improving accuracy while providing transparent, clinically aligned explanations. Furthermore, concept analyses highlight disorder-specific connectivity patterns that align with expert knowledge and suggest new hypotheses for future investigation, establishing CONCEPTNEURO as an interpretable, domain-informed framework for psychiatric disorder diagnosis.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-02</td>
<td style='padding: 8px;'>NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes</td>
<td style='padding: 6px;'>Shiyi Zhang, Dong Liang, Yihang Zhou</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.02266v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Reconstructing visual information from brain activity via computer vision technology provides an intuitive understanding of visual neural mechanisms. Despite progress in decoding fMRI data with generative models, achieving accurate cross-subject reconstruction of visual stimuli remains challenging and computationally demanding. This difficulty arises from inter-subject variability in neural representations and the brain's abstract encoding of core semantic features in complex visual inputs. To address these challenges, we propose NeuroSwift, which integrates complementary adapters via diffusion: AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter is trained on Stable Diffusion generated images paired with COCO captions to emulate higher visual cortex encoding. For cross-subject generalization, we pretrain on one subject and then fine-tune only 17 percent of parameters (fully connected layers) for new subjects, while freezing other components. This enables state-of-the-art performance with only one hour of training per subject on lightweight GPUs (three RTX 4090), and it outperforms existing methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-03</td>
<td style='padding: 8px;'>VarCoNet: A variability-aware self-supervised framework for functional connectome extraction from resting-state fMRI</td>
<td style='padding: 6px;'>Charalampos Lamprou, Aamna Alshehhi, Leontios J. Hadjileontiadis, Mohamed L. Seghier</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.02120v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accounting for inter-individual variability in brain function is key to precision medicine. Here, by considering functional inter-individual variability as meaningful data rather than noise, we introduce VarCoNet, an enhanced self-supervised framework for robust functional connectome (FC) extraction from resting-state fMRI (rs-fMRI) data. VarCoNet employs self-supervised contrastive learning to exploit inherent functional inter-individual variability, serving as a brain function encoder that generates FC embeddings readily applicable to downstream tasks even in the absence of labeled data. Contrastive learning is facilitated by a novel augmentation strategy based on segmenting rs-fMRI signals. At its core, VarCoNet integrates a 1D-CNN-Transformer encoder for advanced time-series processing, enhanced with a robust Bayesian hyperparameter optimization. Our VarCoNet framework is evaluated on two downstream tasks: (i) subject fingerprinting, using rs-fMRI data from the Human Connectome Project, and (ii) autism spectrum disorder (ASD) classification, using rs-fMRI data from the ABIDE I and ABIDE II datasets. Using different brain parcellations, our extensive testing against state-of-the-art methods, including 13 deep learning methods, demonstrates VarCoNet's superiority, robustness, interpretability, and generalizability. Overall, VarCoNet provides a versatile and robust framework for FC analysis in rs-fMRI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-02</td>
<td style='padding: 8px;'>Stacked Regression using Off-the-shelf, Stimulus-tuned and Fine-tuned Neural Networks for Predicting fMRI Brain Responses to Movies (Algonauts 2025 Report)</td>
<td style='padding: 6px;'>Robert Scholz, Kunal Bagga, Christine Ahrends, Carlo Alberto Barbano</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.06235v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We present our submission to the Algonauts 2025 Challenge, where the goal is to predict fMRI brain responses to movie stimuli. Our approach integrates multimodal representations from large language models, video encoders, audio models, and vision-language models, combining both off-the-shelf and fine-tuned variants. To improve performance, we enhanced textual inputs with detailed transcripts and summaries, and we explored stimulus-tuning and fine-tuning strategies for language and vision models. Predictions from individual models were combined using stacked regression, yielding solid results. Our submission, under the team name Seinfeld, ranked 10th. We make all code and resources publicly available, contributing to ongoing efforts in developing multimodal encoding models for brain activity.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-07</td>
<td style='padding: 8px;'>The Gamma-ray Luminosity Function of Flat-Spectrum Radio Quasars</td>
<td style='padding: 6px;'>Garima Rajguru, Lea Marcotulli, Marco Ajello, Mattia Di Mauro, Meg Urry</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.05515v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We have utilized the largest sample of $\gamma$-ray selected Fermi flat-spectrum radio quasars (FSRQs) ever used (519 sources) to construct the luminosity function and its evolution through the cosmic history. In addition to spanning large redshift ($0<z\lesssim 4$) and luminosity ranges ($2.9\times10^{43}$ erg s$^{-1}$ - $7.3\times10^{48}$ erg s$^{-1}$), this sample also has a robust calculation of the detection efficiency associated with its observation, making its selection effects and biases well understood. We confirm that the local luminosity function is best explained by a double power law. The evolution of the luminosity function of FSRQs follows a luminosity-dependent density evolution. FSRQs experience positive evolution with their space density growing with increasing redshift up to a maximum redshift, after which the numbers decrease. This peak in redshift occurs at larger redshifts for higher luminosity sources and at lower redshifts for lower luminosity sources. We find an unexpected similarity between the luminosity function of FSRQs and that of BL Lacertae objects at intermediate luminosity. This could be a sign of a strong genetic link between the two blazar sub-classes or that BL Lac samples are contaminated by large amounts of FSRQs with their jets nearly perfectly aligned with our line of sight.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-18</td>
<td style='padding: 8px;'>UMind: A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding</td>
<td style='padding: 6px;'>Chengjian Xu, Yonghao Song, Zelin Liao, Haochuan Zhang, Qiong Wang, Qingqing Zheng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.14772v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Decoding visual information from time-resolved brain recordings, such as EEG and MEG, plays a pivotal role in real-time brain-computer interfaces. However, existing approaches primarily focus on direct brain-image feature alignment and are limited to single-task frameworks or task-specific models. In this paper, we propose a Unified MultItask Network for zero-shot M/EEG visual Decoding (referred to UMind), including visual stimulus retrieval, classification, and reconstruction, where multiple tasks mutually enhance each other. Our method learns robust neural-visual and semantic representations through multimodal alignment with both image and text modalities. The integration of both coarse and fine-grained texts enhances the extraction of these neural representations, enabling more detailed semantic and visual decoding. These representations then serve as dual conditional inputs to a pre-trained diffusion model, guiding visual reconstruction from both visual and semantic perspectives. Extensive evaluations on MEG and EEG datasets demonstrate the effectiveness, robustness, and biological plausibility of our approach in capturing spatiotemporal neural dynamics. Our approach sets a multitask pipeline for brain visual decoding, highlighting the synergy of semantic information in visual feature extraction.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Quantum-like representation of neuronal networks' activity: modeling "mental entanglement"</td>
<td style='padding: 6px;'>Andrei Khrennikov, Makiko Yamada</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.16253v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Quantum-like modeling (QLM) - quantum theory applications outside of physics - are intensively developed with applications in biology, cognition, psychology, and decision-making. For cognition, QLM should be distinguished from quantum reductionist models in the spirit of Hameroff and Penrose and well as Umezawa and Vitiello. QLM is not concerned with just quantum physical processes in the brain but also QL information processing by macroscopic neuronal structures. Although QLM of cognition and decision-making has seen some success, it suffers from a knowledge gap that exists between oscillatory neuronal network functioning in the brain and QL behavioral patterns. Recently, steps toward closing this gap have been taken using the generalized probability theory and prequantum classical statistical field theory (PCSFT) - a random field model beyond the complex Hilbert space formalism. PCSFT is used to move from the classical ``oscillatory cognition'' of the neuronal networks to QLM for decision.making. In this study, we addressed the most difficult problem within this construction: QLM for entanglement generation by classical networks, i.e., mental entanglement. We started with the observational approach to entanglement based on operator algebras describing local observables and bringing into being the tensor product structure in the space of QL states. Moreover, we applied the standard states entanglement approach: entanglement generation by spatially separated networks in the brain. Finally, we discussed possible future experiments on mental entanglement detection using the EEG/MEG technique.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-17</td>
<td style='padding: 8px;'>Multi-Source Neural Activity Indices and Spatial Filters for EEG/MEG Inverse Problem: An Extension to MNE-Python</td>
<td style='padding: 6px;'>Julia Jurkowska, Joanna Dreszer, Monika Lewandowska, Krzysztof Tołpa, Tomasz Piotrowski</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.14118v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurate EEG/MEG source localization is essential for understanding brain function, yet remains challenging because the inverse problem is inherently ill-posed. In spatial filtering (beamforming) approaches, single-source LCMV spatial filters, though widely used, suffer from source cancellation when sources are correlated - a common experimental scenario. Multi-source frameworks, such as the multi-source minimum-variance pseudo-unbiased reduced-rank (MV-PURE) method, offer improved reconstruction and robust neural activity indices, yet their adoption has been limited by incomplete theory and lack of accessible implementations. In this paper, we present a rigorous derivation of multi-source neural activity indices and spatial filters, establishing a complete analytical framework with automated parameter selection. The resulting compact algebraic forms enable straightforward implementation. To facilitate adoption, we provide a full implementation extending MNE-Python, along with an accompanying tutorial, and demonstrate its utility on EEG experimental data, highlighting the practical advantages of multi-source spatial filtering for source localization and reconstruction.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-23</td>
<td style='padding: 8px;'>MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning</td>
<td style='padding: 6px;'>Jiarui Chen, Yikeng Chen, Yingshuang Zou, Ye Huang, Peng Wang, Yuan Liu, Yujing Sun, Wenping Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.07021v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis technique, but its high memory consumption severely limits its applicability on edge devices. A growing number of 3DGS compression methods have been proposed to make 3DGS more efficient, yet most only focus on storage compression and fail to address the critical bottleneck of rendering memory. To address this problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that tackles this challenge by jointly optimizing two key factors: the total primitive number and the parameters per primitive, achieving unprecedented memory compression. Specifically, we replace the memory-intensive spherical harmonics with lightweight, arbitrarily oriented spherical Gaussian lobes as our color representations. More importantly, we propose a unified soft pruning framework that models primitive-number and lobe-number pruning as a single constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a 50% static VRAM reduction and a 40% rendering VRAM reduction compared to existing methods, while maintaining comparable rendering quality. Project page: https://megs-2.github.io/</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-15</td>
<td style='padding: 8px;'>A fast machine learning tool to predict the composition of astronomical ices from infrared absorption spectra</td>
<td style='padding: 6px;'>Andrés Megías, Izaskun Jiménez-Serra, François Dulieu, Julie Vitorino, Belén Maté, David Ciudad, Will R. M. Rocha, Marcos Martínez Jiménez, Jacobo Aguirre</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.04331v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Current observations taken by James Webb Space Telescope (JWST) allow us to observe the absorption features of icy mantles that cover interstellar dust grains, which are mainly composed of $\mathrm{H_2O}$, $\mathrm{CO}$, and $\mathrm{CO_2}$, along with other minor species. Thanks to its sensitivity and spectral resolution, JWST has the potential to observe ice features towards hundreds of sources at different stages along the process of star formation. However, identifying the spectral features of the different species and quantifying the ice composition is not trivial and requires complex spectroscopic analysis. We present Automatic Ice Composition Estimator (AICE), a new tool based on artificial neural networks. Based on the infrared (IR) ice absorption spectrum between 2.5 and 10 microns, AICE predicts the ice fractional composition in terms of $\mathrm{H_2O}$, $\mathrm{CO}$, $\mathrm{CO_2}$, $\mathrm{CH_3OH}$, $\mathrm{NH_3}$, and $\mathrm{CH_4}$. To train the model, we used hundreds of laboratory experiments of ice mixtures from different databases, which were reprocessed with baseline subtraction and normalisation. Once trained, AICE takes less than one second on a conventional computer to predict the ice composition associated with the observed IR absorption spectrum, with typical errors of $\sim$3 $\%$ in the species fraction. We tested its performance on two spectra reported towards the NIR38 and J110621 background stars observed within the JWST Ice Age program, demonstrating a good agreement with previous estimations of the ice composition. The fast and accurate performance of AICE enables the systematic analysis of hundreds of different ice spectra with a modest time investment. In addition, this model can be enhanced and re-trained with more laboratory data, improving the precision of the predictions and expanding the list of predicted species.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Towards a 384-channel magnetoencephalography system based on optically pumped magnetometers</td>
<td style='padding: 6px;'>Holly Schofield, Ryan M. Hill, Lukas Rier, Ewan Kennett, Gonzalo Reina Rivero, Joseph Gibson, Ashley Tyler, Zoe Tanner, Frank Worcester, Tyler Hayward, James Osborne, Cody Doyle, Vishal Shah, Elena Boto, Niall Holmes, Matthew J. Brookes</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03107v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Magnetoencephalography using optically pumped magnetometers (OPM-MEG) is gaining significant traction as a neuroimaging tool, with the potential for improved performance and practicality compared to conventional instrumentation. However, OPM-MEG has so far lagged conventional-MEG in terms of the number of independent measurements of magnetic field that can be made across the scalp (i.e. the number of channels). This is important since increasing channel count offers improvements in sensitivity, spatial resolution and coverage. Unfortunately, increasing channel count also poses significant technical and practical challenges. Here, we describe a new OPM-MEG array which exploits triaxial sensors and integrated miniaturised electronic control units to measure MEG data from up to 384 channels. We also introduce a high-speed calibration method to ensure the that the fields measured by this array are high fidelity. The system was validated using a phantom, with results showing that dipole localisation accuracy is better than 1 mm, and correlation between the measured magnetic fields and a dipole model is >0.998. We then demonstrate utility in human MEG acquisition: via measurement of visual gamma oscillations we demonstrate the improvements in sensitivity that are afforded by high channel density, and via a moviewatching paradigm we quantify improvements in spatial resolution. In sum, we show the first OPM-MEG system with a channel count larger than that of typical conventional MEG devices. This represents a significant step on the path towards OPMs becoming the sensor of choice for MEG measurement.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-29</td>
<td style='padding: 8px;'>Quantile Function-Based Models for Neuroimaging Classification Using Wasserstein Regression</td>
<td style='padding: 6px;'>Jie Li, Gary Green, Jian Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.21523v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We propose a novel quantile function-based approach for neuroimaging classification using Wasserstein-Fr\'echet regression, specifically applied to the detection of mild traumatic brain injury (mTBI) based on the MEG and MRI data. Conventional neuroimaging classification methods for mTBI detection typically extract summary statistics from brain signals across the different epochs, which may result in the loss of important distributional information, such as variance, skewness, kurtosis, etc. Our approach treats complete probability density functions of epoch space results as functional response variables within a Wasserstein-Fr\'echet regression framework, thereby preserving the full distributional characteristics of epoch results from $L_{1}$ minimum norm solutions. The global Wasserstein-Fr\'echet regression model incorporating covariates (age and gender) allows us to directly compare the distributional patterns between healthy control subjects and mTBI patients. The classification procedure computes Wasserstein distances between estimated quantile functions from control and patient groups, respectively. These distances are then used as the basis for diagnostic decisions. This framework offers a statistically principled approach to improving diagnostic accuracy in mTBI detection. In practical applications, the test accuracy on unseen data from Innovision IP's dataset achieves up to 98\%.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-25</td>
<td style='padding: 8px;'>Disentangling the Factors of Convergence between Brains and Computer Vision Models</td>
<td style='padding: 6px;'>Joséphine Raugel, Marc Szafraniec, Huy V. Vo, Camille Couprie, Patrick Labatut, Piotr Bojanowski, Valentin Wyart, Jean-Rémi King</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.18226v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Many AI models trained on natural images develop representations that resemble those of the human brain. However, the factors that drive this brain-model similarity remain poorly understood. To disentangle how the model, training and data independently lead a neural network to develop brain-like representations, we trained a family of self-supervised vision transformers (DINOv3) that systematically varied these different factors. We compare their representations of images to those of the human brain recorded with both fMRI and MEG, providing high resolution in spatial and temporal analyses. We assess the brain-model similarity with three complementary metrics focusing on overall representational similarity, topographical organization, and temporal dynamics. We show that all three factors - model size, training amount, and image type - independently and interactively impact each of these brain similarity metrics. In particular, the largest DINOv3 models trained with the most human-centric images reach the highest brain-similarity. This emergence of brain-like representations in AI models follows a specific chronology during training: models first align with the early representations of the sensory cortices, and only align with the late and prefrontal representations of the brain with considerably more training. Finally, this developmental trajectory is indexed by both structural and functional properties of the human cortex: the representations that are acquired last by the models specifically align with the cortical areas with the largest developmental expansion, thickness, least myelination, and slowest timescales. Overall, these findings disentangle the interplay between architecture and experience in shaping how artificial neural networks come to see the world as humans do, thus offering a promising framework to understand how the human brain comes to represent its visual world.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-21</td>
<td style='padding: 8px;'>Probing $0νββ$ and $μ\to eγ$ via Fully Determined Dirac Mass Terms in LRSM with Double Seesaw</td>
<td style='padding: 6px;'>Pratik Adarsh, Rajrupa Banerjee, Purushottam Sahu, Utkarsh Patel, Sudhanwa Patra</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.15893v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neutrinoless double beta decay ($0\nu\beta\beta$) and charged lepton flavor violation (cLFV) experiments provide promising avenues to probe new physics contributions from extended neutrino sectors in beyond Standard Model (BSM) scenarios. We consider a Left--Right Symmetric Model (LRSM) extended with three generations of sterile neutrinos to realize a double type-I seesaw mechanism for light neutrino mass generation. The double seesaw induces maximal lepton number violation in the right-handed sector and facilitates enhanced Majorana masses for right-handed neutrinos, thereby leading to their dominant contributions in both cLFV and $0\nu\beta\beta$ processes. We perform a comprehensive exploration of the parameter space for new-physics contributions to the cLFV decay $\mu \to e \gamma$ and to $0\nu\beta\beta$, considering two different textures for the Dirac mass matrices: one proportional to the identity matrix and another fully determined by the model framework. A detailed analysis of the common parameter regions accessible to current experiments like KamLAND-Zen and LEGEND-200, and upcoming experiments, such as MEG-II and LEGEND-1000, is presented, underscoring the phenomenological relevance of this framework. Our results aim to provide optimistic benchmarks for future searches targeting right-handed current-mediated neutrino interactions.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>A Computational Perspective on NeuroAI and Synthetic Biological Intelligence</td>
<td style='padding: 6px;'>Dhruvik Patel, Md Sayed Tanveer, Jesus Gonzalez-Ferrer, Alon Loeffler, Brett J. Kagan, Mohammed A. Mostajo-Radji, Ge Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.23896v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>NeuroAI is an emerging field at the intersection of neuroscience and artificial intelligence, where insights from brain function guide the design of intelligent systems. A central area within this field is synthetic biological intelligence (SBI), which combines the adaptive learning properties of biological neural networks with engineered hardware and software. SBI systems provide a platform for modeling neural computation, developing biohybrid architectures, and enabling new forms of embodied intelligence. In this review, we organize the NeuroAI landscape into three interacting domains: hardware, software, and wetware. We outline computational frameworks that integrate biological and non-biological systems and highlight recent advances in organoid intelligence, neuromorphic computing, and neuro-symbolic learning. These developments collectively point toward a new class of systems that compute through interactions between living neural tissue and digital algorithms.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-09</td>
<td style='padding: 8px;'>Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers</td>
<td style='padding: 6px;'>Thomas Klein, Sascha Meyen, Wieland Brendel, Felix A. Wichmann, Kristof Meding</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.06645v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Benchmarking models is a key factor for the rapid progress in machine learning (ML) research. Thus, further progress depends on improving benchmarking metrics. A standard metric to measure the behavioral alignment between ML models and human observers is error consistency (EC). EC allows for more fine-grained comparisons of behavior than other metrics such as e.g. accuracy, and has been used in the influential Brain-Score benchmark to rank different DNNs by their behavioral consistency with humans. Previously, EC values have been reported without confidence intervals. However, empirically measured EC values are typically noisy -- thus, without confidence intervals, valid benchmarking conclusions are problematic. Here we improve on standard EC in two ways: First, we show how to obtain confidence intervals for EC using a bootstrapping technique, allowing us to derive significance tests for EC. Second, we propose a new computational model relating the EC between two classifiers to the implicit probability that one of them copies responses from the other. This view of EC allows us to give practical guidance to scientists regarding the number of trials required for sufficiently powerful, conclusive experiments. Finally, we use our methodology to revisit popular NeuroAI-results. We find that while the general trend of behavioral differences between humans and machines holds up to scrutiny, many reported differences between deep vision models are statistically insignificant. Our methodology enables researchers to design adequately powered experiments that can reliably detect behavioral differences between models, providing a foundation for more rigorous benchmarking of behavioral alignment.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-02</td>
<td style='padding: 8px;'>What Neuroscience Can Teach AI About Learning in Continuously Changing Environments</td>
<td style='padding: 6px;'>Daniel Durstewitz, Bruno Averbeck, Georgia Koppe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.02103v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-06-12</td>
<td style='padding: 8px;'>NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models</td>
<td style='padding: 6px;'>Luca Ghafourpour, Valentin Duruisseaux, Bahareh Tolooshams, Philip H. Wong, Costas A. Anastassiou, Anima Anandkumar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2506.04536v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Characterizing the diverse computational properties of human neurons via multimodal electrophysiological, transcriptomic, and morphological data provides the foundation for constructing and validating bio-realistic neuron models that can advance our understanding of fundamental mechanisms underlying brain function. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. To capture variability, ensembles of deterministic models are often used, but are difficult to scale as model generation requires repeating computationally expensive optimization for each neuron. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on data generated from biophysically realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE is the first scaled-up deep learning framework validated on real experimental data, enabling efficient generation of synthetic neurons that exhibit trial-to-trial variability and achieve a $4200\times$ speedup over numerical solvers. To this end, NOBLE captures fundamental neural properties, opening the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-21</td>
<td style='padding: 8px;'>SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation</td>
<td style='padding: 6px;'>Jiayue Liu, Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Kuo Yang, Xu Wang, Yang Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.16080v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-02-22</td>
<td style='padding: 8px;'>Brain-Model Evaluations Need the NeuroAI Turing Test</td>
<td style='padding: 6px;'>Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2502.16238v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-04</td>
<td style='padding: 8px;'>Asynchronous Hebbian/anti-Hebbian networks</td>
<td style='padding: 6px;'>Henrique Reis Aguiar, Matthias H. Hennig</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02402v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-03</td>
<td style='padding: 8px;'>NeuroAI for AI Safety</td>
<td style='padding: 6px;'>Patrick Mineault, Niccolò Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.18526v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-14</td>
<td style='padding: 8px;'>Evaluating Representational Similarity Measures from the Lens of Functional Correspondence</td>
<td style='padding: 6px;'>Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.14633v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-09</td>
<td style='padding: 8px;'>Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models</td>
<td style='padding: 6px;'>Emily Cheng, Richard J. Antonello</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.05771v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first "composition" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>CaRT: Teaching LLM Agents to Know When They Know Enough</td>
<td style='padding: 6px;'>Grace Liu, Yuxiao Qu, Jeff Schneider, Aarti Singh, Aviral Kumar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08517v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Many tasks require learned models to strategically gather relevant information over multiple rounds of interaction before actually acting on a task. Strategic information gathering requires models to know not only how to effectively acquire information, but also when to stop gathering information and make a decision, in order to avoid overthinking or getting derailed when acting. In this paper, we formalize this problem and introduce Counterfactuals and Reasoning for Termination (CaRT), an approach for teaching LLMs when to stop seeking information. To appropriately learn when to terminate, CaRT fine-tunes LLMs using counterfactual pairs of trajectories, one where termination is appropriate and a minimally modified version of the same trajectory where it is not. It trains the LLM to explain the rationale for the termination decision in either case via verbal reasoning, and imbues this capability into the base LLM via fine-tuning. We instantiate CaRT in two domains: interactive medical diagnosis and math problem solving. In both domains, we find that CaRT improves the efficiency of information gathering and task success rate compared to other fine-tuning methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>AI-Driven Radiology Report Generation for Traumatic Brain Injuries</td>
<td style='padding: 6px;'>Riadh Bouslimi, Houda Trabelsi, Wahiba Ben Abdssalem Karaa, Hana Hedhli</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08498v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Traumatic brain injuries present significant diagnostic challenges in emergency medicine, where the timely interpretation of medical images is crucial for patient outcomes. In this paper, we propose a novel AI-based approach for automatic radiology report generation tailored to cranial trauma cases. Our model integrates an AC-BiFPN with a Transformer architecture to capture and process complex medical imaging data such as CT and MRI scans. The AC-BiFPN extracts multi-scale features, enabling the detection of intricate anomalies like intracranial hemorrhages, while the Transformer generates coherent, contextually relevant diagnostic reports by modeling long-range dependencies. We evaluate the performance of our model on the RSNA Intracranial Hemorrhage Detection dataset, where it outperforms traditional CNN-based models in both diagnostic accuracy and report generation. This solution not only supports radiologists in high-pressure environments but also provides a powerful educational tool for trainee physicians, offering real-time feedback and enhancing their learning experience. Our findings demonstrate the potential of combining advanced feature extraction with transformer-based text generation to improve clinical decision-making in the diagnosis of traumatic brain injuries.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>Robust Source-Free Domain Adaptation for Medical Image Segmentation based on Curriculum Learning</td>
<td style='padding: 6px;'>Ziqi Zhang, Yuexiang Li, Yawen Huang, Nanjun He, Tao Xu, Liwei Lin, Yefeng Zheng, Shaoxin Li, Feiyue Huang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08393v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent studies have uncovered a new research line, namely source-free domain adaptation, which adapts a model to target domains without using the source data. Such a setting can address the concerns on data privacy and security issues of medical images. However, current source-free domain adaptation frameworks mainly focus on the pseudo label refinement for target data without the consideration of learning procedure. Indeed, a progressive learning process from source to target domain will benefit the knowledge transfer during model adaptation. To this end, we propose a curriculum-based framework, namely learning from curriculum (LFC), for source-free domain adaptation, which consists of easy-to-hard and source-to-target curricula. Concretely, the former curriculum enables the framework to start learning with `easy' samples and gradually tune the optimization direction of model adaption by increasing the sample difficulty. While, the latter can stablize the adaptation process, which ensures smooth transfer of the model from the source domain to the target. We evaluate the proposed source-free domain adaptation approach on the public cross-domain datasets for fundus segmentation and polyp segmentation. The extensive experimental results show that our framework surpasses the existing approaches and achieves a new state-of-the-art.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>Impact of protein corona morphology on nanoparticle diffusion in biological fluids: insights from a mesoscale approach</td>
<td style='padding: 6px;'>Beatrice Cipriani, Hender Lopez</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08340v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Nanoparticles (NPs) demonstrate considerable potential in medical applications, including targeted drug delivery and diagnostic probes. However, their efficacy depends on their ability to navigate through the complex biological environments inside living organisms. In such environments, NPs interact with a dense mixture of biomolecules, which can reduce their mobility and hinder diffusion. Understanding the factors influencing NP diffusion in these environments is key to improving nanomedicine design and predicting toxicological effects. In this study, we propose a computational approach to model NP diffusion in crowded environments. We introduce a mesoscale model that accounts for the combined effects of the Protein Corona (PC) and the crowded medium on NP movement. By including volume-exclusion interactions and modelling the PC both explicitly and implicitly, we identify key macromolecular descriptors that affect NP diffusion. Our results show that the morphology of the PC can significantly affect the diffusion of NPs, and the role of the occupied volume fraction and the size ratio between tracers and crowders are analysed. The results also show that approximating large macromolecular assemblies with a hydrodynamic single-sphere model leads to inexact diffusion estimates. To overcome the limitations of single-sphere representations, a strategy for an accurate parametrization of NP-PC systems using a single-sphere model is presented.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions</td>
<td style='padding: 6px;'>XuHao Hu, Peng Wang, Xiaoya Lu, Dongrui Liu, Xuanjing Huang, Jing Shao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08211v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Previous research has shown that LLMs finetuned on malicious or incorrect completions within narrow domains (e.g., insecure code or incorrect medical advice) can become broadly misaligned to exhibit harmful behaviors, which is called emergent misalignment. In this work, we investigate whether this phenomenon can extend beyond safety behaviors to a broader spectrum of dishonesty and deception under high-stakes scenarios (e.g., lying under pressure and deceptive behavior). To explore this, we finetune open-sourced LLMs on misaligned completions across diverse domains. Experimental results demonstrate that LLMs show broadly misaligned behavior in dishonesty. Additionally, we further explore this phenomenon in a downstream combined finetuning setting, and find that introducing as little as 1% of misalignment data into a standard downstream task is sufficient to decrease honest behavior over 20%. Furthermore, we consider a more practical human-AI interaction environment where we simulate both benign and biased users to interact with the assistant LLM. Notably, we find that the assistant can be misaligned unintentionally to exacerbate its dishonesty with only 10% biased user population. In summary, we extend the study of emergent misalignment to the domain of dishonesty and deception under high-stakes scenarios, and demonstrate that this risk arises not only through direct finetuning, but also in downstream mixture tasks and practical human-AI interactions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation</td>
<td style='padding: 6px;'>Eirik A. Østmo, Kristoffer K. Wickstrøm, Keyur Radiya, Michael C. Kampffmeyer, Karl Øyvind Mikalsen, Robert Jenssen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08116v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Contrast-enhanced Computed Tomography (CT) is important for diagnosis and treatment planning for various medical conditions. Deep learning (DL) based segmentation models may enable automated medical image analysis for detecting and delineating tumors in CT images, thereby reducing clinicians' workload. Achieving generalization capabilities in limited data domains, such as radiology, requires modern DL models to be trained with image augmentation. However, naively applying augmentation methods developed for natural images to CT scans often disregards the nature of the CT modality, where the intensities measure Hounsfield Units (HU) and have important physical meaning. This paper challenges the use of such intensity augmentations for CT imaging and shows that they may lead to artifacts and poor generalization. To mitigate this, we propose a CT-specific augmentation technique, called Random windowing, that exploits the available HU distribution of intensities in CT images. Random windowing encourages robustness to contrast-enhancement and significantly increases model performance on challenging images with poor contrast or timing. We perform ablations and analysis of our method on multiple datasets, and compare to, and outperform, state-of-the-art alternatives, while focusing on the challenge of liver tumor segmentation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>Beyond hospital reach: Autonomous lightweight ultrasound robot for liver sonography</td>
<td style='padding: 6px;'>Zihan Li, Yixiao Xu, Lei Zhang, Taiyu Han, Xinshan Yang, Yingni Wang, Mingxuan Liu, Shenghai Xin, Linxun Liu, Hongen Liao, Guochen Ning</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08106v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Liver disease is a major global health burden. While ultrasound is the first-line diagnostic tool, liver sonography requires locating multiple non-continuous planes from positions where target structures are often not visible, for biometric assessment and lesion detection, requiring significant expertise. However, expert sonographers are severely scarce in resource-limited regions. Here, we develop an autonomous lightweight ultrasound robot comprising an AI agent that integrates multi-modal perception with memory attention for localization of unseen target structures, and a 588-gram 6-degrees-of-freedom cable-driven robot. By mounting on the abdomen, the system enhances robustness against motion. Our robot can autonomously acquire expert-level standard liver ultrasound planes and detect pathology in patients, including two from Xining, a 2261-meter-altitude city with limited medical resources. Our system performs effectively on rapid-motion individuals and in wilderness environments. This work represents the first demonstration of autonomous sonography across multiple challenging scenarios, potentially transforming access to expert-level diagnostics in underserved regions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>MRI-derived quantification of hepatic vessel-to-volume ratios in chronic liver disease using a deep learning approach</td>
<td style='padding: 6px;'>Alexander Herold, Daniel Sobotka, Lucian Beer, Nina Bastati, Sarah Poetter-Lang, Michael Weber, Thomas Reiberger, Mattias Mandorfer, Georg Semmler, Benedikt Simbrunner, Barbara D. Wichtmann, Sami A. Ba-Ssalamah, Michael Trauner, Ahmed Ba-Ssalamah, Georg Langs</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.08039v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Background: We aimed to quantify hepatic vessel volumes across chronic liver disease stages and healthy controls using deep learning-based magnetic resonance imaging (MRI) analysis, and assess correlations with biomarkers for liver (dys)function and fibrosis/portal hypertension.   Methods: We assessed retrospectively healthy controls, non-advanced and advanced chronic liver disease (ACLD) patients using a 3D U-Net model for hepatic vessel segmentation on portal venous phase gadoxetic acid-enhanced 3-T MRI. Total (TVVR), hepatic (HVVR), and intrahepatic portal vein-to-volume ratios (PVVR) were compared between groups and correlated with: albumin-bilirubin (ALBI) and model for end-stage liver disease-sodium (MELD-Na) score, and fibrosis/portal hypertension (Fibrosis-4 [FIB-4] score, liver stiffness measurement [LSM], hepatic venous pressure gradient [HVPG], platelet count [PLT], and spleen volume).   Results: We included 197 subjects, aged 54.9 $\pm$ 13.8 years (mean $\pm$ standard deviation), 111 males (56.3\%): 35 healthy controls, 44 non-ACLD, and 118 ACLD patients. TVVR and HVVR were highest in controls (3.9; 2.1), intermediate in non-ACLD (2.8; 1.7), and lowest in ACLD patients (2.3; 1.0) ($p \leq 0.001$). PVVR was reduced in both non-ACLD and ACLD patients (both 1.2) compared to controls (1.7) ($p \leq 0.001$), but showed no difference between CLD groups ($p = 0.999$). HVVR significantly correlated indirectly with FIB-4, ALBI, MELD-Na, LSM, and spleen volume ($\rho$ ranging from -0.27 to -0.40), and directly with PLT ($\rho = 0.36$). TVVR and PVVR showed similar but weaker correlations.   Conclusions: Deep learning-based hepatic vessel volumetry demonstrated differences between healthy liver and chronic liver disease stages and shows correlations with established markers of disease severity.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation</td>
<td style='padding: 6px;'>Chongmyung Kwon, Yujin Kim, Seoeun Park, Yunji Lee, Charmgil Hong</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.07910v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Drug recommendation is an essential task in machine learning-based clinical decision support systems. However, the risk of drug-drug interactions (DDI) between co-prescribed medications remains a significant challenge. Previous studies have used graph neural networks (GNNs) to represent drug structures. Regardless, their simplified discrete forms cannot fully capture the molecular binding affinity and reactivity. Therefore, we propose Multimodal DDI Prediction with Molecular Electron Localization Function (ELF) Maps (MMM), a novel framework that integrates three-dimensional (3D) quantum-chemical information into drug representation learning. It generates 3D electron density maps using the ELF. To capture both therapeutic relevance and interaction risks, MMM combines ELF-derived features that encode global electronic properties with a bipartite graph encoder that models local substructure interactions. This design enables learning complementary characteristics of drug molecules. We evaluate MMM in the MIMIC-III dataset (250 drugs, 442 substructures), comparing it with several baseline models. In particular, a comparison with the GNN-based SafeDrug model demonstrates statistically significant improvements in the F1-score (p = 0.0387), Jaccard (p = 0.0112), and the DDI rate (p = 0.0386). These results demonstrate the potential of ELF-based 3D representations to enhance prediction accuracy and support safer combinatorial drug prescribing in clinical practice.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>Haibu Mathematical-Medical Intelligent Agent:Enhancing Large Language Model Reliability in Medical Tasks via Verifiable Reasoning Chains</td>
<td style='padding: 6px;'>Yilun Zhang, Dexing Kong</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.07748v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large Language Models (LLMs) show promise in medicine but are prone to factual and logical errors, which is unacceptable in this high-stakes field. To address this, we introduce the "Haibu Mathematical-Medical Intelligent Agent" (MMIA), an LLM-driven architecture that ensures reliability through a formally verifiable reasoning process. MMIA recursively breaks down complex medical tasks into atomic, evidence-based steps. This entire reasoning chain is then automatically audited for logical coherence and evidence traceability, similar to theorem proving. A key innovation is MMIA's "bootstrapping" mode, which stores validated reasoning chains as "theorems." Subsequent tasks can then be efficiently solved using Retrieval-Augmented Generation (RAG), shifting from costly first-principles reasoning to a low-cost verification model. We validated MMIA across four healthcare administration domains, including DRG/DIP audits and medical insurance adjudication, using expert-validated benchmarks. Results showed MMIA achieved an error detection rate exceeding 98% with a false positive rate below 1%, significantly outperforming baseline LLMs. Furthermore, the RAG matching mode is projected to reduce average processing costs by approximately 85% as the knowledge base matures. In conclusion, MMIA's verifiable reasoning framework is a significant step toward creating trustworthy, transparent, and cost-effective AI systems, making LLM technology viable for critical applications in medicine.</td>
</tr>
</tbody>
</table>

