<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2024-10-20</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>Linear-Threshold Network Models for Describing and Analyzing Brain Dynamics</td>
<td style='padding: 6px;'>Michael McCreesh, Erfan Nozari, Jorge Cortes</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13710v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Over the past two decades, an increasing array of control-theoretic methods have been used to study the brain as a complex dynamical system and better understand its structure-function relationship. This article provides an overview on one such family of methods, based on the linear-threshold rate (LTR) dynamics, which arises when modeling the spiking activity of neuronal populations and their impact on each other. LTR dynamics exhibit a wide range of behaviors based on network topologies and inputs, including mono- and multi-stability, limit cycles, and chaos, allowing it to be used to model many complex brain processes involving fast and slow inhibition, multiple time and spatial scales, different types of neural behavior, and higher-order interactions. Here we investigate how the versatility of LTR dynamics paired with concepts and tools from systems and control can provide a computational theory for explaining the dynamical mechanisms enabling different brain processes. Specifically, we illustrate stability and stabilization properties of LTR dynamics and how they are related to goal-driven selective attention, multistability and its relationship with declarative memory, and bifurcations and oscillations and their role in modeling seizure dynamics in epilepsy. We conclude with a discussion on additional properties of LTR dynamics and an outlook on other brain processess that for which they might be play a similar role.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>Theta and/or alpha? Neural oscillational substrates for dynamic inter-brain synchrony during mother-child cooperation</td>
<td style='padding: 6px;'>Jiayang Xu, Yamin Li, Ruxin Su, Saishuang Wu, Chengcheng Wu, Haiwa Wang, Qi Zhu, Yue Fang, Fan Jiang, Shanbao Tong, Yunting Zhang, Xiaoli Guo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13669v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Mother-child interaction is a highly dynamic process neurally characterized by inter-brain synchrony (IBS) at {\theta} and/or {\alpha} rhythms. However, their establishment, dynamic changes, and roles in mother-child interactions remain unknown. Through dynamic analysis of dual-EEG from 40 mother-child dyads during turn-taking cooperation, we uncover that {\theta}-IBS and {\alpha}-IBS alternated with interactive behaviors, with EEG frequency-shift as a prerequisite for IBS transitions. When mothers attempt to track their children's attention and/or predict their intentions, they will adjust their EEG frequencies to align with their children's {\theta} oscillations, leading to a higher occurrence of the {\theta}-IBS state. Conversely, the {\alpha}-IBS state, accompanied by the EEG frequency-shift to the {\alpha} range, is more prominent during mother-led interactions. Further exploratory analysis reveals greater presence and stability of the {\theta}-IBS state during cooperative than non-cooperative conditions, particularly in dyads with stronger emotional attachments and more frequent interactions in their daily lives. Our findings shed light on the neural oscillational substrates underlying the IBS dynamics during mother-child interactions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>Ornstein-Uhlenbeck Adaptation as a Mechanism for Learning in Brains and Machines</td>
<td style='padding: 6px;'>Jesus Garcia Fernandez, Nasir Ahmad, Marcel van Gerven</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13563v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Learning is a fundamental property of intelligent systems, observed across biological organisms and engineered systems. While modern intelligent systems typically rely on gradient descent for learning, the need for exact gradients and complex information flow makes its implementation in biological and neuromorphic systems challenging. This has motivated the exploration of alternative learning mechanisms that can operate locally and do not rely on exact gradients. In this work, we introduce a novel approach that leverages noise in the parameters of the system and global reinforcement signals. Using an Ornstein-Uhlenbeck process with adaptive dynamics, our method balances exploration and exploitation during learning, driven by deviations from error predictions, akin to reward prediction error. Operating in continuous time, Orstein-Uhlenbeck adaptation (OUA) is proposed as a general mechanism for learning dynamic, time-evolving environments. We validate our approach across diverse tasks, including supervised learning and reinforcement learning in feedforward and recurrent systems. Additionally, we demonstrate that it can perform meta-learning, adjusting hyper-parameters autonomously. Our results indicate that OUA provides a viable alternative to traditional gradient-based methods, with potential applications in neuromorphic computing. It also hints at a possible mechanism for noise-driven learning in the brain, where stochastic neurotransmitter release may guide synaptic adjustments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>Advocate -- Trustworthy Evidence in Cloud Systems</td>
<td style='padding: 6px;'>Sebastian Werner, Sepideh Masoudi, Fernando Castillo, Fabian Piper, Jonathan Heiss</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13477v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The rapid evolution of cloud-native applications, characterized by dynamic, interconnected services, presents significant challenges for maintaining trustworthy and auditable systems, especially in sensitive contexts, such as finance or healthcare. Traditional methods of verification and certification are often inadequate due to the fast-past and dynamic development practices common in cloud computing. This paper introduces Advocate, a novel agent-based system designed to generate verifiable evidence of cloud-native application operations. By integrating with existing infrastructure tools, such as Kubernetes and distributed tracing systems, Advocate captures, authenticates, and stores evidence trails in a tamper-resistant manner. This approach not only supports the auditing process but also allows for privacy-preserving evidence aggregation. Advocate's extensible architecture facilitates its deployment in diverse environments, enabling the verification and adherence to policies and enhance trust in cloud services.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>Seizure Onset Zone Localisation Algorithms with Intracranial EEG: Evaluating Methodological Variations and Targeted Features</td>
<td style='padding: 6px;'>Sarah J. Gascoigne, Manel Vila-Vidal, Nathan Evans, Anderson Brito Da Silva, Rhys H. Thomas, Christopher Thornton, Kevin Wilson, Peter N. Taylor, Adrià Tauste, Yujiang Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13466v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>During clinical treatment for epilepsy, the area of the brain thought to be responsible for pathological activity - known as the Seizure Onset Zone (SOZ) - is identified. This is typically performed through visual assessment of EEG recordings; however, this is time consuming and prone to subjective inconsistency. Automated SOZ localisation algorithms provide objective identification of the SOZ by highlighting changes in signal features associated with seizure onset. In this work we investigate how methodological differences in such algorithms can result in different SOZ localisations.   We analysed ictal intracranial EEG (icEEG) recordings in 16 subjects (100 seizures) with drug-resistant epilepsy from the SWEZ-ETHZ public database. Through our analysis, we identified a series of key methodological differences that must be considered when designing or selecting a SOZ localisation algorithm. These differences were demonstrated using three distinct algorithms that capture different, but complementary, seizure onset features: Imprint, Epileptogenicity Index (EI), and Low Entropy Map (LEM). We assessed methodological differences at each Decision Point, comparing the resultant SOZ localisations.   Our independent application of all three algorithms to the same ictal icEEG dataset revealed low agreement between algorithms: 27-60% of seizure onsets were as minimally or non-overlapping (<25% overlap). Therefore, we investigated the effect of three key methodological differences (or Decision Points). Changes at each Decision Point were found to result in significantly different SOZ localisations (p<0.05).   Our results demonstrate how seemingly small methodological changes can result in large differences in SOZ localisations. We propose that key Decision Points must be considered when using or designing a SOZ localisation algorithm.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>Unsupervised Skull Segmentation via Contrastive MR-to-CT Modality Translation</td>
<td style='padding: 6px;'>Kamil Kwarciak, Mateusz Daniol, Daria Hemmerling, Marek Wodzinski</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13427v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The skull segmentation from CT scans can be seen as an already solved problem. However, in MR this task has a significantly greater complexity due to the presence of soft tissues rather than bones. Capturing the bone structures from MR images of the head, where the main visualization objective is the brain, is very demanding. The attempts that make use of skull stripping seem to not be well suited for this task and fail to work in many cases. On the other hand, supervised approaches require costly and time-consuming skull annotations. To overcome the difficulties we propose a fully unsupervised approach, where we do not perform the segmentation directly on MR images, but we rather perform a synthetic CT data generation via MR-to-CT translation and perform the segmentation there. We address many issues associated with unsupervised skull segmentation including the unpaired nature of MR and CT datasets (contrastive learning), low resolution and poor quality (super-resolution), and generalization capabilities. The research has a significant value for downstream tasks requiring skull segmentation from MR volumes such as craniectomy or surgery planning and can be seen as an important step towards the utilization of synthetic data in medical imaging.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>Adversarial Neural Networks in Medical Imaging Advancements and Challenges in Semantic Segmentation</td>
<td style='padding: 6px;'>Houze Liu, Bo Zhang, Yanlin Xiang, Yuxiang Hu, Aoran Shen, Yang Lin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13099v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent advancements in artificial intelligence (AI) have precipitated a paradigm shift in medical imaging, particularly revolutionizing the domain of brain imaging. This paper systematically investigates the integration of deep learning -- a principal branch of AI -- into the semantic segmentation of brain images. Semantic segmentation serves as an indispensable technique for the delineation of discrete anatomical structures and the identification of pathological markers, essential for the diagnosis of complex neurological disorders. Historically, the reliance on manual interpretation by radiologists, while noteworthy for its accuracy, is plagued by inherent subjectivity and inter-observer variability. This limitation becomes more pronounced with the exponential increase in imaging data, which traditional methods struggle to process efficiently and effectively. In response to these challenges, this study introduces the application of adversarial neural networks, a novel AI approach that not only automates but also refines the semantic segmentation process. By leveraging these advanced neural networks, our approach enhances the precision of diagnostic outputs, reducing human error and increasing the throughput of imaging data analysis. The paper provides a detailed discussion on how adversarial neural networks facilitate a more robust, objective, and scalable solution, thereby significantly improving diagnostic accuracies in neurological evaluations. This exploration highlights the transformative impact of AI on medical imaging, setting a new benchmark for future research and clinical practice in neurology.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-16</td>
<td style='padding: 8px;'>AADNet: An End-to-End Deep Learning Model for Auditory Attention Decoding</td>
<td style='padding: 6px;'>Nhan Duc Thanh Nguyen, Huy Phan, Simon Geirnaert, Kaare Mikkelsen, Preben Kidmose</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13059v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Auditory attention decoding (AAD) is the process of identifying the attended speech in a multi-talker environment using brain signals, typically recorded through electroencephalography (EEG). Over the past decade, AAD has undergone continuous development, driven by its promising application in neuro-steered hearing devices. Most AAD algorithms are relying on the increase in neural entrainment to the envelope of attended speech, as compared to unattended speech, typically using a two-step approach. First, the algorithm predicts representations of the attended speech signal envelopes; second, it identifies the attended speech by finding the highest correlation between the predictions and the representations of the actual speech signals. In this study, we proposed a novel end-to-end neural network architecture, named AADNet, which combines these two stages into a direct approach to address the AAD problem. We compare the proposed network against the traditional approaches, including linear stimulus reconstruction, canonical correlation analysis, and an alternative non-linear stimulus reconstruction using two different datasets. AADNet shows a significant performance improvement for both subject-specific and subject-independent models. Notably, the average subject-independent classification accuracies from 56.1 % to 82.7 % with analysis window lengths ranging from 1 to 40 seconds, respectively, show a significantly improved ability to generalize to data from unseen subjects. These results highlight the potential of deep learning models for advancing AAD, with promising implications for future hearing aids, assistive devices, and clinical assessments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-16</td>
<td style='padding: 8px;'>Machine Learning Approach to Brain Tumor Detection and Classification</td>
<td style='padding: 6px;'>Alice Oh, Inyoung Noh, Jian Choo, Jihoo Lee, Justin Park, Kate Hwang, Sanghyeon Kim, Soo Min Oh</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.12692v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain tumor detection and classification are critical tasks in medical image analysis, particularly in early-stage diagnosis, where accurate and timely detection can significantly improve treatment outcomes. In this study, we apply various statistical and machine learning models to detect and classify brain tumors using brain MRI images. We explore a variety of statistical models including linear, logistic, and Bayesian regressions, and the machine learning models including decision tree, random forest, single-layer perceptron, multi-layer perceptron, convolutional neural network (CNN), recurrent neural network, and long short-term memory. Our findings show that CNN outperforms other models, achieving the best performance. Additionally, we confirm that the CNN model can also work for multi-class classification, distinguishing between four categories of brain MRI images such as normal, glioma, meningioma, and pituitary tumor images. This study demonstrates that machine learning approaches are suitable for brain tumor detection and classification, facilitating real-world medical applications in assisting radiologists with early and accurate diagnosis.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-16</td>
<td style='padding: 8px;'>iFuzzyTL: Interpretable Fuzzy Transfer Learning for SSVEP BCI System</td>
<td style='padding: 6px;'>Xiaowei Jiang, Beining Cao, Liang Ou, Yu-Cheng Chang, Thomas Do, Chin-Teng Lin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.12267v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The rapid evolution of Brain-Computer Interfaces (BCIs) has significantly influenced the domain of human-computer interaction, with Steady-State Visual Evoked Potentials (SSVEP) emerging as a notably robust paradigm. This study explores advanced classification techniques leveraging interpretable fuzzy transfer learning (iFuzzyTL) to enhance the adaptability and performance of SSVEP-based systems. Recent efforts have strengthened to reduce calibration requirements through innovative transfer learning approaches, which refine cross-subject generalizability and minimize calibration through strategic application of domain adaptation and few-shot learning strategies. Pioneering developments in deep learning also offer promising enhancements, facilitating robust domain adaptation and significantly improving system responsiveness and accuracy in SSVEP classification. However, these methods often require complex tuning and extensive data, limiting immediate applicability. iFuzzyTL introduces an adaptive framework that combines fuzzy logic principles with neural network architectures, focusing on efficient knowledge transfer and domain adaptation. iFuzzyTL refines input signal processing and classification in a human-interpretable format by integrating fuzzy inference systems and attention mechanisms. This approach bolsters the model's precision and aligns with real-world operational demands by effectively managing the inherent variability and uncertainty of EEG data. The model's efficacy is demonstrated across three datasets: 12JFPM (89.70% accuracy for 1s with an information transfer rate (ITR) of 149.58), Benchmark (85.81% accuracy for 1s with an ITR of 213.99), and eldBETA (76.50% accuracy for 1s with an ITR of 94.63), achieving state-of-the-art results and setting new benchmarks for SSVEP BCI performance.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>Theta and/or alpha? Neural oscillational substrates for dynamic inter-brain synchrony during mother-child cooperation</td>
<td style='padding: 6px;'>Jiayang Xu, Yamin Li, Ruxin Su, Saishuang Wu, Chengcheng Wu, Haiwa Wang, Qi Zhu, Yue Fang, Fan Jiang, Shanbao Tong, Yunting Zhang, Xiaoli Guo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13669v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Mother-child interaction is a highly dynamic process neurally characterized by inter-brain synchrony (IBS) at {\theta} and/or {\alpha} rhythms. However, their establishment, dynamic changes, and roles in mother-child interactions remain unknown. Through dynamic analysis of dual-EEG from 40 mother-child dyads during turn-taking cooperation, we uncover that {\theta}-IBS and {\alpha}-IBS alternated with interactive behaviors, with EEG frequency-shift as a prerequisite for IBS transitions. When mothers attempt to track their children's attention and/or predict their intentions, they will adjust their EEG frequencies to align with their children's {\theta} oscillations, leading to a higher occurrence of the {\theta}-IBS state. Conversely, the {\alpha}-IBS state, accompanied by the EEG frequency-shift to the {\alpha} range, is more prominent during mother-led interactions. Further exploratory analysis reveals greater presence and stability of the {\theta}-IBS state during cooperative than non-cooperative conditions, particularly in dyads with stronger emotional attachments and more frequent interactions in their daily lives. Our findings shed light on the neural oscillational substrates underlying the IBS dynamics during mother-child interactions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>Neural Correlates of Augmented Reality Safety Warnings: EEG Analysis of Situational Awareness and Cognitive Performance in Roadway Work Zones</td>
<td style='padding: 6px;'>Fatemeh Banani Ardecani, Amit Kumar, Sepehr Sabeti, Omidreza Shoghli</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13623v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Despite the research and implementation efforts involving various safety strategies, protocols, and technologies, work zone crashes and fatalities continue to occur at an alarming rate each year. This study investigates the neurophysiological responses to Augmented Reality safety warnings in roadway work zones under varying workload conditions. Using electroencephalogram (EEG) technology, we objectively assessed situational awareness, attention, and cognitive load in simulated low-intensity (LA) and moderate-intensity (MA) work activities. The research analyzed key EEG indicators including beta, gamma, alpha, and theta waves, as well as various combined wave ratios. Results revealed that AR warnings effectively triggered neurological responses associated with increased situational awareness and attention across both workload conditions. However, significant differences were observed in the timing and intensity of these responses. In the LA condition, peak responses occurred earlier (within 125 ms post-warning) and were more pronounced, suggesting a more robust cognitive response when physical demands were lower. Conversely, the MA condition showed delayed peak responses (125-250 ms post-warning) and more gradual changes, indicating a potential impact of increased physical activity on cognitive processing speed. These findings underscore the importance of considering physical workload when designing AR-based safety systems for roadway work zones. The research contributes to the understanding of how AR can enhance worker safety and provides insights for developing more effective, context-aware safety interventions in high-risk work environments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>Seizure Onset Zone Localisation Algorithms with Intracranial EEG: Evaluating Methodological Variations and Targeted Features</td>
<td style='padding: 6px;'>Sarah J. Gascoigne, Manel Vila-Vidal, Nathan Evans, Anderson Brito Da Silva, Rhys H. Thomas, Christopher Thornton, Kevin Wilson, Peter N. Taylor, Adrià Tauste, Yujiang Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13466v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>During clinical treatment for epilepsy, the area of the brain thought to be responsible for pathological activity - known as the Seizure Onset Zone (SOZ) - is identified. This is typically performed through visual assessment of EEG recordings; however, this is time consuming and prone to subjective inconsistency. Automated SOZ localisation algorithms provide objective identification of the SOZ by highlighting changes in signal features associated with seizure onset. In this work we investigate how methodological differences in such algorithms can result in different SOZ localisations.   We analysed ictal intracranial EEG (icEEG) recordings in 16 subjects (100 seizures) with drug-resistant epilepsy from the SWEZ-ETHZ public database. Through our analysis, we identified a series of key methodological differences that must be considered when designing or selecting a SOZ localisation algorithm. These differences were demonstrated using three distinct algorithms that capture different, but complementary, seizure onset features: Imprint, Epileptogenicity Index (EI), and Low Entropy Map (LEM). We assessed methodological differences at each Decision Point, comparing the resultant SOZ localisations.   Our independent application of all three algorithms to the same ictal icEEG dataset revealed low agreement between algorithms: 27-60% of seizure onsets were as minimally or non-overlapping (<25% overlap). Therefore, we investigated the effect of three key methodological differences (or Decision Points). Changes at each Decision Point were found to result in significantly different SOZ localisations (p<0.05).   Our results demonstrate how seemingly small methodological changes can result in large differences in SOZ localisations. We propose that key Decision Points must be considered when using or designing a SOZ localisation algorithm.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-16</td>
<td style='padding: 8px;'>AADNet: An End-to-End Deep Learning Model for Auditory Attention Decoding</td>
<td style='padding: 6px;'>Nhan Duc Thanh Nguyen, Huy Phan, Simon Geirnaert, Kaare Mikkelsen, Preben Kidmose</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13059v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Auditory attention decoding (AAD) is the process of identifying the attended speech in a multi-talker environment using brain signals, typically recorded through electroencephalography (EEG). Over the past decade, AAD has undergone continuous development, driven by its promising application in neuro-steered hearing devices. Most AAD algorithms are relying on the increase in neural entrainment to the envelope of attended speech, as compared to unattended speech, typically using a two-step approach. First, the algorithm predicts representations of the attended speech signal envelopes; second, it identifies the attended speech by finding the highest correlation between the predictions and the representations of the actual speech signals. In this study, we proposed a novel end-to-end neural network architecture, named AADNet, which combines these two stages into a direct approach to address the AAD problem. We compare the proposed network against the traditional approaches, including linear stimulus reconstruction, canonical correlation analysis, and an alternative non-linear stimulus reconstruction using two different datasets. AADNet shows a significant performance improvement for both subject-specific and subject-independent models. Notably, the average subject-independent classification accuracies from 56.1 % to 82.7 % with analysis window lengths ranging from 1 to 40 seconds, respectively, show a significantly improved ability to generalize to data from unseen subjects. These results highlight the potential of deep learning models for advancing AAD, with promising implications for future hearing aids, assistive devices, and clinical assessments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-16</td>
<td style='padding: 8px;'>On the Role of Activation Functions in EEG-To-Text Decoder</td>
<td style='padding: 6px;'>Zenon Lamprou, Iakovos Tenedios, Yashar Moshfeghi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.12572v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In recent years, much interdisciplinary research has been conducted exploring potential use cases of neuroscience to advance the field of information retrieval. Initial research concentrated on the use of fMRI data, but fMRI was deemed to be not suitable for real-world applications, and soon, research shifted towards using EEG data. In this paper, we try to improve the original performance of a first attempt at generating text using EEG by focusing on the less explored area of optimising neural network performance. We test a set of different activation functions and compare their performance. Our results show that introducing a higher degree polynomial activation function can enhance model performance without changing the model architecture. We also show that the learnable 3rd-degree activation function performs better on the 1-gram evaluation compared to a 3rd-degree non-learnable function. However, when evaluating the model on 2-grams and above, the polynomial function lacks in performance, whilst the leaky ReLU activation function outperforms the baseline.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-16</td>
<td style='padding: 8px;'>iFuzzyTL: Interpretable Fuzzy Transfer Learning for SSVEP BCI System</td>
<td style='padding: 6px;'>Xiaowei Jiang, Beining Cao, Liang Ou, Yu-Cheng Chang, Thomas Do, Chin-Teng Lin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.12267v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The rapid evolution of Brain-Computer Interfaces (BCIs) has significantly influenced the domain of human-computer interaction, with Steady-State Visual Evoked Potentials (SSVEP) emerging as a notably robust paradigm. This study explores advanced classification techniques leveraging interpretable fuzzy transfer learning (iFuzzyTL) to enhance the adaptability and performance of SSVEP-based systems. Recent efforts have strengthened to reduce calibration requirements through innovative transfer learning approaches, which refine cross-subject generalizability and minimize calibration through strategic application of domain adaptation and few-shot learning strategies. Pioneering developments in deep learning also offer promising enhancements, facilitating robust domain adaptation and significantly improving system responsiveness and accuracy in SSVEP classification. However, these methods often require complex tuning and extensive data, limiting immediate applicability. iFuzzyTL introduces an adaptive framework that combines fuzzy logic principles with neural network architectures, focusing on efficient knowledge transfer and domain adaptation. iFuzzyTL refines input signal processing and classification in a human-interpretable format by integrating fuzzy inference systems and attention mechanisms. This approach bolsters the model's precision and aligns with real-world operational demands by effectively managing the inherent variability and uncertainty of EEG data. The model's efficacy is demonstrated across three datasets: 12JFPM (89.70% accuracy for 1s with an information transfer rate (ITR) of 149.58), Benchmark (85.81% accuracy for 1s with an ITR of 213.99), and eldBETA (76.50% accuracy for 1s with an ITR of 94.63), achieving state-of-the-art results and setting new benchmarks for SSVEP BCI performance.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-16</td>
<td style='padding: 8px;'>NSSI-Net: Multi-Concept Generative Adversarial Network for Non-Suicidal Self-Injury Detection Using High-Dimensional EEG Signals in a Semi-Supervised Learning Framework</td>
<td style='padding: 6px;'>Zhen Liang, Weishan Ye, Qile Liu, Li Zhang, Gan Huang, Yongjie Zhou</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.12159v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Non-suicidal self-injury (NSSI) is a serious threat to the physical and mental health of adolescents, significantly increasing the risk of suicide and attracting widespread public concern. Electroencephalography (EEG), as an objective tool for identifying brain disorders, holds great promise. However, extracting meaningful and reliable features from high-dimensional EEG data, especially by integrating spatiotemporal brain dynamics into informative representations, remains a major challenge. In this study, we introduce an advanced semi-supervised adversarial network, NSSI-Net, to effectively model EEG features related to NSSI. NSSI-Net consists of two key modules: a spatial-temporal feature extraction module and a multi-concept discriminator. In the spatial-temporal feature extraction module, an integrated 2D convolutional neural network (2D-CNN) and a bi-directional Gated Recurrent Unit (BiGRU) are used to capture both spatial and temporal dynamics in EEG data. In the multi-concept discriminator, signal, gender, domain, and disease levels are fully explored to extract meaningful EEG features, considering individual, demographic, disease variations across a diverse population. Based on self-collected NSSI data (n=114), the model's effectiveness and reliability are demonstrated, with a 7.44% improvement in performance compared to existing machine learning and deep learning methods. This study advances the understanding and early diagnosis of NSSI in adolescents with depression, enabling timely intervention. The source code is available at https://github.com/Vesan-yws/NSSINet.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-15</td>
<td style='padding: 8px;'>EEG-based 90-Degree Turn Intention Detection for Brain-Computer Interface</td>
<td style='padding: 6px;'>Pradyot Anand, Anant Jain, Suriya Prakash Muthukrishnan, Shubhendu Bhasin, Sitikantha Roy, Mohanavelu Kalathe, Lalan Kumar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.11339v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG)--based turn intention prediction for lower limb movement is important to build an efficient brain-computer interface (BCI) system. This study investigates the feasibility of intention detection of left-turn, right-turn, and straight walk by utilizing EEG signals obtained before the event occurrence. Synchronous data was collected using 31-channel EEG and IMU-based motion capture systems for nine healthy participants while performing left-turn, right-turn, and straight walk movements. EEG data was preprocessed with steps including Artifact Subspace Reconstruction (ASR), re-referencing, and Independent Component Analysis (ICA) to remove data noise. Feature extraction from the preprocessed EEG data involved computing various statistical measures (mean, median, standard deviation, skew, and kurtosis), and Hjorth parameters (activity, mobility, and complexity). Further, the feature selection was performed using the Random forest algorithm for the dimensionality reduction. The feature set obtained was utilized for 3-class classification using XG boost, gradient boosting, and support vector machine (SVM) with RBF kernel classifiers in a five-fold cross-validation scheme. Using the proposed intention detection methodology, the SVM classifier using an EEG window of 1.5 s and 0 s time-lag has the best decoding performance with mean accuracy, precision, and recall of 81.23%, 85.35%, and 83.92%, respectively, across the nine participants. The decoding analysis shows the feasibility of turn intention prediction for lower limb movement using the EEG signal before the event onset.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-15</td>
<td style='padding: 8px;'>SplitSEE: A Splittable Self-supervised Framework for Single-Channel EEG Representation Learning</td>
<td style='padding: 6px;'>Rikuto Kotoge, Zheng Chen, Tasuku Kimura, Yasuko Matsubara, Takufumi Yanagisawa, Haruhiko Kishima, Yasushi Sakurai</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.11200v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While end-to-end multi-channel electroencephalography (EEG) learning approaches have shown significant promise, their applicability is often constrained in neurological diagnostics, such as intracranial EEG resources. When provided with a single-channel EEG, how can we learn representations that are robust to multi-channels and scalable across varied tasks, such as seizure prediction? In this paper, we present SplitSEE, a structurally splittable framework designed for effective temporal-frequency representation learning in single-channel EEG. The key concept of SplitSEE is a self-supervised framework incorporating a deep clustering task. Given an EEG, we argue that the time and frequency domains are two distinct perspectives, and hence, learned representations should share the same cluster assignment. To this end, we first propose two domain-specific modules that independently learn domain-specific representation and address the temporal-frequency tradeoff issue in conventional spectrogram-based methods. Then, we introduce a novel clustering loss to measure the information similarity. This encourages representations from both domains to coherently describe the same input by assigning them a consistent cluster. SplitSEE leverages a pre-training-to-fine-tuning framework within a splittable architecture and has following properties: (a) Effectiveness: it learns representations solely from single-channel EEG but has even outperformed multi-channel baselines. (b) Robustness: it shows the capacity to adapt across different channels with low performance variance. Superior performance is also achieved with our collected clinical dataset. (c) Scalability: With just one fine-tuning epoch, SplitSEE achieves high and stable performance using partial model layers.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-15</td>
<td style='padding: 8px;'>DARNet: Dual Attention Refinement Network with Spatiotemporal Construction for Auditory Attention Detection</td>
<td style='padding: 6px;'>Sheng Yan, Cunhang fan, Hongyu Zhang, Xiaoke Yang, Jianhua Tao, Zhao Lv</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.11181v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>At a cocktail party, humans exhibit an impressive ability to direct their attention. The auditory attention detection (AAD) approach seeks to identify the attended speaker by analyzing brain signals, such as EEG signals. However, current AAD algorithms overlook the spatial distribution information within EEG signals and lack the ability to capture long-range latent dependencies, limiting the model's ability to decode brain activity. To address these issues, this paper proposes a dual attention refinement network with spatiotemporal construction for AAD, named DARNet, which consists of the spatiotemporal construction module, dual attention refinement module, and feature fusion \& classifier module. Specifically, the spatiotemporal construction module aims to construct more expressive spatiotemporal feature representations, by capturing the spatial distribution characteristics of EEG signals. The dual attention refinement module aims to extract different levels of temporal patterns in EEG signals and enhance the model's ability to capture long-range latent dependencies. The feature fusion \& classifier module aims to aggregate temporal patterns and dependencies from different levels and obtain the final classification results. The experimental results indicate that compared to the state-of-the-art models, DARNet achieves an average classification accuracy improvement of 5.9\% for 0.1s, 4.6\% for 1s, and 3.9\% for 2s on the DTU dataset. While maintaining excellent classification performance, DARNet significantly reduces the number of required parameters. Compared to the state-of-the-art models, DARNet reduces the parameter count by 91\%. Code is available at: https://github.com/fchest/DARNet.git.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-16</td>
<td style='padding: 8px;'>iFuzzyTL: Interpretable Fuzzy Transfer Learning for SSVEP BCI System</td>
<td style='padding: 6px;'>Xiaowei Jiang, Beining Cao, Liang Ou, Yu-Cheng Chang, Thomas Do, Chin-Teng Lin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.12267v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The rapid evolution of Brain-Computer Interfaces (BCIs) has significantly influenced the domain of human-computer interaction, with Steady-State Visual Evoked Potentials (SSVEP) emerging as a notably robust paradigm. This study explores advanced classification techniques leveraging interpretable fuzzy transfer learning (iFuzzyTL) to enhance the adaptability and performance of SSVEP-based systems. Recent efforts have strengthened to reduce calibration requirements through innovative transfer learning approaches, which refine cross-subject generalizability and minimize calibration through strategic application of domain adaptation and few-shot learning strategies. Pioneering developments in deep learning also offer promising enhancements, facilitating robust domain adaptation and significantly improving system responsiveness and accuracy in SSVEP classification. However, these methods often require complex tuning and extensive data, limiting immediate applicability. iFuzzyTL introduces an adaptive framework that combines fuzzy logic principles with neural network architectures, focusing on efficient knowledge transfer and domain adaptation. iFuzzyTL refines input signal processing and classification in a human-interpretable format by integrating fuzzy inference systems and attention mechanisms. This approach bolsters the model's precision and aligns with real-world operational demands by effectively managing the inherent variability and uncertainty of EEG data. The model's efficacy is demonstrated across three datasets: 12JFPM (89.70% accuracy for 1s with an information transfer rate (ITR) of 149.58), Benchmark (85.81% accuracy for 1s with an ITR of 213.99), and eldBETA (76.50% accuracy for 1s with an ITR of 94.63), achieving state-of-the-art results and setting new benchmarks for SSVEP BCI performance.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-15</td>
<td style='padding: 8px;'>EEG-based 90-Degree Turn Intention Detection for Brain-Computer Interface</td>
<td style='padding: 6px;'>Pradyot Anand, Anant Jain, Suriya Prakash Muthukrishnan, Shubhendu Bhasin, Sitikantha Roy, Mohanavelu Kalathe, Lalan Kumar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.11339v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG)--based turn intention prediction for lower limb movement is important to build an efficient brain-computer interface (BCI) system. This study investigates the feasibility of intention detection of left-turn, right-turn, and straight walk by utilizing EEG signals obtained before the event occurrence. Synchronous data was collected using 31-channel EEG and IMU-based motion capture systems for nine healthy participants while performing left-turn, right-turn, and straight walk movements. EEG data was preprocessed with steps including Artifact Subspace Reconstruction (ASR), re-referencing, and Independent Component Analysis (ICA) to remove data noise. Feature extraction from the preprocessed EEG data involved computing various statistical measures (mean, median, standard deviation, skew, and kurtosis), and Hjorth parameters (activity, mobility, and complexity). Further, the feature selection was performed using the Random forest algorithm for the dimensionality reduction. The feature set obtained was utilized for 3-class classification using XG boost, gradient boosting, and support vector machine (SVM) with RBF kernel classifiers in a five-fold cross-validation scheme. Using the proposed intention detection methodology, the SVM classifier using an EEG window of 1.5 s and 0 s time-lag has the best decoding performance with mean accuracy, precision, and recall of 81.23%, 85.35%, and 83.92%, respectively, across the nine participants. The decoding analysis shows the feasibility of turn intention prediction for lower limb movement using the EEG signal before the event onset.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-13</td>
<td style='padding: 8px;'>Towards Homogeneous Lexical Tone Decoding from Heterogeneous Intracranial Recordings</td>
<td style='padding: 6px;'>Di Wu, Siyuan Li, Chen Feng, Lu Cao, Yue Zhang, Jie Yang, Mohamad Sawan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.12866v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent advancements in brain-computer interfaces (BCIs) have enabled the decoding of lexical tones from intracranial recordings, offering the potential to restore the communication abilities of speech-impaired tonal language speakers. However, data heterogeneity induced by both physiological and instrumental factors poses a significant challenge for unified invasive brain tone decoding. Traditional subject-specific models, which operate under a heterogeneous decoding paradigm, fail to capture generalized neural representations and cannot effectively leverage data across subjects. To address these limitations, we introduce Homogeneity-Heterogeneity Disentangled Learning for neural Representations (H2DiLR), a novel framework that disentangles and learns both the homogeneity and heterogeneity from intracranial recordings across multiple subjects. To evaluate H2DiLR, we collected stereoelectroencephalography (sEEG) data from multiple participants reading Mandarin materials comprising 407 syllables, representing nearly all Mandarin characters. Extensive experiments demonstrate that H2DiLR, as a unified decoding paradigm, significantly outperforms the conventional heterogeneous decoding approach. Furthermore, we empirically confirm that H2DiLR effectively captures both homogeneity and heterogeneity during neural representation learning.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-13</td>
<td style='padding: 8px;'>EEG-based AI-BCI Wheelchair Advancement: A Brain-Computer Interfacing Wheelchair System Using Machine Learning Mechanism with Right and Left Voluntary Hand Movement</td>
<td style='padding: 6px;'>Biplov Paneru, Bishwash Paneru, Khem Narayan Poudyal</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.09763v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper presents an Artificial Intelligence (AI) integrated novel approach to Brain-Computer Interface (BCI)-based wheelchair development, utilizing a voluntary Right Left Hand Movement mechanism for control. The system is designed to simulate wheelchair navigation based on voluntary right and left-hand movements using electroencephalogram (EEG) data. A pre-filtered dataset, obtained from an open-source EEG repository, was segmented into arrays of 19x200 to capture the onset of hand movements. The data was acquired at a sampling frequency 200Hz in the laboratory experiment. The system integrates a Tkinter-based interface for simulating wheelchair movements, offering users a functional and intuitive control system. Various machine learning models, including Support Vector Machines (SVM), XGBoost, random forest, and a Bi-directional Long Short-Term Memory (Bi-LSTM) attention-based model, were developed. The random forest model obtained 79% accuracy. Great performance was seen on the Logistic Regression model which outperforms other models with 92% accuracy and 91% accuracy on the Multi-Layer Perceptron (MLP) model. The Bi-LSTM attention-based model achieved a mean accuracy of 86% through cross-validation, showcasing the potential of attention mechanisms in BCI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-11</td>
<td style='padding: 8px;'>Towards Effective Deep Neural Network Approach for Multi-Trial P300-based Character Recognition in Brain-Computer Interfaces</td>
<td style='padding: 6px;'>Praveen Kumar Shukla, Hubert Cecotti, Yogesh Kumar Meena</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.08561v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-computer interfaces (BCIs) enable direct interaction between users and computers by decoding brain signals. This study addresses the challenges of detecting P300 event-related potentials in electroencephalograms (EEGs) and integrating these P300 responses for character spelling, particularly within oddball paradigms characterized by uneven P300 distribution, low target probability, and poor signal-to-noise ratio (SNR). This work proposes a weighted ensemble spatio-sequential convolutional neural network (WE-SPSQ-CNN) to improve classification accuracy and SNR by mitigating signal variability for character identification. We evaluated the proposed WE-SPSQ-CNN on dataset II from the BCI Competition III, achieving P300 classification accuracies of 69.7\% for subject A and 79.9\% for subject B across fifteen epochs. For character recognition, the model achieved average accuracies of 76.5\%, 87.5\%, and 94.5\% with five, ten, and fifteen repetitions, respectively. Our proposed model outperformed state-of-the-art models in the five-repetition and delivered comparable performance in the ten and fifteen repetitions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-10</td>
<td style='padding: 8px;'>Post-Training Quantization in Brain-Computer Interfaces based on Event-Related Potential Detection</td>
<td style='padding: 6px;'>Hubert Cecotti, Dalvir Dhaliwal, Hardip Singh, Yogesh Kumar Meena</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.07920v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Post-training quantization (PTQ) is a technique used to optimize and reduce the memory footprint and computational requirements of machine learning models. It has been used primarily for neural networks. For Brain-Computer Interfaces (BCI) that are fully portable and usable in various situations, it is necessary to provide approaches that are lightweight for storage and computation. In this paper, we propose the evaluation of post-training quantization on state-of-the-art approaches in brain-computer interfaces and assess their impact on accuracy. We evaluate the performance of the single-trial detection of event-related potentials representing one major BCI paradigm. The area under the receiver operating characteristic curve drops from 0.861 to 0.825 with PTQ when applied on both spatial filters and the classifier, while reducing the size of the model by about $\times$ 15. The results support the conclusion that PTQ can substantially reduce the memory footprint of the models while keeping roughly the same level of accuracy.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-08</td>
<td style='padding: 8px;'>Neural Signal Operated Intelligent Robot: Human-guided Robot Maze Navigation through SSVEP</td>
<td style='padding: 6px;'>Jiarui Tang, Tingrui Sun, Siwen Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.11867v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-computer Interface (BCI) applications based on steady-state visual evoked potentials (SSVEP) have the advantages of being fast, accurate and mobile. SSVEP is the EEG response evoked by visual stimuli that are presented at a specific frequency, which results in an increase in the EEG at that same frequency. In this paper, we proposed a novel human-guided maze solving robot navigation system based on SSVEP. By integrating human's intelligence which sees the entirety of the maze, maze solving time could be significantly reduced. Our methods involve training an offline SSVEP classification model, implementing the robot self-navigation algorithm, and finally deploy the model online for real-time robot operation. Our results demonstrated such system to be feasible, and it has the potential to impact the life of many elderly people by helping them carrying out simple daily tasks at home with just the look of their eyes.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-08</td>
<td style='padding: 8px;'>Bayesian model of individual learning to control a motor imagery BCI</td>
<td style='padding: 6px;'>Côme Annicchiarico, Fabien Lotte, Jérémie Mattout</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.05926v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The cognitive mechanisms underlying subjects' self-regulation in Brain-Computer Interface (BCI) and neurofeedback (NF) training remain poorly understood. Yet, a mechanistic computational model of each individual learning trajectory is required to improve the reliability of BCI applications. The few existing attempts mostly rely on model-free (reinforcement learning) approaches. Hence, they cannot capture the strategy developed by each subject and neither finely predict their learning curve. In this study, we propose an alternative, model-based approach rooted in cognitive skill learning within the Active Inference framework. We show how BCI training may be framed as an inference problem under high uncertainties. We illustrate the proposed approach on a previously published synthetic Motor Imagery ERD laterality training. We show how simple changes in model parameters allow us to qualitatively match experimental results and account for various subject. In the near future, this approach may provide a powerful computational to model individual skill learning and thus optimize and finely characterize BCI training.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-02</td>
<td style='padding: 8px;'>SHAP-CAT: A interpretable multi-modal framework enhancing WSI classification via virtual staining and shapley-value-based multimodal fusion</td>
<td style='padding: 6px;'>Jun Wang, Yu Mao, Nan Guan, Chun Jason Xue</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.01408v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The multimodal model has demonstrated promise in histopathology. However, most multimodal models are based on H\&E and genomics, adopting increasingly complex yet black-box designs. In our paper, we propose a novel interpretable multimodal framework named SHAP-CAT, which uses a Shapley-value-based dimension reduction technique for effective multimodal fusion. Starting with two paired modalities -- H\&E and IHC images, we employ virtual staining techniques to enhance limited input data by generating a new clinical-related modality. Lightweight bag-level representations are extracted from image modalities and a Shapley-value-based mechanism is used for dimension reduction. For each dimension of the bag-level representation, attribution values are calculated to indicate how changes in the specific dimensions of the input affect the model output. In this way, we select a few top important dimensions of bag-level representation for each image modality to late fusion. Our experimental results demonstrate that the proposed SHAP-CAT framework incorporating synthetic modalities significantly enhances model performance, yielding a 5\% increase in accuracy for the BCI, an 8\% increase for IHC4BC-ER, and an 11\% increase for the IHC4BC-PR dataset.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-30</td>
<td style='padding: 8px;'>Professor X: Manipulating EEG BCI with Invisible and Robust Backdoor Attack</td>
<td style='padding: 6px;'>Xuan-Hao Liu, Xinhao Song, Dexuan He, Bao-Liang Lu, Wei-Long Zheng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.20158v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While electroencephalogram (EEG) based brain-computer interface (BCI) has been widely used for medical diagnosis, health care, and device control, the safety of EEG BCI has long been neglected. In this paper, we propose Professor X, an invisible and robust "mind-controller" that can arbitrarily manipulate the outputs of EEG BCI through backdoor attack, to alert the EEG community of the potential hazard. However, existing EEG attacks mainly focus on single-target class attacks, and they either require engaging the training stage of the target BCI, or fail to maintain high stealthiness. Addressing these limitations, Professor X exploits a three-stage clean label poisoning attack: 1) selecting one trigger for each class; 2) learning optimal injecting EEG electrodes and frequencies strategy with reinforcement learning for each trigger; 3) generating poisoned samples by injecting the corresponding trigger's frequencies into poisoned data for each class by linearly interpolating the spectral amplitude of both data according to previously learned strategies. Experiments on datasets of three common EEG tasks demonstrate the effectiveness and robustness of Professor X, which also easily bypasses existing backdoor defenses.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>L1-Regularized ICA: A Novel Method for Analysis of Task-related fMRI Data</td>
<td style='padding: 6px;'>Yusuke Endo, Koujin Takeda</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13171v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We propose a new method of independent component analysis (ICA) in order to extract appropriate features from high-dimensional data. In general, matrix factorization methods including ICA have a problem regarding the interpretability of extracted features. For the improvement of interpretability, it is considered that sparse constraint on a factorized matrix is helpful. With this background, we construct a new ICA method with sparsity. In our method, the L1-regularization term is added to the cost function of ICA, and minimization of the cost function is performed by difference of convex functions algorithm. For the validity of our proposed method, we apply it to synthetic data and real functional magnetic resonance imaging data.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-16</td>
<td style='padding: 8px;'>On the Role of Activation Functions in EEG-To-Text Decoder</td>
<td style='padding: 6px;'>Zenon Lamprou, Iakovos Tenedios, Yashar Moshfeghi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.12572v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In recent years, much interdisciplinary research has been conducted exploring potential use cases of neuroscience to advance the field of information retrieval. Initial research concentrated on the use of fMRI data, but fMRI was deemed to be not suitable for real-world applications, and soon, research shifted towards using EEG data. In this paper, we try to improve the original performance of a first attempt at generating text using EEG by focusing on the less explored area of optimising neural network performance. We test a set of different activation functions and compare their performance. Our results show that introducing a higher degree polynomial activation function can enhance model performance without changing the model architecture. We also show that the learnable 3rd-degree activation function performs better on the 1-gram evaluation compared to a 3rd-degree non-learnable function. However, when evaluating the model on 2-grams and above, the polynomial function lacks in performance, whilst the leaky ReLU activation function outperforms the baseline.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-15</td>
<td style='padding: 8px;'>Improving semantic understanding in speech language models via brain-tuning</td>
<td style='padding: 6px;'>Omer Moussa, Dietrich Klakow, Mariya Toneva</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.09230v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Speech language models align with human brain responses to natural language to an impressive degree. However, current models rely heavily on low-level speech features, indicating they lack brain-relevant semantics which limits their utility as model organisms of semantic processing in the brain. In this work, we address this limitation by inducing brain-relevant bias directly into the models via fine-tuning with fMRI recordings of people listening to natural stories, a process we name brain-tuning. After testing it on 3 different pretrained model families, we show that brain-tuning not only improves overall alignment with new brain recordings in semantic language regions, but also reduces the reliance on low-level speech features for this alignment. Excitingly, we further show that brain-tuning leads to 1) consistent improvements in performance on a range of downstream tasks and 2) a representational space with increased semantic preference. Our results provide converging evidence, for the first time, that incorporating brain signals into the training of language models improves the models' semantic understanding.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-08</td>
<td style='padding: 8px;'>Contrastive Learning to Fine-Tune Feature Extraction Models for the Visual Cortex</td>
<td style='padding: 6px;'>Alex Mulrooney, Austin J. Brockmeier</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.06067v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Predicting the neural response to natural images in the visual cortex requires extracting relevant features from the images and relating those feature to the observed responses. In this work, we optimize the feature extraction in order to maximize the information shared between the image features and the neural response across voxels in a given region of interest (ROI) extracted from the BOLD signal measured by fMRI. We adapt contrastive learning (CL) to fine-tune a convolutional neural network, which was pretrained for image classification, such that a mapping of a given image's features are more similar to the corresponding fMRI response than to the responses to other images. We exploit the recently released Natural Scenes Dataset (Allen et al., 2022) as organized for the Algonauts Project (Gifford et al., 2023), which contains the high-resolution fMRI responses of eight subjects to tens of thousands of naturalistic images. We show that CL fine-tuning creates feature extraction models that enable higher encoding accuracy in early visual ROIs as compared to both the pretrained network and a baseline approach that uses a regression loss at the output of the network to tune it for fMRI response encoding. We investigate inter-subject transfer of the CL fine-tuned models, including subjects from another, lower-resolution dataset (Gong et al., 2023). We also pool subjects for fine-tuning to further improve the encoding performance. Finally, we examine the performance of the fine-tuned models on common image classification tasks, explore the landscape of ROI-specific models by applying dimensionality reduction on the Bhattacharya dissimilarity matrix created using the predictions on those tasks (Mao et al., 2024), and investigate lateralization of the processing for early visual ROIs using salience maps of the classifiers built on the CL-tuned models.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-08</td>
<td style='padding: 8px;'>Node-reconfiguring multilayer networks of human brain function</td>
<td style='padding: 6px;'>Tarmo Nurmi, Pietro De Luca, Mikko Kivelä, Onerva Korhonen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.05972v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The properties of functional brain networks are heavily influenced by how the network nodes are defined. A common approach uses Regions of Interest (ROIs), which are predetermined collections of fMRI measurement voxels, as network nodes. Their definition is always a compromise, as static ROIs cannot capture the dynamics and the temporal reconfigurations of the brain areas. Consequently, the ROIs do not align with the functionally homogeneous regions, which can explain the very low functional homogeneity values observed for the ROIs. This is in violation of the underlying homogeneity assumption in functional brain network analysis pipelines and it can cause serious problems such as spurious network structure. We introduce the node-reconfiguring multilayer network model, where nodes represent ROIs with boundaries optimized for high functional homogeneity in each time window. In this representation, network layers correspond to time windows, intralayer links depict functional connectivity between ROIs, and interlayer link weights quantify overlap between ROIs on different layers. The ROI optimization approach increases functional homogeneity notably, yielding an over 10-fold increase in the fraction of ROIs with high homogeneity compared to static ROIs from the Brainnetome atlas. The optimized ROIs reorganize non-trivially at short time scales of consecutive time windows and across several windows. The amount of reorganization across time windows is connected to intralayer hubness: ROIs that show intermediate levels of reorganization have stronger intralayer links than extremely stable or unstable ROIs. Our results demonstrate that reconfiguring parcellations yield more accurate network models of brain function. This supports the ongoing paradigm shift towards the chronnectome that sees the brain as a set of sources with continuously reconfiguring spatial and connectivity profiles.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-08</td>
<td style='padding: 8px;'>Vector-ICL: In-context Learning with Continuous Vector Representations</td>
<td style='padding: 6px;'>Yufan Zhuang, Chandan Singh, Liyuan Liu, Jingbo Shang, Jianfeng Gao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.05629v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large language models (LLMs) have shown remarkable in-context learning (ICL) capabilities on textual data. We explore whether these capabilities can be extended to continuous vectors from diverse domains, obtained from black-box pretrained encoders. By aligning input data with an LLM's embedding space through lightweight projectors, we observe that LLMs can effectively process and learn from these projected vectors, which we term Vector-ICL. In particular, we find that pretraining projectors with general language modeling objectives enables Vector-ICL, while task-specific finetuning further enhances performance. In our experiments across various tasks and modalities, including text reconstruction, numerical function regression, text classification, summarization, molecule captioning, time-series classification, graph classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL and domain-specific model or tuning. We further conduct analyses and case studies, indicating the potential of LLMs to process vector representations beyond traditional token-based paradigms.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-07</td>
<td style='padding: 8px;'>Multi-Stage Graph Learning for fMRI Analysis to Diagnose Neuro-Developmental Disorders</td>
<td style='padding: 6px;'>Wenjing Gao, Yuanyuan Yang, Jianrui Wei, Xuntao Yin, Xinhan Di</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.05342v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The insufficient supervision limit the performance of the deep supervised models for brain disease diagnosis. It is important to develop a learning framework that can capture more information in limited data and insufficient supervision. To address these issues at some extend, we propose a multi-stage graph learning framework which incorporates 1) pretrain stage : self-supervised graph learning on insufficient supervision of the fmri data 2) fine-tune stage : supervised graph learning for brain disorder diagnosis. Experiment results on three datasets, Autism Brain Imaging Data Exchange ABIDE I, ABIDE II and ADHD with AAL1,demonstrating the superiority and generalizability of the proposed framework compared to the state of art of models.(ranging from 0.7330 to 0.9321,0.7209 to 0.9021,0.6338 to 0.6699)</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-07</td>
<td style='padding: 8px;'>NeuroBOLT: Resting-state EEG-to-fMRI Synthesis with Multi-dimensional Feature Mapping</td>
<td style='padding: 6px;'>Yamin Li, Ange Lou, Ziyuan Xu, Shengchao Zhang, Shiyu Wang, Dario J. Englot, Soheil Kolouri, Daniel Moyer, Roza G. Bayrak, Catie Chang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.05341v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional magnetic resonance imaging (fMRI) is an indispensable tool in modern neuroscience, providing a non-invasive window into whole-brain dynamics at millimeter-scale spatial resolution. However, fMRI is constrained by issues such as high operation costs and immobility. With the rapid advancements in cross-modality synthesis and brain decoding, the use of deep neural networks has emerged as a promising solution for inferring whole-brain, high-resolution fMRI features directly from electroencephalography (EEG), a more widely accessible and portable neuroimaging modality. Nonetheless, the complex projection from neural activity to fMRI hemodynamic responses and the spatial ambiguity of EEG pose substantial challenges both in modeling and interpretability. Relatively few studies to date have developed approaches for EEG-fMRI translation, and although they have made significant strides, the inference of fMRI signals in a given study has been limited to a small set of brain areas and to a single condition (i.e., either resting-state or a specific task). The capability to predict fMRI signals in other brain areas, as well as to generalize across conditions, remain critical gaps in the field. To tackle these challenges, we introduce a novel and generalizable framework: NeuroBOLT, i.e., Neuro-to-BOLD Transformer, which leverages multi-dimensional representation learning from temporal, spatial, and spectral domains to translate raw EEG data to the corresponding fMRI activity signals across the brain. Our experiments demonstrate that NeuroBOLT effectively reconstructs resting-state fMRI signals from primary sensory, high-level cognitive areas, and deep subcortical brain regions, achieving state-of-the-art accuracy and significantly advancing the integration of these two modalities.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-06</td>
<td style='padding: 8px;'>BrainCodec: Neural fMRI codec for the decoding of cognitive brain states</td>
<td style='padding: 6px;'>Yuto Nishimura, Masataka Sawayama, Ayumu Yamashita, Hideki Nakayama, Kaoru Amano</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.04383v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recently, leveraging big data in deep learning has led to significant performance improvements, as confirmed in applications like mental state decoding using fMRI data. However, fMRI datasets remain relatively small in scale, and the inherent issue of low signal-to-noise ratios (SNR) in fMRI data further exacerbates these challenges. To address this, we apply compression techniques as a preprocessing step for fMRI data. We propose BrainCodec, a novel fMRI codec inspired by the neural audio codec. We evaluated BrainCodec's compression capability in mental state decoding, demonstrating further improvements over previous methods. Furthermore, we analyzed the latent representations obtained through BrainCodec, elucidating the similarities and differences between task and resting state fMRI, highlighting the interpretability of BrainCodec. Additionally, we demonstrated that fMRI reconstructions using BrainCodec can enhance the visibility of brain activity by achieving higher SNR, suggesting its potential as a novel denoising method. Our study shows that BrainCodec not only enhances performance over previous methods but also offers new analytical possibilities for neuroscience. Our codes, dataset, and model weights are available at https://github.com/amano-k-lab/BrainCodec.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-05</td>
<td style='padding: 8px;'>TV-based Deep 3D Self Super-Resolution for fMRI</td>
<td style='padding: 6px;'>Fernando Pérez-Bueno, Hongwei Bran Li, Shahin Nasr, Cesar Caballero-Gaudes, Juan Eugenio Iglesias</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.04097v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While functional Magnetic Resonance Imaging (fMRI) offers valuable insights into cognitive processes, its inherent spatial limitations pose challenges for detailed analysis of the fine-grained functional architecture of the brain. More specifically, MRI scanner and sequence specifications impose a trade-off between temporal resolution, spatial resolution, signal-to-noise ratio, and scan time. Deep Learning (DL) Super-Resolution (SR) methods have emerged as a promising solution to enhance fMRI resolution, generating high-resolution (HR) images from low-resolution (LR) images typically acquired with lower scanning times. However, most existing SR approaches depend on supervised DL techniques, which require training ground truth (GT) HR data, which is often difficult to acquire and simultaneously sets a bound for how far SR can go. In this paper, we introduce a novel self-supervised DL SR model that combines a DL network with an analytical approach and Total Variation (TV) regularization. Our method eliminates the need for external GT images, achieving competitive performance compared to supervised DL techniques and preserving the functional maps.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-11</td>
<td style='padding: 8px;'>Determining sensor geometry and gain in a wearable MEG system</td>
<td style='padding: 6px;'>Ryan M. Hill, Gonzalo Reina Rivero, Ashley J. Tyler, Holly Schofield, Cody Doyle, James Osborne, David Bobela, Lukas Rier, Joseph Gibson, Zoe Tanner, Elena Boto, Richard Bowtell, Matthew J. Brookes, Vishal Shah, Niall Holmes</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.08718v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Optically pumped magnetometers (OPMs) are compact and lightweight sensors that can measure magnetic fields generated by current flow in neuronal assemblies in the brain. Such sensors enable construction of magnetoencephalography (MEG) instrumentation, with significant advantages over conventional MEG devices including adaptability to head size, enhanced movement tolerance, lower complexity and improved data quality. However, realising the potential of OPMs depends on our ability to perform system calibration, which means finding sensor locations, orientations, and the relationship between the sensor output and magnetic field (termed sensor gain). Such calibration is complex in OPMMEG since, for example, OPM placement can change from subject to subject (unlike in conventional MEG where sensor locations or orientations are fixed). Here, we present two methods for calibration, both based on generating well-characterised magnetic fields across a sensor array. Our first device (the HALO) is a head mounted system that generates dipole like fields from a set of coils. Our second (the matrix coil (MC)) generates fields using coils embedded in the walls of a magnetically shielded room. Our results show that both methods offer an accurate means to calibrate an OPM array (e.g. sensor locations within 2 mm of the ground truth) and that the calibrations produced by the two methods agree strongly with each other. When applied to data from human MEG experiments, both methods offer improved signal to noise ratio after beamforming suggesting that they give calibration parameters closer to the ground truth than factory settings and presumed physical sensor coordinates and orientations. Both techniques are practical and easy to integrate into real world MEG applications. This advances the field significantly closer to the routine use of OPMs for MEG recording.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-09</td>
<td style='padding: 8px;'>Nested Deep Learning Model Towards A Foundation Model for Brain Signal Data</td>
<td style='padding: 6px;'>Fangyi Wei, Jiajie Mo, Kai Zhang, Haipeng Shen, Srikantan Nagarajan, Fei Jiang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.03191v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Epilepsy affects over 50 million people globally, with EEG/MEG-based spike detection playing a crucial role in diagnosis and treatment. Manual spike identification is time-consuming and requires specialized training, limiting the number of professionals available to analyze EEG/MEG data. To address this, various algorithmic approaches have been developed. However, current methods face challenges in handling varying channel configurations and in identifying the specific channels where spikes originate. This paper introduces a novel Nested Deep Learning (NDL) framework designed to overcome these limitations. NDL applies a weighted combination of signals across all channels, ensuring adaptability to different channel setups, and allows clinicians to identify key channels more accurately. Through theoretical analysis and empirical validation on real EEG/MEG datasets, NDL demonstrates superior accuracy in spike detection and channel localization compared to traditional methods. The results show that NDL improves prediction accuracy, supports cross-modality data integration, and can be fine-tuned for various neurophysiological applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-23</td>
<td style='padding: 8px;'>Dual Stream Graph Transformer Fusion Networks for Enhanced Brain Decoding</td>
<td style='padding: 6px;'>Lucas Goene, Siamak Mehrkanoon</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.07189v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper presents the novel Dual Stream Graph-Transformer Fusion (DS-GTF) architecture designed specifically for classifying task-based Magnetoencephalography (MEG) data. In the spatial stream, inputs are initially represented as graphs, which are then passed through graph attention networks (GAT) to extract spatial patterns. Two methods, TopK and Thresholded Adjacency are introduced for initializing the adjacency matrix used in the GAT. In the temporal stream, the Transformer Encoder receives concatenated windowed input MEG data and learns new temporal representations. The learned temporal and spatial representations from both streams are fused before reaching the output layer. Experimental results demonstrate an enhancement in classification performance and a reduction in standard deviation across multiple test subjects compared to other examined models.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-16</td>
<td style='padding: 8px;'>MEGS: Morphological Evaluation of Galactic Structure</td>
<td style='padding: 6px;'>Ufuk Çakır, Tobias Buck</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.10346v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding the morphology of galaxies is a critical aspect of astrophysics research, providing insight into the formation, evolution, and physical properties of these vast cosmic structures. Various observational and computational methods have been developed to quantify galaxy morphology, and with the advent of large galaxy simulations, the need for automated and effective classification methods has become increasingly important. This paper investigates the use of Principal Component Analysis (PCA) as an interpretable dimensionality reduction algorithm for galaxy morphology using the IllustrisTNG cosmological simulation dataset with the aim of developing a generative model for galaxies. We first generate a dataset of 2D images and 3D cubes of galaxies from the IllustrisTNG simulation, focusing on the mass, metallicity, and stellar age distribution of each galaxy. PCA is then applied to this data, transforming it into a lower-dimensional image space, where closeness of data points corresponds to morphological similarity. We find that PCA can effectively capture the key morphological features of galaxies, with a significant proportion of the variance in the data being explained by a small number of components. With our method we achieve a dimensionality reduction by a factor of $\sim200$ for 2D images and $\sim3650$ for 3D cubes at a reconstruction accuracy below five percent. Our results illustrate the potential of PCA in compressing large cosmological simulations into an interpretable generative model for galaxies that can easily be used in various downstream tasks such as galaxy classification and analysis.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-13</td>
<td style='padding: 8px;'>Characterizing the Molecular Gas in Infrared Bright Galaxies with CARMA</td>
<td style='padding: 6px;'>Katherine Alatalo, Andreea O. Petric, Lauranne Lanz, Kate Rowlands, Vivian U, Kirsten L. Larson, Lee Armus, Loreto Barcos-Muñoz, Aaron S. Evans, Jin Koda, Yuanze Luo, Anne M. Medling, Kristina E. Nyland, Justin A. Otter, Pallavi Patil, Fernando Peñaloza, Diane Salim, David B. Sanders, Elizaveta Sazonova, Maya Skarbinski, Yiqing Song, Ezequiel Treister, C. Meg Urry</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.09116v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We present the CO(1-0) maps of 28 infrared-bright galaxies from the Great Observatories All-Sky Luminous Infrared Galaxy Survey (GOALS) taken with the Combined Array for Research in Millimeter Astronomy (CARMA). We detect 100GHz continuum in 16 of 28 galaxies, which trace both active galactic nuclei (AGNs) and compact star-forming cores. The GOALS galaxies show a variety of molecular gas morphologies, though in the majority of cases, the average velocity fields show a gradient consistent with rotation. We fit the full continuum SEDs of each of the source using either MAGPHYS or SED3FIT (if there are signs of an AGN) to derive the total stellar mass, dust mass, and star formation rates of each object. We adopt a value determined from luminous and ultraluminous infrared galaxies (LIRGs and ULIRGs) of $\alpha_{\rm CO}=1.5^{+1.3}_{-0.8}~M_\odot$ (K km s$^{-1}$ pc$^2)^{-1}$, which leads to more physical values for $f_{\rm mol}$ and the gas-to-dust ratio. Mergers tend to have the highest gas-to-dust ratios. We assume the cospatiality of the molecular gas and star formation, and plot the sample on the Schmidt-Kennicutt relation, we find that they preferentially lie above the line set by normal star-forming galaxies. This hyper-efficiency is likely due to the increased turbulence in these systems, which decreases the freefall time compared to star-forming galaxies, leading to "enhanced" star formation efficiency. Line wings are present in a non-negligible subsample (11/28) of the CARMA GOALS sources and are likely due to outflows driven by AGNs or star formation, gas inflows, or additional decoupled gas components.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-06</td>
<td style='padding: 8px;'>Lepton-flavor changing decays and non-unitarity in the inverse seesaw mechanism</td>
<td style='padding: 6px;'>Adrián González-Quiterio, Héctor Novales-Sánchez</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.03952v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The pursuit of the genuine fundamental description, governing nature at some high-energy scale, must invariably consider the yet-unknown mechanism behind the generation of neutrino mass. Lepton-flavor violating decays $l_\alpha\to\gamma\,l_\beta$, allowed in the presence of neutrino mass and mixing, provide a mean to look for physics beyond the Standard Model. In the present work we consider the inverse seesaw mechanism and then revisit the calculation of its contributions to the branching ratios of the aforementioned decay processes, among which we find $\mu\to\gamma\,e$ to be more promising, in the light of current bounds by the MEG Collaboration. Deviations from unitarity in the mixing of light neutrinos are related to the branching ratios ${\rm Br}\big( l_\alpha\to\gamma\,l_\beta \big)$ in a simple manner, which we address, then finding that, while experimental data are consistent with current bounds on non-unitarity effects, the upcoming MEG II update shall be able to improve restrictions on such effects by a factor $\sim\frac{1}{3}$.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-10</td>
<td style='padding: 8px;'>Bayesian Inference General Procedures for A Single-subject Test Study</td>
<td style='padding: 6px;'>Jie Li, Gary Green, Sarah J. A. Carr, Peng Liu, Jian Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.15419v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Abnormality detection in the identification of a single-subject which deviates from the majority of the dataset that comes from a control group is a critical problem. A common approach is to assume that the control group can be characterised in terms of standard Normal statistics and the detection of single abnormal subject is in that context. But in many situations the control group can not be described in terms of Normal statistics and the use of standard statistics is inappropriate. This paper presents a Bayesian Inference General Procedures for A Single-Subject Test (BIGPAST), designed to mitigate the effects of skewness under the assumption that the dataset of control group comes from the skewed Student's \( t \) distribution. BIGPAST operates under the null hypothesis that the single-subject follows the same distribution as the control group. We assess BIGPAST's performance against other methods through a series of simulation studies. The results demonstrate that BIGPAST is robust against deviations from normality and outperforms the existing approaches in terms of accuracy. This is because BIGPAST can effectively reduce model misspecification errors under the skewed Student's \( t \) assumption. We apply BIGPAST to a MEG dataset consisting of an individual with mild traumatic brain injury and an age and gender-matched control group, demonstrating its effectiveness in detecting abnormalities in the single-subject.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-23</td>
<td style='padding: 8px;'>Enabling Distributed Generative Artificial Intelligence in 6G: Mobile Edge Generation</td>
<td style='padding: 6px;'>Ruikang Zhong, Xidong Mu, Mona Jaber, Yuanwei Liu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.05870v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Mobile edge generation (MEG) is an emerging technology that allows the network to meet the challenging traffic load expectations posed by the rise of generative artificial intelligence~(GAI). A novel MEG model is proposed for deploying GAI models on edge servers (ES) and user equipment~(UE) to jointly complete text-to-image generation tasks. In the generation task, the ES and UE will cooperatively generate the image according to the text prompt given by the user. To enable the MEG, a pre-trained latent diffusion model (LDM) is invoked to generate the latent feature, and an edge-inferencing MEG protocol is employed for data transmission exchange between the ES and the UE. A compression coding technique is proposed for compressing the latent features to produce seeds. Based on the above seed-enabled MEG model, an image quality optimization problem with transmit power constraint is formulated. The transmitting power of the seed is dynamically optimized by a deep reinforcement learning agent over the fading channel. The proposed MEG enabled text-to-image generation system is evaluated in terms of image quality and transmission overhead. The numerical results indicate that, compared to the conventional centralized generation-and-downloading scheme, the symbol number of the transmission of MEG is materially reduced. In addition, the proposed compression coding approach can improve the quality of generated images under low signal-to-noise ratio (SNR) conditions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-09</td>
<td style='padding: 8px;'>Towards improving Alzheimer's intervention: a machine learning approach for biomarker detection through combining MEG and MRI pipelines</td>
<td style='padding: 6px;'>Alwani Liyana Ahmad, Jose Sanchez-Bornot, Roberto C. Sotero, Damien Coyle, Zamzuri Idris, Ibrahima Faye</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.04815v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>MEG are non invasive neuroimaging techniques with excellent temporal and spatial resolution, crucial for studying brain function in dementia and Alzheimer Disease. They identify changes in brain activity at various Alzheimer stages, including preclinical and prodromal phases. MEG may detect pathological changes before clinical symptoms, offering potential biomarkers for intervention. This study evaluates classification techniques using MEG features to distinguish between healthy controls and mild cognitive impairment participants from the BioFIND study. We compare MEG based biomarkers with MRI based anatomical features, both independently and combined. We used 3 Tesla MRI and MEG data from 324 BioFIND participants;158 MCI and 166 HC. Analyses were performed using MATLAB with SPM12 and OSL toolboxes. Machine learning analyses, including 100 Monte Carlo replications of 10 fold cross validation, were conducted on sensor and source spaces. Combining MRI with MEG features achieved the best performance; 0.76 accuracy and AUC of 0.82 for GLMNET using LCMV source based MEG. MEG only analyses using LCMV and eLORETA also performed well, suggesting that combining uncorrected MEG with z-score-corrected MRI features is optimal.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-05</td>
<td style='padding: 8px;'>Classification of Raw MEG/EEG Data with Detach-Rocket Ensemble: An Improved ROCKET Algorithm for Multivariate Time Series Analysis</td>
<td style='padding: 6px;'>Adrià Solana, Erik Fransén, Gonzalo Uribarri</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.02760v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Multivariate Time Series Classification (MTSC) is a ubiquitous problem in science and engineering, particularly in neuroscience, where most data acquisition modalities involve the simultaneous time-dependent recording of brain activity in multiple brain regions. In recent years, Random Convolutional Kernel models such as ROCKET and MiniRocket have emerged as highly effective time series classification algorithms, capable of achieving state-of-the-art accuracy results with low computational load. Despite their success, these types of models face two major challenges when employed in neuroscience: 1) they struggle to deal with high-dimensional data such as EEG and MEG, and 2) they are difficult to interpret. In this work, we present a novel ROCKET-based algorithm, named Detach-Rocket Ensemble, that is specifically designed to address these two problems in MTSC. Our algorithm leverages pruning to provide an integrated estimation of channel importance, and ensembles to achieve better accuracy and provide a label probability. Using a synthetic multivariate time series classification dataset in which we control the amount of information carried by each of the channels, we first show that our algorithm is able to correctly recover the channel importance for classification. Then, using two real-world datasets, a MEG dataset and an EEG dataset, we show that Detach-Rocket Ensemble is able to provide both interpretable channel relevance and competitive classification accuracy, even when applied directly to the raw brain data, without the need for feature engineering.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-09</td>
<td style='padding: 8px;'>Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models</td>
<td style='padding: 6px;'>Emily Cheng, Richard J. Antonello</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.05771v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first "composition" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-07-22</td>
<td style='padding: 8px;'>Predictive Coding Networks and Inference Learning: Tutorial and Survey</td>
<td style='padding: 6px;'>Björn van Zwol, Ro Jefferson, Egon L. van den Broek</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2407.04117v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent years have witnessed a growing call for renewed emphasis on neuroscience-inspired approaches in artificial intelligence research, under the banner of NeuroAI. A prime example of this is predictive coding networks (PCNs), based on the neuroscientific framework of predictive coding. This framework views the brain as a hierarchical Bayesian inference model that minimizes prediction errors through feedback connections. Unlike traditional neural networks trained with backpropagation (BP), PCNs utilize inference learning (IL), a more biologically plausible algorithm that explains patterns of neural activity that BP cannot. Historically, IL has been more computationally intensive, but recent advancements have demonstrated that it can achieve higher efficiency than BP with sufficient parallelization. Furthermore, PCNs can be mathematically considered a superset of traditional feedforward neural networks (FNNs), significantly extending the range of trainable architectures. As inherently probabilistic (graphical) latent variable models, PCNs provide a versatile framework for both supervised learning and unsupervised (generative) modeling that goes beyond traditional artificial neural networks. This work provides a comprehensive review and detailed formal specification of PCNs, particularly situating them within the context of modern ML methods. Additionally, we introduce a Python library (PRECO) for practical implementation. This positions PC as a promising framework for future ML innovations.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-10-29</td>
<td style='padding: 8px;'>Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis</td>
<td style='padding: 6px;'>Mitchell Ostrow, Adam Eisen, Leo Kozachkov, Ila Fiete</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2306.10168v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-05-25</td>
<td style='padding: 8px;'>Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture</td>
<td style='padding: 6px;'>Galen Pogoncheff, Jacob Granley, Michael Beyeler</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2305.11275v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-03-11</td>
<td style='padding: 8px;'>Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks</td>
<td style='padding: 6px;'>Feng-Lei Fan, Yingxin Li, Hanchuan Peng, Tieyong Zeng, Fei Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2301.09245v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Throughout history, the development of artificial intelligence, particularly artificial neural networks, has been open to and constantly inspired by the increasingly deepened understanding of the brain, such as the inspiration of neocognitron, which is the pioneering work of convolutional neural networks. Per the motives of the emerging field: NeuroAI, a great amount of neuroscience knowledge can help catalyze the next generation of AI by endowing a network with more powerful capabilities. As we know, the human brain has numerous morphologically and functionally different neurons, while artificial neural networks are almost exclusively built on a single neuron type. In the human brain, neuronal diversity is an enabling factor for all kinds of biological intelligent behaviors. Since an artificial network is a miniature of the human brain, introducing neuronal diversity should be valuable in terms of addressing those essential problems of artificial networks such as efficiency, interpretability, and memory. In this Primer, we first discuss the preliminaries of biological neuronal diversity and the characteristics of information transmission and processing in a biological neuron. Then, we review studies of designing new neurons for artificial networks. Next, we discuss what gains can neuronal diversity bring into artificial networks and exemplary applications in several important fields. Lastly, we discuss the challenges and future directions of neuronal diversity to explore the potential of NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2022-12-08</td>
<td style='padding: 8px;'>A Rubric for Human-like Agents and NeuroAI</td>
<td style='padding: 6px;'>Ida Momennejad</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2212.04401v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Researchers across cognitive, neuro-, and computer sciences increasingly reference human-like artificial intelligence and neuroAI. However, the scope and use of the terms are often inconsistent. Contributed research ranges widely from mimicking behaviour, to testing machine learning methods as neurally plausible hypotheses at the cellular or functional levels, or solving engineering problems. However, it cannot be assumed nor expected that progress on one of these three goals will automatically translate to progress in others. Here a simple rubric is proposed to clarify the scope of individual contributions, grounded in their commitments to human-like behaviour, neural plausibility, or benchmark/engineering goals. This is clarified using examples of weak and strong neuroAI and human-like agents, and discussing the generative, corroborate, and corrective ways in which the three dimensions interact with one another. The author maintains that future progress in artificial intelligence will need strong interactions across the disciplines, with iterative feedback loops and meticulous validity tests, leading to both known and yet-unknown advances that may span decades to come.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-02-22</td>
<td style='padding: 8px;'>Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolution</td>
<td style='padding: 6px;'>Anthony Zador, Sean Escola, Blake Richards, Bence Ölveczky, Yoshua Bengio, Kwabena Boahen, Matthew Botvinick, Dmitri Chklovskii, Anne Churchland, Claudia Clopath, James DiCarlo, Surya Ganguli, Jeff Hawkins, Konrad Koerding, Alexei Koulakov, Yann LeCun, Timothy Lillicrap, Adam Marblestone, Bruno Olshausen, Alexandre Pouget, Cristina Savin, Terrence Sejnowski, Eero Simoncelli, Sara Solla, David Sussillo, Andreas S. Tolias, Doris Tsao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2210.08340v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience has long been an essential driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI. A core component of this is the embodied Turing test, which challenges AI animal models to interact with the sensorimotor world at skill levels akin to their living counterparts. The embodied Turing test shifts the focus from those capabilities like game playing and language that are especially well-developed or uniquely human to those capabilities, inherited from over 500 million years of evolution, that are shared with all animals. Building models that can pass the embodied Turing test will provide a roadmap for the next generation of AI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2022-04-11</td>
<td style='padding: 8px;'>Social Neuro AI: Social Interaction as the "dark matter" of AI</td>
<td style='padding: 6px;'>Samuele Bolotta, Guillaume Dumas</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2112.15459v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This article introduces a three-axis framework indicating how AI can be informed by biological examples of social learning mechanisms. We argue that the complex human cognitive architecture owes a large portion of its expressive power to its ability to engage in social and cultural learning. However, the field of AI has mostly embraced a solipsistic perspective on intelligence. We thus argue that social interactions not only are largely unexplored in this field but also are an essential element of advanced cognitive ability, and therefore constitute metaphorically the dark matter of AI. In the first section, we discuss how social learning plays a key role in the development of intelligence. We do so by discussing social and cultural learning theories and empirical findings from social neuroscience. Then, we discuss three lines of research that fall under the umbrella of Social NeuroAI and can contribute to developing socially intelligent embodied agents in complex environments. First, neuroscientific theories of cognitive architecture, such as the global workspace theory and the attention schema theory, can enhance biological plausibility and help us understand how we could bridge individual and social theories of intelligence. Second, intelligence occurs in time as opposed to over time, and this is naturally incorporated by dynamical systems. Third, embodiment has been demonstrated to provide more sophisticated array of communicative signals. To conclude, we discuss the example of active inference, which offers powerful insights for developing agents that possess biological realism, can self-organize in time, and are socially embodied.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2021-10-23</td>
<td style='padding: 8px;'>Predictive Coding, Variational Autoencoders, and Biological Connections</td>
<td style='padding: 6px;'>Joseph Marino</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2011.07464v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper reviews predictive coding, from theoretical neuroscience, and variational autoencoders, from machine learning, identifying the common origin and mathematical framework underlying both areas. As each area is prominent within its respective field, more firmly connecting these areas could prove useful in the dialogue between neuroscience and machine learning. After reviewing each area, we discuss two possible correspondences implied by this perspective: cortical pyramidal dendrites as analogous to (non-linear) deep networks and lateral inhibition as analogous to normalizing flows. These connections may provide new directions for further investigations in each field.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2019-09-13</td>
<td style='padding: 8px;'>Additive function approximation in the brain</td>
<td style='padding: 6px;'>Kameron Decker Harris</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/1909.02603v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Many biological learning systems such as the mushroom body, hippocampus, and cerebellum are built from sparsely connected networks of neurons. For a new understanding of such networks, we study the function spaces induced by sparse random features and characterize what functions may and may not be learned. A network with $d$ inputs per neuron is found to be equivalent to an additive model of order $d$, whereas with a degree distribution the network combines additive terms of different orders. We identify three specific advantages of sparsity: additive function approximation is a powerful inductive bias that limits the curse of dimensionality, sparse networks are stable to outlier noise in the inputs, and sparse random features are scalable. Thus, even simple brain architectures can be powerful function approximators. Finally, we hope that this work helps popularize kernel theories of networks among computational neuroscientists.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>The Disparate Benefits of Deep Ensembles</td>
<td style='padding: 6px;'>Kajetan Schweighofer, Adrian Arnaiz-Rodriguez, Sepp Hochreiter, Nuria Oliver</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13831v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Ensembles of Deep Neural Networks, Deep Ensembles, are widely used as a simple way to boost predictive performance. However, their impact on algorithmic fairness is not well understood yet. Algorithmic fairness investigates how a model's performance varies across different groups, typically defined by protected attributes such as age, gender, or race. In this work, we investigate the interplay between the performance gains from Deep Ensembles and fairness. Our analysis reveals that they unevenly favor different groups in what we refer to as a disparate benefits effect. We empirically investigate this effect with Deep Ensembles applied to popular facial analysis and medical imaging datasets, where protected group attributes are given and find that it occurs for multiple established group fairness metrics, including statistical parity and equal opportunity. Furthermore, we identify the per-group difference in predictive diversity of ensemble members as the potential cause of the disparate benefits effect. Finally, we evaluate different approaches to reduce unfairness due to the disparate benefits effect. Our findings show that post-processing is an effective method to mitigate this unfairness while preserving the improved performance of Deep Ensembles.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>Deep Generative Models Unveil Patterns in Medical Images Through Vision-Language Conditioning</td>
<td style='padding: 6px;'>Xiaodan Xing, Junzhi Ning, Yang Nan, Guang Yang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13823v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Deep generative models have significantly advanced medical imaging analysis by enhancing dataset size and quality. Beyond mere data augmentation, our research in this paper highlights an additional, significant capacity of deep generative models: their ability to reveal and demonstrate patterns in medical images. We employ a generative structure with hybrid conditions, combining clinical data and segmentation masks to guide the image synthesis process. Furthermore, we innovatively transformed the tabular clinical data into textual descriptions. This approach simplifies the handling of missing values and also enables us to leverage large pre-trained vision-language models that investigate the relations between independent clinical entries and comprehend general terms, such as gender and smoking status. Our approach differs from and presents a more challenging task than traditional medical report-guided synthesis due to the less visual correlation of our clinical information with the images. To overcome this, we introduce a text-visual embedding mechanism that strengthens the conditions, ensuring the network effectively utilizes the provided information. Our pipeline is generalizable to both GAN-based and diffusion models. Experiments on chest CT, particularly focusing on the smoking status, demonstrated a consistent intensity shift in the lungs which is in agreement with clinical observations, indicating the effectiveness of our method in capturing and visualizing the impact of specific attributes on medical image patterns. Our methods offer a new avenue for the early detection and precise visualization of complex clinical conditions with deep generative models. All codes are https://github.com/junzhin/DGM-VLC.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>Optimizing Probabilistic Conformal Prediction with Vectorized Non-Conformity Scores</td>
<td style='padding: 6px;'>Minxing Zheng, Shixiang Zhu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13735v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Generative models have shown significant promise in critical domains such as medical diagnosis, autonomous driving, and climate science, where reliable decision-making hinges on accurate uncertainty quantification. While probabilistic conformal prediction (PCP) offers a powerful framework for this purpose, its coverage efficiency -- the size of the uncertainty set -- is limited when dealing with complex underlying distributions and a finite number of generated samples. In this paper, we propose a novel PCP framework that enhances efficiency by first vectorizing the non-conformity scores with ranked samples and then optimizing the shape of the prediction set by varying the quantiles for samples at the same rank. Our method delivers valid coverage while producing discontinuous and more efficient prediction sets, making it particularly suited for high-stakes applications. We demonstrate the effectiveness of our approach through experiments on both synthetic and real-world datasets.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World Multilingual Settings</td>
<td style='padding: 6px;'>Varun Gumma, Anandhita Raghunath, Mohit Jain, Sunayana Sitaram</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13671v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Assessing the capabilities and limitations of large language models (LLMs) has garnered significant interest, yet the evaluation of multiple models in real-world scenarios remains rare. Multilingual evaluation often relies on translated benchmarks, which typically do not capture linguistic and cultural nuances present in the source language. This study provides an extensive assessment of 24 LLMs on real world data collected from Indian patients interacting with a medical chatbot in Indian English and 4 other Indic languages. We employ a uniform Retrieval Augmented Generation framework to generate responses, which are evaluated using both automated techniques and human evaluators on four specific metrics relevant to our application. We find that models vary significantly in their performance and that instruction tuned Indic models do not always perform well on Indic language queries. Further, we empirically show that factual correctness is generally lower for responses to Indic queries compared to English queries. Finally, our qualitative work shows that code-mixed and culturally relevant queries in our dataset pose challenges to evaluated models.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>Multimodal growth and development assessment model</td>
<td style='padding: 6px;'>Ying Li, Zichen Song, Zijie Gong, Sitan Huang, Jiewei Ge</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13647v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>With the development of social economy and the improvement of people's attention to health, the growth and development of children and adolescents has become an important indicator to measure the level of national health. Therefore, accurate and timely assessment of children's growth and development has become increasingly important. At the same time, global health inequalities, especially child malnutrition and stunting in developing countries, urgently require effective assessment tools to monitor and intervene. In recent years, the rapid development of technologies such as big data, artificial intelligence, and cloud computing, and the cross-integration of multiple disciplines such as biomedicine, statistics, and computer science have promoted the rapid development of large-scale models for growth and development assessment. However, there are still problems such as too single evaluation factors, inaccurate diagnostic results, and inability to give accurate and reasonable recommendations. The multi-modal growth and development assessment model uses the public data set of RSNA ( North American College of Radiology ) as the training set, and the data set of the Department of Pediatrics of Huaibei People's Hospital as the open source test set. The embedded ICL module enables the model to quickly adapt and identify the tasks that need to be done to ensure that under the premise of considering multiple evaluation factors, accurate diagnosis results and reasonable medical recommendations are given, so as to provide solutions to the above problems and promote the development of the medical field.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling</td>
<td style='padding: 6px;'>Yakun Zhu, Shaohang Wei, Xu Wang, Kui Xue, Xiaofan Zhang, Shaoting Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13610v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Integrating tools into Large Language Models (LLMs) has facilitated the widespread application. Despite this, in specialized downstream task contexts, reliance solely on tools is insufficient to fully address the complexities of the real world. This particularly restricts the effective deployment of LLMs in fields such as medicine. In this paper, we focus on the downstream tasks of medical calculators, which use standardized tests to assess an individual's health status. We introduce MeNTi, a universal agent architecture for LLMs. MeNTi integrates a specialized medical toolkit and employs meta-tool and nested calling mechanisms to enhance LLM tool utilization. Specifically, it achieves flexible tool selection and nested tool calling to address practical issues faced in intricate medical scenarios, including calculator selection, slot filling, and unit conversion. To assess the capabilities of LLMs for quantitative assessment throughout the clinical process of calculator scenarios, we introduce CalcQA. This benchmark requires LLMs to use medical calculators to perform calculations and assess patient health status. CalcQA is constructed by professional physicians and includes 100 case-calculator pairs, complemented by a toolkit of 281 medical tools. The experimental results demonstrate significant performance improvements with our framework. This research paves new directions for applying LLMs in demanding scenarios of medicine.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>OAH-Net: A Deep Neural Network for Hologram Reconstruction of Off-axis Digital Holographic Microscope</td>
<td style='padding: 6px;'>Wei Liu, Kerem Delikoyun, Qianyu Chen, Alperen Yildiz, Si Ko Myo, Win Sen Kuan, John Tshon Yit Soong, Matthew Edward Cove, Oliver Hayden, Hweekuan Lee</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13592v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Off-axis digital holographic microscopy is a high-throughput, label-free imaging technology that provides three-dimensional, high-resolution information about samples, particularly useful in large-scale cellular imaging. However, the hologram reconstruction process poses a significant bottleneck for timely data analysis. To address this challenge, we propose a novel reconstruction approach that integrates deep learning with the physical principles of off-axis holography. We initialized part of the network weights based on the physical principle and then fine-tuned them via weakly supersized learning. Our off-axis hologram network (OAH-Net) retrieves phase and amplitude images with errors that fall within the measurement error range attributable to hardware, and its reconstruction speed significantly surpasses the microscope's acquisition rate. Crucially, OAH-Net demonstrates remarkable external generalization capabilities on unseen samples with distinct patterns and can be seamlessly integrated with other models for downstream tasks to achieve end-to-end real-time hologram analysis. This capability further expands off-axis holography's applications in both biological and medical studies.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>Three-Input Ciphertext Multiplication for Homomorphic Encryption</td>
<td style='padding: 6px;'>Sajjad Akherati, Yok Jye Tang, Xinmiao Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13545v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Homomorphic encryption (HE) allows computations to be directly carried out on ciphertexts and is essential to privacy-preserving computing, such as neural network inference, medical diagnosis, and financial data analysis. Only addition and 2-input multiplication are defined over ciphertexts in popular HE schemes. However, many HE applications involve non-linear functions and they need to be approximated using high-order polynomials to maintain precision. To reduce the complexity of these computations, this paper proposes 3-input ciphertext multiplication. One extra evaluation key is introduced to carry out the relinearization step of ciphertext multiplication, and new formulas are proposed to combine computations and share intermediate results. Compared to using two consecutive 2- input multiplications, computing the product of three ciphertexts utilizing the proposed scheme leads to almost a half of the latency, 29% smaller silicon area, and lower noise without scarifying the throughput.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?</td>
<td style='padding: 6px;'>Che Liu, Zhongwei Wan, Haozhe Wang, Yinda Chen, Talha Qaiser, Chen Jin, Fariba Yousefi, Nikolay Burlutskiy, Rossella Arcucci</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13523v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Medical Vision-Language Pre-training (MedVLP) has made significant progress in enabling zero-shot tasks for medical image understanding. However, training MedVLP models typically requires large-scale datasets with paired, high-quality image-text data, which are scarce in the medical domain. Recent advancements in Large Language Models (LLMs) and diffusion models have made it possible to generate large-scale synthetic image-text pairs. This raises the question: *Can MedVLP succeed using purely synthetic data?* To address this, we use off-the-shelf generative models to create synthetic radiology reports and paired Chest X-ray (CXR) images, and propose an automated pipeline to build a diverse, high-quality synthetic dataset, enabling a rigorous study that isolates model and training settings, focusing entirely from the data perspective. Our results show that MedVLP models trained *exclusively on synthetic data* outperform those trained on real data by **3.8%** in averaged AUC on zero-shot classification. Moreover, using a combination of synthetic and real data leads to a further improvement of **9.07%**. Additionally, MedVLP models trained on synthetic or mixed data consistently outperform those trained on real data in zero-shot grounding, as well as in fine-tuned classification and segmentation tasks. Our analysis suggests MedVLP trained on well-designed synthetic data can outperform models trained on real datasets, which may be limited by low-quality samples and long-tailed distributions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-17</td>
<td style='padding: 8px;'>SemSim: Revisiting Weak-to-Strong Consistency from a Semantic Similarity Perspective for Semi-supervised Medical Image Segmentation</td>
<td style='padding: 6px;'>Shiao Xie, Hongyi Wang, Ziwei Niu, Hao Sun, Shuyi Ouyang, Yen-Wei Chen, Lanfen Lin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.13486v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Semi-supervised learning (SSL) for medical image segmentation is a challenging yet highly practical task, which reduces reliance on large-scale labeled dataset by leveraging unlabeled samples. Among SSL techniques, the weak-to-strong consistency framework, popularized by FixMatch, has emerged as a state-of-the-art method in classification tasks. Notably, such a simple pipeline has also shown competitive performance in medical image segmentation. However, two key limitations still persist, impeding its efficient adaptation: (1) the neglect of contextual dependencies results in inconsistent predictions for similar semantic features, leading to incomplete object segmentation; (2) the lack of exploitation of semantic similarity between labeled and unlabeled data induces considerable class-distribution discrepancy. To address these limitations, we propose a novel semi-supervised framework based on FixMatch, named SemSim, powered by two appealing designs from semantic similarity perspective: (1) rectifying pixel-wise prediction by reasoning about the intra-image pair-wise affinity map, thus integrating contextual dependencies explicitly into the final prediction; (2) bridging labeled and unlabeled data via a feature querying mechanism for compact class representation learning, which fully considers cross-image anatomical similarities. As the reliable semantic similarity extraction depends on robust features, we further introduce an effective spatial-aware fusion module (SFM) to explore distinctive information from multiple scales. Extensive experiments show that SemSim yields consistent improvements over the state-of-the-art methods across three public segmentation benchmarks.</td>
</tr>
</tbody>
</table>

