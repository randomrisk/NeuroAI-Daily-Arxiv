<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2025-01-27</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>A mathematical model of CAR-T cell therapy in combination with chemotherapy for malignant gliomas</td>
<td style='padding: 6px;'>Dmitry Sinelshchikov, Juan Belmonte-Beitia, Matteo Italia</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13774v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We study the dynamics and interactions between combined chemotherapy and chimeric antigen receptor (CAR-T) cells therapy and malignant gliomas (MG). MG is one of the most common primary brain tumor, with high resistance to therapy and unfavorable prognosis. Here, we develop a mathematical model that describes the application of chemo- and CAR-T cell therapies and the dynamics of sensitive and resistant populations of tumor cells. This model is a five-dimensional dynamical system with impulsive inputs corresponding to clinical administration of chemo- and immunotherapy. We provide a proof of non-negativeness of solutions of the proposed model for non-negative initial data. We demonstrate that if we apply both therapies only once, the trajectories will be attracted to an invariant surface that corresponds to the tumor carrying capacity. On the other hand, if we apply both treatments constantly, we find regions of the parameter where the tumor is eradicated. Moreover, we study applications of different combinations of the above treatments in order to find an optimal combination at the population level. To this aim, we generate a population of $10^{4}$ virtual patients with the model parameters uniformly distributed in the medically relevant ranges and perform \emph{in silico} trials with different combinations of treatments. We obtain optimal protocols for several different relations of tumor growth rates between sensitive and drug resistant cells. We demonstrate that the tumor growth rate, efficacy of chemotherapy, and tumor immunosuppression are the parameters that mostly impact survival time in \emph{in silico} trials. We believe that our results provide new theoretical insights to guide the design of clinical trials for MG therapies.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>Integrating Causality with Neurochaos Learning: Proposed Approach and Research Agenda</td>
<td style='padding: 6px;'>Nanjangud C. Narendra, Nithin Nagaraj</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13763v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Deep learning implemented via neural networks, has revolutionized machine learning by providing methods for complex tasks such as object detection/classification and prediction. However, architectures based on deep neural networks have started to yield diminishing returns, primarily due to their statistical nature and inability to capture causal structure in the training data. Another issue with deep learning is its high energy consumption, which is not that desirable from a sustainability perspective.   Therefore, alternative approaches are being considered to address these issues, both of which are inspired by the functioning of the human brain. One approach is causal learning, which takes into account causality among the items in the dataset on which the neural network is trained. It is expected that this will help minimize the spurious correlations that are prevalent in the learned representations of deep neural networks. The other approach is Neurochaos Learning, a recent development, which draws its inspiration from the nonlinear chaotic firing intrinsic to neurons in biological neural networks (brain/central nervous system). Both approaches have shown improved results over just deep learning alone.   To that end, in this position paper, we investigate how causal and neurochaos learning approaches can be integrated together to produce better results, especially in domains that contain linked data. We propose an approach for this integration to enhance classification, prediction and reinforcement learning. We also propose a set of research questions that need to be investigated in order to make this integration a reality.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>Language modulates vision: Evidence from neural networks and human brain-lesion models</td>
<td style='padding: 6px;'>Haoyang Chen, Bo Liu, Shuyue Wang, Xiaosha Wang, Wenjuan Han, Yixin Zhu, Xiaochun Wang, Yanchao Bi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13628v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Comparing information structures in between deep neural networks (DNNs) and the human brain has become a key method for exploring their similarities and differences. Recent research has shown better alignment of vision-language DNN models, such as CLIP, with the activity of the human ventral occipitotemporal cortex (VOTC) than earlier vision models, supporting the idea that language modulates human visual perception. However, interpreting the results from such comparisons is inherently limited due to the "black box" nature of DNNs. To address this, we combined model-brain fitness analyses with human brain lesion data to examine how disrupting the communication pathway between the visual and language systems causally affects the ability of vision-language DNNs to explain the activity of the VOTC. Across four diverse datasets, CLIP consistently outperformed both label-supervised (ResNet) and unsupervised (MoCo) models in predicting VOTC activity. This advantage was left-lateralized, aligning with the human language network. Analyses of the data of 33 stroke patients revealed that reduced white matter integrity between the VOTC and the language region in the left angular gyrus was correlated with decreased CLIP performance and increased MoCo performance, indicating a dynamic influence of language processing on the activity of the VOTC. These findings support the integration of language modulation in neurocognitive models of human vision, reinforcing concepts from vision-language DNN models. The sensitivity of model-brain similarity to specific brain lesions demonstrates that leveraging manipulation of the human brain is a promising framework for evaluating and developing brain-like computer models.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>Funnelling super-resolution STED microscopy through multimode fibres</td>
<td style='padding: 6px;'>André Gomes, Miroslav Stibůrek, Sergey Turtaev, Katharina Reglinski, Christian Eggeling, Tomáš Čižmár</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13572v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Holographic multimode fibre endoscopes have recently shown their ability to unveil and monitor deep brain structures with sub-micrometre resolution, establishing themselves as a minimally-invasive technology with promising applications in neurobiology. In this approach, holographic control of the input light field entering the multimode fibres is achieved by means of wavefront shaping, usually treating the fibre as a complex medium. In contrast to other unpredictable and highly scattering complex media, multimode fibres feature symmetries and strong correlations between their input and output fields. Both step-index and graded-index multimode fibres offer a specific set of such correlations which, when appropriately leveraged, enable generating high-quality focused pulses with minimal intermodal dispersion. With this, we funnelled pulsed super-resolution STED microscopy with time-gated detection through a custom multimode fibre probe, combining the correlations of both multimode fibre types. We demonstrate resolution improvements over 3-times beyond the diffraction limit and showcase its applicability in bioimaging. This work provides not only a solution for delivering short pulses through step-index multimode fibre segments but also marks a step towards bringing advanced super-resolution imaging techniques with virtually no depth limitations.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>Capturing heterogeneous time-variation in covariate effects in non-proportional hazard regression models</td>
<td style='padding: 6px;'>Niklas Hagemann, Thomas Kneib, Kathrin Möllenhoff</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13525v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A central focus in survival analysis is examining how covariates influence survival time. These covariate effects are often found to be either time-varying, heterogeneous - such as being specific to patients, treatments, or subgroups - or exhibit both characteristics simultaneously. While the standard model, the Cox proportional hazards model, allows neither time-varying nor heterogeneous effects, several extensions to the Cox model as well as alternative modeling frameworks have been introduced. However, no unified framework for incorporating heterogeneously time-varying effects of covariates has been proposed yet. Such effects occur when a covariate influences survival not only in a heterogeneous and time-varying manner, but when the time-variation is also heterogeneous. We propose to model such effects by introducing heterogeneously time-varying coefficients to piecewise exponential additive mixed models. We deploy functional random effects, also known as factor smooths, to model such coefficients as the interaction effect of heterogeneity and time-variation. Our approach allows for non-linear time-effects due to being based on penalized splines and uses an efficient random effects basis to model the heterogeneity. Using a penalized basis prevents overfitting in case of absence of such effects. In addition, the penalization mostly solves the problem of choosing the number of intervals which is usually present in unregularized piecewise exponential approaches. We demonstrate the superiority of our approach in comparison to competitors by means of a simulation study. Finally, the practical application and relevance are outlined by presenting a brain tumor case study.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>Unraveling Normal Anatomy via Fluid-Driven Anomaly Randomization</td>
<td style='padding: 6px;'>Peirong Liu, Ana Lawry Aguila, Juan E. Iglesias</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13370v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Data-driven machine learning has made significant strides in medical image analysis. However, most existing methods are tailored to specific modalities and assume a particular resolution (often isotropic). This limits their generalizability in clinical settings, where variations in scan appearance arise from differences in sequence parameters, resolution, and orientation. Furthermore, most general-purpose models are designed for healthy subjects and suffer from performance degradation when pathology is present. We introduce UNA (Unraveling Normal Anatomy), the first modality-agnostic learning approach for normal brain anatomy reconstruction that can handle both healthy scans and cases with pathology. We propose a fluid-driven anomaly randomization method that generates an unlimited number of realistic pathology profiles on-the-fly. UNA is trained on a combination of synthetic and real data, and can be applied directly to real images with potential pathology without the need for fine-tuning. We demonstrate UNA's effectiveness in reconstructing healthy brain anatomy and showcase its direct application to anomaly detection, using both simulated and real images from 3D healthy and stroke datasets, including CT and MRI scans. By bridging the gap between healthy and diseased images, UNA enables the use of general-purpose models on diseased images, opening up new opportunities for large-scale analysis of uncurated clinical images in the presence of pathology. Code is available at https://github.com/peirong26/UNA.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>Polyhedra Encoding Transformers: Enhancing Diffusion MRI Analysis Beyond Voxel and Volumetric Embedding</td>
<td style='padding: 6px;'>Tianyuan Yao, Zhiyuan Li, Praitayini Kanakaraj, Derek B. Archer, Kurt Schilling, Lori Beason-Held, Susan Resnick, Bennett A. Landman, Yuankai Huo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13352v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Diffusion-weighted Magnetic Resonance Imaging (dMRI) is an essential tool in neuroimaging. It is arguably the sole noninvasive technique for examining the microstructural properties and structural connectivity of the brain. Recent years have seen the emergence of machine learning and data-driven approaches that enhance the speed, accuracy, and consistency of dMRI data analysis. However, traditional deep learning models often fell short, as they typically utilize pixel-level or volumetric patch-level embeddings similar to those used in structural MRI, and do not account for the unique distribution of various gradient encodings. In this paper, we propose a novel method called Polyhedra Encoding Transformer (PE-Transformer) for dMRI, designed specifically to handle spherical signals. Our approach involves projecting an icosahedral polygon onto a unit sphere to resample signals from predetermined directions. These resampled signals are then transformed into embeddings, which are processed by a transformer encoder that incorporates orientational information reflective of the icosahedral structure. Through experimental validation with various gradient encoding protocols, our method demonstrates superior accuracy in estimating multi-compartment models and Fiber Orientation Distributions (FOD), outperforming both conventional CNN architectures and standard transformers.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-22</td>
<td style='padding: 8px;'>Learning accurate rigid registration for longitudinal brain MRI from synthetic data</td>
<td style='padding: 6px;'>Jingru Fu, Adrian V. Dalca, Bruce Fischl, Rodrigo Moreno, Malte Hoffmann</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13010v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Rigid registration aims to determine the translations and rotations necessary to align features in a pair of images. While recent machine learning methods have become state-of-the-art for linear and deformable registration across subjects, they have demonstrated limitations when applied to longitudinal (within-subject) registration, where achieving precise alignment is critical. Building on an existing framework for anatomy-aware, acquisition-agnostic affine registration, we propose a model optimized for longitudinal, rigid brain registration. By training the model with synthetic within-subject pairs augmented with rigid and subtle nonlinear transforms, the model estimates more accurate rigid transforms than previous cross-subject networks and performs robustly on longitudinal registration pairs within and across magnetic resonance imaging (MRI) contrasts.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-22</td>
<td style='padding: 8px;'>AMM-Diff: Adaptive Multi-Modality Diffusion Network for Missing Modality Imputation</td>
<td style='padding: 6px;'>Aghiles Kebaili, Jérôme Lapuyade-Lahorgue, Pierre Vera, Su Ruan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.12840v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In clinical practice, full imaging is not always feasible, often due to complex acquisition protocols, stringent privacy regulations, or specific clinical needs. However, missing MR modalities pose significant challenges for tasks like brain tumor segmentation, especially in deep learning-based segmentation, as each modality provides complementary information crucial for improving accuracy. A promising solution is missing data imputation, where absent modalities are generated from available ones. While generative models have been widely used for this purpose, most state-of-the-art approaches are limited to single or dual target translations, lacking the adaptability to generate missing modalities based on varying input configurations. To address this, we propose an Adaptive Multi-Modality Diffusion Network (AMM-Diff), a novel diffusion-based generative model capable of handling any number of input modalities and generating the missing ones. We designed an Image-Frequency Fusion Network (IFFN) that learns a unified feature representation through a self-supervised pretext task across the full input modalities and their selected high-frequency Fourier components. The proposed diffusion model leverages this representation, encapsulating prior knowledge of the complete modalities, and combines it with an adaptive reconstruction strategy to achieve missing modality completion. Experimental results on the BraTS 2021 dataset demonstrate the effectiveness of our approach.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-22</td>
<td style='padding: 8px;'>Machine Learning Modeling for Multi-order Human Visual Motion Processing</td>
<td style='padding: 6px;'>Zitang Sun, Yen-Ju Chen, Yung-Hao Yang, Yuan Li, Shin'ya Nishida</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.12810v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Our research aims to develop machines that learn to perceive visual motion as do humans. While recent advances in computer vision (CV) have enabled DNN-based models to accurately estimate optical flow in naturalistic images, a significant disparity remains between CV models and the biological visual system in both architecture and behavior. This disparity includes humans' ability to perceive the motion of higher-order image features (second-order motion), which many CV models fail to capture because of their reliance on the intensity conservation law. Our model architecture mimics the cortical V1-MT motion processing pathway, utilizing a trainable motion energy sensor bank and a recurrent graph network. Supervised learning employing diverse naturalistic videos allows the model to replicate psychophysical and physiological findings about first-order (luminance-based) motion perception. For second-order motion, inspired by neuroscientific findings, the model includes an additional sensing pathway with nonlinear preprocessing before motion energy sensing, implemented using a simple multilayer 3D CNN block. When exploring how the brain acquired the ability to perceive second-order motion in natural environments, in which pure second-order signals are rare, we hypothesized that second-order mechanisms were critical when estimating robust object motion amidst optical fluctuations, such as highlights on glossy surfaces. We trained our dual-pathway model on novel motion datasets with varying material properties of moving objects. We found that training to estimate object motion from non-Lambertian materials naturally endowed the model with the capacity to perceive second-order motion, as can humans. The resulting model effectively aligns with biological systems while generalizing to both first- and second-order motion phenomena in natural scenes.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-19</td>
<td style='padding: 8px;'>Robust Functional Ward's Linkages with Applications in EEG data Clustering</td>
<td style='padding: 6px;'>Tianbo Chen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.11081v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper proposes two new distance measures, called functional Ward's linkages, for functional data clustering that are robust against outliers. Conventional Ward's linkage defines the distance between two clusters as the increase in sum of squared errors (SSE) upon merging, which can be interpreted graphically as an increase in the diameter. Analogously, functional Ward's linkage defines the distance of two clusters as the increased width of the band delimited by the merged clusters. To address the limitations of conventional Ward's linkage in handling outliers and contamination, the proposed linkages focus exclusively on the most central curves by leveraging magnitude-shape outlyingness measures and modified band depth, respectively. Simulations and real-world electroencephalogram (EEG) data analysis demonstrate that the proposed methods outperform other competitive approaches, particularly in the presence of various types of outliers and contamination.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-18</td>
<td style='padding: 8px;'>CEReBrO: Compact Encoder for Representations of Brain Oscillations Using Efficient Alternating Attention</td>
<td style='padding: 6px;'>Alexandru Dimofte, Glenn Anta Bucagu, Thorir Mar Ingolfsson, Xiaying Wang, Andrea Cossettini, Luca Benini, Yawei Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.10885v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalograph (EEG) is a crucial tool for studying brain activity. Recently, self-supervised learning methods leveraging large unlabeled datasets have emerged as a potential solution to the scarcity of widely available annotated EEG data. However, current methods suffer from at least one of the following limitations: i) sub-optimal EEG signal modeling, ii) model sizes in the hundreds of millions of trainable parameters, and iii) reliance on private datasets and/or inconsistent public benchmarks, hindering reproducibility. To address these challenges, we introduce a Compact Encoder for Representations of Brain Oscillations using alternating attention (CEReBrO), a new small EEG foundation model. Our tokenization scheme represents EEG signals at a per-channel patch granularity. We propose an alternating attention mechanism that jointly models intra-channel temporal dynamics and inter-channel spatial correlations, achieving 2x speed improvement with 6x less memory required compared to standard self-attention. We present several model sizes ranging from 3.6 million to 85 million parameters. Pre-trained on over 20,000 hours of publicly available scalp EEG recordings with diverse channel configurations, our models set new benchmarks in emotion detection and seizure detection tasks, with competitive performance in anomaly classification and gait prediction. This validates our models' effectiveness and effictiveness.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-17</td>
<td style='padding: 8px;'>Hybrid Deep Learning Model for epileptic seizure classification by using 1D-CNN with multi-head attention mechanism</td>
<td style='padding: 6px;'>Mohammed Guhdar, Ramadhan J. Mstafa, Abdulhakeem O. Mohammed</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.10342v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Epilepsy is a prevalent neurological disorder globally, impacting around 50 million people \cite{WHO_epilepsy_50million}. Epileptic seizures result from sudden abnormal electrical activity in the brain, which can be read as sudden and significant changes in the EEG signal of the brain. The signal can vary in severity and frequency, which results in loss of consciousness and muscle contractions for a short period of time \cite{epilepsyfoundation_myoclonic}. Individuals with epilepsy often face significant employment challenges due to safety concerns in certain work environments. Many jobs that involve working at heights, operating heavy machinery, or in other potentially hazardous settings may be restricted for people with seizure disorders. This certainly limits job options and economic opportunities for those living with epilepsy.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-16</td>
<td style='padding: 8px;'>Cueless EEG imagined speech for subject identification: dataset and benchmarks</td>
<td style='padding: 6px;'>Ali Derakhshesh, Zahra Dehghanian, Reza Ebrahimpour, Hamid R. Rabiee</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.09700v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalogram (EEG) signals have emerged as a promising modality for biometric identification. While previous studies have explored the use of imagined speech with semantically meaningful words for subject identification, most have relied on additional visual or auditory cues. In this study, we introduce a cueless EEG-based imagined speech paradigm, where subjects imagine the pronunciation of semantically meaningful words without any external cues. This innovative approach addresses the limitations of prior methods by requiring subjects to select and imagine words from a predefined list naturally. The dataset comprises over 4,350 trials from 11 subjects across five sessions. We assess a variety of classification methods, including traditional machine learning techniques such as Support Vector Machines (SVM) and XGBoost, as well as time-series foundation models and deep learning architectures specifically designed for EEG classification, such as EEG Conformer and Shallow ConvNet. A session-based hold-out validation strategy was employed to ensure reliable evaluation and prevent data leakage. Our results demonstrate outstanding classification accuracy, reaching 97.93%. These findings highlight the potential of cueless EEG paradigms for secure and reliable subject identification in real-world applications, such as brain-computer interfaces (BCIs).</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-16</td>
<td style='padding: 8px;'>Multi-task deep-learning for sleep event detection and stage classification</td>
<td style='padding: 6px;'>Adriana Anido-Alonso, Diego Alvarez-Estevez</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.09519v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Polysomnographic sleep analysis is the standard clinical method to accurately diagnose and treat sleep disorders. It is an intricate process which involves the manual identification, classification, and location of multiple sleep event patterns. This is complex, for which identification of different types of events involves focusing on different subsets of signals, resulting on an iterative time-consuming process entailing several visual analysis passes. In this paper we propose a multi-task deep-learning approach for the simultaneous detection of sleep events and hypnogram construction in one single pass. Taking as reference state-of-the-art methodology for object-detection in the field of Computer Vision, we reformulate the problem for the analysis of multi-variate time sequences, and more specifically for pattern detection in the sleep analysis scenario. We investigate the performance of the resulting method in identifying different assembly combinations of EEG arousals, respiratory events (apneas and hypopneas) and sleep stages, also considering different input signal montage configurations. Furthermore, we evaluate our approach using two independent datasets, assessing true-generalization effects involving local and external validation scenarios. Based on our results, we analyze and discuss our method's capabilities and its potential wide-range applicability across different settings and datasets.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-15</td>
<td style='padding: 8px;'>Subject Disentanglement Neural Network for Speech Envelope Reconstruction from EEG</td>
<td style='padding: 6px;'>Li Zhang, Jiyao Liu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.08693v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Reconstructing speech envelopes from EEG signals is essential for exploring neural mechanisms underlying speech perception. Yet, EEG variability across subjects and physiological artifacts complicate accurate reconstruction. To address this problem, we introduce Subject Disentangling Neural Network (SDN-Net), which disentangles subject identity information from reconstructed speech envelopes to enhance cross-subject reconstruction accuracy. SDN-Net integrates three key components: MLA-Codec, MPN-MI, and CTA-MTDNN. The MLA-Codec, a fully convolutional neural network, decodes EEG signals into speech envelopes. The CTA-MTDNN module, a multi-scale time-delay neural network with channel and temporal attention, extracts subject identity features from EEG signals. Lastly, the MPN-MI module, a mutual information estimator with a multi-layer perceptron, supervises the removal of subject identity information from the reconstructed speech envelope. Experiments on the Auditory EEG Decoding Dataset demonstrate that SDN-Net achieves superior performance in inner- and cross-subject speech envelope reconstruction compared to recent state-of-the-art methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-15</td>
<td style='padding: 8px;'>Easing Seasickness through Attention Redirection with a Mindfulness-Based Brain--Computer Interface</td>
<td style='padding: 6px;'>Xiaoyu Bao, Kailin Xu, Jiawei Zhu, Haiyun Huang, Kangning Li, Qiyun Huang, Yuanqing Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.08518v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Seasickness is a prevalent issue that adversely impacts both passenger experiences and the operational efficiency of maritime crews. While techniques that redirect attention have proven effective in alleviating motion sickness symptoms in terrestrial environments, applying similar strategies to manage seasickness poses unique challenges due to the prolonged and intense motion environment associated with maritime travel. In this study, we propose a mindfulness brain-computer interface (BCI), specifically designed to redirect attention with the aim of mitigating seasickness symptoms in real-world settings. Our system utilizes a single-channel headband to capture prefrontal EEG signals, which are then wirelessly transmitted to computing devices for the assessment of mindfulness states. The results are transferred into real-time feedback as mindfulness scores and audiovisual stimuli, facilitating a shift in attentional focus from physiological discomfort to mindfulness practices. A total of 43 individuals participated in a real-world maritime experiment consisted of three sessions: a real-feedback mindfulness session, a resting session, and a pseudofeedback mindfulness session. Notably, 81.39% of participants reported that the mindfulness BCI intervention was effective, and there was a significant reduction in the severity of seasickness, as measured by the Misery Scale (MISC). Furthermore, EEG analysis revealed a decrease in the theta/beta ratio, corresponding with the alleviation of seasickness symptoms. A decrease in overall EEG band power during the real-feedback mindfulness session suggests that the mindfulness BCI fosters a more tranquil and downregulated state of brain activity. Together, this study presents a novel nonpharmacological, portable, and effective approach for seasickness intervention, with the potential to enhance the cruising experience for both passengers and crews.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-14</td>
<td style='padding: 8px;'>Cognitive Assessment and Training in Extended Reality: Multimodal Systems, Clinical Utility, and Current Challenges</td>
<td style='padding: 6px;'>Palmira Victoria González-Erena, Sara Fernández-Guinea, Panagiotis Kourtesis</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.08237v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Extended reality (XR) technologies-encompassing virtual reality (VR), augmented reality (AR), and mixed reality (MR) are transforming cognitive assessment and training by offering immersive, interactive environments that simulate real-world tasks. XR enhances ecological validity while enabling real-time, multimodal data collection through tools such as galvanic skin response (GSR), electroencephalography (EEG), eye tracking (ET), hand tracking, and body tracking. This allows for a more comprehensive understanding of cognitive and emotional processes, as well as adaptive, personalized interventions for users. Despite these advancements, current XR applications often underutilize the full potential of multimodal integration, relying primarily on visual and auditory inputs. Challenges such as cybersickness, usability concerns, and accessibility barriers further limit the widespread adoption of XR tools in cognitive science and clinical practice. This review examines XR-based cognitive assessment and training, focusing on its advantages over traditional methods, including ecological validity, engagement, and adaptability. It also explores unresolved challenges such as system usability, cost, and the need for multimodal feedback integration. The review concludes by identifying opportunities for optimizing XR tools to improve cognitive evaluation and rehabilitation outcomes, particularly for diverse populations, including older adults and individuals with cognitive impairments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-14</td>
<td style='padding: 8px;'>EEG-ReMinD: Enhancing Neurodegenerative EEG Decoding through Self-Supervised State Reconstruction-Primed Riemannian Dynamics</td>
<td style='padding: 6px;'>Zirui Wang, Zhenxi Song, Yi Guo, Yuxin Liu, Guoyang Xu, Min Zhang, Zhiguo Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.08139v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The development of EEG decoding algorithms confronts challenges such as data sparsity, subject variability, and the need for precise annotations, all of which are vital for advancing brain-computer interfaces and enhancing the diagnosis of diseases. To address these issues, we propose a novel two-stage approach named Self-Supervised State Reconstruction-Primed Riemannian Dynamics (EEG-ReMinD) , which mitigates reliance on supervised learning and integrates inherent geometric features. This approach efficiently handles EEG data corruptions and reduces the dependency on labels. EEG-ReMinD utilizes self-supervised and geometric learning techniques, along with an attention mechanism, to analyze the temporal dynamics of EEG features within the framework of Riemannian geometry, referred to as Riemannian dynamics. Comparative analyses on both intact and corrupted datasets from two different neurodegenerative disorders underscore the enhanced performance of EEG-ReMinD.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-14</td>
<td style='padding: 8px;'>Neural Speech Tracking in a Virtual Acoustic Environment: Audio-Visual Benefit for Unscripted Continuous Speech</td>
<td style='padding: 6px;'>Mareike Daeglau, Juergen Otten, Giso Grimm, Bojana Mirkovic, Volker Hohmann, Stefan Debener</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.08124v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The audio visual benefit in speech perception, where congruent visual input enhances auditory processing, is well documented across age groups, particularly in challenging listening conditions and among individuals with varying hearing abilities. However, most studies rely on highly controlled laboratory environments with scripted stimuli. Here, we examine the audio visual benefit using unscripted, natural speech from untrained speakers within a virtual acoustic environment. Using electroencephalography (EEG) and cortical speech tracking, we assessed neural responses across audio visual, audio only, visual only, and masked lip conditions to isolate the role of lip movements. Additionally, we analysed individual differences in acoustic and visual features of the speakers, including pitch, jitter, and lip openness, to explore their influence on the audio visual speech tracking benefit. Results showed a significant audio visual enhancement in speech tracking with background noise, with the masked lip condition performing similarly to the audio-only condition, emphasizing the importance of lip movements in adverse listening situations. Our findings reveal the feasibility of cortical speech tracking with naturalistic stimuli and underscore the impact of individual speaker characteristics on audio-visual integration in real world listening contexts.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-16</td>
<td style='padding: 8px;'>Cueless EEG imagined speech for subject identification: dataset and benchmarks</td>
<td style='padding: 6px;'>Ali Derakhshesh, Zahra Dehghanian, Reza Ebrahimpour, Hamid R. Rabiee</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.09700v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalogram (EEG) signals have emerged as a promising modality for biometric identification. While previous studies have explored the use of imagined speech with semantically meaningful words for subject identification, most have relied on additional visual or auditory cues. In this study, we introduce a cueless EEG-based imagined speech paradigm, where subjects imagine the pronunciation of semantically meaningful words without any external cues. This innovative approach addresses the limitations of prior methods by requiring subjects to select and imagine words from a predefined list naturally. The dataset comprises over 4,350 trials from 11 subjects across five sessions. We assess a variety of classification methods, including traditional machine learning techniques such as Support Vector Machines (SVM) and XGBoost, as well as time-series foundation models and deep learning architectures specifically designed for EEG classification, such as EEG Conformer and Shallow ConvNet. A session-based hold-out validation strategy was employed to ensure reliable evaluation and prevent data leakage. Our results demonstrate outstanding classification accuracy, reaching 97.93%. These findings highlight the potential of cueless EEG paradigms for secure and reliable subject identification in real-world applications, such as brain-computer interfaces (BCIs).</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-16</td>
<td style='padding: 8px;'>Teaching Wav2Vec2 the Language of the Brain</td>
<td style='padding: 6px;'>Tobias Fiedler, Leon Hermann, Florian Müller, Sarel Cohen, Peter Chin, Tobias Friedrich, Eilon Vaadia</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.09459v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The decoding of continuously spoken speech from neuronal activity has the potential to become an important clinical solution for paralyzed patients. Deep Learning Brain Computer Interfaces (BCIs) have recently successfully mapped neuronal activity to text contents in subjects who attempted to formulate speech. However, only small BCI datasets are available. In contrast, labeled data and pre-trained models for the closely related task of speech recognition from audio are widely available. One such model is Wav2Vec2 which has been trained in a self-supervised fashion to create meaningful representations of speech audio data. In this study, we show that patterns learned by Wav2Vec2 are transferable to brain data. Specifically, we replace its audio feature extractor with an untrained Brain Feature Extractor (BFE) model. We then execute full fine-tuning with pre-trained weights for Wav2Vec2, training ''from scratch'' without pre-trained weights as well as freezing a pre-trained Wav2Vec2 and training only the BFE each for 45 different BFE architectures. Across these experiments, the best run is from full fine-tuning with pre-trained weights, achieving a Character Error Rate (CER) of 18.54\%, outperforming the best training from scratch run by 20.46\% and that of frozen Wav2Vec2 training by 15.92\% percentage points. These results indicate that knowledge transfer from audio speech recognition to brain decoding is possible and significantly improves brain decoding performance for the same architectures. Related source code is available at https://github.com/tfiedlerdev/Wav2Vec2ForBrain.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-15</td>
<td style='padding: 8px;'>Easing Seasickness through Attention Redirection with a Mindfulness-Based Brain--Computer Interface</td>
<td style='padding: 6px;'>Xiaoyu Bao, Kailin Xu, Jiawei Zhu, Haiyun Huang, Kangning Li, Qiyun Huang, Yuanqing Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.08518v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Seasickness is a prevalent issue that adversely impacts both passenger experiences and the operational efficiency of maritime crews. While techniques that redirect attention have proven effective in alleviating motion sickness symptoms in terrestrial environments, applying similar strategies to manage seasickness poses unique challenges due to the prolonged and intense motion environment associated with maritime travel. In this study, we propose a mindfulness brain-computer interface (BCI), specifically designed to redirect attention with the aim of mitigating seasickness symptoms in real-world settings. Our system utilizes a single-channel headband to capture prefrontal EEG signals, which are then wirelessly transmitted to computing devices for the assessment of mindfulness states. The results are transferred into real-time feedback as mindfulness scores and audiovisual stimuli, facilitating a shift in attentional focus from physiological discomfort to mindfulness practices. A total of 43 individuals participated in a real-world maritime experiment consisted of three sessions: a real-feedback mindfulness session, a resting session, and a pseudofeedback mindfulness session. Notably, 81.39% of participants reported that the mindfulness BCI intervention was effective, and there was a significant reduction in the severity of seasickness, as measured by the Misery Scale (MISC). Furthermore, EEG analysis revealed a decrease in the theta/beta ratio, corresponding with the alleviation of seasickness symptoms. A decrease in overall EEG band power during the real-feedback mindfulness session suggests that the mindfulness BCI fosters a more tranquil and downregulated state of brain activity. Together, this study presents a novel nonpharmacological, portable, and effective approach for seasickness intervention, with the potential to enhance the cruising experience for both passengers and crews.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-10</td>
<td style='padding: 8px;'>On Creating A Brain-To-Text Decoder</td>
<td style='padding: 6px;'>Zenon Lamprou, Yashar Moshfeghi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.06326v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain decoding has emerged as a rapidly advancing and extensively utilized technique within neuroscience. This paper centers on the application of raw electroencephalogram (EEG) signals for decoding human brain activity, offering a more expedited and efficient methodology for enhancing our understanding of the human brain. The investigation specifically scrutinizes the efficacy of brain-computer interfaces (BCI) in deciphering neural signals associated with speech production, with particular emphasis on the impact of vocabulary size, electrode density, and training data on the framework's performance. The study reveals the competitive word error rates (WERs) achievable on the Librispeech benchmark through pre-training on unlabelled data for speech processing. Furthermore, the study evaluates the efficacy of voice recognition under configurations with limited labeled data, surpassing previous state-of-the-art techniques while utilizing significantly fewer labels. Additionally, the research provides a comprehensive analysis of error patterns in voice recognition and the influence of model size and unlabelled training data. It underscores the significance of factors such as vocabulary size and electrode density in enhancing BCI performance, advocating for an increase in microelectrodes and refinement of language models.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-09</td>
<td style='padding: 8px;'>Towards Probabilistic Inference of Human Motor Intentions by Assistive Mobile Robots Controlled via a Brain-Computer Interface</td>
<td style='padding: 6px;'>Xiaoshan Zhou, Carol M. Menassa, Vineet R. Kamat</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.05610v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Assistive mobile robots are a transformative technology that helps persons with disabilities regain the ability to move freely. Although autonomous wheelchairs significantly reduce user effort, they still require human input to allow users to maintain control and adapt to changing environments. Brain Computer Interface (BCI) stands out as a highly user-friendly option that does not require physical movement. Current BCI systems can understand whether users want to accelerate or decelerate, but they implement these changes in discrete speed steps rather than allowing for smooth, continuous velocity adjustments. This limitation prevents the systems from mimicking the natural, fluid speed changes seen in human self-paced motion. The authors aim to address this limitation by redesigning the perception-action cycle in a BCI controlled robotic system: improving how the robotic agent interprets the user's motion intentions (world state) and implementing these actions in a way that better reflects natural physical properties of motion, such as inertia and damping. The scope of this paper focuses on the perception aspect. We asked and answered a normative question "what computation should the robotic agent carry out to optimally perceive incomplete or noisy sensory observations?" Empirical EEG data were collected, and probabilistic representation that served as world state distributions were learned and evaluated in a Generative Adversarial Network framework. The ROS framework was established that connected with a Gazebo environment containing a digital twin of an indoor space and a virtual model of a robotic wheelchair. Signal processing and statistical analyses were implemented to identity the most discriminative features in the spatial-spectral-temporal dimensions, which are then used to construct the world model for the robotic agent to interpret user motion intentions as a Bayesian observer.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-09</td>
<td style='padding: 8px;'>LGL-BCI: A Motor-Imagery-Based Brain-Computer Interface with Geometric Learning</td>
<td style='padding: 6px;'>Jianchao Lu, Yuzhe Tian, Yang Zhang, Quan Z. Sheng, Xi Zheng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.05589v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain--computer interfaces are groundbreaking technology whereby brain signals are used to control external devices. Despite some advances in recent years, electroencephalogram (EEG)-based motor-imagery tasks face challenges, such as amplitude and phase variability and complex spatial correlations, with a need for smaller models and faster inference. In this study, we develop a prototype, called the Lightweight Geometric Learning Brain--Computer Interface (LGL-BCI), which uses our customized geometric deep learning architecture for swift model inference without sacrificing accuracy. LGL-BCI contains an EEG channel selection module via a feature decomposition algorithm to reduce the dimensionality of a symmetric positive definite matrix, providing adaptiveness among the continuously changing EEG signal. Meanwhile, a built-in lossless transformation helps boost the inference speed. The performance of our solution was evaluated using two real-world EEG devices and two public EEG datasets. LGL-BCI demonstrated significant improvements, achieving an accuracy of 82.54% compared to 62.22% for the state-of-the-art approach. Furthermore, LGL-BCI uses fewer parameters (64.9K vs. 183.7K), highlighting its computational efficiency. These findings underscore both the superior accuracy and computational efficiency of LGL-BCI, demonstrating the feasibility and robustness of geometric deep learning in motor-imagery brain--computer interface applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-09</td>
<td style='padding: 8px;'>MECASA: Motor Execution Classification using Additive Self-Attention for Hybrid EEG-fNIRS Data</td>
<td style='padding: 6px;'>Gourav Siddhad, Juhi Singh, Partha Pratim Roy</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.05525v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Motor execution, a fundamental aspect of human behavior, has been extensively studied using BCI technologies. EEG and fNIRS have been utilized to provide valuable insights, but their individual limitations have hindered performance. This study investigates the effectiveness of fusing electroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS) data for classifying rest versus task states in a motor execution paradigm. Using the SMR Hybrid BCI dataset, this work compares unimodal (EEG and fNIRS) classifiers with a multimodal fusion approach. It proposes Motor Execution using Convolutional Additive Self-Attention Mechanisms (MECASA), a novel architecture leveraging convolutional operations and self-attention to capture complex patterns in multimodal data. MECASA, built upon the CAS-ViT architecture, employs a computationally efficient, convolutional-based self-attention module (CASA), a hybrid block design, and a dedicated fusion network to combine features from separate EEG and fNIRS processing streams. Experimental results demonstrate that MECASA consistently outperforms established methods across all modalities (EEG, fNIRS, and fused), with fusion consistently improving accuracy compared to single-modality approaches. fNIRS generally achieved higher accuracy than EEG alone. Ablation studies revealed optimal configurations for MECASA, with embedding dimensions of 64-128 providing the best performance for EEG data and OD128 (upsampled optical density) yielding superior results for fNIRS data. This work highlights the potential of deep learning, specifically MECASA, to enhance EEG-fNIRS fusion for BCI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-08</td>
<td style='padding: 8px;'>Enhancing Listened Speech Decoding from EEG via Parallel Phoneme Sequence Prediction</td>
<td style='padding: 6px;'>Jihwan Lee, Tiantian Feng, Aditya Kommineni, Sudarsana Reddy Kadiri, Shrikanth Narayanan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.04844v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-computer interfaces (BCI) offer numerous human-centered application possibilities, particularly affecting people with neurological disorders. Text or speech decoding from brain activities is a relevant domain that could augment the quality of life for people with impaired speech perception. We propose a novel approach to enhance listened speech decoding from electroencephalography (EEG) signals by utilizing an auxiliary phoneme predictor that simultaneously decodes textual phoneme sequences. The proposed model architecture consists of three main parts: EEG module, speech module, and phoneme predictor. The EEG module learns to properly represent EEG signals into EEG embeddings. The speech module generates speech waveforms from the EEG embeddings. The phoneme predictor outputs the decoded phoneme sequences in text modality. Our proposed approach allows users to obtain decoded listened speech from EEG signals in both modalities (speech waveforms and textual phoneme sequences) simultaneously, eliminating the need for a concatenated sequential pipeline for each modality. The proposed approach also outperforms previous methods in both modalities. The source code and speech samples are publicly available.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-08</td>
<td style='padding: 8px;'>Decoding EEG Speech Perception with Transformers and VAE-based Data Augmentation</td>
<td style='padding: 6px;'>Terrance Yu-Hao Chen, Yulin Chen, Pontus Soederhaell, Sadrishya Agrawal, Kateryna Shapovalenko</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.04359v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Decoding speech from non-invasive brain signals, such as electroencephalography (EEG), has the potential to advance brain-computer interfaces (BCIs), with applications in silent communication and assistive technologies for individuals with speech impairments. However, EEG-based speech decoding faces major challenges, such as noisy data, limited datasets, and poor performance on complex tasks like speech perception. This study attempts to address these challenges by employing variational autoencoders (VAEs) for EEG data augmentation to improve data quality and applying a state-of-the-art (SOTA) sequence-to-sequence deep learning architecture, originally successful in electromyography (EMG) tasks, to EEG-based speech decoding. Additionally, we adapt this architecture for word classification tasks. Using the Brennan dataset, which contains EEG recordings of subjects listening to narrated speech, we preprocess the data and evaluate both classification and sequence-to-sequence models for EEG-to-words/sentences tasks. Our experiments show that VAEs have the potential to reconstruct artificial EEG data for augmentation. Meanwhile, our sequence-to-sequence model achieves more promising performance in generating sentences compared to our classification model, though both remain challenging tasks. These findings lay the groundwork for future research on EEG speech perception decoding, with possible extensions to speech production tasks such as silent or imagined speech.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-07</td>
<td style='padding: 8px;'>Exploring EEG and Eye Movement Fusion for Multi-Class Target RSVP-BCI</td>
<td style='padding: 6px;'>Xujin Li, Wei Wei, Kun Zhao, Jiayu Mao, Yizhuo Lu, Shuang Qiu, Huiguang He</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.03596v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Rapid Serial Visual Presentation (RSVP)-based Brain-Computer Interfaces (BCIs) facilitate high-throughput target image detection by identifying event-related potentials (ERPs) evoked in EEG signals. The RSVP-BCI systems effectively detect single-class targets within a stream of images but have limited applicability in scenarios that require detecting multiple target categories. Multi-class RSVP-BCI systems address this limitation by simultaneously identifying the presence of a target and distinguishing its category. However, existing multi-class RSVP decoding algorithms predominantly rely on single-modality EEG decoding, which restricts their performance improvement due to the high similarity between ERPs evoked by different target categories. In this work, we introduce eye movement (EM) modality into multi-class RSVP decoding and explore EEG and EM fusion to enhance decoding performance. First, we design three independent multi-class target RSVP tasks and build an open-source dataset comprising EEG and EM signals from 43 subjects. Then, we propose the Multi-class Target RSVP EEG and EM fusion Network (MTREE-Net) to enhance multi-class RSVP decoding. Specifically, a dual-complementary module is proposed to strengthen the differentiation of uni-modal features across categories. To improve multi-modal fusion performance, we adopt a dynamic reweighting fusion strategy guided by theoretically derived modality contribution ratios. Furthermore, we reduce the misclassification of non-target samples through knowledge transfer between two hierarchical classifiers. Extensive experiments demonstrate the feasibility of integrating EM signals into multi-class RSVP decoding and highlight the superior performance of MTREE-Net compared to existing RSVP decoding methods. The proposed MTREE-Net and open-source dataset provide a promising framework for developing practical multi-class RSVP-BCI systems.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-22</td>
<td style='padding: 8px;'>Peak Inference for Gaussian Random Fields on a Lattice</td>
<td style='padding: 6px;'>Tuo Lin, Armin Schwartzman, Samuel Davenport</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13239v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In this work we develop a Monte Carlo method to compute the height distribution of local maxima of a stationary Gaussian or Gaussian-related random field that is observed on a regular lattice. We show that our method can be used to provide valid peak based inference in datasets with low levels of smoothness, where existing formulae derived for continuous domains are not accurate. We also extend the methods in Worsley (2005) and Taylor et al. (2007) to compute the peak height distribution and compare them with our approach. Lastly, we apply our method to a task fMRI dataset to show how it can be used in practice.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-21</td>
<td style='padding: 8px;'>Using Space-Filling Curves and Fractals to Reveal Spatial and Temporal Patterns in Neuroimaging Data</td>
<td style='padding: 6px;'>Jacek Grela, Zbigniew Drogosz, Jakub Janarek, Jeremi K. Ochab, Ignacio Cifre, Ewa Gudowska-Nowak, Maciej A. Nowak, Paweł Oświęcimka, Dante R. Chialvo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.12111v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We present a novel method, Fractal Space-Curve Analysis (FSCA), which combines Space-Filling Curve (SFC) mapping for dimensionality reduction with fractal Detrended Fluctuation Analysis (DFA). The method is suitable for multidimensional geometrically embedded data, especially for neuroimaging data which is highly correlated temporally and spatially. We conduct extensive feasibility studies on diverse, artificially generated data with known fractal characteristics: the fractional Brownian motion, Cantor sets, and Gaussian processes. We compare the suitability of dimensionality reduction via Hilbert SFC and a data-driven alternative. FSCA is then successfully applied to real-world magnetic resonance imaging (MRI) and functional MRI (fMRI) scans.   The method utilizing Hilbert curves is optimized for computational efficiency, proven robust against boundary effects typical in experimental data analysis, and resistant to data sub-sampling. It is able to correctly quantify and discern correlations in both stationary and dynamic two-dimensional images. In MRI Alzheimer's dataset, patients reveal a progression of the disease associated with a systematic decrease of the Hurst exponent. In fMRI recording of breath-holding task, the change in the exponent allows distinguishing different experimental phases.   This study introduces a robust method for fractal characterization of spatial and temporal correlations in many types of multidimensional neuroimaging data. Very few assumptions allow it to be generalized to more dimensions than typical for neuroimaging and utilized in other scientific fields. The method can be particularly useful in analyzing fMRI experiments to compute markers of pathological conditions resulting from neurodegeneration. We also showcase its potential for providing insights into brain dynamics in task-related experiments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-16</td>
<td style='padding: 8px;'>Multiplex Nodal Modularity: A novel network metric for the regional analysis of amnestic mild cognitive impairment during a working memory binding task</td>
<td style='padding: 6px;'>Avalon Campbell-Cousins, Federica Guazzo, Mark Bastin, Mario A. Parra, Javier Escudero</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.09805v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Modularity is a well-established concept for assessing community structures in various single and multi-layer networks, including those in biological and social domains. Biological networks, such as the brain, are known to exhibit group structure at a variety of scales -- local, meso, and global scale. Modularity, while useful in describing mesoscale brain organization, is limited as a metric to a global scale describing the overall strength of community structure. This approach, while valuable, overlooks important localized variations in community structure at the node level. To address this limitation, we extended modularity to individual nodes. This novel measure of nodal modularity ($nQ$) captures both meso and local scale changes in modularity. We hypothesized that $nQ$ illuminates granular changes in the brain due to diseases such as Alzheimer's disease (AD), which are known to disrupt the brain's modular structure. We explored $nQ$ in multiplex networks of a visual short-term memory binding task in fMRI and DTI data in the early stages of AD. Observed changes in $nQ$ in fMRI and DTI networks aligned with known trajectories of AD and were linked to common biomarkers of the disease, including amyloid-$\beta$ and tau. Additionally, $nQ$ clearly differentiated MCI from MCI converters showing indications that $nQ$ may be a useful diagnostic tool for characterizing disease stages. Our findings demonstrate the utility of $nQ$ as a measure of localized group structure, providing novel insights into temporal and disease related variability at the node level. Given the widespread application of modularity as a global measure, $nQ$ represents a significant advancement, providing a granular measure of network organization applicable to a wide range of disciplines.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-16</td>
<td style='padding: 8px;'>Making Your Dreams A Reality: Decoding the Dreams into a Coherent Video Story from fMRI Signals</td>
<td style='padding: 6px;'>Yanwei Fu, Jianxiong Gao, Baofeng Yang, Jianfeng Feng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.09350v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper studies the brave new idea for Multimedia community, and proposes a novel framework to convert dreams into coherent video narratives using fMRI data. Essentially, dreams have intrigued humanity for centuries, offering glimpses into our subconscious minds. Recent advancements in brain imaging, particularly functional magnetic resonance imaging (fMRI), have provided new ways to explore the neural basis of dreaming. By combining subjective dream experiences with objective neurophysiological data, we aim to understand the visual aspects of dreams and create complete video narratives. Our process involves three main steps: reconstructing visual perception, decoding dream imagery, and integrating dream stories. Using innovative techniques in fMRI analysis and language modeling, we seek to push the boundaries of dream research and gain deeper insights into visual experiences during sleep. This technical report introduces a novel approach to visually decoding dreams using fMRI signals and weaving dream visuals into narratives using language models. We gather a dataset of dreams along with descriptions to assess the effectiveness of our framework.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-15</td>
<td style='padding: 8px;'>Kolmogorov-Arnold Networks for Time Series Granger Causality Inference</td>
<td style='padding: 6px;'>Meiliang Liu, Yunfang Xu, Zijin Li, Zhengye Si, Xiaoxiao Yang, Xinyue Yang, Zhiwen Zhao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.08958v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We introduce Granger Causality Kolmogorov-Arnold Networks (GCKAN), an innovative architecture that extends the recently proposed Kolmogorov-Arnold Networks (KAN) to the domain of causal inference. By extracting base weights from KAN layers and incorporating the sparsity-inducing penalty along with ridge regularization, GCKAN infers the Granger causality from time series while enabling automatic time lag selection. Additionally, we propose an algorithm leveraging time-reversed Granger causality to enhance inference accuracy. The algorithm compares prediction and sparse-inducing losses derived from the original and time-reversed series, automatically selecting the casual relationship with the higher score or integrating the results to mitigate spurious connectivities. Comprehensive experiments conducted on Lorenz-96, gene regulatory networks, fMRI BOLD signals, and VAR datasets demonstrate that the proposed model achieves competitive performance to state-of-the-art methods in inferring Granger causality from nonlinear, high-dimensional, and limited-sample time series.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-16</td>
<td style='padding: 8px;'>A new perspective on brain stimulation interventions: Optimal stochastic tracking control of brain network dynamics</td>
<td style='padding: 6px;'>Kangli Dong, Siya Chen, Ying Dan, Lu Zhang, Xinyi Li, Wei Liang, Yue Zhao, Yu Sun</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.08567v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Network control theory (NCT) has recently been utilized in neuroscience to facilitate our understanding of brain stimulation effects. A particularly useful branch of NCT is optimal control, which focuses on applying theoretical and computational principles of control theory to design optimal strategies to achieve specific goals in neural processes. However, most existing research focuses on optimally controlling brain network dynamics from the original state to a target state at a specific time point. In this paper, we present the first investigation of introducing optimal stochastic tracking control strategy to synchronize the dynamics of the brain network to a target dynamics rather than to a target state at a specific time point. We utilized fMRI data from healthy groups, and cases of stroke and post-stroke aphasia. For all participants, we utilized a gradient descent optimization method to estimate the parameters for the brain network dynamic system. We then utilized optimal stochastic tracking control techniques to drive original unhealthy dynamics by controlling a certain number of nodes to synchronize with target healthy dynamics. Results show that the energy associated with optimal stochastic tracking control is negatively correlated with the intrinsic average controllability of the brain network system, while the energy of the optimal state approaching control is significantly related to the target state value. For a 100-dimensional brain network system, controlling the five nodes with the lowest tracking energy can achieve relatively acceptable dynamics control effects. Our results suggest that stochastic tracking control is more aligned with the objective of brain stimulation interventions, and is closely related to the intrinsic characteristics of the brain network system, potentially representing a new direction for future brain network optimal control research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>THOI: An efficient and accessible library for computing higher-order interactions enhanced by batch-processing</td>
<td style='padding: 6px;'>Laouen Belloli, Pedro Mediano, Rodrigo Cofré, Diego Fernandez Slezak, Rubén Herzog</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.03381v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Complex systems are characterized by nonlinear dynamics, multi-level interactions, and emergent collective behaviors. Traditional analyses that focus solely on pairwise interactions often oversimplify these systems, neglecting the higher-order interactions critical for understanding their full collective dynamics. Recent advances in multivariate information theory provide a principled framework for quantifying these higher-order interactions, capturing key properties such as redundancy, synergy, shared randomness, and collective constraints. However, two major challenges persist: accurately estimating joint entropies and addressing the combinatorial explosion of interacting terms. To overcome these challenges, we introduce THOI (Torch-based High-Order Interactions), a novel, accessible, and efficient Python library for computing high-order interactions in continuous-valued systems. THOI leverages the well-established Gaussian copula method for joint entropy estimation, combined with state-of-the-art batch and parallel processing techniques to optimize performance across CPU, GPU, and TPU environments. Our results demonstrate that THOI significantly outperforms existing tools in terms of speed and scalability. For larger systems, where exhaustive analysis is computationally impractical, THOI integrates optimization strategies that make higher-order interaction analysis feasible. We validate THOI accuracy using synthetic datasets with parametrically controlled interactions and further illustrate its utility by analyzing fMRI data from human subjects in wakeful resting states and under deep anesthesia. Finally, we analyzed over 900 real-world and synthetic datasets, establishing a comprehensive framework for applying higher-order interaction (HOI) analysis in complex systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-05</td>
<td style='padding: 8px;'>Decoding fMRI Data into Captions using Prefix Language Modeling</td>
<td style='padding: 6px;'>Vyacheslav Shen, Kassymzhomart Kunanbayev, Dae-Shik Kim</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02570v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>With the advancements in Large Language and Latent Diffusion models, brain decoding has achieved remarkable results in recent years. The works on the NSD dataset, with stimuli images from the COCO dataset, leverage the embeddings from the CLIP model for image reconstruction and GIT for captioning. However, the current captioning approach introduces the challenge of potential data contamination given that the GIT model was trained on the COCO dataset. In this work, we present an alternative method for decoding brain signals into image captions by predicting a DINOv2 model's embedding of an image from the corresponding fMRI signal and then providing its [CLS] token as the prefix to the GPT-2 language model which decreases computational requirements considerably. Additionally, instead of commonly used Linear Regression, we explore 3D Convolutional Neural Network mapping of fMRI signals to image embedding space for better accounting positional information of voxels.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-01</td>
<td style='padding: 8px;'>Sparse identification of evolution equations via Bayesian model selection</td>
<td style='padding: 6px;'>Tim W. Kroll, Oliver Kamps</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.01476v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The quantitative formulation of evolution equations is the backbone for prediction, control, and understanding of dynamical systems across diverse scientific fields. Besides deriving differential equations for dynamical systems based on basic scientific reasoning or prior knowledge in recent times a growing interest emerged to infer these equations purely from data. In this article, we introduce a novel method for the sparse identification of nonlinear dynamical systems from observational data, based on the observation how the key challenges of the quality of time derivatives and sampling rates influence this problem. Our approach combines system identification based on thresholded least squares minimization with additional error measures that account for both the deviation between the model and the time derivative of the data, and the integrated performance of the model in forecasting dynamics. Specifically, we integrate a least squares error as well as the Wasserstein metric for estimated models and combine them within a Bayesian optimization framework to efficiently determine optimal hyperparameters for thresholding and weighting of the different error norms. Additionally, we employ distinct regularization parameters for each differential equation in the system, enhancing the method's precision and flexibility. We demonstrate the capabilities of our approach through applications to dynamical fMRI data and the prototypical example of a wake flow behind a cylinder. In the wake flow problem, our method identifies a sparse, accurate model that correctly captures transient dynamics, oscillation periods, and phase information, outperforming existing methods. In the fMRI example, we show how our approach extracts insights from a trained recurrent neural network, offering a novel avenue for explainable AI by inferring differential equations that capture potentially causal relationships.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-06</td>
<td style='padding: 8px;'>The Algonauts Project 2025 Challenge: How the Human Brain Makes Sense of Multimodal Movies</td>
<td style='padding: 6px;'>Alessandro T. Gifford, Domenic Bersch, Marie St-Laurent, Basile Pinsard, Julie Boyle, Lune Bellec, Aude Oliva, Gemma Roig, Radoslaw M. Cichy</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.00504v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>There is growing symbiosis between artificial and biological intelligence sciences: neural principles inspire new intelligent machines, which are in turn used to advance our theoretical understanding of the brain. To promote further collaboration between biological and artificial intelligence researchers, we introduce the 2025 edition of the Algonauts Project challenge: How the Human Brain Makes Sense of Multimodal Movies (https://algonautsproject.com/). In collaboration with the Courtois Project on Neuronal Modelling (CNeuroMod), this edition aims to bring forth a new generation of brain encoding models that are multimodal and that generalize well beyond their training distribution, by training them on the largest dataset of fMRI responses to movie watching available to date. Open to all, the 2025 challenge provides transparent, directly comparable results through a public leaderboard that is updated automatically after each submission to facilitate rapid model assessment and guide development. The challenge will end with a session at the 2025 Cognitive Computational Neuroscience (CCN) conference that will feature winning models. We welcome researchers interested in collaborating with the Algonauts Project by contributing ideas and datasets for future challenges.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-21</td>
<td style='padding: 8px;'>Probing Type II Seesaw Leptogenesis Through Lepton Flavor Violation</td>
<td style='padding: 6px;'>Chengcheng Han, Yijun Han, Sihui Huang, Zhanhong Lei</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.12184v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Lepton flavor violation (LFV) offers a powerful probe of physics beyond the Standard Model, particularly in models addressing neutrino masses and the baryon asymmetry of the universe. In this study, we investigate LFV processes within the framework of type II seesaw leptogenesis, where the Standard Model is extended by an $SU(2)_L$ triplet Higgs field. We focus on key LFV processes including $\mu^+\to e^+\gamma$, $\mu^+ \to e^+e^-e^+$, and $\mu \rightarrow e$ conversion in nuclei, deriving stringent constraints on the parameter space from current experimental data. We scan the 3$\sigma$ range of neutrino oscillation parameters and identify the most conservative bounds consistent with existing measurements. Our results reveal that the MEG experiment currently provides the strongest constraints in the normal ordering (NO) scenario, while the SINDRUM experiment offers comparable sensitivity in the inverted ordering (IO) case. Future experiments, such as MEG II, Mu3e, Mu2e, and COMET, are predicted to significantly improve the sensitivity, testing larger regions of the parameter space. This work underscores the crucial role of LFV experiments in probing type II seesaw leptogenesis, providing an avenue to explore the connections between neutrino mass generation, baryogenesis, and inflation at experimentally accessible energy scales.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-20</td>
<td style='padding: 8px;'>Artificial Neural Networks for Magnetoencephalography: A review of an emerging field</td>
<td style='padding: 6px;'>Arthur Dehgan, Hamza Abdelhedi, Vanessa Hadid, Irina Rish, Karim Jerbi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.11566v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Magnetoencephalography (MEG) is a cutting-edge neuroimaging technique that measures the intricate brain dynamics underlying cognitive processes with an unparalleled combination of high temporal and spatial precision. MEG data analytics has always relied on advanced signal processing and mathematical and statistical tools for various tasks ranging from data cleaning to probing the signals' rich dynamics and estimating the neural sources underlying the surface-level recordings. Like in most domains, the surge in Artificial Intelligence (AI) has led to the increased use of Machine Learning (ML) methods for MEG data classification. More recently, an emerging trend in this field is using Artificial Neural Networks (ANNs) to address many MEG-related tasks. This review provides a comprehensive overview of how ANNs are being used with MEG data from three vantage points: First, we review work that employs ANNs for MEG signal classification, i.e., for brain decoding. Second, we report on work that has used ANNs as putative models of information processing in the human brain. Finally, we examine studies that use ANNs as techniques to tackle methodological questions in MEG, including artifact correction and source estimation. Furthermore, we assess the current strengths and limitations of using ANNs with MEG and discuss future challenges and opportunities in this field. Finally, by establishing a detailed portrait of the field and providing practical recommendations for the future, this review seeks to provide a helpful reference for both seasoned MEG researchers and newcomers to the field who are interested in using ANNs to enhance the exploration of the complex dynamics of the human brain with MEG.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-13</td>
<td style='padding: 8px;'>MVICAD2: Multi-View Independent Component Analysis with Delays and Dilations</td>
<td style='padding: 6px;'>Ambroise Heurtebise, Omar Chehab, Pierre Ablin, Alexandre Gramfort</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.07426v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Machine learning techniques in multi-view settings face significant challenges, particularly when integrating heterogeneous data, aligning feature spaces, and managing view-specific biases. These issues are prominent in neuroscience, where data from multiple subjects exposed to the same stimuli are analyzed to uncover brain activity dynamics. In magnetoencephalography (MEG), where signals are captured at the scalp level, estimating the brain's underlying sources is crucial, especially in group studies where sources are assumed to be similar for all subjects. Common methods, such as Multi-View Independent Component Analysis (MVICA), assume identical sources across subjects, but this assumption is often too restrictive due to individual variability and age-related changes. Multi-View Independent Component Analysis with Delays (MVICAD) addresses this by allowing sources to differ up to a temporal delay. However, temporal dilation effects, particularly in auditory stimuli, are common in brain dynamics, making the estimation of time delays alone insufficient. To address this, we propose Multi-View Independent Component Analysis with Delays and Dilations (MVICAD2), which allows sources to differ across subjects in both temporal delays and dilations. We present a model with identifiable sources, derive an approximation of its likelihood in closed form, and use regularization and optimization techniques to enhance performance. Through simulations, we demonstrate that MVICAD2 outperforms existing multi-view ICA methods. We further validate its effectiveness using the Cam-CAN dataset, and showing how delays and dilations are related to aging.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-18</td>
<td style='padding: 8px;'>Exploring the distribution of connectivity weights in resting-state EEG networks</td>
<td style='padding: 6px;'>Shiang Hu, Xiao Gong, Xiaolong Huang, Jie Ruan, Pedro Antonio Valdes-Sosa</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.07394v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The resting-state brain networks (RSNs) reflects the functional connectivity patterns between brain modules, providing essential foundations for decoding intrinsic neural information within the brain. It serves as one of the primary tools for describing the spatial dynamics of the brain using various neuroimaging techniques, such as electroencephalography (EEG) and magnetoencephalography (MEG). However, the distribution rules or potential modes of functional connectivity weights in the resting state remain unclear. In this context, we first start from simulation, using forward solving model to generate scalp EEG with four channel densities (19, 32, 64, 128). Subsequently, we construct scalp brain networks using five coupling measures, aiming to explore whether different channel density or coupling measures affect the distribution pattern of functional connectivity weights. Next, we quantify the distribution pattern by calculating the skewness, kurtosis, and Shannon entropy of the functional connectivity network weights. Finally, the results of the simulation were validated in a normative database. We observed that: 1) The functional connection weights exhibit a right-skewed distribution, and are not influenced by channel density or coupling measures; 2) The functional connection weights exhibit a relatively uniform distribution, with the potential for volume conduction to affect the degree of uniformity in the distribution; 3) Networks constructed using coupling measures influenced by volume conduction exhibit significant correlations between the average connection weight and measures of skewness, kurtosis, and Shannon entropy. This study contributes to a deeper understanding of RSNs, providing valuable insights for research in the field of neuroscience, and holds promise for being associated with brain cognition and disease diagnosis.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-09</td>
<td style='padding: 8px;'>On the Atomki nuclear anomaly after the MEG-II result</td>
<td style='padding: 6px;'>Daniele Barducci, Davide Germani, Marco Nardecchia, Stefano Scacco, Claudio Toni</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.05507v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent experimental results from the Atomki collaboration have reported the observation of anomalous effects in Beryllium, Helium and Carbon nuclear transitions that could hint at physics beyond the Standard Model. However, the MEG-II experiment has recently found no significant anomalous signal in the Beryllium transition ${^8}\text{Be}^\star\to{^8}\text{Be}+e^+e^-$. In view of this result, we critically re-examine the possible theoretical interpretations of the anomalies observed by the Atomki experiment in terms of a new boson $X$ with mass around $17\;$MeV. The present work aims to study the phenomenology of a spin-2 state and revisit the possibility of a pure CP-even scalar, which was initially dismissed due to its inability to explain the Beryllium anomalous signal. Our analysis shows that a spin-2 state is highly disfavoured by the SINDRUM constraint while a scalar boson could explain the Helium and Carbon anomalies while being compatible with other experimental constraints.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-10</td>
<td style='padding: 8px;'>Development of the high-rate capable DLC-RPC based on the current evacuation pattern</td>
<td style='padding: 6px;'>Masato Takahashi, Sei Ban, Weiyuan Li, Atsuhiko Ochi, Wataru Ootani, Atsushi Oya, Hiromu Suzuki, Kensuke Yamamoto</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.05128v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The Resistive Plate Chamber using Diamond-Like Carbon electrodes (DLC-RPC) has been developed as a background tagging detector in the MEG$~$II experiment. The DLC-RPC is planned to be installed in a high-intensity and low-momentum muon beam. This detector is required to have a detection efficiency of above 90 % with four active gaps in the muon beam due to the limitation of the material budget. In such an environment, the high current flowing through the resistive electrodes causes a voltage drop, which reduces the performance of the DLC-RPC. This voltage drop can be suppressed by implementing a current evacuation pattern, though discharges are more likely to occur near the pattern. Therefore the pattern must be covered by a protection cover made of an insulator. In this study, electrode samples with the current evacuation pattern and different widths of protection cover (0.2 mm and 0.8 mm) have been produced, and their performance and stability were measured. The detection efficiency of the single-gap for $\beta$-ray from a $^{90}$Sr source was measured to be up to approximately 60 % in both electrode samples. The target efficiency can be achieved even with a drop of 100 $-$ 150 V. On the other hand, after more than a dozen hours of operation, discharges suddenly occurred and the detector was prevented from further operation. These discharges created current paths on the spacing pillars. This serious problem must be investigated and solved in the future.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-05</td>
<td style='padding: 8px;'>Automated Detection of Epileptic Spikes and Seizures Incorporating a Novel Spatial Clustering Prior</td>
<td style='padding: 6px;'>Hanyang Dong, Shurong Sheng, Xiongfei Wang, Jiahong Gao, Yi Sun, Wanli Yang, Kuntao Xiao, Pengfei Teng, Guoming Luan, Zhao Lv</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.10404v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A Magnetoencephalography (MEG) time-series recording consists of multi-channel signals collected by superconducting sensors, with each signal's intensity reflecting magnetic field changes over time at the sensor location. Automating epileptic MEG spike detection significantly reduces manual assessment time and effort, yielding substantial clinical benefits. Existing research addresses MEG spike detection by encoding neural network inputs with signals from all channel within a time segment, followed by classification. However, these methods overlook simultaneous spiking occurred from nearby sensors. We introduce a simple yet effective paradigm that first clusters MEG channels based on their sensor's spatial position. Next, a novel convolutional input module is designed to integrate the spatial clustering and temporal changes of the signals. This module is fed into a custom MEEG-ResNet3D developed by the authors, which learns to extract relevant features and classify the input as a spike clip or not. Our method achieves an F1 score of 94.73% on a large real-world MEG dataset Sanbo-CMR collected from two centers, outperforming state-of-the-art approaches by 1.85%. Moreover, it demonstrates efficacy and stability in the Electroencephalographic (EEG) seizure detection task, yielding an improved weighted F1 score of 1.4% compared to current state-of-the-art techniques evaluated on TUSZ, whch is the largest EEG seizure dataset.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-27</td>
<td style='padding: 8px;'>Signatures of prediction during natural listening in MEG data?</td>
<td style='padding: 6px;'>Sahel Azizpour, Britta U. Westner, Jakub Szewczyk, Umut Güçlü, Linda Geerligs</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.19622v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The brain uses contextual information and prior knowledge to anticipate upcoming content during language comprehension. Recent research has shown predictive signals can be revealed in pre-onset ECoG activity during naturalistic narrative listening, by building encoding models based on word embeddings from Large Language Models (LLMs). Similarly, evidence for long-range predictive encoding has been observed in fMRI data, where incorporating embeddings for multiple upcoming words in a narrative improves alignment with brain activity. This study examines whether similar predictive information can be detected in MEG, a technique with higher temporal resolution than fMRI but a lower signal-to-noise ratio than ECoG. Our findings indicate that MEG captures pre-onset representations up to 1 second before word onset, consistent with ECoG results. However, unlike fMRI findings, incorporating future word embeddings did not enhance MEG encoding, even for one word into the future, which suggests that the pre-onset encoding may not reflect predictive processing. This work demonstrates that MEG combined with LLMs is a valuable approach for studying language processing in naturalistic narratives and highlights the need to study further what constitutes evidence for prediction during natural listening.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-24</td>
<td style='padding: 8px;'>Low count of optically pumped magnetometers furnishes a reliable real-time access to sensorimotor rhythm</td>
<td style='padding: 6px;'>Nikita Fedosov, Daria Medvedeva, Oleg Shevtsov, Alexei Ossadtchi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2412.18353v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study presents an analysis of sensorimotor rhythms using an advanced, optically-pumped magnetoencephalography (OPM-MEG) system - a novel and rapidly developing technology. We conducted real-movement and motor imagery experiments with nine participants across two distinct magnetically-shielded environments: one featuring an analog active suppression system and the other a digital implementation. Our findings demonstrate that, under optimal recording conditions, OPM sensors provide highly informative signals, suitable for use in practical motor imagery brain-computer interface (BCI) applications. We further examine the feasibility of a portable, low-sensor-count OPM-based BCI under varied experimental setups, highlighting its potential for real-time control of external devices via user intentions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-12-22</td>
<td style='padding: 8px;'>Bridging Auditory Perception and Language Comprehension through MEG-Driven Encoding Models</td>
<td style='padding: 6px;'>Matteo Ciferri, Matteo Ferrante, Nicola Toschi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.03246v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding the neural mechanisms behind auditory and linguistic processing is key to advancing cognitive neuroscience. In this study, we use Magnetoencephalography (MEG) data to analyze brain responses to spoken language stimuli. We develop two distinct encoding models: an audio-to-MEG encoder, which uses time-frequency decompositions (TFD) and wav2vec2 latent space representations, and a text-to-MEG encoder, which leverages CLIP and GPT-2 embeddings. Both models successfully predict neural activity, demonstrating significant correlations between estimated and observed MEG signals. However, the text-to-MEG model outperforms the audio-based model, achieving higher Pearson Correlation (PC) score. Spatially, we identify that auditory-based embeddings (TFD and wav2vec2) predominantly activate lateral temporal regions, which are responsible for primary auditory processing and the integration of auditory signals. In contrast, textual embeddings (CLIP and GPT-2) primarily engage the frontal cortex, particularly Broca's area, which is associated with higher-order language processing, including semantic integration and language production, especially in the 8-30 Hz frequency range. The strong involvement of these regions suggests that auditory stimuli are processed through more direct sensory pathways, while linguistic information is encoded via networks that integrate meaning and cognitive control. Our results reveal distinct neural pathways for auditory and linguistic information processing, with higher encoding accuracy for text representations in the frontal regions. These insights refine our understanding of the brain's functional architecture in processing auditory and textual information, offering quantitative advancements in the modelling of neural responses to complex language stimuli.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-04</td>
<td style='padding: 8px;'>Asynchronous Hebbian/anti-Hebbian networks</td>
<td style='padding: 6px;'>Henrique Reis Aguiar, Matthias H. Hennig</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02402v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-27</td>
<td style='padding: 8px;'>NeuroAI for AI Safety</td>
<td style='padding: 6px;'>Patrick Mineault, Niccolò Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.18526v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-21</td>
<td style='padding: 8px;'>Evaluating Representational Similarity Measures from the Lens of Functional Correspondence</td>
<td style='padding: 6px;'>Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.14633v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-10-25</td>
<td style='padding: 8px;'>A prescriptive theory for brain-like inference</td>
<td style='padding: 6px;'>Hadi Vafaii, Dekel Galor, Jacob L. Yates</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2410.19315v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The Evidence Lower Bound (ELBO) is a widely used objective for training deep generative models, such as Variational Autoencoders (VAEs). In the neuroscience literature, an identical objective is known as the variational free energy, hinting at a potential unified framework for brain function and machine learning. Despite its utility in interpreting generative models, including diffusion models, ELBO maximization is often seen as too broad to offer prescriptive guidance for specific architectures in neuroscience or machine learning. In this work, we show that maximizing ELBO under Poisson assumptions for general sequence data leads to a spiking neural network that performs Bayesian posterior inference through its membrane potential dynamics. The resulting model, the iterative Poisson VAE (iP-VAE), has a closer connection to biological neurons than previous brain-inspired predictive coding models based on Gaussian assumptions. Compared to amortized and iterative VAEs, iP-VAElearns sparser representations and exhibits superior generalization to out-of-distribution samples. These findings suggest that optimizing ELBO, combined with Poisson assumptions, provides a solid foundation for developing prescriptive theories in NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-09</td>
<td style='padding: 8px;'>Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models</td>
<td style='padding: 6px;'>Emily Cheng, Richard J. Antonello</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.05771v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first "composition" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-07-22</td>
<td style='padding: 8px;'>Predictive Coding Networks and Inference Learning: Tutorial and Survey</td>
<td style='padding: 6px;'>Björn van Zwol, Ro Jefferson, Egon L. van den Broek</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2407.04117v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent years have witnessed a growing call for renewed emphasis on neuroscience-inspired approaches in artificial intelligence research, under the banner of NeuroAI. A prime example of this is predictive coding networks (PCNs), based on the neuroscientific framework of predictive coding. This framework views the brain as a hierarchical Bayesian inference model that minimizes prediction errors through feedback connections. Unlike traditional neural networks trained with backpropagation (BP), PCNs utilize inference learning (IL), a more biologically plausible algorithm that explains patterns of neural activity that BP cannot. Historically, IL has been more computationally intensive, but recent advancements have demonstrated that it can achieve higher efficiency than BP with sufficient parallelization. Furthermore, PCNs can be mathematically considered a superset of traditional feedforward neural networks (FNNs), significantly extending the range of trainable architectures. As inherently probabilistic (graphical) latent variable models, PCNs provide a versatile framework for both supervised learning and unsupervised (generative) modeling that goes beyond traditional artificial neural networks. This work provides a comprehensive review and detailed formal specification of PCNs, particularly situating them within the context of modern ML methods. Additionally, we introduce a Python library (PRECO) for practical implementation. This positions PC as a promising framework for future ML innovations.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-10-29</td>
<td style='padding: 8px;'>Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis</td>
<td style='padding: 6px;'>Mitchell Ostrow, Adam Eisen, Leo Kozachkov, Ila Fiete</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2306.10168v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-05-25</td>
<td style='padding: 8px;'>Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture</td>
<td style='padding: 6px;'>Galen Pogoncheff, Jacob Granley, Michael Beyeler</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2305.11275v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-09</td>
<td style='padding: 8px;'>A Deep Probabilistic Spatiotemporal Framework for Dynamic Graph Representation Learning with Application to Brain Disorder Identification</td>
<td style='padding: 6px;'>Sin-Yee Yap, Junn Yong Loo, Chee-Ming Ting, Fuad Noman, Raphael C. -W. Phan, Adeel Razi, David L. Dowe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2302.07243v4' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent applications of pattern recognition techniques on brain connectome classification using functional connectivity (FC) are shifting towards acknowledging the non-Euclidean topology and dynamic aspects of brain connectivity across time. In this paper, a deep spatiotemporal variational Bayes (DSVB) framework is proposed to learn time-varying topological structures in dynamic FC networks for identifying autism spectrum disorder (ASD) in human participants. The framework incorporates a spatial-aware recurrent neural network with an attention-based message passing scheme to capture rich spatiotemporal patterns across dynamic FC networks. To overcome model overfitting on limited training datasets, an adversarial training strategy is introduced to learn graph embedding models that generalize well to unseen brain networks. Evaluation on the ABIDE resting-state functional magnetic resonance imaging dataset shows that our proposed framework substantially outperforms state-of-the-art methods in identifying patients with ASD. Dynamic FC analyses with DSVB-learned embeddings reveal apparent group differences between ASD and healthy controls in brain network connectivity patterns and switching dynamics of brain states. The code is available at https://github.com/Monash-NeuroAI/Deep-Spatiotemporal-Variational-Bayes.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-03-11</td>
<td style='padding: 8px;'>Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks</td>
<td style='padding: 6px;'>Feng-Lei Fan, Yingxin Li, Hanchuan Peng, Tieyong Zeng, Fei Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2301.09245v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Throughout history, the development of artificial intelligence, particularly artificial neural networks, has been open to and constantly inspired by the increasingly deepened understanding of the brain, such as the inspiration of neocognitron, which is the pioneering work of convolutional neural networks. Per the motives of the emerging field: NeuroAI, a great amount of neuroscience knowledge can help catalyze the next generation of AI by endowing a network with more powerful capabilities. As we know, the human brain has numerous morphologically and functionally different neurons, while artificial neural networks are almost exclusively built on a single neuron type. In the human brain, neuronal diversity is an enabling factor for all kinds of biological intelligent behaviors. Since an artificial network is a miniature of the human brain, introducing neuronal diversity should be valuable in terms of addressing those essential problems of artificial networks such as efficiency, interpretability, and memory. In this Primer, we first discuss the preliminaries of biological neuronal diversity and the characteristics of information transmission and processing in a biological neuron. Then, we review studies of designing new neurons for artificial networks. Next, we discuss what gains can neuronal diversity bring into artificial networks and exemplary applications in several important fields. Lastly, we discuss the challenges and future directions of neuronal diversity to explore the potential of NeuroAI.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>The Lock Generative Adversarial Network for Medical Waveform Anomaly Detection</td>
<td style='padding: 6px;'>Wenjie Xu, Scott Dick</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13858v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Waveform signal analysis is a complex and important task in medical care. For example, mechanical ventilators are critical life-support machines, but they can cause serious injury to patients if they are out of synchronization with the patients' own breathing reflex. This asynchrony is revealed by the waveforms showing flow and pressure histories. Likewise, electrocardiograms record the electrical activity of a patients' heart as a set of waveforms, and anomalous waveforms can reveal important disease states. In both cases, subtle variations in a complex waveform are important information for patient care; signals which may be missed or mis-interpreted by human caregivers.   We report on the design of a novel Lock Generative Adversarial Network architecture for anomaly detection in raw or summarized medical waveform data. The proposed architecture uses alternating optimization of the generator and discriminator networks to solve the convergence dilemma. Furthermore, the fidelity of the generator networks' outputs to the actual distribution of anomalous data is improved via synthetic minority oversampling. We evaluate this new architecture on one ventilator asynchrony dataset, and two electrocardiogram datasets, finding that the performance was either equal or superior to the state-of-the art on all three.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>PhotoGAN: Generative Adversarial Neural Network Acceleration with Silicon Photonics</td>
<td style='padding: 6px;'>Tharini Suresh, Salma Afifi, Sudeep Pasricha</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13828v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Generative Adversarial Networks (GANs) are at the forefront of AI innovation, driving advancements in areas such as image synthesis, medical imaging, and data augmentation. However, the unique computational operations within GANs, such as transposed convolutions and instance normalization, introduce significant inefficiencies when executed on traditional electronic accelerators, resulting in high energy consumption and suboptimal performance. To address these challenges, we introduce PhotoGAN, the first silicon-photonic accelerator designed to handle the specialized operations of GAN models. By leveraging the inherent high throughput and energy efficiency of silicon photonics, PhotoGAN offers an innovative, reconfigurable architecture capable of accelerating transposed convolutions and other GAN-specific layers. The accelerator also incorporates a sparse computation optimization technique to reduce redundant operations, improving computational efficiency. Our experimental results demonstrate that PhotoGAN achieves at least 4.4x higher GOPS and 2.18x lower energy-per-bit (EPB) compared to state-of-the-art accelerators, including GPUs and TPUs. These findings showcase PhotoGAN as a promising solution for the next generation of GAN acceleration, providing substantial gains in both performance and energy efficiency.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>Ensuring Medical AI Safety: Explainable AI-Driven Detection and Mitigation of Spurious Model Behavior and Associated Data</td>
<td style='padding: 6px;'>Frederik Pahde, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13818v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Deep neural networks are increasingly employed in high-stakes medical applications, despite their tendency for shortcut learning in the presence of spurious correlations, which can have potentially fatal consequences in practice. Detecting and mitigating shortcut behavior is a challenging task that often requires significant labeling efforts from domain experts. To alleviate this problem, we introduce a semi-automated framework for the identification of spurious behavior from both data and model perspective by leveraging insights from eXplainable Artificial Intelligence (XAI). This allows the retrieval of spurious data points and the detection of model circuits that encode the associated prediction rules. Moreover, we demonstrate how these shortcut encodings can be used for XAI-based sample- and pixel-level data annotation, providing valuable information for bias mitigation methods to unlearn the undesired shortcut behavior. We show the applicability of our framework using four medical datasets across two modalities, featuring controlled and real-world spurious correlations caused by data artifacts. We successfully identify and mitigate these biases in VGG16, ResNet50, and contemporary Vision Transformer models, ultimately increasing their robustness and applicability for real-world medical tasks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>Generation of reusable learning objects from digital medical collections: An analysis based on the MASMDOA framework</td>
<td style='padding: 6px;'>Félix Buendía, Joaquín Gayoso-Cabada, José-Luis Sierra</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13806v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Learning Objects represent a widespread approach to structuring instructional materials in a large variety of educational contexts. The main aim of this work consists of analyzing from a qualitative point of view the process of generating reusable learning objects (RLOs) followed by Clavy, a tool that can be used to retrieve data from multiple medical knowledge sources and reconfigure such sources in diverse multimedia-based structures and organizations. From these organizations, Clavy is able to generate learning objects which can be adapted to various instructional healthcare scenarios with several types of user profiles and distinct learning requirements. Moreover, Clavy provides the capability of exporting these learning objects through educational standard specifications, which improves their reusability features. The analysis insights highlight the importance of having a tool able to transfer knowledge from the available digital medical collections to learning objects that can be easily accessed by medical students and healthcare practitioners through the most popular e-learning platforms.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>A mathematical model of CAR-T cell therapy in combination with chemotherapy for malignant gliomas</td>
<td style='padding: 6px;'>Dmitry Sinelshchikov, Juan Belmonte-Beitia, Matteo Italia</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13774v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We study the dynamics and interactions between combined chemotherapy and chimeric antigen receptor (CAR-T) cells therapy and malignant gliomas (MG). MG is one of the most common primary brain tumor, with high resistance to therapy and unfavorable prognosis. Here, we develop a mathematical model that describes the application of chemo- and CAR-T cell therapies and the dynamics of sensitive and resistant populations of tumor cells. This model is a five-dimensional dynamical system with impulsive inputs corresponding to clinical administration of chemo- and immunotherapy. We provide a proof of non-negativeness of solutions of the proposed model for non-negative initial data. We demonstrate that if we apply both therapies only once, the trajectories will be attracted to an invariant surface that corresponds to the tumor carrying capacity. On the other hand, if we apply both treatments constantly, we find regions of the parameter where the tumor is eradicated. Moreover, we study applications of different combinations of the above treatments in order to find an optimal combination at the population level. To this aim, we generate a population of $10^{4}$ virtual patients with the model parameters uniformly distributed in the medically relevant ranges and perform \emph{in silico} trials with different combinations of treatments. We obtain optimal protocols for several different relations of tumor growth rates between sensitive and drug resistant cells. We demonstrate that the tumor growth rate, efficacy of chemotherapy, and tumor immunosuppression are the parameters that mostly impact survival time in \emph{in silico} trials. We believe that our results provide new theoretical insights to guide the design of clinical trials for MG therapies.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>Reconciling Binary Replicates: Beyond the Average</td>
<td style='padding: 6px;'>Manuela Royer-Carenzi, Hadrien Lorenzo, Pierre Pudlo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13745v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Binary observations are often repeated to improve data quality, creating technical replicates. Several scoring methods are commonly used to infer the actual individual state and obtain a probability for each state. The common practice of averaging replicates has limitations, and alternative methods for scoring and classifying individuals are proposed. Additionally, an indecisive response might be wiser than classifying all individuals based on their replicates in the medical context, where 1 indicates a particular health condition. Building on the inherent limitations of the averaging approach, three alternative methods are examined: the median, maximum penalized likelihood estimation, and a Bayesian algorithm. The theoretical analysis suggests that the proposed alternatives outperform the averaging approach, especially the Bayesian method, which incorporates uncertainty and provides credible intervals. Simulations and real-world medical datasets are used to demonstrate the practical implications of these methods for improving diagnostic accuracy and disease prevalence estimation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>Point Cloud Surface Parametrization with HAND and LEG: Hausdorff Approximation from Node-wise Distances and Localized Energy for Geometry</td>
<td style='padding: 6px;'>Ka Ho Lai, Lok Ming Lui</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13737v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Surface parametrization plays a crucial role in various fields, such as computer graphics and medical imaging, and computational science and engineering. However, most existing techniques rely on the discretization of the surface into a triangular mesh. This paper addresses the problem of point cloud surface parametrization and presents two novel loss functions and a framework for point cloud surface parametrization based on deep neural networks. The first loss function aims to provide a soft constraint on parameter domain, allowing the handling of parameter domains with complex shapes or geometries. This loss function can also be used in generalizing landmark matching. The second loss function focuses on minimizing local distortion on the point cloud surface, demonstrating effectiveness in preserving the surface's local shape characteristics. We parametrized the functions involved using neural networks, and developed an algorithm for the minimization. Numerical experiments for shape matching, free-boundary and fixed-boundary surface parametrization and landmark matching, along with applications including surface reconstruction and boundary detection, are presented to demonstrate the effectiveness of our proposed methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>Question Answering on Patient Medical Records with Private Fine-Tuned LLMs</td>
<td style='padding: 6px;'>Sara Kothari, Ayush Gupta</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13687v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Healthcare systems continuously generate vast amounts of electronic health records (EHRs), commonly stored in the Fast Healthcare Interoperability Resources (FHIR) standard. Despite the wealth of information in these records, their complexity and volume make it difficult for users to retrieve and interpret crucial health insights. Recent advances in Large Language Models (LLMs) offer a solution, enabling semantic question answering (QA) over medical data, allowing users to interact with their health records more effectively. However, ensuring privacy and compliance requires edge and private deployments of LLMs.   This paper proposes a novel approach to semantic QA over EHRs by first identifying the most relevant FHIR resources for a user query (Task1) and subsequently answering the query based on these resources (Task2). We explore the performance of privately hosted, fine-tuned LLMs, evaluating them against benchmark models such as GPT-4 and GPT-4o. Our results demonstrate that fine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by 0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we examine advanced aspects of LLM usage, including sequential fine-tuning, model self-evaluation (narcissistic evaluation), and the impact of training data size on performance. The models and datasets are available here: https://huggingface.co/genloop</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>How to Complete Domain Tuning while Keeping General Ability in LLM: Adaptive Layer-wise and Element-wise Regularization</td>
<td style='padding: 6px;'>Shezheng Song, Hao Xu, Jun Ma, Shasha Li, Long Peng, Qian Wan, Xiaodong Liu, Jie Yu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13669v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large Language Models (LLMs) exhibit strong general-purpose language capabilities. However, fine-tuning these models on domain-specific tasks often leads to catastrophic forgetting, where the model overwrites or loses essential knowledge acquired during pretraining. This phenomenon significantly limits the broader applicability of LLMs. To address this challenge, we propose a novel approach to compute the element-wise importance of model parameters crucial for preserving general knowledge during fine-tuning. Our method utilizes a dual-objective optimization strategy: (1) regularization loss to retain the parameter crucial for general knowledge; (2) cross-entropy loss to adapt to domain-specific tasks. Additionally, we introduce layer-wise coefficients to account for the varying contributions of different layers, dynamically balancing the dual-objective optimization. Extensive experiments on scientific, medical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our approach mitigates catastrophic forgetting while enhancing model adaptability. Compared to previous methods, our solution is approximately 20 times faster and requires only 10%-15% of the storage, highlighting the practical efficiency. The code will be released.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-23</td>
<td style='padding: 8px;'>Enhancing Medical Image Analysis through Geometric and Photometric transformations</td>
<td style='padding: 6px;'>Khadija Rais, Mohamed Amroune, Mohamed Yassine Haouam</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.13643v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Medical image analysis suffers from a lack of labeled data due to several challenges including patient privacy and lack of experts. Although some AI models only perform well with large amounts of data, we will move to data augmentation where there is a solution to improve the performance of our models and increase the dataset size through traditional or advanced techniques. In this paper, we evaluate the effectiveness of data augmentation techniques on two different medical image datasets. In the first step, we applied some transformation techniques to the skin cancer dataset containing benign and malignant classes. Then, we trained the convolutional neural network (CNN) on the dataset before and after augmentation, which significantly improved test accuracy from 90.74% to 96.88% and decreased test loss from 0.7921 to 0.1468 after augmentation. In the second step, we used the Mixup technique by mixing two random images and their corresponding masks using the retina and blood vessels dataset, then we trained the U-net model and obtained the Dice coefficient which increased from 0 before augmentation to 0.4163 after augmentation. The result shows the effect of using data augmentation to increase the dataset size on the classification and segmentation performance.</td>
</tr>
</tbody>
</table>

