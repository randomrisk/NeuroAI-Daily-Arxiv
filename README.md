<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2025-09-05</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>A Brain-Inspired Gating Mechanism Unlocks Robust Computation in Spiking Neural Networks</td>
<td style='padding: 6px;'>Qianyi Bai, Haiteng Wang, Qiang Yu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03281v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While spiking neural networks (SNNs) provide a biologically inspired and energy-efficient computational framework, their robustness and the dynamic advantages inherent to biological neurons remain significantly underutilized owing to oversimplified neuron models. In particular, conventional leaky integrate-and-fire (LIF) neurons often omit the dynamic conductance mechanisms inherent in biological neurons, thereby limiting their capacity to cope with noise and temporal variability. In this work, we revisit dynamic conductance from a functional perspective and uncover its intrinsic role as a biologically plausible gating mechanism that modulates information flow. Building on this insight, we introduce the Dynamic Gated Neuron~(DGN), a novel spiking unit in which membrane conductance evolves in response to neuronal activity, enabling selective input filtering and adaptive noise suppression. We provide a theoretical analysis showing that DGN possess enhanced stochastic stability compared to standard LIF models, with dynamic conductance intriguingly acting as a disturbance rejection mechanism. DGN-based SNNs demonstrate superior performance across extensive evaluations on anti-noise tasks and temporal-related benchmarks such as TIDIGITS and SHD, consistently exhibiting excellent robustness. Our results highlight, for the first time, a biologically plausible dynamic gating as a key mechanism for robust spike-based computation, providing not only theoretical guarantees but also strong empirical validations. This work thus paves the way for more resilient, efficient, and biologically inspired spiking neural networks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>RTGMFF: Enhanced fMRI-based Brain Disorder Diagnosis via ROI-driven Text Generation and Multimodal Feature Fusion</td>
<td style='padding: 6px;'>Junhao Jia, Yifei Sun, Yunyou Liu, Cheng Yang, Changmiao Wang, Feiwei Qin, Yong Peng, Wenwen Min</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03214v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional magnetic resonance imaging (fMRI) is a powerful tool for probing brain function, yet reliable clinical diagnosis is hampered by low signal-to-noise ratios, inter-subject variability, and the limited frequency awareness of prevailing CNN- and Transformer-based models. Moreover, most fMRI datasets lack textual annotations that could contextualize regional activation and connectivity patterns. We introduce RTGMFF, a framework that unifies automatic ROI-level text generation with multimodal feature fusion for brain-disorder diagnosis. RTGMFF consists of three components: (i) ROI-driven fMRI text generation deterministically condenses each subject's activation, connectivity, age, and sex into reproducible text tokens; (ii) Hybrid frequency-spatial encoder fuses a hierarchical wavelet-mamba branch with a cross-scale Transformer encoder to capture frequency-domain structure alongside long-range spatial dependencies; and (iii) Adaptive semantic alignment module embeds the ROI token sequence and visual features in a shared space, using a regularized cosine-similarity loss to narrow the modality gap. Extensive experiments on the ADHD-200 and ABIDE benchmarks show that RTGMFF surpasses current methods in diagnostic accuracy, achieving notable gains in sensitivity, specificity, and area under the ROC curve. Code is available at https://github.com/BeistMedAI/RTGMFF.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Temporally-Aware Diffusion Model for Brain Progression Modelling with Bidirectional Temporal Regularisation</td>
<td style='padding: 6px;'>Mattia Litrico, Francesco Guarnera, Mario Valerio Giuffrida, Daniele Rav√¨, Sebastiano Battiato</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03141v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Generating realistic MRIs to accurately predict future changes in the structure of brain is an invaluable tool for clinicians in assessing clinical outcomes and analysing the disease progression at the patient level. However, current existing methods present some limitations: (i) some approaches fail to explicitly capture the relationship between structural changes and time intervals, especially when trained on age-imbalanced datasets; (ii) others rely only on scan interpolation, which lack clinical utility, as they generate intermediate images between timepoints rather than future pathological progression; and (iii) most approaches rely on 2D slice-based architectures, thereby disregarding full 3D anatomical context, which is essential for accurate longitudinal predictions. We propose a 3D Temporally-Aware Diffusion Model (TADM-3D), which accurately predicts brain progression on MRI volumes. To better model the relationship between time interval and brain changes, TADM-3D uses a pre-trained Brain-Age Estimator (BAE) that guides the diffusion model in the generation of MRIs that accurately reflect the expected age difference between baseline and generated follow-up scans. Additionally, to further improve the temporal awareness of TADM-3D, we propose the Back-In-Time Regularisation (BITR), by training TADM-3D to predict bidirectionally from the baseline to follow-up (forward), as well as from the follow-up to baseline (backward). Although predicting past scans has limited clinical applications, this regularisation helps the model generate temporally more accurate scans. We train and evaluate TADM-3D on the OASIS-3 dataset, and we validate the generalisation performance on an external test set from the NACC dataset. The code will be available upon acceptance.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Handwriting Imagery EEG Classification based on Convolutional Neural Networks</td>
<td style='padding: 6px;'>Hao Yang, Guang Ouyang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03111v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Handwriting imagery has emerged as a promising paradigm for brain-computer interfaces (BCIs) aimed at translating brain activity into text output. Compared with invasively recorded electroencephalography (EEG), non-invasive recording offers a more practical and feasible approach to capturing brain signals for BCI. This study explores the limit of decoding non-invasive EEG associated with handwriting imagery into English letters using deep neural networks. To this end, five participants were instructed to imagine writing the 26 English letters with their EEG being recorded from the scalp. A measurement of EEG similarity across letters was conducted to investigate letter-specific patterns in the dataset. Subsequently, four convolutional neural network (CNN) models were trained for EEG classification. Descriptively, the EEG data clearly exhibited letter-specific patterns serving as a proof-of-concept for EEG-to-text translation. Under the chance level of accuracy at 3.85%, the CNN classifiers trained on each participant reached the highest limit of around 20%. This study marks the first attempt to decode non-invasive EEG associated with handwriting imagery. Although the achieved accuracy is not sufficient for a usable brain-to-text BCI, the model's performance is noteworthy in revealing the potential for translating non-invasively recorded brain signals into text outputs and establishing a baseline for future research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Binary Quantization For LLMs Through Dynamic Grouping</td>
<td style='padding: 6px;'>Xinzhe Zheng, Zhen-Qun Yang, Haoran Xie, S. Joe Qin, Arlene Chen, Fangzhen Lin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03054v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of Natural Language Processing (NLP) tasks, but require substantial memory and computational resources. Binary quantization, which compresses model weights from 16-bit Brain Float to 1-bit representations in {-1, 1}, offers significant reductions in storage and inference costs. However, such aggressive quantization often leads to notable performance degradation compared to more conservative 4-bit quantization methods. In this research, we propose a novel optimization objective tailored for binary quantization, along with three algorithms designed to realize it effectively. Our method enhances blocked quantization by dynamically identifying optimal unstructured sub-matrices through adaptive grouping strategies. Experimental results demonstrate that our approach achieves an average bit length of just 1.007 bits, while maintaining high model quality. Specifically, our quantized LLaMA 3.2 3B model attains a perplexity of 8.23, remarkably close to the original 7.81, and surpasses previous SOTA BiLLM with a perplexity of only 123.90. Furthermore, our method is competitive with SOTA 4-bit approaches such as GPTQ in both performance and efficiency. The compression process is highly efficient, requiring only 14 seconds to quantize the full LLaMA 3.2 3B weights on a single CPU core, with the entire process completing in under 100 minutes and exhibiting embarrassingly parallel properties.   Code - https://github.com/johnnyzheng0636/WGM_bi_quan</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Remember when? Deciphering Ediacaran-Cambrian Metazoan behaviour and temporal memory using fossil movement paths</td>
<td style='padding: 6px;'>Brittany A. Laing, M. Gabriela M√°ngano, Luis A. Buatois, Glenn A. Brock, Romain Gougeon, Zoe Vestrum, Luke C. Strotz, Lyndon Koens</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.02940v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Evaluating the timing and trajectory of sensory system innovations is crucial for understanding the increase in phylogenetic, behavioural, and ecological diversity during the Ediacaran-Cambrian transition. Elucidation of sensory adaptations has relied on either body-fossil evidence based on anatomical features or qualitative descriptions of trace-fossil morphology, leaving a gap in the record of sensory system innovations between the development of basic sensory capacities and that of more advanced sensory organs and brains. Here, we examine fossil movement trajectories of Ediacaran and Cambrian grazers for the presence of autocorrelation. Our analysis reveals a lack of temporal correlation in the studied Ediacaran trajectories and its presence in both analysed Cambrian trajectories, indicating time-tuned behaviours were in place by the early Cambrian. These results support the Cambrian Information Revolution hypothesis and indicates that increases in cognitive complexity and behavioural strategies were yet another important evolutionary innovation that occurred during the Ediacaran Cambrian transition.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Simulacra Naturae: Generative Ecosystem driven by Agent-Based Simulations and Brain Organoid Collective Intelligence</td>
<td style='padding: 6px;'>Nefeli Manoudaki, Mert Toka, Iason Paterakis, Diarmid Flatley</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.02924v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Simulacra Naturae is a data-driven media installation that explores collective care through the entanglement of biological computation, material ecologies, and generative systems. The work translates pre-recorded neural activity from brain organoids, lab-grown three-dimensional clusters of neurons, into a multi-sensory environment composed of generative visuals, spatial audio, living plants, and fabricated clay artifacts. These biosignals, streamed through a real-time system, modulate emergent agent behaviors inspired by natural systems such as termite colonies and slime molds. Rather than using biosignals as direct control inputs, Simulacra Naturae treats organoid activity as a co-creative force, allowing neural rhythms to guide the growth, form, and atmosphere of a generative ecosystem. The installation features computationally fabricated clay prints embedded with solenoids, adding physical sound resonances to the generative surround composition. The spatial environment, filled with live tropical plants and a floor-level projection layer featuring real-time generative AI visuals, invites participants into a sensory field shaped by nonhuman cognition. By grounding abstract data in living materials and embodied experience, Simulacra Naturae reimagines visualization as a practice of care, one that decentralizes human agency and opens new spaces for ethics, empathy, and ecological attunement within hybrid computational systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-04</td>
<td style='padding: 8px;'>Data-driven mean-field within whole-brain models</td>
<td style='padding: 6px;'>Martin Breyton, Viktor Sip, Marmaduke Woodman, Meysam Hashemi, Spase Petkoski, Viktor Jirsa</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.02799v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Mean-field models provide a link between microscopic neuronal activity and macroscopic brain dynamics. Their derivation depends on simplifying assumptions, such as all-to-all connectivity, limiting their biological realism. To overcome this, we introduce a data-driven framework in which a multi-layer perceptron (MLP) learns the macroscopic dynamics directly from simulations of a network of spiking neurons. The network connection probability serves here as a new parameter, inaccessible to purely analytical treatment, which is validated against ground truth analytical solutions. Through bifurcation analysis on the trained MLP, we demonstrate the existence of new cusp bifurcation that systematically reshapes the system's phase diagram in a degenerate manner with synaptic coupling. By integrating this data-driven mean-field model into a whole-brain computational framework, we show that it extends beyond the macroscopic emergent dynamics generated by the analytical model. For validation, we use simulation-based inference on synthetic functional magnetic resonance imaging (fMRI) data and demonstrate accurate parameter recovery for the novel mean-field model, while the current state-of-the-art models lead to biased estimates. This work presents a flexible and generic framework for building more realistic whole-brain models, bridging the gap between microscale mechanisms and macroscopic brain recordings.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-02</td>
<td style='padding: 8px;'>Mix-modal Federated Learning for MRI Image Segmentation</td>
<td style='padding: 6px;'>Guyue Hu, Siyuan Song, Jingpeng Sun, Zhe Jin, Chenglong Li, Jin Tang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.02541v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Magnetic resonance imaging (MRI) image segmentation is crucial in diagnosing and treating many diseases, such as brain tumors. Existing MRI image segmentation methods mainly fall into a centralized multimodal paradigm, which is inapplicable in engineering non-centralized mix-modal medical scenarios. In this situation, each distributed client (hospital) processes multiple mixed MRI modalities, and the modality set and image data for each client are diverse, suffering from extensive client-wise modality heterogeneity and data heterogeneity. In this paper, we first formulate non-centralized mix-modal MRI image segmentation as a new paradigm for federated learning (FL) that involves multiple modalities, called mix-modal federated learning (MixMFL). It distinguishes from existing multimodal federating learning (MulMFL) and cross-modal federating learning (CroMFL) paradigms. Then, we proposed a novel modality decoupling and memorizing mix-modal federated learning framework (MDM-MixMFL) for MRI image segmentation, which is characterized by a modality decoupling strategy and a modality memorizing mechanism. Specifically, the modality decoupling strategy disentangles each modality into modality-tailored and modality-shared information. During mix-modal federated updating, corresponding modality encoders undergo tailored and shared updating, respectively. It facilitates stable and adaptive federating aggregation of heterogeneous data and modalities from distributed clients. Besides, the modality memorizing mechanism stores client-shared modality prototypes dynamically refreshed from every modality-tailored encoder to compensate for incomplete modalities in each local client. It further benefits modality aggregation and fusion processes during mixmodal federated learning. Extensive experiments on two public datasets for MRI image segmentation demonstrate the effectiveness and superiority of our methods.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-02</td>
<td style='padding: 8px;'>OmniActor: A Generalist GUI and Embodied Agent for 2D&3D Worlds</td>
<td style='padding: 6px;'>Longrong Yang, Zhixiong Zeng, Yufeng Zhong, Jing Huang, Liming Zheng, Lei Chen, Haibo Qiu, Zequn Qin, Lin Ma, Xi Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.02322v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Multimodal large language models are evolving toward multimodal agents capable of proactively executing tasks. Most agent research focuses on GUI or embodied scenarios, which correspond to agents interacting with 2D virtual worlds or 3D real worlds, respectively. However, many complex tasks typically require agents to interleavely interact with these two types of environment. We initially mix GUI and embodied data to train, but find the performance degeneration brought by the data conflict. Further analysis reveals that GUI and embodied data exhibit synergy and conflict at the shallow and deep layers, respectively, which resembles the cerebrum-cerebellum mechanism in the human brain. To this end, we propose a high-performance generalist agent OmniActor, designed from both structural and data perspectives. First, we propose Layer-heterogeneity MoE to eliminate the conflict between GUI and embodied data by separating deep-layer parameters, while leverage their synergy by sharing shallow-layer parameters. By successfully leveraging the synergy and eliminating the conflict, OmniActor outperforms agents only trained by GUI or embodied data in GUI or embodied tasks. Furthermore, we unify the action spaces of GUI and embodied tasks, and collect large-scale GUI and embodied data from various sources for training. This significantly improves OmniActor under different scenarios, especially in GUI tasks. The code will be publicly available.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Decoding Visual Neural Representations by Multimodal with Dynamic Balancing</td>
<td style='padding: 6px;'>Kaili sun, Xingyu Miao, Bing Zhai, Haoran Duan, Yang Long</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03433v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In this work, we propose an innovative framework that integrates EEG, image, and text data, aiming to decode visual neural representations from low signal-to-noise ratio EEG signals. Specifically, we introduce text modality to enhance the semantic correspondence between EEG signals and visual content. With the explicit semantic labels provided by text, image and EEG features of the same category can be more closely aligned with the corresponding text representations in a shared multimodal space. To fully utilize pre-trained visual and textual representations, we propose an adapter module that alleviates the instability of high-dimensional representation while facilitating the alignment and fusion of cross-modal features. Additionally, to alleviate the imbalance in multimodal feature contributions introduced by the textual representations, we propose a Modal Consistency Dynamic Balance (MCDB) strategy that dynamically adjusts the contribution weights of each modality. We further propose a stochastic perturbation regularization (SPR) term to enhance the generalization ability of semantic perturbation-based models by introducing dynamic Gaussian noise in the modality optimization process. The evaluation results on the ThingsEEG dataset show that our method surpasses previous state-of-the-art methods in both Top-1 and Top-5 accuracy metrics, improving by 2.0\% and 4.7\% respectively.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Handwriting Imagery EEG Classification based on Convolutional Neural Networks</td>
<td style='padding: 6px;'>Hao Yang, Guang Ouyang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03111v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Handwriting imagery has emerged as a promising paradigm for brain-computer interfaces (BCIs) aimed at translating brain activity into text output. Compared with invasively recorded electroencephalography (EEG), non-invasive recording offers a more practical and feasible approach to capturing brain signals for BCI. This study explores the limit of decoding non-invasive EEG associated with handwriting imagery into English letters using deep neural networks. To this end, five participants were instructed to imagine writing the 26 English letters with their EEG being recorded from the scalp. A measurement of EEG similarity across letters was conducted to investigate letter-specific patterns in the dataset. Subsequently, four convolutional neural network (CNN) models were trained for EEG classification. Descriptively, the EEG data clearly exhibited letter-specific patterns serving as a proof-of-concept for EEG-to-text translation. Under the chance level of accuracy at 3.85%, the CNN classifiers trained on each participant reached the highest limit of around 20%. This study marks the first attempt to decode non-invasive EEG associated with handwriting imagery. Although the achieved accuracy is not sufficient for a usable brain-to-text BCI, the model's performance is noteworthy in revealing the potential for translating non-invasively recorded brain signals into text outputs and establishing a baseline for future research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails</td>
<td style='padding: 6px;'>Hritik Arasu, Faisal R Jahangiri</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.02982v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Sleep staging models often degrade when deployed on patients with unseen physiology or recording conditions. We propose a streaming, source-free test-time adaptation (TTA) recipe that combines entropy minimization (Tent) with Batch-Norm statistic refresh and two safety rails: an entropy gate to pause adaptation on uncertain windows and an EMA-based reset to reel back drift. On Sleep-EDF Expanded, using single-lead EEG (Fpz-Cz, 100 Hz, 30s epochs; R&K to AASM mapping), we show consistent gains over a frozen baseline at seconds-level latency and minimal memory, reporting per-stage metrics and Cohen's k. The method is model-agnostic, requires no source data or patient calibration, and is practical for on-device or bedside use.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-02</td>
<td style='padding: 8px;'>Mentality: A Mamba-based Approach towards Foundation Models for EEG</td>
<td style='padding: 6px;'>Saarang Panchavati, Corey Arnold, William Speier</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.02746v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This work explores the potential of foundation models, specifically a Mamba-based selective state space model, for enhancing EEG analysis in neurological disorder diagnosis. EEG, crucial for diagnosing conditions like epilepsy, presents significant challenges due to its noisy, high-dimensional, and nonlinear nature. Traditional machine learning methods have made advances in automating EEG analysis but often fail to capture its complex spatio-temporal dynamics. Recent advances in deep learning, particularly in sequence modeling, offer new avenues for creating more generalized and expressive models capable of handling such complexities. By training a Mamba-based model on a large dataset containing seizure and non-seizure EEG recordings through a self-supervised reconstruction task followed by a seizure detection task, we demonstrate the model's effectiveness, achieving an AUROC of 0.72 on a held-out test set. This approach marks a significant step toward developing large-scale, clinically applicable foundation models for EEG data analysis.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-02</td>
<td style='padding: 8px;'>Improving Electroencephalogram-Based Deception Detection in Concealed Information Test under Low Stimulus Heterogeneity</td>
<td style='padding: 6px;'>Suhye Kim, Jaehoon Cheon, Taehee Kim, Seok Chan Kim, Chang-Hwan Im</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.02234v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The concealed information test (CIT) is widely used for detecting deception in criminal investigations, primarily leveraging the P300 component of electroencephalogram (EEG) signals. However, the traditional bootstrapped amplitude difference (BAD) method struggles to accurately differentiate deceptive individuals from innocent ones when irrelevant stimuli carry familiarity or inherent meaning, thus limiting its practical applicability in real-world investigations. This study aimed to enhance the deception detection capability of the P300-based CIT, particularly under conditions of low stimulus heterogeneity. To closely simulate realistic investigative scenarios, we designed a realistic mock-crime setup in which participants were familiarized with all CIT stimuli except the target stimulus. EEG data acquired during CIT sessions were analyzed using the BAD method, machine learning algorithms, and deep learning (DL) methods (ShallowNet and EEGNet). Among these techniques, EEGNet demonstrated the highest deception detection accuracy at 86.67%, when employing our proposed data augmentation approach. Overall, DL methods could significantly improve the accuracy of deception detection under challenging conditions of low stimulus heterogeneity by effectively capturing subtle cognitive responses not accessible through handcrafted features. To the best of our knowledge, this is the first study that employed DL approaches for subject-independent deception classification using the CIT paradigm.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-01</td>
<td style='padding: 8px;'>Convolutional Monge Mapping between EEG Datasets to Support Independent Component Labeling</td>
<td style='padding: 6px;'>Austin Meek, Carlos H. Mendoza-Cardenas, Austin J. Brockmeier</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.01721v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>EEG recordings contain rich information about neural activity but are subject to artifacts, noise, and superficial differences due to sensors, amplifiers, and filtering. Independent component analysis and automatic labeling of independent components (ICs) enable artifact removal in EEG pipelines. Convolutional Monge Mapping Normalization (CMMN) is a recent tool used to achieve spectral conformity of EEG signals, which was shown to improve deep neural network approaches for sleep staging. Here we propose a novel extension of the CMMN method with two alternative approaches to computing the source reference spectrum the target signals are mapped to: (1) channel-averaged and $l_1$-normalized barycenter, and (2) a subject-to-subject mapping that finds the source subject with the closest spectrum to the target subject. Notably, our extension yields space-time separable filters that can be used to map between datasets with different numbers of EEG channels. We apply these filters in an IC classification task, and show significant improvement in recognizing brain versus non-brain ICs.   Clinical relevance - EEG recordings are used in the diagnosis and monitoring of multiple neuropathologies, including epilepsy and psychosis. While EEG analysis can benefit from automating artifact removal through independent component analysis and labeling, differences in recording equipment and context (the presence of noise from electrical wiring and other devices) may impact the performance of machine learning models, but these differences can be minimized by appropriate spectral normalization through filtering.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-01</td>
<td style='padding: 8px;'>Body Ownership Affects the Processing of Sensorimotor Contingencies in Virtual Reality</td>
<td style='padding: 6px;'>Evan G. Center, Matti Pouke, Alessandro Nardi, Lukas Gehrke, Klaus Gramann, Timo Ojala, Steven M. LaValle</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.01420v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Presence in virtual reality (VR), the subjective sense of "being there" in a virtual environment, is notoriously difficult to measure. Electroencephalography (EEG) may offer a promising, unobtrusive means of assessing a user's momentary state of presence. Unlike traditional questionnaires, EEG does not interrupt the experience or rely on users' retrospective self-reports, thereby avoiding interference with the very state it aims to capture. Previous research has attempted to quantify presence in virtual environments using event-related potentials (ERPs). We contend, however, that previous efforts have fallen short of fully realizing this goal, failing to either A) independently manipulate presence, B) validate their measure of presence against traditional techniques, C) adequately separate the constructs of presence and attention, and/or D) implement a realistic and immersive environment and task. We address these shortcomings in a preregistered ERP experiment in which participants play an engaging target shooting game in VR. ERPs are time-locked to the release of a ball from a sling. We induce breaks in presence (BIPs) by freezing the ball's release on a minority of trials. Embodiment is manipulated by allowing manual manipulation of the sling with a realistic avatar in one condition (embodied condition) and passive manipulation with only controllers in another (non-embodied condition). We support our predictions that the N2, the P3b, and the N400, are selectively sensitive towards specific components of these manipulations. The pattern of findings carries significant implications for theories of presence, which have been seldom addressed in previous ERP investigations on this topic.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-01</td>
<td style='padding: 8px;'>DynaMind: Reconstructing Dynamic Visual Scenes from EEG by Aligning Temporal Dynamics and Multimodal Semantics to Guided Diffusion</td>
<td style='padding: 6px;'>Junxiang Liu, Junming Lin, Jiangtong Li, Jie Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.01177v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Reconstruction dynamic visual scenes from electroencephalography (EEG) signals remains a primary challenge in brain decoding, limited by the low spatial resolution of EEG, a temporal mismatch between neural recordings and video dynamics, and the insufficient use of semantic information within brain activity. Therefore, existing methods often inadequately resolve both the dynamic coherence and the complex semantic context of the perceived visual stimuli. To overcome these limitations, we introduce DynaMind, a novel framework that reconstructs video by jointly modeling neural dynamics and semantic features via three core modules: a Regional-aware Semantic Mapper (RSM), a Temporal-aware Dynamic Aligner (TDA), and a Dual-Guidance Video Reconstructor (DGVR). The RSM first utilizes a regional-aware encoder to extract multimodal semantic features from EEG signals across distinct brain regions, aggregating them into a unified diffusion prior. In the mean time, the TDA generates a dynamic latent sequence, or blueprint, to enforce temporal consistency between the feature representations and the original neural recordings. Together, guided by the semantic diffusion prior, the DGVR translates the temporal-aware blueprint into a high-fidelity video reconstruction. On the SEED-DV dataset, DynaMind sets a new state-of-the-art (SOTA), boosting reconstructed video accuracies (video- and frame-based) by 12.5 and 10.3 percentage points, respectively. It also achieves a leap in pixel-level quality, showing exceptional visual fidelity and temporal coherence with a 9.4% SSIM improvement and a 19.7% FVMD reduction. This marks a critical advancement, bridging the gap between neural dynamics and high-fidelity visual semantics.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-01</td>
<td style='padding: 8px;'>MATL-DC: A Multi-domain Aggregation Transfer Learning Framework for EEG Emotion Recognition with Domain-Class Prototype under Unseen Targets</td>
<td style='padding: 6px;'>Guangli Li, Canbiao Wu, Zhehao Zhou, Na Tian, Zhen Liang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.01135v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Emotion recognition based on electroencephalography (EEG) signals is increasingly becoming a key research hotspot in affective Brain-Computer Interfaces (aBCIs). However, the current transfer learning model greatly depends on the source domain and target domain data, which hinder the practical application of emotion recognition. Therefore, we propose a Multi-domain Aggregation Transfer Learning framework for EEG emotion recognition with Domain-Class prototype under unseen targets (MATL-DC). We design the feature decoupling module to decouple class-invariant domain features from domain-invariant class features from shallow features. In the model training stage, the multi-domain aggregation mechanism aggregates the domain feature space to form a superdomain, which enhances the characteristics of emotional EEG signals. In each superdomain, we further extract the class prototype representation by class features. In addition, we adopt the pairwise learning strategy to transform the sample classification problem into the similarity problem between sample pairs, which effectively alleviates the influence of label noise. It is worth noting that the target domain is completely unseen during the training process. In the inference stage, we use the trained domain-class prototypes for inference, and then realize emotion recognition. We rigorously validate it on the publicly available databases (SEED, SEED-IV and SEED-V). The results show that the accuracy of MATL-DC model is 84.70\%, 68.11\% and 61.08\%, respectively. MATL-DC achieves comparable or even better performance than methods that rely on both source and target domains. The source code is available at https://github.com/WuCB-BCI/MATL-DC.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-01</td>
<td style='padding: 8px;'>IMU-Enhanced EEG Motion Artifact Removal with Fine-Tuned Large Brain Models</td>
<td style='padding: 6px;'>Yuhong Zhang, Xusheng Zhu, Yuchen Xu, ChiaEn Lu, Hsinyu Shih, Gert Cauwenberghs, Tzyy-Ping Jung</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.01073v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) is a non-invasive method for measuring brain activity with high temporal resolution; however, EEG signals often exhibit low signal-to-noise ratios because of contamination from physiological and environmental artifacts. One of the major challenges hindering the real-world deployment of brain-computer interfaces (BCIs) involves the frequent occurrence of motion-related EEG artifacts. Most prior studies on EEG motion artifact removal rely on single-modality approaches, such as Artifact Subspace Reconstruction (ASR) and Independent Component Analysis (ICA), without incorporating simultaneously recorded modalities like inertial measurement units (IMUs), which directly capture the extent and dynamics of motion. This work proposes a fine-tuned large brain model (LaBraM)-based correlation attention mapping method that leverages spatial channel relationships in IMU data to identify motion-related artifacts in EEG signals. The fine-tuned model contains approximately 9.2 million parameters and uses 5.9 hours of EEG and IMU recordings for training, just 0.2346\% of the 2500 hours used to train the base model. We compare our results against the established ASR-ICA benchmark across varying time scales and motion activities, showing that incorporating IMU reference signals significantly improves robustness under diverse motion scenarios.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Handwriting Imagery EEG Classification based on Convolutional Neural Networks</td>
<td style='padding: 6px;'>Hao Yang, Guang Ouyang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03111v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Handwriting imagery has emerged as a promising paradigm for brain-computer interfaces (BCIs) aimed at translating brain activity into text output. Compared with invasively recorded electroencephalography (EEG), non-invasive recording offers a more practical and feasible approach to capturing brain signals for BCI. This study explores the limit of decoding non-invasive EEG associated with handwriting imagery into English letters using deep neural networks. To this end, five participants were instructed to imagine writing the 26 English letters with their EEG being recorded from the scalp. A measurement of EEG similarity across letters was conducted to investigate letter-specific patterns in the dataset. Subsequently, four convolutional neural network (CNN) models were trained for EEG classification. Descriptively, the EEG data clearly exhibited letter-specific patterns serving as a proof-of-concept for EEG-to-text translation. Under the chance level of accuracy at 3.85%, the CNN classifiers trained on each participant reached the highest limit of around 20%. This study marks the first attempt to decode non-invasive EEG associated with handwriting imagery. Although the achieved accuracy is not sufficient for a usable brain-to-text BCI, the model's performance is noteworthy in revealing the potential for translating non-invasively recorded brain signals into text outputs and establishing a baseline for future research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-01</td>
<td style='padding: 8px;'>MATL-DC: A Multi-domain Aggregation Transfer Learning Framework for EEG Emotion Recognition with Domain-Class Prototype under Unseen Targets</td>
<td style='padding: 6px;'>Guangli Li, Canbiao Wu, Zhehao Zhou, Na Tian, Zhen Liang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.01135v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Emotion recognition based on electroencephalography (EEG) signals is increasingly becoming a key research hotspot in affective Brain-Computer Interfaces (aBCIs). However, the current transfer learning model greatly depends on the source domain and target domain data, which hinder the practical application of emotion recognition. Therefore, we propose a Multi-domain Aggregation Transfer Learning framework for EEG emotion recognition with Domain-Class prototype under unseen targets (MATL-DC). We design the feature decoupling module to decouple class-invariant domain features from domain-invariant class features from shallow features. In the model training stage, the multi-domain aggregation mechanism aggregates the domain feature space to form a superdomain, which enhances the characteristics of emotional EEG signals. In each superdomain, we further extract the class prototype representation by class features. In addition, we adopt the pairwise learning strategy to transform the sample classification problem into the similarity problem between sample pairs, which effectively alleviates the influence of label noise. It is worth noting that the target domain is completely unseen during the training process. In the inference stage, we use the trained domain-class prototypes for inference, and then realize emotion recognition. We rigorously validate it on the publicly available databases (SEED, SEED-IV and SEED-V). The results show that the accuracy of MATL-DC model is 84.70\%, 68.11\% and 61.08\%, respectively. MATL-DC achieves comparable or even better performance than methods that rely on both source and target domains. The source code is available at https://github.com/WuCB-BCI/MATL-DC.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-01</td>
<td style='padding: 8px;'>IMU-Enhanced EEG Motion Artifact Removal with Fine-Tuned Large Brain Models</td>
<td style='padding: 6px;'>Yuhong Zhang, Xusheng Zhu, Yuchen Xu, ChiaEn Lu, Hsinyu Shih, Gert Cauwenberghs, Tzyy-Ping Jung</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.01073v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) is a non-invasive method for measuring brain activity with high temporal resolution; however, EEG signals often exhibit low signal-to-noise ratios because of contamination from physiological and environmental artifacts. One of the major challenges hindering the real-world deployment of brain-computer interfaces (BCIs) involves the frequent occurrence of motion-related EEG artifacts. Most prior studies on EEG motion artifact removal rely on single-modality approaches, such as Artifact Subspace Reconstruction (ASR) and Independent Component Analysis (ICA), without incorporating simultaneously recorded modalities like inertial measurement units (IMUs), which directly capture the extent and dynamics of motion. This work proposes a fine-tuned large brain model (LaBraM)-based correlation attention mapping method that leverages spatial channel relationships in IMU data to identify motion-related artifacts in EEG signals. The fine-tuned model contains approximately 9.2 million parameters and uses 5.9 hours of EEG and IMU recordings for training, just 0.2346\% of the 2500 hours used to train the base model. We compare our results against the established ASR-ICA benchmark across varying time scales and motion activities, showing that incorporating IMU reference signals significantly improves robustness under diverse motion scenarios.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-31</td>
<td style='padding: 8px;'>PyNoetic: A modular python framework for no-code development of EEG brain-computer interfaces</td>
<td style='padding: 6px;'>Gursimran Singh, Aviral Chharia, Rahul Upadhyay, Vinay Kumar, Luca Longo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.00670v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG)-based Brain-Computer Interfaces (BCIs) have emerged as a transformative technology with applications spanning robotics, virtual reality, medicine, and rehabilitation. However, existing BCI frameworks face several limitations, including a lack of stage-wise flexibility essential for experimental research, steep learning curves for researchers without programming expertise, elevated costs due to reliance on proprietary software, and a lack of all-inclusive features leading to the use of multiple external tools affecting research outcomes. To address these challenges, we present PyNoetic, a modular BCI framework designed to cater to the diverse needs of BCI research. PyNoetic is one of the very few frameworks in Python that encompasses the entire BCI design pipeline, from stimulus presentation and data acquisition to channel selection, filtering, feature extraction, artifact removal, and finally simulation and visualization. Notably, PyNoetic introduces an intuitive and end-to-end GUI coupled with a unique pick-and-place configurable flowchart for no-code BCI design, making it accessible to researchers with minimal programming experience. For advanced users, it facilitates the seamless integration of custom functionalities and novel algorithms with minimal coding, ensuring adaptability at each design stage. PyNoetic also includes a rich array of analytical tools such as machine learning models, brain-connectivity indices, systematic testing functionalities via simulation, and evaluation methods of novel paradigms. PyNoetic's strengths lie in its versatility for both offline and real-time BCI development, which streamlines the design process, allowing researchers to focus on more intricate aspects of BCI development and thus accelerate their research endeavors. Project Website: https://neurodiag.github.io/PyNoetic</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-29</td>
<td style='padding: 8px;'>A Single Subject Machine Learning Based Classification of Motor Imagery EEGs</td>
<td style='padding: 6px;'>Dario Sanalitro, Marco Finocchiaro, Pasquale Memmolo, Emanuela Cutuli, Maide Bucolo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.21724v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Motor Imagery-Based Brain-Computer Interfaces (MI-BCIs) are systems that detect and interpret brain activity patterns linked to the mental visualization of movement, and then translate these into instructions for controlling external robotic or domotic devices. Such devices have the potential to be useful in a broad variety of applications. While implementing a system that would help individuals restore some freedom levels, the interpretation of (Electroencephalography) EEG data remains a complex and unsolved problem. In the literature, the classification of left and right imagined movements has been extensively studied. This study introduces a novel pipeline that makes use of machine learning techniques for classifying MI EEG data. The entire framework is capable of accurately categorizing left and imagined motions, as well as rest phases, for a set of 52 subjects who performed a MI task. We trained a within subject model on each individual subject. The methodology has been offline evaluated and compared to four studies that are currently the state-of-the-art regarding the specified dataset. The results show that our proposed framework could be used with MI-BCI systems in light of its failsafe classification performances, i.e. 99.5% in accuracy</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-27</td>
<td style='padding: 8px;'>Alljoined-1.6M: A Million-Trial EEG-Image Dataset for Evaluating Affordable Brain-Computer Interfaces</td>
<td style='padding: 6px;'>Jonathan Xu, Ugo Bruzadin Nunes, Wangshu Jiang, Samuel Ryther, Jordan Pringle, Paul S. Scotti, Arnaud Delorme, Reese Kneeland</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.18571v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We present a new large-scale electroencephalography (EEG) dataset as part of the THINGS initiative, comprising over 1.6 million visual stimulus trials collected from 20 participants, and totaling more than twice the size of the most popular current benchmark dataset, THINGS-EEG2. Crucially, our data was recorded using a 32-channel consumer-grade wet electrode system costing ~$2.2k, around 27x cheaper than research-grade EEG systems typically used in cognitive neuroscience labs. Our work is one of the first open-source, large-scale EEG resource designed to closely reflect the quality of hardware that is practical to deploy in real-world, downstream applications of brain-computer interfaces (BCIs). We aim to explore the specific question of whether deep neural network-based BCI research and semantic decoding methods can be effectively conducted with such affordable systems, filling an important gap in current literature that is extremely relevant for future research. In our analysis, we not only demonstrate that decoding of high-level semantic information from EEG of visualized images is possible at consumer-grade hardware, but also that our data can facilitate effective EEG-to-Image reconstruction even despite significantly lower signal-to-noise ratios. In addition to traditional benchmarks, we also conduct analyses of EEG-to-Image models that demonstrate log-linear decoding performance with increasing data volume on our data, and discuss the trade-offs between hardware cost, signal fidelity, and the scale of data collection efforts in increasing the size and utility of currently available datasets. Our contributions aim to pave the way for large-scale, cost-effective EEG research with widely accessible equipment, and position our dataset as a unique resource for the democratization and development of effective deep neural models of visual cognition.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-22</td>
<td style='padding: 8px;'>EEG Study of the Influence of Imagined Temperature Sensations on Neuronal Activity in the Sensorimotor Cortex</td>
<td style='padding: 6px;'>Anton Belichenko, Daria Trinitatova, Aigul Nasibullina, Lev Yakovlev, Dzmitry Tsetserukou</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.16274v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding the neural correlates of sensory imagery is crucial for advancing cognitive neuroscience and developing novel Brain-Computer Interface (BCI) paradigms. This study investigated the influence of imagined temperature sensations (ITS) on neural activity within the sensorimotor cortex. The experimental study involved the evaluation of neural activity using electroencephalography (EEG) during both real thermal stimulation (TS: 40{\deg}C Hot, 20{\deg}C Cold) applied to the participants' hand, and the mental temperature imagination (ITS) of the corresponding hot and cold sensations. The analysis focused on quantifying the event-related desynchronization (ERD) of the sensorimotor mu-rhythm (8-13 Hz). The experimental results revealed a characteristic mu-ERD localized over central scalp regions (e.g., C3) during both TS and ITS conditions. Although the magnitude of mu-ERD during ITS was slightly lower than during TS, this difference was not statistically significant (p>.05). However, ERD during both ITS and TS was statistically significantly different from the resting baseline (p<.001). These findings demonstrate that imagining temperature sensations engages sensorimotor cortical mechanisms in a manner comparable to actual thermal perception. This insight expands our understanding of the neurophysiological basis of sensory imagery and suggests the potential utility of ITS for non-motor BCI control and neurorehabilitation technologies.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-22</td>
<td style='padding: 8px;'>Motor Imagery EEG Signal Classification Using Minimally Random Convolutional Kernel Transform and Hybrid Deep Learning</td>
<td style='padding: 6px;'>Jamal Hwaidi, Mohamed Chahine Ghanem</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.16179v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The brain-computer interface (BCI) establishes a non-muscle channel that enables direct communication between the human body and an external device. Electroencephalography (EEG) is a popular non-invasive technique for recording brain signals. It is critical to process and comprehend the hidden patterns linked to a specific cognitive or motor task, for instance, measured through the motor imagery brain-computer interface (MI-BCI). A significant challenge is presented by classifying motor imagery-based electroencephalogram (MI-EEG) tasks, given that EEG signals exhibit nonstationarity, time-variance, and individual diversity. Obtaining good classification accuracy is also very difficult due to the growing number of classes and the natural variability among individuals. To overcome these issues, this paper proposes a novel method for classifying EEG motor imagery signals that extracts features efficiently with Minimally Random Convolutional Kernel Transform (MiniRocket), a linear classifier then uses the extracted features for activity recognition. Furthermore, a novel deep learning based on Convolutional Neural Network (CNN) and Long Short Term Memory (LSTM) architecture to serve as a baseline was proposed and demonstrated that classification via MiniRocket's features achieves higher performance than the best deep learning models at lower computational cost. The PhysioNet dataset was used to evaluate the performance of the proposed approaches. The proposed models achieved mean accuracy values of 98.63% and 98.06% for the MiniRocket and CNN-LSTM, respectively. The findings demonstrate that the proposed approach can significantly enhance motor imagery EEG accuracy and provide new insights into the feature extraction and classification of MI-EEG.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-20</td>
<td style='padding: 8px;'>Detecting Reading-Induced Confusion Using EEG and Eye Tracking</td>
<td style='padding: 6px;'>Haojun Zhuang, D√ºnya Baradari, Nataliya Kosmyna, Arnav Balyan, Constanze Albrecht, Stephanie Chen, Pattie Maes</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.14442v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Humans regularly navigate an overwhelming amount of information via text media, whether reading articles, browsing social media, or interacting with chatbots. Confusion naturally arises when new information conflicts with or exceeds a reader's comprehension or prior knowledge, posing a challenge for learning. In this study, we present a multimodal investigation of reading-induced confusion using EEG and eye tracking. We collected neural and gaze data from 11 adult participants as they read short paragraphs sampled from diverse, real-world sources. By isolating the N400 event-related potential (ERP), a well-established neural marker of semantic incongruence, and integrating behavioral markers from eye tracking, we provide a detailed analysis of the neural and behavioral correlates of confusion during naturalistic reading. Using machine learning, we show that multimodal (EEG + eye tracking) models improve classification accuracy by 4-22% over unimodal baselines, reaching an average weighted participant accuracy of 77.3% and a best accuracy of 89.6%. Our results highlight the dominance of the brain's temporal regions in these neural signatures of confusion, suggesting avenues for wearable, low-electrode brain-computer interfaces (BCI) for real-time monitoring. These findings lay the foundation for developing adaptive systems that dynamically detect and respond to user confusion, with potential applications in personalized learning, human-computer interaction, and accessibility.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-18</td>
<td style='padding: 8px;'>Cyber Risks to Next-Gen Brain-Computer Interfaces: Analysis and Recommendations</td>
<td style='padding: 6px;'>Tyler Schroder, Renee Sirbu, Sohee Park, Jessica Morley, Sam Street, Luciano Floridi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.12571v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-computer interfaces (BCIs) show enormous potential for advancing personalized medicine. However, BCIs also introduce new avenues for cyber-attacks or security compromises. In this article, we analyze the problem and make recommendations for device manufacturers to better secure devices and to help regulators understand where more guidance is needed to protect patient safety and data confidentiality. Device manufacturers should implement the prior suggestions in their BCI products. These recommendations help protect BCI users from undue risks, including compromised personal health and genetic information, unintended BCI-mediated movement, and many other cybersecurity breaches. Regulators should mandate non-surgical device update methods, strong authentication and authorization schemes for BCI software modifications, encryption of data moving to and from the brain, and minimize network connectivity where possible. We also design a hypothetical, average-case threat model that identifies possible cybersecurity threats to BCI patients and predicts the likeliness of risk for each category of threat. BCIs are at less risk of physical compromise or attack, but are vulnerable to remote attack; we focus on possible threats via network paths to BCIs and suggest technical controls to limit network connections.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>RTGMFF: Enhanced fMRI-based Brain Disorder Diagnosis via ROI-driven Text Generation and Multimodal Feature Fusion</td>
<td style='padding: 6px;'>Junhao Jia, Yifei Sun, Yunyou Liu, Cheng Yang, Changmiao Wang, Feiwei Qin, Yong Peng, Wenwen Min</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03214v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional magnetic resonance imaging (fMRI) is a powerful tool for probing brain function, yet reliable clinical diagnosis is hampered by low signal-to-noise ratios, inter-subject variability, and the limited frequency awareness of prevailing CNN- and Transformer-based models. Moreover, most fMRI datasets lack textual annotations that could contextualize regional activation and connectivity patterns. We introduce RTGMFF, a framework that unifies automatic ROI-level text generation with multimodal feature fusion for brain-disorder diagnosis. RTGMFF consists of three components: (i) ROI-driven fMRI text generation deterministically condenses each subject's activation, connectivity, age, and sex into reproducible text tokens; (ii) Hybrid frequency-spatial encoder fuses a hierarchical wavelet-mamba branch with a cross-scale Transformer encoder to capture frequency-domain structure alongside long-range spatial dependencies; and (iii) Adaptive semantic alignment module embeds the ROI token sequence and visual features in a shared space, using a regularized cosine-similarity loss to narrow the modality gap. Extensive experiments on the ADHD-200 and ABIDE benchmarks show that RTGMFF surpasses current methods in diagnostic accuracy, achieving notable gains in sensitivity, specificity, and area under the ROC curve. Code is available at https://github.com/BeistMedAI/RTGMFF.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-04</td>
<td style='padding: 8px;'>Data-driven mean-field within whole-brain models</td>
<td style='padding: 6px;'>Martin Breyton, Viktor Sip, Marmaduke Woodman, Meysam Hashemi, Spase Petkoski, Viktor Jirsa</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.02799v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Mean-field models provide a link between microscopic neuronal activity and macroscopic brain dynamics. Their derivation depends on simplifying assumptions, such as all-to-all connectivity, limiting their biological realism. To overcome this, we introduce a data-driven framework in which a multi-layer perceptron (MLP) learns the macroscopic dynamics directly from simulations of a network of spiking neurons. The network connection probability serves here as a new parameter, inaccessible to purely analytical treatment, which is validated against ground truth analytical solutions. Through bifurcation analysis on the trained MLP, we demonstrate the existence of new cusp bifurcation that systematically reshapes the system's phase diagram in a degenerate manner with synaptic coupling. By integrating this data-driven mean-field model into a whole-brain computational framework, we show that it extends beyond the macroscopic emergent dynamics generated by the analytical model. For validation, we use simulation-based inference on synthetic functional magnetic resonance imaging (fMRI) data and demonstrate accurate parameter recovery for the novel mean-field model, while the current state-of-the-art models lead to biased estimates. This work presents a flexible and generic framework for building more realistic whole-brain models, bridging the gap between microscale mechanisms and macroscopic brain recordings.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-01</td>
<td style='padding: 8px;'>DCA: Graph-Guided Deep Embedding Clustering for Brain Atlases</td>
<td style='padding: 6px;'>Mo Wang, Kaining Peng, Jingsheng Tang, Hongkai Wen, Quanying Liu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.01426v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain atlases are essential for reducing the dimensionality of neuroimaging data and enabling interpretable analysis. However, most existing atlases are predefined, group-level templates with limited flexibility and resolution. We present Deep Cluster Atlas (DCA), a graph-guided deep embedding clustering framework for generating individualized, voxel-wise brain parcellations. DCA combines a pretrained autoencoder with spatially regularized deep clustering to produce functionally coherent and spatially contiguous regions. Our method supports flexible control over resolution and anatomical scope, and generalizes to arbitrary brain structures. We further introduce a standardized benchmarking platform for atlas evaluation, using multiple large-scale fMRI datasets. Across multiple datasets and scales, DCA outperforms state-of-the-art atlases, improving functional homogeneity by 98.8\% and silhouette coefficient by 29\%, and achieves superior performance in downstream tasks such as autism diagnosis and cognitive decoding. Codes and models will be released soon.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-31</td>
<td style='padding: 8px;'>Resting-state fMRI Analysis using Quantum Time-series Transformer</td>
<td style='padding: 6px;'>Junghoon Justin Park, Jungwoo Seo, Sangyoon Bae, Samuel Yen-Chi Chen, Huan-Hsin Tseng, Jiook Cha, Shinjae Yoo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.00711v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Resting-state functional magnetic resonance imaging (fMRI) has emerged as a pivotal tool for revealing intrinsic brain network connectivity and identifying neural biomarkers of neuropsychiatric conditions. However, classical self-attention transformer models--despite their formidable representational power--struggle with quadratic complexity, large parameter counts, and substantial data requirements. To address these barriers, we introduce a Quantum Time-series Transformer, a novel quantum-enhanced transformer architecture leveraging Linear Combination of Unitaries and Quantum Singular Value Transformation. Unlike classical transformers, Quantum Time-series Transformer operates with polylogarithmic computational complexity, markedly reducing training overhead and enabling robust performance even with fewer parameters and limited sample sizes. Empirical evaluation on the largest-scale fMRI datasets from the Adolescent Brain Cognitive Development Study and the UK Biobank demonstrates that Quantum Time-series Transformer achieves comparable or superior predictive performance compared to state-of-the-art classical transformer models, with especially pronounced gains in small-sample scenarios. Interpretability analyses using SHapley Additive exPlanations further reveal that Quantum Time-series Transformer reliably identifies clinically meaningful neural biomarkers of attention-deficit/hyperactivity disorder (ADHD). These findings underscore the promise of quantum-enhanced transformers in advancing computational neuroscience by more efficiently modeling complex spatio-temporal dynamics and improving clinical interpretability.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-25</td>
<td style='padding: 8px;'>Disentangling the Factors of Convergence between Brains and Computer Vision Models</td>
<td style='padding: 6px;'>Jos√©phine Raugel, Marc Szafraniec, Huy V. Vo, Camille Couprie, Patrick Labatut, Piotr Bojanowski, Valentin Wyart, Jean-R√©mi King</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.18226v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Many AI models trained on natural images develop representations that resemble those of the human brain. However, the factors that drive this brain-model similarity remain poorly understood. To disentangle how the model, training and data independently lead a neural network to develop brain-like representations, we trained a family of self-supervised vision transformers (DINOv3) that systematically varied these different factors. We compare their representations of images to those of the human brain recorded with both fMRI and MEG, providing high resolution in spatial and temporal analyses. We assess the brain-model similarity with three complementary metrics focusing on overall representational similarity, topographical organization, and temporal dynamics. We show that all three factors - model size, training amount, and image type - independently and interactively impact each of these brain similarity metrics. In particular, the largest DINOv3 models trained with the most human-centric images reach the highest brain-similarity. This emergence of brain-like representations in AI models follows a specific chronology during training: models first align with the early representations of the sensory cortices, and only align with the late and prefrontal representations of the brain with considerably more training. Finally, this developmental trajectory is indexed by both structural and functional properties of the human cortex: the representations that are acquired last by the models specifically align with the cortical areas with the largest developmental expansion, thickness, least myelination, and slowest timescales. Overall, these findings disentangle the interplay between architecture and experience in shaping how artificial neural networks come to see the world as humans do, thus offering a promising framework to understand how the human brain comes to represent its visual world.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-20</td>
<td style='padding: 8px;'>The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models</td>
<td style='padding: 6px;'>Hend Al-Khalifa, Raneem Almansour, Layan Abdulrahman Alhuasini, Alanood Alsaleh, Mohamad-Hani Temsah, Mohamad-Hani_Temsah, Ashwag Rafea S Alruwaili</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.14869v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Prompt engineering has rapidly emerged as a critical skill for effective interaction with large language models (LLMs). However, the cognitive and neural underpinnings of this expertise remain largely unexplored. This paper presents findings from a cross-sectional pilot fMRI study investigating differences in brain functional connectivity and network activity between experts and intermediate prompt engineers. Our results reveal distinct neural signatures associated with higher prompt engineering literacy, including increased functional connectivity in brain regions such as the left middle temporal gyrus and the left frontal pole, as well as altered power-frequency dynamics in key cognitive networks. These findings offer initial insights into the neurobiological basis of prompt engineering proficiency. We discuss the implications of these neurocognitive markers in Natural Language Processing (NLP). Understanding the neural basis of human expertise in interacting with LLMs can inform the design of more intuitive human-AI interfaces, contribute to cognitive models of LLM interaction, and potentially guide the development of AI systems that better align with human cognitive workflows. This interdisciplinary approach aims to bridge the gap between human cognition and machine intelligence, fostering a deeper understanding of how humans learn and adapt to complex AI systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-19</td>
<td style='padding: 8px;'>ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery</td>
<td style='padding: 6px;'>Mohammad Izadi, Mehran Safayani</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.14005v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition marked by disruptions in brain connectivity. Functional MRI (fMRI) offers a non-invasive window into large-scale neural dynamics by measuring blood-oxygen-level-dependent (BOLD) signals across the brain. These signals can be modeled as interactions among Regions of Interest (ROIs), which are grouped into functional communities based on their underlying roles in brain function. Emerging evidence suggests that connectivity patterns within and between these communities are particularly sensitive to ASD-related alterations. Effectively capturing these patterns and identifying interactions that deviate from typical development is essential for improving ASD diagnosis and enabling biomarker discovery. In this work, we introduce ASDFormer, a Transformer-based architecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to capture neural signatures associated with ASD. By integrating multiple specialized expert branches with attention mechanisms, ASDFormer adaptively emphasizes different brain regions and connectivity patterns relevant to autism. This enables both improved classification performance and more interpretable identification of disorder-related biomarkers. Applied to the ABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and reveals robust insights into functional connectivity disruptions linked to ASD, highlighting its potential as a tool for biomarker discovery.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-18</td>
<td style='padding: 8px;'>A Dual-Attention Graph Network for fMRI Data Classification</td>
<td style='padding: 6px;'>Amirali Arbab, Zeinab Davarani, Mehran Safayani</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.13328v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding the complex neural activity dynamics is crucial for the development of the field of neuroscience. Although current functional MRI classification approaches tend to be based on static functional connectivity or cannot capture spatio-temporal relationships comprehensively, we present a new framework that leverages dynamic graph creation and spatiotemporal attention mechanisms for Autism Spectrum Disorder(ASD) diagnosis. The approach used in this research dynamically infers functional brain connectivity in each time interval using transformer-based attention mechanisms, enabling the model to selectively focus on crucial brain regions and time segments. By constructing time-varying graphs that are then processed with Graph Convolutional Networks (GCNs) and transformers, our method successfully captures both localized interactions and global temporal dependencies. Evaluated on the subset of ABIDE dataset, our model achieves 63.2 accuracy and 60.0 AUC, outperforming static graph-based approaches (e.g., GCN:51.8). This validates the efficacy of joint modeling of dynamic connectivity and spatio-temporal context for fMRI classification. The core novelty arises from (1) attention-driven dynamic graph creation that learns temporal brain region interactions and (2) hierarchical spatio-temporal feature fusion through GCNtransformer fusion.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-17</td>
<td style='padding: 8px;'>Defining and Benchmarking a Data-Centric Design Space for Brain Graph Construction</td>
<td style='padding: 6px;'>Qinwen Ge, Roza G. Bayrak, Anwar Said, Catie Chang, Xenofon Koutsoukos, Tyler Derr</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.12533v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The construction of brain graphs from functional Magnetic Resonance Imaging (fMRI) data plays a crucial role in enabling graph machine learning for neuroimaging. However, current practices often rely on rigid pipelines that overlook critical data-centric choices in how brain graphs are constructed. In this work, we adopt a Data-Centric AI perspective and systematically define and benchmark a data-centric design space for brain graph construction, constrasting with primarily model-centric prior work. We organize this design space into three stages: temporal signal processing, topology extraction, and graph featurization. Our contributions lie less in novel components and more in evaluating how combinations of existing and modified techniques influence downstream performance. Specifically, we study high-amplitude BOLD signal filtering, sparsification and unification strategies for connectivity, alternative correlation metrics, and multi-view node and edge features, such as incorporating lagged dynamics. Experiments on the HCP1200 and ABIDE datasets show that thoughtful data-centric configurations consistently improve classification accuracy over standard pipelines. These findings highlight the critical role of upstream data decisions and underscore the importance of systematically exploring the data-centric design space for graph-based neuroimaging. Our code is available at https://github.com/GeQinwen/DataCentricBrainGraphs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-16</td>
<td style='padding: 8px;'>A Wavelet-Based Framework for Mapping Long Memory in Resting-State fMRI: Age-Related Changes in the Hippocampus from the ADHD-200 Dataset</td>
<td style='padding: 6px;'>Yasaman Shahhosseini, C√©dric Beaulac, Farouk S. Nathoo, Michelle F. Miranda</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.11920v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Functional magnetic resonance imaging (fMRI) time series are known to exhibit long-range temporal dependencies that challenge traditional modeling approaches. In this study, we propose a novel computational pipeline to characterize and interpret these dependencies using a long-memory (LM) framework, which captures the slow, power-law decay of autocorrelation in resting-state fMRI (rs-fMRI) signals. The pipeline involves voxelwise estimation of LM parameters via a wavelet-based Bayesian method, yielding spatial maps that reflect temporal dependence across the brain. These maps are then projected onto a lower-dimensional space via a composite basis and are then related to individual-level covariates through group-level regression. We applied this approach to the ADHD-200 dataset and found significant positive associations between age in children and the LM parameter in the hippocampus, after adjusting for ADHD symptom severity and medication status. These findings complement prior neuroimaging work by linking long-range temporal dependence to developmental changes in memory-related brain regions. Overall, the proposed methodology enables detailed mapping of intrinsic temporal dynamics in rs-fMRI and offers new insights into the relationship between functional signal memory and brain development.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Towards a 384-channel magnetoencephalography system based on optically pumped magnetometers</td>
<td style='padding: 6px;'>Holly Schofield, Ryan M. Hill, Lukas Rier, Ewan Kennett, Gonzalo Reina Rivero, Joseph Gibson, Ashley Tyler, Zoe Tanner, Frank Worcester, Tyler Hayward, James Osborne, Cody Doyle, Vishal Shah, Elena Boto, Niall Holmes, Matthew J. Brookes</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03107v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Magnetoencephalography using optically pumped magnetometers (OPM-MEG) is gaining significant traction as a neuroimaging tool, with the potential for improved performance and practicality compared to conventional instrumentation. However, OPM-MEG has so far lagged conventional-MEG in terms of the number of independent measurements of magnetic field that can be made across the scalp (i.e. the number of channels). This is important since increasing channel count offers improvements in sensitivity, spatial resolution and coverage. Unfortunately, increasing channel count also poses significant technical and practical challenges. Here, we describe a new OPM-MEG array which exploits triaxial sensors and integrated miniaturised electronic control units to measure MEG data from up to 384 channels. We also introduce a high-speed calibration method to ensure the that the fields measured by this array are high fidelity. The system was validated using a phantom, with results showing that dipole localisation accuracy is better than 1 mm, and correlation between the measured magnetic fields and a dipole model is >0.998. We then demonstrate utility in human MEG acquisition: via measurement of visual gamma oscillations we demonstrate the improvements in sensitivity that are afforded by high channel density, and via a moviewatching paradigm we quantify improvements in spatial resolution. In sum, we show the first OPM-MEG system with a channel count larger than that of typical conventional MEG devices. This represents a significant step on the path towards OPMs becoming the sensor of choice for MEG measurement.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-31</td>
<td style='padding: 8px;'>Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention Mechanisms for Visual Prostheses</td>
<td style='padding: 6px;'>Ganxi Xu, Jinyi Long, Jia Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.00787v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Visual prostheses have shown great potential in restoring vision for blind individuals. On the one hand, researchers have been continuously improving the brain decoding framework of visual prostheses by leveraging the powerful image generation capabilities of diffusion models. On the other hand, the brain encoding stage of visual prostheses struggles to generate brain signals with sufficient biological similarity. Although existing works have recognized this problem, the quality of predicted stimuli still remains a critical issue, as existing approaches typically lack supervised signals from real brain responses to validate the biological plausibility of predicted stimuli. To address this issue, we propose a novel image-to-brain framework based on denoising diffusion probabilistic models (DDPMs) enhanced with cross-attention mechanisms. Our framework consists of two key architectural components: a pre-trained CLIP visual encoder that extracts rich semantic representations from input images, and a cross-attention enhanced U-Net diffusion model that learns to reconstruct biologically plausible brain signals through iterative denoising. Unlike conventional generative models that rely on simple concatenation for conditioning, our cross-attention modules enable dynamic interaction between visual features and brain signal representations, facilitating fine-grained alignment during the generation process. We evaluate our framework on two multimodal datasets (THINGS-EEG2 and THINGS-MEG) to demonstrate its effectiveness in generating biologically plausible brain signals. Moreover, we visualize the training and test M/EEG topographies for all subjects on both datasets to intuitively demonstrate the intra-subject variations and inter-subject variations in M/EEG signals.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-29</td>
<td style='padding: 8px;'>Quantile Function-Based Models for Neuroimaging Classification Using Wasserstein Regression</td>
<td style='padding: 6px;'>Jie Li, Gary Green, Jian Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.21523v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We propose a novel quantile function-based approach for neuroimaging classification using Wasserstein-Fr\'echet regression, specifically applied to the detection of mild traumatic brain injury (mTBI) based on the MEG and MRI data. Conventional neuroimaging classification methods for mTBI detection typically extract summary statistics from brain signals across the different epochs, which may result in the loss of important distributional information, such as variance, skewness, kurtosis, etc. Our approach treats complete probability density functions of epoch space results as functional response variables within a Wasserstein-Fr\'echet regression framework, thereby preserving the full distributional characteristics of epoch results from $L_{1}$ minimum norm solutions. The global Wasserstein-Fr\'echet regression model incorporating covariates (age and gender) allows us to directly compare the distributional patterns between healthy control subjects and mTBI patients. The classification procedure computes Wasserstein distances between estimated quantile functions from control and patient groups, respectively. These distances are then used as the basis for diagnostic decisions. This framework offers a statistically principled approach to improving diagnostic accuracy in mTBI detection. In practical applications, the test accuracy on unseen data from Innovision IP's dataset achieves up to 98\%.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-25</td>
<td style='padding: 8px;'>Disentangling the Factors of Convergence between Brains and Computer Vision Models</td>
<td style='padding: 6px;'>Jos√©phine Raugel, Marc Szafraniec, Huy V. Vo, Camille Couprie, Patrick Labatut, Piotr Bojanowski, Valentin Wyart, Jean-R√©mi King</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.18226v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Many AI models trained on natural images develop representations that resemble those of the human brain. However, the factors that drive this brain-model similarity remain poorly understood. To disentangle how the model, training and data independently lead a neural network to develop brain-like representations, we trained a family of self-supervised vision transformers (DINOv3) that systematically varied these different factors. We compare their representations of images to those of the human brain recorded with both fMRI and MEG, providing high resolution in spatial and temporal analyses. We assess the brain-model similarity with three complementary metrics focusing on overall representational similarity, topographical organization, and temporal dynamics. We show that all three factors - model size, training amount, and image type - independently and interactively impact each of these brain similarity metrics. In particular, the largest DINOv3 models trained with the most human-centric images reach the highest brain-similarity. This emergence of brain-like representations in AI models follows a specific chronology during training: models first align with the early representations of the sensory cortices, and only align with the late and prefrontal representations of the brain with considerably more training. Finally, this developmental trajectory is indexed by both structural and functional properties of the human cortex: the representations that are acquired last by the models specifically align with the cortical areas with the largest developmental expansion, thickness, least myelination, and slowest timescales. Overall, these findings disentangle the interplay between architecture and experience in shaping how artificial neural networks come to see the world as humans do, thus offering a promising framework to understand how the human brain comes to represent its visual world.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-21</td>
<td style='padding: 8px;'>Probing $0ŒΩŒ≤Œ≤$ and $Œº\to eŒ≥$ via Fully Determined Dirac Mass Terms in LRSM with Double Seesaw</td>
<td style='padding: 6px;'>Pratik Adarsh, Rajrupa Banerjee, Purushottam Sahu, Utkarsh Patel, Sudhanwa Patra</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.15893v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neutrinoless double beta decay ($0\nu\beta\beta$) and charged lepton flavor violation (cLFV) experiments provide promising avenues to probe new physics contributions from extended neutrino sectors in beyond Standard Model (BSM) scenarios. We consider a Left--Right Symmetric Model (LRSM) extended with three generations of sterile neutrinos to realize a double type-I seesaw mechanism for light neutrino mass generation. The double seesaw induces maximal lepton number violation in the right-handed sector and facilitates enhanced Majorana masses for right-handed neutrinos, thereby leading to their dominant contributions in both cLFV and $0\nu\beta\beta$ processes. We perform a comprehensive exploration of the parameter space for new-physics contributions to the cLFV decay $\mu \to e \gamma$ and to $0\nu\beta\beta$, considering two different textures for the Dirac mass matrices: one proportional to the identity matrix and another fully determined by the model framework. A detailed analysis of the common parameter regions accessible to current experiments like KamLAND-Zen and LEGEND-200, and upcoming experiments, such as MEG-II and LEGEND-1000, is presented, underscoring the phenomenological relevance of this framework. Our results aim to provide optimistic benchmarks for future searches targeting right-handed current-mediated neutrino interactions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-08-19</td>
<td style='padding: 8px;'>Reduction of Electromagnetic Interference in ultra-low noise Bimodal MEG & EEG</td>
<td style='padding: 6px;'>Jim Barnes, Lukasz Radzinski, Soudabeh Arsalani, Gunnar Waterstraat, Gabriel Curio, Jens Haueisen, Rainer K√∂rber</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2508.13758v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Single-channel SQUID system technology, operating at a noise level of 100s of aT/$\sqrt{\textrm{Hz}}$, enables the non-invasive detection of synchronized spiking activity at the single-trial level via magnetoencephalography (MEG). However, when combined with simultaneous electroencephalography (EEG) recordings, the noise performance of the ultrasensitive MEG system can be greatly diminished. This issue negates some of the complementary qualities of these two recording methods. In addition, typical electrical components required for electrical stimulation of peripheral nerves, a common method for evoking specific brain responses, are also observed to have a detrimental influence on ultra-low MEG noise performance. These effects are caused by electromagnetic interference (EMI) and typically preclude single-trial detection. This work outlines, how careful design allows a significant reduction of the impact of EMI when these different electronic systems are operated concurrently. This optimization enabled the simultaneous single-trial detection of synchronized spiking activity using these two highly sensitive recording modalities.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-31</td>
<td style='padding: 8px;'>Wave Turbulence and Cortical Dynamics</td>
<td style='padding: 6px;'>Gerald Kaushallye Cooray</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.23525v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Cortical activity recorded through EEG and MEG reflects complex dynamics that span multiple temporal and spatial scales. Spectral analyses of these signals consistently reveal power-law behaviour, a hallmark of turbulent systems. In this paper, we derive a kinetic equation for neural field activity based on wave turbulence theory, highlighting how quantities such as energy and pseudo-particle density flow through wave-space (k-space) via direct and inverse cascades. We explore how different forms of nonlinearity, particularly 3-wave and 4-wave interactions, shape spectral features, including harmonic generation, spectral dispersion, and transient dynamics. While the observed power-law decays in empirical data are broadly consistent with turbulent cascades, variations across studies, such as the presence of dual decay rates or harmonic structures, point to a diversity of underlying mechanisms. We argue that although no single model fully explains all spectral observations, key constraints emerge: namely, that cortical dynamics exhibit features consistent with turbulent wave systems involving both single and dual cascades and a mixture of 3- and 4-wave interactions. This turbulence-based framework offers a principled and unifying approach to interpreting large-scale brain activity, including state transitions and seizure dynamics.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-29</td>
<td style='padding: 8px;'>Following the Committor Flow: A Data-Driven Discovery of Transition Pathways</td>
<td style='padding: 6px;'>Cheng Giuseppe Chen, Chenyu Tang, Alberto Meg√≠as, Radu A. Talmazan, Sergio Contreras Arredondo, Beno√Æt Roux, Christophe Chipot</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.21961v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The discovery of transition pathways to unravel distinct reaction mechanisms and, in general, rare events that occur in molecular systems is still a challenge. Recent advances have focused on analyzing the transition path ensemble using the committor probability, widely regarded as the most informative one-dimensional reaction coordinate. Consistency between transition pathways and the committor function is essential for accurate mechanistic insight. In this work, we propose an iterative framework to infer the committor and, subsequently, to identify the most relevant transition pathways. Starting from an initial guess for the transition path, we generate biased sampling from which we train a neural network to approximate the committor probability. From this learned committor, we extract dominant transition channels as discretized strings lying on isocommittor surfaces. These pathways are then used to enhance sampling and iteratively refine both the committor and the transition paths until convergence. The resulting committor enables accurate estimation of the reaction rate constant. We demonstrate the effectiveness of our approach on benchmark systems, including a two-dimensional model potential, peptide conformational transitions, and a Diels--Alder reaction.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-23</td>
<td style='padding: 8px;'>From Atoms to Dynamics: Learning the Committor Without Collective Variables</td>
<td style='padding: 6px;'>Sergio Contreras Arredondo, Chenyu Tang, Radu A. Talmazan, Alberto Meg√≠as, Cheng Giuseppe Chen, Christophe Chipot</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.17700v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This Brief Communication introduces a graph-neural-network architecture built on geometric vector perceptrons to predict the committor function directly from atomic coordinates, bypassing the need for hand-crafted collective variables (CVs). The method offers atom-level interpretability, pinpointing the key atomic players in complex transitions without relying on prior assumptions. Applied across diverse molecular systems, the method accurately infers the committor function and highlights the importance of each heavy atom in the transition mechanism. It also yields precise estimates of the rate constants for the underlying processes. The proposed approach opens new avenues for understanding and modeling complex dynamics, by enabling CV-free learning and automated identification of physically meaningful reaction coordinates of complex molecular processes.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-16</td>
<td style='padding: 8px;'>Diffusion-based translation between unpaired spontaneous premature neonatal EEG and fetal MEG</td>
<td style='padding: 6px;'>Beno√Æt Brebion, Alban Gallard, Katrin Sippel, Amer Zaylaa, Hubert Preissl, Sahar Moghimi, Fabrice Wallois, Ya√´l Fr√©gier</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.14224v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Background and objective: Brain activity in premature newborns has traditionally been studied using electroencephalography (EEG), leading to substantial advances in our understanding of early neural development. However, since brain development takes root at the fetal stage, a critical window of this process remains largely unknown. The only technique capable of recording neural activity in the intrauterine environment is fetal magnetoencephalography (fMEG), but this approach presents challenges in terms of data quality and scarcity. Using artificial intelligence, the present research aims to transfer the well-established knowledge from EEG studies to fMEG to improve understanding of prenatal brain development, laying the foundations for better detection and treatment of potential pathologies. Methods: We developed an unpaired diffusion translation method based on dual diffusion bridges, which notably includes numerical integration improvements to obtain more qualitative results at a lower computational cost. Models were trained on our unpaired dataset of bursts of spontaneous activity from 30 high-resolution premature newborns EEG recordings and 44 fMEG recordings. Results: We demonstrate that our method achieves significant improvement upon previous results obtained with Generative Adversarial Networks (GANs), by almost 5% on the mean squared error in the time domain, and completely eliminating the mode collapse problem in the frequency domain, thus achieving near-perfect signal fidelity. Conclusion: We set a new state of the art in the EEG-fMEG unpaired translation problem, as our developed tool completely paves the way for early brain activity analysis. Overall, we also believe that our method could be reused for other unpaired signal translation applications.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-09</td>
<td style='padding: 8px;'>Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers</td>
<td style='padding: 6px;'>Thomas Klein, Sascha Meyen, Wieland Brendel, Felix A. Wichmann, Kristof Meding</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.06645v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Benchmarking models is a key factor for the rapid progress in machine learning (ML) research. Thus, further progress depends on improving benchmarking metrics. A standard metric to measure the behavioral alignment between ML models and human observers is error consistency (EC). EC allows for more fine-grained comparisons of behavior than other metrics such as e.g. accuracy, and has been used in the influential Brain-Score benchmark to rank different DNNs by their behavioral consistency with humans. Previously, EC values have been reported without confidence intervals. However, empirically measured EC values are typically noisy -- thus, without confidence intervals, valid benchmarking conclusions are problematic. Here we improve on standard EC in two ways: First, we show how to obtain confidence intervals for EC using a bootstrapping technique, allowing us to derive significance tests for EC. Second, we propose a new computational model relating the EC between two classifiers to the implicit probability that one of them copies responses from the other. This view of EC allows us to give practical guidance to scientists regarding the number of trials required for sufficiently powerful, conclusive experiments. Finally, we use our methodology to revisit popular NeuroAI-results. We find that while the general trend of behavioral differences between humans and machines holds up to scrutiny, many reported differences between deep vision models are statistically insignificant. Our methodology enables researchers to design adequately powered experiments that can reliably detect behavioral differences between models, providing a foundation for more rigorous benchmarking of behavioral alignment.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-02</td>
<td style='padding: 8px;'>What Neuroscience Can Teach AI About Learning in Continuously Changing Environments</td>
<td style='padding: 6px;'>Daniel Durstewitz, Bruno Averbeck, Georgia Koppe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.02103v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-06-12</td>
<td style='padding: 8px;'>NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models</td>
<td style='padding: 6px;'>Luca Ghafourpour, Valentin Duruisseaux, Bahareh Tolooshams, Philip H. Wong, Costas A. Anastassiou, Anima Anandkumar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2506.04536v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Characterizing the diverse computational properties of human neurons via multimodal electrophysiological, transcriptomic, and morphological data provides the foundation for constructing and validating bio-realistic neuron models that can advance our understanding of fundamental mechanisms underlying brain function. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. To capture variability, ensembles of deterministic models are often used, but are difficult to scale as model generation requires repeating computationally expensive optimization for each neuron. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on data generated from biophysically realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE is the first scaled-up deep learning framework validated on real experimental data, enabling efficient generation of synthetic neurons that exhibit trial-to-trial variability and achieve a $4200\times$ speedup over numerical solvers. To this end, NOBLE captures fundamental neural properties, opening the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-21</td>
<td style='padding: 8px;'>SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation</td>
<td style='padding: 6px;'>Jiayue Liu, Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Kuo Yang, Xu Wang, Yang Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.16080v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-03-08</td>
<td style='padding: 8px;'>A 7T fMRI dataset of synthetic images for out-of-distribution modeling of vision</td>
<td style='padding: 6px;'>Alessandro T. Gifford, Radoslaw M. Cichy, Thomas Naselaris, Kendrick Kay</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2503.06286v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD) are boosting NeuroAI research by enabling computational models of the brain with performances beyond what was possible just a decade ago. However, these datasets lack out-of-distribution (OOD) components, which are crucial for the development of more robust models. Here, we address this limitation by releasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the eight NSD subjects for 284 carefully controlled synthetic images. We show that NSD-synthetic's fMRI responses reliably encode stimulus-related information and are OOD with respect to NSD. Furthermore, OOD generalization tests on NSD-synthetic reveal differences between models of the brain that are not detected with NSD - specifically, self-supervised deep neural networks better explain neural responses than their task-supervised counterparts. These results showcase how NSD-synthetic enables OOD generalization tests that facilitate the development of more robust models of visual processing, and the formulation of more accurate theories of human vision.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-02-22</td>
<td style='padding: 8px;'>Brain-Model Evaluations Need the NeuroAI Turing Test</td>
<td style='padding: 6px;'>Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2502.16238v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-04</td>
<td style='padding: 8px;'>Asynchronous Hebbian/anti-Hebbian networks</td>
<td style='padding: 6px;'>Henrique Reis Aguiar, Matthias H. Hennig</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02402v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-03</td>
<td style='padding: 8px;'>NeuroAI for AI Safety</td>
<td style='padding: 6px;'>Patrick Mineault, Niccol√≤ Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.18526v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-21</td>
<td style='padding: 8px;'>Evaluating Representational Similarity Measures from the Lens of Functional Correspondence</td>
<td style='padding: 6px;'>Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.14633v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-09</td>
<td style='padding: 8px;'>Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models</td>
<td style='padding: 6px;'>Emily Cheng, Richard J. Antonello</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.05771v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first "composition" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>From Image Denoisers to Regularizing Imaging Inverse Problems: An Overview</td>
<td style='padding: 6px;'>Hong Ye Tan, Subhadip Mukherjee, Junqi Tang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03475v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Inverse problems lie at the heart of modern imaging science, with broad applications in areas such as medical imaging, remote sensing, and microscopy. Recent years have witnessed a paradigm shift in solving imaging inverse problems, where data-driven regularizers are used increasingly, leading to remarkably high-fidelity reconstruction. A particularly notable approach for data-driven regularization is to use learned image denoisers as implicit priors in iterative image reconstruction algorithms. This survey presents a comprehensive overview of this powerful and emerging class of algorithms, commonly referred to as plug-and-play (PnP) methods. We begin by providing a brief background on image denoising and inverse problems, followed by a short review of traditional regularization strategies. We then explore how proximal splitting algorithms, such as the alternating direction method of multipliers (ADMM) and proximal gradient descent (PGD), can naturally accommodate learned denoisers in place of proximal operators, and under what conditions such replacements preserve convergence. The role of Tweedie's formula in connecting optimal Gaussian denoisers and score estimation is discussed, which lays the foundation for regularization-by-denoising (RED) and more recent diffusion-based posterior sampling methods. We discuss theoretical advances regarding the convergence of PnP algorithms, both within the RED and proximal settings, emphasizing the structural assumptions that the denoiser must satisfy for convergence, such as non-expansiveness, Lipschitz continuity, and local homogeneity. We also address practical considerations in algorithm design, including choices of denoiser architecture and acceleration strategies.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Cost-Optimized Systems Engineering for IoT-Enabled Robot Nurse in Infectious Pandemic Management</td>
<td style='padding: 6px;'>Md Mhamud Hussen Sifat, Md Maruf, Md Rokunuzzaman</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03436v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The utilization of robotic technology has gained traction in healthcare facilities due to progress in the field that enables time and cost savings, minimizes waste, and improves patient care. Digital healthcare technologies that leverage automation, such as robotics and artificial intelligence, have the potential to enhance the sustainability and profitability of healthcare systems in the long run. However, the recent COVID-19 pandemic has amplified the need for cyber-physical robots to automate check-ups and medication administration. A robot nurse is controlled by the Internet of Things (IoT) and can serve as an automated medical assistant while also allowing supervisory control based on custom commands. This system helps reduce infection risk and improves outcomes in pandemic settings. This research presents a test case with a nurse robot that can assess a patient's health status and take action accordingly. We also evaluate the system's performance in medication administration, health-status monitoring, and life-cycle considerations.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Generalist versus Specialist Vision Foundation Models for Ocular Disease and Oculomics</td>
<td style='padding: 6px;'>Yukun Zhou, Paul Nderitu, Jocelyn Hui Lin Goh, Justin Engelmann, Siegfried K. Wagner, Anran Ran, Hongyang Jiang, Lie Ju, Ke Zou, Sahana Srinivasan, Hyunmin Kim, Takahiro Ninomiya, Zheyuan Wang, Gabriel Dawei Yang, Eden Ruffell, Dominic Williamson, Rui Santos, Gabor Mark Somfai, Carol Y. Cheung, Tien Yin Wong, Daniel C. Alexander, Yih Chung Tham, Pearse A. Keane</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03421v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Medical foundation models, pre-trained with large-scale clinical data, demonstrate strong performance in diverse clinically relevant applications. RETFound, trained on nearly one million retinal images, exemplifies this approach in applications with retinal images. However, the emergence of increasingly powerful and multifold larger generalist foundation models such as DINOv2 and DINOv3 raises the question of whether domain-specific pre-training remains essential, and if so, what gap persists. To investigate this, we systematically evaluated the adaptability of DINOv2 and DINOv3 in retinal image applications, compared to two specialist RETFound models, RETFound-MAE and RETFound-DINOv2. We assessed performance on ocular disease detection and systemic disease prediction using two adaptation strategies: fine-tuning and linear probing. Data efficiency and adaptation efficiency were further analysed to characterise trade-offs between predictive performance and computational cost. Our results show that although scaling generalist models yields strong adaptability across diverse tasks, RETFound-DINOv2 consistently outperforms these generalist foundation models in ocular-disease detection and oculomics tasks, demonstrating stronger generalisability and data efficiency. These findings suggest that specialist retinal foundation models remain the most effective choice for clinical applications, while the narrowing gap with generalist foundation models suggests that continued data and model scaling can deliver domain-relevant gains and position them as strong foundations for future medical foundation models.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Image-Guided Surgery: Technology, Quality, Innovation, and Opportunities for Medical Physics</td>
<td style='padding: 6px;'>Jeffrey H. Siewerdsen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03420v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The science and clinical practice of medical physics has been integral to the advancement of radiology and radiation therapy for over a century. In parallel, advances in surgery - including intraoperative imaging, registration, and other technologies within the expertise of medical physicists - have advanced primarily in connection to other disciplines, such as biomedical engineering and computer science, and via somewhat distinct translational paths. This review article briefly traces the parallel and convergent evolution of such scientific, engineering, and clinical domains with an eye to a potentially broader, more impactful role of medical physics in research and clinical practice of surgery. A review of image-guided surgery technologies is offered, including intraoperative imaging, tracking / navigation, image registration, visualization, and surgical robotics across a spectrum of surgical applications. Trends and drivers for research and innovation are traced, including federal funding and academic-industry partnership, and some of the major challenges to achieving major clinical impact are described. Opportunities for medical physicists to expand expertise and contribute to the advancement of surgery in the decade ahead are outlined, including research and innovation, data science approaches, improving efficiency through operations research and optimization, improving patient safety, and bringing rigorous quality assurance to technologies and processes in the circle of care for surgery. Challenges abound but appear tractable, including domain knowledge, professional qualifications, and the need for investment and clinical partnership.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Machine Learning-Enhanced Colorimetric Sensing: Achieving over 5700-fold Accuracy Improvement via Full-Spectrum Modeling</td>
<td style='padding: 6px;'>Majid Aalizadeh, Chinmay Raut, Ali Tabartehfarahani, Xudong Fan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03398v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Conventional colorimetric sensing methods typically rely on signal intensity at a single wavelength, often selected heuristically based on peak visual modulation. This approach overlooks the structured information embedded in full-spectrum transmission profiles, particularly in intensity-based systems where linear models may be highly effective. In this study, we experimentally demonstrate that applying a forward feature selection strategy to normalized transmission spectra, combined with linear regression and ten-fold cross-validation, yields significant improvements in predictive accuracy. Using food dye dilutions as a model system, the mean squared error was reduced from over 22,000 with a single wavelength to 3.87 using twelve selected features, corresponding to a more than 5,700-fold enhancement. These results validate that full-spectrum modeling enables precise concentration prediction without requiring changes to the sensing hardware. The approach is broadly applicable to colorimetric assays used in medical diagnostics, environmental monitoring, and industrial analysis, offering a scalable pathway to improve sensitivity and reliability in existing platforms.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Advancements in Monte Carlo simulations with gMicroMC: reactive species build-up promotes radical-radical reactions at Flash dose rates</td>
<td style='padding: 6px;'>Miguel Molina-Hernandez, Patricia Gon√ßalves, Yujie Chi, Joao Seco</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03313v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Ultra-high dose rate irradiations to water indicate an enhancement of radical-radical reactions, which could potentially correlate with the Flash effect. The purpose of this work was to extend gMicroMC to support multiple pulse simulations and Flash dose rates, and to investigate, in a pure water model, the mechanisms underlying the enhancement of radical-radical reactions under Flash conditions. gMicroMC, a GPU-based Monte Carlo track-structure algorithm, was extended to simulate multiple pulses. Pure water was exposed to multiple 70 MeV protons pulses delivering up to 20 Gy. The pulse dose rate was set to 2*10^5 and 10^6 Gy/s, while the average dose rate ranged from 0.01 to 100000 Gy/s. The G-values of H2O2 were used to monitor the influence of dose rate on radical-radical reactions. The multiple pulse extension of gMicroMC was validated against Kinetiscope. Multiple pulse simulations indicated an average dose rate threshold. Below it, complete radical depletion occurred within the pulses, leading to constant G-values. Above it, reactive species accumulated throughout the irradiation, resulting in an increase of radical-radical reactions and thus the G-values of H2O2. The average dose rate thresholds were in the order of 10 and 100 Gy/s for pulse dose rates of 2*10^5 and 10^6 Gy/s, respectively. At ultra-high dose rates, the brief intervals between pulses led to a reactive species build-up, which enhanced radical-radical reactions. This build-up is more likely to promote radical-radical reactions than the inter-track mechanism. The advancements in gMicroMC provide a sophisticated tool to study chemical dose rate dependencies.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D Diffusion Model</td>
<td style='padding: 6px;'>Hongxu Yang, Edina Timko, Levente Lippenszky, Vanda Czipczer, Lehel Ferenczi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03267v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Synthetic tumors in medical images offer controllable characteristics that facilitate the training of machine learning models, leading to an improved segmentation performance. However, the existing methods of tumor synthesis yield suboptimal performances when tumor occupies a large spatial volume, such as breast tumor segmentation in MRI with a large field-of-view (FOV), while commonly used tumor generation methods are based on small patches. In this paper, we propose a 3D medical diffusion model, called SynBT, to generate high-quality breast tumor (BT) in contrast-enhanced MRI images. The proposed model consists of a patch-to-volume autoencoder, which is able to compress the high-resolution MRIs into compact latent space, while preserving the resolution of volumes with large FOV. Using the obtained latent space feature vector, a mask-conditioned diffusion model is used to synthesize breast tumors within selected regions of breast tissue, resulting in realistic tumor appearances. We evaluated the proposed method for a tumor segmentation task, which demonstrated the proposed high-quality tumor synthesis method can facilitate the common segmentation models with performance improvement of 2-3% Dice Score on a large public dataset, and therefore provides benefits for tumor segmentation in MRI images.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Anti-scatter grid prototype manufactured via laser powder bed fusion of pure tungsten</td>
<td style='padding: 6px;'>I. Diachkov, S. Chernyshikhin, S. Dadabaev, S. Kholodenko, A. Kulik, V. Obraztsov, E. Shmanin, D. Strekalina</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03255v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper describes the production and tests performed with the tungsten collimator prototype produced using laser powder bed fusion technology in a converging geometry. This prototype has been tested for being the anti-scatter grid for X-ray imaging to reduce background from the scattered photons. The purpose is reducing patient radiation exposure in medical applications. Obtained results are positive, grid provides definitive enhancement in marker detection which proves to be especially useful with artificially created obstacles to imitate harsher working environments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Deep Learning for High Speed Optical Coherence Elastography with a Fiber Scanning Endoscope</td>
<td style='padding: 6px;'>Maximilian Neidhardt, Sarah Latus, Tim Eixmann, Gereon H√ºttmann, Alexander Schlaefer</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03193v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Tissue stiffness is related to soft tissue pathologies and can be assessed through palpation or via clinical imaging systems, e.g., ultrasound or magnetic resonance imaging. Typically, the image based approaches are not suitable during interventions, particularly for minimally invasive surgery. To this end, we present a miniaturized fiber scanning endoscope for fast and localized elastography. Moreover, we propose a deep learning based signal processing pipeline to account for the intricate data and the need for real-time estimates. Our elasticity estimation approach is based on imaging complex and diffuse wave fields that encompass multiple wave frequencies and propagate in various directions. We optimize the probe design to enable different scan patterns. To maximize temporal sampling while maintaining three-dimensional information we define a scan pattern in a conical shape with a temporal frequency of 5.05 kHz. To efficiently process the image sequences of complex wave fields we consider a spatio-temporal deep learning network. We train the network in an end-to-end fashion on measurements from phantoms representing multiple elasticities. The network is used to obtain localized and robust elasticity estimates, allowing to create elasticity maps in real-time. For 2D scanning, our approach results in a mean absolute error of 6.31+-5.76 kPa compared to 11.33+-12.78 kPa for conventional phase tracking. For scanning without estimating the wave direction, the novel 3D method reduces the error to 4.48+-3.63 kPa compared to 19.75+-21.82 kPa for the conventional 2D method. Finally, we demonstrate feasibility of elasticity estimates in ex-vivo porcine tissue.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-03</td>
<td style='padding: 8px;'>Prompt-Guided Patch UNet-VAE with Adversarial Supervision for Adrenal Gland Segmentation in Computed Tomography Medical Images</td>
<td style='padding: 6px;'>Hania Ghouse, Muzammil Behzad</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.03188v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Segmentation of small and irregularly shaped abdominal organs, such as the adrenal glands in CT imaging, remains a persistent challenge due to severe class imbalance, poor spatial context, and limited annotated data. In this work, we propose a unified framework that combines variational reconstruction, supervised segmentation, and adversarial patch-based feedback to address these limitations in a principled and scalable manner. Our architecture is built upon a VAE-UNet backbone that jointly reconstructs input patches and generates voxel-level segmentation masks, allowing the model to learn disentangled representations of anatomical structure and appearance. We introduce a patch-based training pipeline that selectively injects synthetic patches generated from the learned latent space, and systematically study the effects of varying synthetic-to-real patch ratios during training. To further enhance output fidelity, the framework incorporates perceptual reconstruction loss using VGG features, as well as a PatchGAN-style discriminator for adversarial supervision over spatial realism. Comprehensive experiments on the BTCV dataset demonstrate that our approach improves segmentation accuracy, particularly in boundary-sensitive regions, while maintaining strong reconstruction quality. Our findings highlight the effectiveness of hybrid generative-discriminative training regimes for small-organ segmentation and provide new insights into balancing realism, diversity, and anatomical consistency in data-scarce scenarios.</td>
</tr>
</tbody>
</table>

