<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2025-05-20</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>Fractal geometry predicts dynamic differences in structural and functional connectomes</td>
<td style='padding: 6px;'>Anca Radulescu, Eva Kaslik, Alexandry Fikl, Johan Nakuci, Sarah Muldoon, Michael Anderson</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11477v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding the intricate architecture of brain networks and its connection to brain function is essential for deciphering the underlying principles of cognition and disease.   While traditional graph-theoretical measures have been widely used to characterize these networks, they often fail to fully capture the emergent properties of large-scale neural dynamics. Here, we introduce an alternative approach to quantify brain networks that is rooted in complex dynamics, fractal geometry, and asymptotic analysis.   We apply these concepts to brain connectomes and demonstrate how quadratic iterations and geometric properties of Mandelbrot-like sets can provide novel insights into structural and functional network dynamics. Our findings reveal fundamental distinctions between structural (positive) and functional (signed) connectomes, such as the shift of cusp orientation and the variability in equi-M set geometry. Notably, structural connectomes exhibit more robust, predictable features, while functional connectomes show increased variability for non-trivial tasks. We further demonstrate that traditional graph-theoretical measures, when applied separately to the positive and negative sub-networks of functional connectomes, fail to fully capture their dynamic complexity. Instead, size and shape-based invariants of the equi-M set effectively differentiate between rest and emotional task states, which highlights their potential as superior markers of emergent network dynamics. These results suggest that incorporating fractal-based methods into network neuroscience provides a powerful tool for understanding how information flows in natural systems beyond static connectivity measures, while maintaining their simplicity.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>GOUHFI: a novel contrast- and resolution-agnostic segmentation tool for Ultra-High Field MRI</td>
<td style='padding: 6px;'>Marc-Antoine Fortin, Anne Louise Kristoffersen, Michael Staff Larsen, Laurent Lamalle, Ruediger Stirnberg, Paal Erik Goa</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11445v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recently, Ultra-High Field MRI (UHF-MRI) has become more available and one of the best tools to study the brain. One common step in quantitative neuroimaging is the brain segmentation. However, the differences between UHF-MRI and 1.5-3T images are such that the automatic segmentation techniques optimized at these field strengths usually produce unsatisfactory segmentation results for UHF images. It has been particularly challenging to perform quantitative analyses as typically done with 1.5-3T data, considerably limiting the potential of UHF-MRI. Hence, we propose a novel Deep Learning (DL)-based segmentation technique called GOUHFI: Generalized and Optimized segmentation tool for Ultra-High Field Images, designed to segment UHF images of various contrasts and resolutions. For training, we used a total of 206 label maps from four datasets acquired at 3T, 7T and 9.4T. In contrast to most DL strategies, we used a previously proposed domain randomization approach, where synthetic images generated from the label maps were used for training a 3D U-Net. GOUHFI was tested on seven different datasets and compared to techniques like FastSurferVINN and CEREBRUM-7T. GOUHFI was able to the segment six contrasts and seven resolutions tested at 3T, 7T and 9.4T. Average Dice-Sorensen Similarity Coefficient (DSC) scores of 0.87, 0.84, 0.91 were computed against the ground truth segmentations at 3T, 7T and 9.4T. Moreover, GOUHFI demonstrated impressive resistance to the typical inhomogeneities observed at UHF-MRI, making it a new powerful segmentation tool that allows to apply the usual quantitative analysis pipelines also at UHF. Ultimately, GOUHFI is a promising new segmentation tool, being the first of its kind proposing a contrast- and resolution-agnostic alternative for UHF-MRI, making it the forthcoming alternative for neuroscientists working with UHF-MRI or even lower field strengths.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>From Fibers to Cells: Fourier-Based Registration Enables Virtual Cresyl Violet Staining From 3D Polarized Light Imaging</td>
<td style='padding: 6px;'>Alexander Oberstrass, Esteban Vaca, Eric Upschulte, Meiqi Niu, Nicola Palomero-Gallagher, David Graessel, Christian Schiffer, Markus Axer, Katrin Amunts, Timo Dickscheid</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11394v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Comprehensive assessment of the various aspects of the brain's microstructure requires the use of complementary imaging techniques. This includes measuring the spatial distribution of cell bodies (cytoarchitecture) and nerve fibers (myeloarchitecture). The gold standard for cytoarchitectonic analysis is light microscopic imaging of cell-body stained tissue sections. To reveal the 3D orientations of nerve fibers, 3D Polarized Light Imaging (3D-PLI) has been introduced as a reliable technique providing a resolution in the micrometer range while allowing processing of series of complete brain sections. 3D-PLI acquisition is label-free and allows subsequent staining of sections after measurement. By post-staining for cell bodies, a direct link between fiber- and cytoarchitecture can potentially be established within the same section. However, inevitable distortions introduced during the staining process make a nonlinear and cross-modal registration necessary in order to study the detailed relationships between cells and fibers in the images. In addition, the complexity of processing histological sections for post-staining only allows for a limited number of samples. In this work, we take advantage of deep learning methods for image-to-image translation to generate a virtual staining of 3D-PLI that is spatially aligned at the cellular level. In a supervised setting, we build on a unique dataset of brain sections, to which Cresyl violet staining has been applied after 3D-PLI measurement. To ensure high correspondence between both modalities, we address the misalignment of training data using Fourier-based registration methods. In this way, registration can be efficiently calculated during training for local image patches of target and predicted staining. We demonstrate that the proposed method enables prediction of a Cresyl violet staining from 3D-PLI, matching individual cell instances.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>Covariance Density Neural Networks</td>
<td style='padding: 6px;'>Om Roy, Yashar Moshfeghi, Keith Smith</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11139v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Graph neural networks have re-defined how we model and predict on network data but there lacks a consensus on choosing the correct underlying graph structure on which to model signals. CoVariance Neural Networks (VNN) address this issue by using the sample covariance matrix as a Graph Shift Operator (GSO). Here, we improve on the performance of VNNs by constructing a Density Matrix where we consider the sample Covariance matrix as a quasi-Hamiltonian of the system in the space of random variables. Crucially, using this density matrix as the GSO allows components of the data to be extracted at different scales, allowing enhanced discriminability and performance. We show that this approach allows explicit control of the stability-discriminability trade-off of the network, provides enhanced robustness to noise compared to VNNs, and outperforms them in useful real-life applications where the underlying covariance matrix is informative. In particular, we show that our model can achieve strong performance in subject-independent Brain Computer Interface EEG motor imagery classification, outperforming EEGnet while being faster. This shows how covariance density neural networks provide a basis for the notoriously difficult task of transferability of BCIs when evaluated on unseen individuals.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>PhiNet v2: A Mask-Free Brain-Inspired Vision Foundation Model from Video</td>
<td style='padding: 6px;'>Makoto Yamada, Kian Ming A. Chai, Ayoub Rhim, Satoki Ishikawa, Mohammad Sabokrou, Yao-Hung Hubert Tsai</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11129v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent advances in self-supervised learning (SSL) have revolutionized computer vision through innovative architectures and learning objectives, yet they have not fully leveraged insights from biological visual processing systems. Recently, a brain-inspired SSL model named PhiNet was proposed; it is based on a ResNet backbone and operates on static image inputs with strong augmentation. In this paper, we introduce PhiNet v2, a novel Transformer-based architecture that processes temporal visual input (that is, sequences of images) without relying on strong augmentation. Our model leverages variational inference to learn robust visual representations from continuous input streams, similar to human visual processing. Through extensive experimentation, we demonstrate that PhiNet v2 achieves competitive performance compared to state-of-the-art vision foundation models, while maintaining the ability to learn from sequential input without strong data augmentation. This work represents a significant step toward more biologically plausible computer vision systems that process visual information in a manner more closely aligned with human cognitive processes.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>Bridging BCI and Communications: A MIMO Framework for EEG-to-ECoG Wireless Channel Modeling</td>
<td style='padding: 6px;'>Jiaheng Wang, Zhenyu Wang, Tianheng Xu, Yuan Si, Ang Li, Ting Zhou, Xi Zhao, Honglin Hu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.10786v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As a method to connect human brain and external devices, Brain-computer interfaces (BCIs) are receiving extensive research attention. Recently, the integration of communication theory with BCI has emerged as a popular trend, offering potential to enhance system performance and shape next-generation communications.   A key challenge in this field is modeling the brain wireless communication channel between intracranial electrocorticography (ECoG) emitting neurons and extracranial electroencephalography (EEG) receiving electrodes. However, the complex physiology of brain challenges the application of traditional channel modeling methods, leaving relevant research in its infancy. To address this gap, we propose a frequency-division multiple-input multiple-output (MIMO) estimation framework leveraging simultaneous macaque EEG and ECoG recordings, while employing neurophysiology-informed regularization to suppress noise interference. This approach reveals profound similarities between neural signal propagation and multi-antenna communication systems. Experimental results show improved estimation accuracy over conventional methods while highlighting a trade-off between frequency resolution and temporal stability determined by signal duration. This work establish a conceptual bridge between neural interfacing and communication theory, accelerating synergistic developments in both fields.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-15</td>
<td style='padding: 8px;'>Predicting Risk of Pulmonary Fibrosis Formation in PASC Patients</td>
<td style='padding: 6px;'>Wanying Dou, Gorkem Durak, Koushik Biswas, Ziliang Hong, Andrea Mia Bejar, Elif Keles, Kaan Akin, Sukru Mehmet Erturk, Alpay Medetalibeyoglu, Marc Sala, Alexander Misharin, Hatice Savas, Mary Salvatore, Sachin Jambawalikar, Drew Torigian, Jayaram K. Udupa, Ulas Bagci</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.10691v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While the acute phase of the COVID-19 pandemic has subsided, its long-term effects persist through Post-Acute Sequelae of COVID-19 (PASC), commonly known as Long COVID. There remains substantial uncertainty regarding both its duration and optimal management strategies. PASC manifests as a diverse array of persistent or newly emerging symptoms--ranging from fatigue, dyspnea, and neurologic impairments (e.g., brain fog), to cardiovascular, pulmonary, and musculoskeletal abnormalities--that extend beyond the acute infection phase. This heterogeneous presentation poses substantial challenges for clinical assessment, diagnosis, and treatment planning. In this paper, we focus on imaging findings that may suggest fibrotic damage in the lungs, a critical manifestation characterized by scarring of lung tissue, which can potentially affect long-term respiratory function in patients with PASC. This study introduces a novel multi-center chest CT analysis framework that combines deep learning and radiomics for fibrosis prediction. Our approach leverages convolutional neural networks (CNNs) and interpretable feature extraction, achieving 82.2% accuracy and 85.5% AUC in classification tasks. We demonstrate the effectiveness of Grad-CAM visualization and radiomics-based feature analysis in providing clinically relevant insights for PASC-related lung fibrosis prediction. Our findings highlight the potential of deep learning-driven computational methods for early detection and risk assessment of PASC-related lung fibrosis--presented for the first time in the literature.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-15</td>
<td style='padding: 8px;'>ROIsGAN: A Region Guided Generative Adversarial Framework for Murine Hippocampal Subregion Segmentation</td>
<td style='padding: 6px;'>Sayed Mehedi Azim, Brian Corbett, Iman Dehzangi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.10687v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The hippocampus, a critical brain structure involved in memory processing and various neurodegenerative and psychiatric disorders, comprises three key subregions: the dentate gyrus (DG), Cornu Ammonis 1 (CA1), and Cornu Ammonis 3 (CA3). Accurate segmentation of these subregions from histological tissue images is essential for advancing our understanding of disease mechanisms, developmental dynamics, and therapeutic interventions. However, no existing methods address the automated segmentation of hippocampal subregions from tissue images, particularly from immunohistochemistry (IHC) images. To bridge this gap, we introduce a novel set of four comprehensive murine hippocampal IHC datasets featuring distinct staining modalities: cFos, NeuN, and multiplexed stains combining cFos, NeuN, and either {\Delta}FosB or GAD67, capturing structural, neuronal activity, and plasticity associated information. Additionally, we propose ROIsGAN, a region-guided U-Net-based generative adversarial network tailored for hippocampal subregion segmentation. By leveraging adversarial learning, ROIsGAN enhances boundary delineation and structural detail refinement through a novel region-guided discriminator loss combining Dice and binary cross-entropy loss. Evaluated across DG, CA1, and CA3 subregions, ROIsGAN consistently outperforms conventional segmentation models, achieving performance gains ranging from 1-10% in Dice score and up to 11% in Intersection over Union (IoU), particularly under challenging staining conditions. Our work establishes foundational datasets and methods for automated hippocampal segmentation, enabling scalable, high-precision analysis of tissue images in neuroscience research. Our generated datasets, proposed model as a standalone tool, and its corresponding source code are publicly available at: https://github.com/MehediAzim/ROIsGAN</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-15</td>
<td style='padding: 8px;'>Evolution imposes an inductive bias that alters and accelerates learning dynamics</td>
<td style='padding: 6px;'>Benjamin Midler, Alejandro Pan Vazquez</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.10651v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The learning dynamics of biological brains and artificial neural networks are of interest to both neuroscience and machine learning. A key difference between them is that neural networks are often trained from a randomly initialized state whereas each brain is the product of generations of evolutionary optimization, yielding innate structures that enable few-shot learning and inbuilt reflexes. Artificial neural networks, by contrast, require non-ethological quantities of training data to attain comparable performance. To investigate the effect of evolutionary optimization on the learning dynamics of neural networks, we combined algorithms simulating natural selection and online learning to produce a method for evolutionarily conditioning artificial neural networks, and applied it to both reinforcement and supervised learning contexts. We found the evolutionary conditioning algorithm, by itself, performs comparably to an unoptimized baseline. However, evolutionarily conditioned networks show signs of unique and latent learning dynamics, and can be rapidly fine-tuned to optimal performance. These results suggest evolution constitutes an inductive bias that tunes neural systems to enable rapid learning.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-15</td>
<td style='padding: 8px;'>Real-World fNIRS-Based Brain-Computer Interfaces: Benchmarking Deep Learning and Classical Models in Interactive Gaming</td>
<td style='padding: 6px;'>Mohammad Ghalavand, Javad Hatami, Seyed Kamaledin Setarehdan, Hananeh Ghalavand</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.10536v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-Computer Interfaces enable direct communication between the brain and external systems, with functional Near-Infrared Spectroscopy emerging as a portable and non-invasive method for capturing cerebral hemodynamics. This study investigates the classification of rest and task states during a realistic, interactive tennis simulation using fNIRS signals and a range of machine learning approaches. We benchmarked traditional classifiers based on engineered features, Long Short-Term Memory networks on raw time-series data, and Convolutional Neural Networks applied to Gramian Angular Field-transformed images. Ensemble models like Extra Trees and Gradient Boosting achieved accuracies above 97 percent, while the ResNet-based CNN reached 95.0 percent accuracy with a near-perfect AUC of 99.2 percent, outperforming both LSTM and EfficientNet architectures. A novel data augmentation strategy was employed to equalize trial durations while preserving physiological integrity. Feature importance analyses revealed that both oxygenated and deoxygenated hemoglobin signals, particularly slope and RMS metrics, were key contributors to classification performance. These findings demonstrate the strong potential of fNIRS-based BCIs for deployment in dynamic, real-world environments and underscore the advantages of deep learning models in decoding complex neural signals.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>Covariance Density Neural Networks</td>
<td style='padding: 6px;'>Om Roy, Yashar Moshfeghi, Keith Smith</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11139v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Graph neural networks have re-defined how we model and predict on network data but there lacks a consensus on choosing the correct underlying graph structure on which to model signals. CoVariance Neural Networks (VNN) address this issue by using the sample covariance matrix as a Graph Shift Operator (GSO). Here, we improve on the performance of VNNs by constructing a Density Matrix where we consider the sample Covariance matrix as a quasi-Hamiltonian of the system in the space of random variables. Crucially, using this density matrix as the GSO allows components of the data to be extracted at different scales, allowing enhanced discriminability and performance. We show that this approach allows explicit control of the stability-discriminability trade-off of the network, provides enhanced robustness to noise compared to VNNs, and outperforms them in useful real-life applications where the underlying covariance matrix is informative. In particular, we show that our model can achieve strong performance in subject-independent Brain Computer Interface EEG motor imagery classification, outperforming EEGnet while being faster. This shows how covariance density neural networks provide a basis for the notoriously difficult task of transferability of BCIs when evaluated on unseen individuals.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>Bridging BCI and Communications: A MIMO Framework for EEG-to-ECoG Wireless Channel Modeling</td>
<td style='padding: 6px;'>Jiaheng Wang, Zhenyu Wang, Tianheng Xu, Yuan Si, Ang Li, Ting Zhou, Xi Zhao, Honglin Hu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.10786v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As a method to connect human brain and external devices, Brain-computer interfaces (BCIs) are receiving extensive research attention. Recently, the integration of communication theory with BCI has emerged as a popular trend, offering potential to enhance system performance and shape next-generation communications.   A key challenge in this field is modeling the brain wireless communication channel between intracranial electrocorticography (ECoG) emitting neurons and extracranial electroencephalography (EEG) receiving electrodes. However, the complex physiology of brain challenges the application of traditional channel modeling methods, leaving relevant research in its infancy. To address this gap, we propose a frequency-division multiple-input multiple-output (MIMO) estimation framework leveraging simultaneous macaque EEG and ECoG recordings, while employing neurophysiology-informed regularization to suppress noise interference. This approach reveals profound similarities between neural signal propagation and multi-antenna communication systems. Experimental results show improved estimation accuracy over conventional methods while highlighting a trade-off between frequency resolution and temporal stability determined by signal duration. This work establish a conceptual bridge between neural interfacing and communication theory, accelerating synergistic developments in both fields.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-15</td>
<td style='padding: 8px;'>ListenNet: A Lightweight Spatio-Temporal Enhancement Nested Network for Auditory Attention Detection</td>
<td style='padding: 6px;'>Cunhang Fan, Xiaoke Yang, Hongyu Zhang, Ying Chen, Lu Li, Jian Zhou, Zhao Lv</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.10348v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Auditory attention detection (AAD) aims to identify the direction of the attended speaker in multi-speaker environments from brain signals, such as Electroencephalography (EEG) signals. However, existing EEG-based AAD methods overlook the spatio-temporal dependencies of EEG signals, limiting their decoding and generalization abilities. To address these issues, this paper proposes a Lightweight Spatio-Temporal Enhancement Nested Network (ListenNet) for AAD. The ListenNet has three key components: Spatio-temporal Dependency Encoder (STDE), Multi-scale Temporal Enhancement (MSTE), and Cross-Nested Attention (CNA). The STDE reconstructs dependencies between consecutive time windows across channels, improving the robustness of dynamic pattern extraction. The MSTE captures temporal features at multiple scales to represent both fine-grained and long-range temporal patterns. In addition, the CNA integrates hierarchical features more effectively through novel dynamic attention mechanisms to capture deep spatio-temporal correlations. Experimental results on three public datasets demonstrate the superiority of ListenNet over state-of-the-art methods in both subject-dependent and challenging subject-independent settings, while reducing the trainable parameter count by approximately 7 times. Code is available at:https://github.com/fchest/ListenNet.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-14</td>
<td style='padding: 8px;'>Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net</td>
<td style='padding: 6px;'>Dongyi He, Shiyang Li, Bin Jiang, He Yan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.09521v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>High-resolution functional magnetic resonance imaging (fMRI) is essential for mapping human brain activity; however, it remains costly and logistically challenging. If comparable volumes could be generated directly from widely available scalp electroencephalography (EEG), advanced neuroimaging would become significantly more accessible. Existing EEG-to-fMRI generators rely on plain CNNs that fail to capture cross-channel time-frequency cues or on heavy transformer/GAN decoders that strain memory and stability. We propose Spec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts these issues via a Multi-directional Time-Frequency Convolutional Attention Encoder, stacking temporal, spectral and joint convolutions with self-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space blocks enable efficient long-range spatial modelling. Trained end-to-end with a hybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on three public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball and 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9% respectively over previous best SSIM scores. Furthermore, it achieves competitive PSNR scores, particularly excelling on the CN-EPFL dataset with a 4.6% improvement over the previous best PSNR, thus striking a better balance in reconstruction quality. The proposed model is lightweight and efficient, making it suitable for real-time applications in clinical and research settings. The code is available at https://github.com/hdy6438/Spec2VolCAMU-Net.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-14</td>
<td style='padding: 8px;'>Adaptive control for multi-scale stochastic dynamical systems with stochastic next generation reservoir computing</td>
<td style='padding: 6px;'>Jiani Cheng, Ting Gao, Jinqiao Duan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.09327v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The rapid advancement of neuroscience and machine learning has established data-driven stochastic dynamical system modeling as a powerful tool for understanding and controlling high-dimensional, spatio-temporal processes. We introduce the stochastic next-generation reservoir computing (NG-RC) controller, a framework that integrates the computational efficiency of NG-RC with stochastic analysis to enable robust event-triggered control in multiscale stochastic systems. The asymptotic stability of the controller is rigorously proven via an extended stochastic LaSalle theorem, providing theoretical guarantees for amplitude regulation in nonlinear stochastic dynamics. Numerical experiments on a stochastic Van-der-Pol system subject to both additive and multiplicative noise validate the algorithm, demonstrating its convergence rate across varying temporal scales and noise intensities. To bridge theoretical insights with real-world applications, we deploy the controller to modulate pathological dynamics reconstructed from epileptic EEG data. This work advances a theoretically guaranteed scalable framework for adaptive control of stochastic systems, with broad potential for data-driven decision making in engineering, neuroscience, and beyond.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-19</td>
<td style='padding: 8px;'>Robust Emotion Recognition via Bi-Level Self-Supervised Continual Learning</td>
<td style='padding: 6px;'>Adnan Ahmad, Bahareh Nakisa, Mohammad Naim Rastgoo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.10575v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Emotion recognition through physiological signals such as electroencephalogram (EEG) has become an essential aspect of affective computing and provides an objective way to capture human emotions. However, physiological data characterized by cross-subject variability and noisy labels hinder the performance of emotion recognition models. Existing domain adaptation and continual learning methods struggle to address these issues, especially under realistic conditions where data is continuously streamed and unlabeled. To overcome these limitations, we propose a novel bi-level self-supervised continual learning framework, SSOCL, based on a dynamic memory buffer. This bi-level architecture iteratively refines the dynamic buffer and pseudo-label assignments to effectively retain representative samples, enabling generalization from continuous, unlabeled physiological data streams for emotion recognition. The assigned pseudo-labels are subsequently leveraged for accurate emotion prediction. Key components of the framework, including a fast adaptation module and a cluster-mapping module, enable robust learning and effective handling of evolving data streams. Experimental validation on two mainstream EEG tasks demonstrates the framework's ability to adapt to continuous data streams while maintaining strong generalization across subjects, outperforming existing approaches.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-13</td>
<td style='padding: 8px;'>Addressing the Current Challenges of Quantum Machine Learning through Multi-Chip Ensembles</td>
<td style='padding: 6px;'>Junghoon Justin Park, Jiook Cha, Samuel Yen-Chi Chen, Huan-Hsin Tseng, Shinjae Yoo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.08782v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Quantum Machine Learning (QML) holds significant promise for solving computational challenges across diverse domains. However, its practical deployment is constrained by the limitations of noisy intermediate-scale quantum (NISQ) devices, including noise, limited scalability, and trainability issues in variational quantum circuits (VQCs). We introduce the multi-chip ensemble VQC framework, which partitions high-dimensional computations across smaller quantum chips to enhance scalability, trainability, and noise resilience. We show that this approach mitigates barren plateaus, reduces quantum error bias and variance, and maintains robust generalization through controlled entanglement. Designed to align with current and emerging quantum hardware, the framework demonstrates strong potential for enabling scalable QML on near-term devices, as validated by experiments on standard benchmark datasets (MNIST, FashionMNIST, CIFAR-10) and real world dataset (PhysioNet EEG).</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-12</td>
<td style='padding: 8px;'>Dynamical phase transitions in postictal generalized EEG suppression</td>
<td style='padding: 6px;'>Subhasanket Dutta, Sarika Jalan, Yash Shashank Vakilna, Sandipan Pati</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.07988v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Postictal generalized EEG suppression (PGES) is a neurological condition that occurs in patients with generalized tonic-clonic seizures. It is marked by suppressed signals just after the seizure before the brain gradually recovers. Recovery from PGES involves a mixed state of amplitude suppression and high-amplitude oscillations, exhibiting a bimodal exponential distribution in power, unlike the unimodal exponential distribution of PGES. In this study, using the subcritical Hopf model, we explain the nature of phase transitions that underlie PGES. Our results reveal that recovery from PGES involves a change from a fixed point state to a bistable state (mixed phase), effectively captured by the noisy fixed-point and bistable regimes of the model. Consistent patterns across patients suggest a universal dynamical signature in PGES recovery. Our findings offer a mechanistic understanding of seizure termination and postictal brain state transitions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-12</td>
<td style='padding: 8px;'>Decoding Chess Puzzle Play and Standard Cognitive Tasks for BCI: A Low-Cost EEG Study</td>
<td style='padding: 6px;'>Matthew Russell, Samuel Youkeles, William Xia, Kenny Zheng, Aman Shah, Robert J. K. Jacob</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.07592v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While consumer-grade electroencephalography (EEG) devices show promise for Brain-Computer Interface (BCI) applications, their efficacy in detecting subtle cognitive states remains understudied. Using a combination of established cognitive paradigms (N-Back, Stroop, and Mental Rotation) and a novel ecological task (Chess puzzles), we demonstrate successful distinctions of workload levels within some tasks, as well as differentiation between task types using the MUSE 2 device. With machine learning we further show reliable predictive power to differentiate between workload levels in the N-Back task, while also achieving effective cross-task classification. These findings demonstrate that consumer-grade EEG devices can effectively detect and differentiate various forms of cognitive workload, and that they can be leveraged with some success towards real-time classification distinguishing workload in some tasks, as well as in differentiating between nuanced cognitive states, supporting their potential use in adaptive BCI applications. Research code and data are further provided for future researchers.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-08</td>
<td style='padding: 8px;'>Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition</td>
<td style='padding: 6px;'>Xiaozhou Ye, Kevin I-Kai Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.06301v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Cross-user variability in Human Activity Recognition (HAR) remains a critical challenge due to differences in sensor placement, body dynamics, and behavioral patterns. Traditional methods often fail to capture biomechanical invariants that persist across users, limiting their generalization capability. We propose an Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG) framework that integrates anatomical correlation knowledge into a unified graph neural network (GNN) architecture. By modeling three biomechanically motivated relationships together-Interconnected Units, Analogous Units, and Lateral Units-our method encodes domain-invariant features while addressing user-specific variability through Variational Edge Feature Extractor. A Gradient Reversal Layer (GRL) enforces adversarial domain generalization, ensuring robustness to unseen users. Extensive experiments on OPPORTUNITY and DSADS datasets demonstrate state-of-the-art performance. Our work bridges biomechanical principles with graph-based adversarial learning by integrating information fusion techniques. This fusion of information underpins our unified and generalized model for cross-user HAR.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>Covariance Density Neural Networks</td>
<td style='padding: 6px;'>Om Roy, Yashar Moshfeghi, Keith Smith</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11139v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Graph neural networks have re-defined how we model and predict on network data but there lacks a consensus on choosing the correct underlying graph structure on which to model signals. CoVariance Neural Networks (VNN) address this issue by using the sample covariance matrix as a Graph Shift Operator (GSO). Here, we improve on the performance of VNNs by constructing a Density Matrix where we consider the sample Covariance matrix as a quasi-Hamiltonian of the system in the space of random variables. Crucially, using this density matrix as the GSO allows components of the data to be extracted at different scales, allowing enhanced discriminability and performance. We show that this approach allows explicit control of the stability-discriminability trade-off of the network, provides enhanced robustness to noise compared to VNNs, and outperforms them in useful real-life applications where the underlying covariance matrix is informative. In particular, we show that our model can achieve strong performance in subject-independent Brain Computer Interface EEG motor imagery classification, outperforming EEGnet while being faster. This shows how covariance density neural networks provide a basis for the notoriously difficult task of transferability of BCIs when evaluated on unseen individuals.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>Bridging BCI and Communications: A MIMO Framework for EEG-to-ECoG Wireless Channel Modeling</td>
<td style='padding: 6px;'>Jiaheng Wang, Zhenyu Wang, Tianheng Xu, Yuan Si, Ang Li, Ting Zhou, Xi Zhao, Honglin Hu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.10786v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As a method to connect human brain and external devices, Brain-computer interfaces (BCIs) are receiving extensive research attention. Recently, the integration of communication theory with BCI has emerged as a popular trend, offering potential to enhance system performance and shape next-generation communications.   A key challenge in this field is modeling the brain wireless communication channel between intracranial electrocorticography (ECoG) emitting neurons and extracranial electroencephalography (EEG) receiving electrodes. However, the complex physiology of brain challenges the application of traditional channel modeling methods, leaving relevant research in its infancy. To address this gap, we propose a frequency-division multiple-input multiple-output (MIMO) estimation framework leveraging simultaneous macaque EEG and ECoG recordings, while employing neurophysiology-informed regularization to suppress noise interference. This approach reveals profound similarities between neural signal propagation and multi-antenna communication systems. Experimental results show improved estimation accuracy over conventional methods while highlighting a trade-off between frequency resolution and temporal stability determined by signal duration. This work establish a conceptual bridge between neural interfacing and communication theory, accelerating synergistic developments in both fields.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-15</td>
<td style='padding: 8px;'>Real-World fNIRS-Based Brain-Computer Interfaces: Benchmarking Deep Learning and Classical Models in Interactive Gaming</td>
<td style='padding: 6px;'>Mohammad Ghalavand, Javad Hatami, Seyed Kamaledin Setarehdan, Hananeh Ghalavand</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.10536v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-Computer Interfaces enable direct communication between the brain and external systems, with functional Near-Infrared Spectroscopy emerging as a portable and non-invasive method for capturing cerebral hemodynamics. This study investigates the classification of rest and task states during a realistic, interactive tennis simulation using fNIRS signals and a range of machine learning approaches. We benchmarked traditional classifiers based on engineered features, Long Short-Term Memory networks on raw time-series data, and Convolutional Neural Networks applied to Gramian Angular Field-transformed images. Ensemble models like Extra Trees and Gradient Boosting achieved accuracies above 97 percent, while the ResNet-based CNN reached 95.0 percent accuracy with a near-perfect AUC of 99.2 percent, outperforming both LSTM and EfficientNet architectures. A novel data augmentation strategy was employed to equalize trial durations while preserving physiological integrity. Feature importance analyses revealed that both oxygenated and deoxygenated hemoglobin signals, particularly slope and RMS metrics, were key contributors to classification performance. These findings demonstrate the strong potential of fNIRS-based BCIs for deployment in dynamic, real-world environments and underscore the advantages of deep learning models in decoding complex neural signals.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-12</td>
<td style='padding: 8px;'>Decoding Chess Puzzle Play and Standard Cognitive Tasks for BCI: A Low-Cost EEG Study</td>
<td style='padding: 6px;'>Matthew Russell, Samuel Youkeles, William Xia, Kenny Zheng, Aman Shah, Robert J. K. Jacob</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.07592v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>While consumer-grade electroencephalography (EEG) devices show promise for Brain-Computer Interface (BCI) applications, their efficacy in detecting subtle cognitive states remains understudied. Using a combination of established cognitive paradigms (N-Back, Stroop, and Mental Rotation) and a novel ecological task (Chess puzzles), we demonstrate successful distinctions of workload levels within some tasks, as well as differentiation between task types using the MUSE 2 device. With machine learning we further show reliable predictive power to differentiate between workload levels in the N-Back task, while also achieving effective cross-task classification. These findings demonstrate that consumer-grade EEG devices can effectively detect and differentiate various forms of cognitive workload, and that they can be leveraged with some success towards real-time classification distinguishing workload in some tasks, as well as in differentiating between nuanced cognitive states, supporting their potential use in adaptive BCI applications. Research code and data are further provided for future researchers.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-11</td>
<td style='padding: 8px;'>HistDiST: Histopathological Diffusion-based Stain Transfer</td>
<td style='padding: 6px;'>Erik Großkopf, Valay Bundele, Mehran Hossienzadeh, Hendrik P. A. Lensch</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.06793v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Hematoxylin and Eosin (H&E) staining is the cornerstone of histopathology but lacks molecular specificity. While Immunohistochemistry (IHC) provides molecular insights, it is costly and complex, motivating H&E-to-IHC translation as a cost-effective alternative. Existing translation methods are mainly GAN-based, often struggling with training instability and limited structural fidelity, while diffusion-based approaches remain underexplored. We propose HistDiST, a Latent Diffusion Model (LDM) based framework for high-fidelity H&E-to-IHC translation. HistDiST introduces a dual-conditioning strategy, utilizing Phikon-extracted morphological embeddings alongside VAE-encoded H&E representations to ensure pathology-relevant context and structural consistency. To overcome brightness biases, we incorporate a rescaled noise schedule, v-prediction, and trailing timesteps, enforcing a zero-SNR condition at the final timestep. During inference, DDIM inversion preserves the morphological structure, while an eta-cosine noise schedule introduces controlled stochasticity, balancing structural consistency and molecular fidelity. Moreover, we propose Molecular Retrieval Accuracy (MRA), a novel pathology-aware metric leveraging GigaPath embeddings to assess molecular relevance. Extensive evaluations on MIST and BCI datasets demonstrate that HistDiST significantly outperforms existing methods, achieving a 28% improvement in MRA on the H&E-to-Ki67 translation task, highlighting its effectiveness in capturing true IHC semantics.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-30</td>
<td style='padding: 8px;'>Real-Time Brain-Computer Interface Control of Walking Exoskeleton with Bilateral Sensory Feedback</td>
<td style='padding: 6px;'>Jeffrey Lim, Po T. Wang, Won Joon Sohn, Derrick Lin, Shravan Thaploo, Luke Bashford, David Bjanes, Angelica Nguyen, Hui Gong, Michelle Armacost, Susan J. Shaw, Spencer Kellis, Brian Lee, Darrin Lee, Payam Heydari, Richard A. Andersen, Zoran Nenadic, Charles Y. Liu, An H. Do</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.00219v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Invasive brain-computer interface (BCI) technology has demonstrated the possibility of restoring brain-controlled walking in paraplegic spinal cord injury patients. However, current implementations of BCI-controlled walking still have significant drawbacks. In particular, prior systems are unidirectional and lack sensory feedback for insensate patients, have suboptimal reliance on brain signals from the bilateral arm areas of the motor cortex, and depend on external systems for signal processing. Motivated by these shortcomings, this study is the first time a bidirectional brain-computer interface (BDBCI) has demonstrated the restoration of both brain-controlled walking and leg sensory feedback while utilizing the bilateral leg motor and sensory cortices. Here, a subject undergoing subdural electrocorticogram electrode implantation for epilepsy surgery evaluation leveraged the leg representation areas of the bilateral interhemispheric primary motor and sensory cortices to operate a BDBCI with high performance. Although electrode implantation in the interhemispheric region is uncommon, electrodes can be safely implanted in this region to access rich leg motor information and deliver bilateral leg sensory feedback. Finally, we demonstrated that all BDBCI operations can be executed on a dedicated, portable embedded system. These results indicate that BDBCIs can potentially provide brain-controlled ambulation and artificial leg sensation to people with paraplegia after spinal cord injury in a manner that emulates full-implantability and is untethered from any external systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-30</td>
<td style='padding: 8px;'>MPEC: Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers</td>
<td style='padding: 6px;'>Shermin Shahbazi, Mohammad-Reza Nasiri, Majid Ramezani</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2504.21427v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Accurate classification of EEG signals is crucial for brain-computer interfaces (BCIs) and neuroprosthetic applications, yet many existing methods fail to account for the non-Euclidean, manifold structure of EEG data, resulting in suboptimal performance. Preserving this manifold information is essential to capture the true geometry of EEG signals, but traditional classification techniques largely overlook this need. To this end, we propose MPEC (Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers), that introduces two key innovations: (1) a feature engineering phase that combines covariance matrices and Radial Basis Function (RBF) kernels to capture both linear and non-linear relationships among EEG channels, and (2) a clustering phase that employs a modified K-means algorithm tailored for the Riemannian manifold space, ensuring local geometric sensitivity. Ensembling multiple clustering-based classifiers, MPEC achieves superior results, validated by significant improvements on the BCI Competition IV dataset 2a.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-03</td>
<td style='padding: 8px;'>Pretraining Large Brain Language Model for Active BCI: Silent Speech</td>
<td style='padding: 6px;'>Jinzhao Zhou, Zehong Cao, Yiqun Duan, Connor Barkley, Daniel Leong, Xiaowei Jiang, Quoc-Toan Nguyen, Ziyi Zhao, Thomas Do, Yu-Cheng Chang, Sheng-Fu Liang, Chin-teng Lin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2504.21214v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper explores silent speech decoding in active brain-computer interface (BCI) systems, which offer more natural and flexible communication than traditional BCI applications. We collected a new silent speech dataset of over 120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing 24 commonly used English words for language model pretraining and decoding. Following the recent success of pretraining large models with self-supervised paradigms to enhance EEG classification performance, we propose Large Brain Language Model (LBLM) pretrained to decode silent speech for active BCI. To pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining paradigm to learn effective representations from unlabeled EEG data. Unlike existing EEG pretraining methods that mainly follow a masked-reconstruction paradigm, our proposed FSTP method employs autoregressive modeling in temporal and frequency domains to capture both temporal and spectral dependencies from EEG signals. After pretraining, we finetune our LBLM on downstream tasks, including word-level and semantic-level classification. Extensive experiments demonstrate significant performance gains of the LBLM over fully-supervised and pretrained baseline models. For instance, in the difficult cross-session setting, our model achieves 47.0\% accuracy on semantic-level classification and 39.6\% in word-level classification, outperforming baseline methods by 5.4\% and 7.3\%, respectively. Our research advances silent speech decoding in active BCI systems, offering an innovative solution for EEG language model pretraining and a new dataset for fundamental research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-24</td>
<td style='padding: 8px;'>The Riemannian Means Field Classifier for EEG-Based BCI Data</td>
<td style='padding: 6px;'>Anton Andreev, Grégoire Cattan, Marco Congedo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2504.17352v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A substantial amount of research has demonstrated the robustness and accuracy of the Riemannian minimum distance to mean (MDM) classifier for all kinds of EEG-based brain--computer interfaces (BCIs). This classifier is simple, fully deterministic, robust to noise, computationally efficient, and prone to transfer learning. Its training is very simple, requiring just the computation of a geometric mean of a symmetric positive-definite (SPD) matrix per class. We propose an improvement of the MDM involving a number of power means of SPD matrices instead of the sole geometric mean. By the analysis of 20 public databases, 10 for the motor-imagery BCI paradigm and 10 for the P300 BCI paradigm, comprising 587 individuals in total, we show that the proposed classifier clearly outperforms the MDM, approaching the state-of-the art in terms of performance while retaining the simplicity and the deterministic behavior. In order to promote reproducible research, our code will be released as open source.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-23</td>
<td style='padding: 8px;'>Transferring Spatial Filters via Tangent Space Alignment in Motor Imagery BCIs</td>
<td style='padding: 6px;'>Tekin Gunasar, Virginia de Sa</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2504.17111v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We propose a method to improve subject transfer in motor imagery BCIs by aligning covariance matrices on a Riemannian manifold, followed by computing a new common spatial patterns (CSP) based spatial filter. We explore various ways to integrate information from multiple subjects and show improved performance compared to standard CSP. Across three datasets, our method shows marginal improvements over standard CSP; however, when training data are limited, the improvements become more significant.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-15</td>
<td style='padding: 8px;'>Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli</td>
<td style='padding: 6px;'>Chunyu Ye, Shaonan Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.10356v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Decoding thoughts from brain activity offers valuable insights into human cognition and enables promising applications in brain-computer interaction. While prior studies have explored language reconstruction from fMRI data, they are typically limited to single-modality inputs such as images or audio. In contrast, human thought is inherently multimodal. To bridge this gap, we propose a unified and flexible framework for reconstructing coherent language from brain recordings elicited by diverse input modalities-visual, auditory, and textual. Our approach leverages visual-language models (VLMs), using modality-specific experts to jointly interpret information across modalities. Experiments demonstrate that our method achieves performance comparable to state-of-the-art systems while remaining adaptable and extensible. This work advances toward more ecologically valid and generalizable mind decoding.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-14</td>
<td style='padding: 8px;'>Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net</td>
<td style='padding: 6px;'>Dongyi He, Shiyang Li, Bin Jiang, He Yan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.09521v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>High-resolution functional magnetic resonance imaging (fMRI) is essential for mapping human brain activity; however, it remains costly and logistically challenging. If comparable volumes could be generated directly from widely available scalp electroencephalography (EEG), advanced neuroimaging would become significantly more accessible. Existing EEG-to-fMRI generators rely on plain CNNs that fail to capture cross-channel time-frequency cues or on heavy transformer/GAN decoders that strain memory and stability. We propose Spec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts these issues via a Multi-directional Time-Frequency Convolutional Attention Encoder, stacking temporal, spectral and joint convolutions with self-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space blocks enable efficient long-range spatial modelling. Trained end-to-end with a hybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on three public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball and 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9% respectively over previous best SSIM scores. Furthermore, it achieves competitive PSNR scores, particularly excelling on the CN-EPFL dataset with a 4.6% improvement over the previous best PSNR, thus striking a better balance in reconstruction quality. The proposed model is lightweight and efficient, making it suitable for real-time applications in clinical and research settings. The code is available at https://github.com/hdy6438/Spec2VolCAMU-Net.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-09</td>
<td style='padding: 8px;'>Reconstructing Brain Causal Dynamics for Subject and Task Fingerprints using fMRI Time-series Data</td>
<td style='padding: 6px;'>Dachuan Song, Li Shen, Duy Duong-Tran, Xuan Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.06392v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Purpose: Recently, there has been a revived interest in system neuroscience causation models, driven by their unique capability to unravel complex relationships in multi-scale brain networks. In this paper, we present a novel method that leverages causal dynamics to achieve effective fMRI-based subject and task fingerprinting. Methods: By applying an implicit-explicit discretization scheme, we develop a two-timescale linear state-space model. Through data-driven identification of its parameters, the model captures causal signatures, including directed interactions among brain regions from a spatial perspective, and disentangled fast and slow dynamic modes of brain activity from a temporal perspective. These causal signatures are then integrated with: (i) a modal decomposition and projection method for model-based subject identification, and (ii) a Graph Neural Network (GNN) framework for learning-based task classification. Furthermore, we introduce the concept of the brain reachability landscape as a novel visualization tool, which quantitatively characterizes the maximum possible activation levels of brain regions under various fMRI tasks. Results: We evaluate the proposed approach using the Human Connectome Project dataset and demonstrate its advantage over non-causality-based methods. The obtained causal signatures are visualized and demonstrate clear biological relevance with established understandings of brain function. Conclusion: We verified the feasibility and effectiveness of utilizing brain causal signatures for subject and task fingerprinting. Additionally, our work paves the way for further studies on causal fingerprints with potential applications in both healthy controls and neurodegenerative diseases.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-06</td>
<td style='padding: 8px;'>Stabilizing 3D EPI time series by servo navigation and phase equalization exploiting repeated shots (PEERS)</td>
<td style='padding: 6px;'>Malte Riedel, Thomas Ulrich, Samuel Bianchi, Klaas P. Pruessmann</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.03637v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Purpose: To enable run-time head motion control and robust frequency corrections for 3D EPI fMRI. Methods: A short 3D orbital navigator (3 ms) is inserted into a 3D EPI sequence. A linear perturbation model is calibrated to estimate rigid motion and frequency parameters per shot. Rigid motion is corrected by scan geometry updates in run-time, while several techniques are investigated to stabilize navigator-based frequency corrections in the reconstruction. An additional method termed PEERS is proposed that exploits the repetitive structure of fMRI scans to fine-tune shot-wise phase and frequency estimates using the motion-corrected EPI data itself. Results: Servo navigation effectively reduces motion in the raw data of in-vivo fMRI scans in six subjects. PEERS provides high-precision frequency parameters for robust phase-corrected reconstructions in the phantom and in-vivo accounting for scanner drifts and slice encoding-related effects on EPI. In combination, servo navigation and PEERS achieve successful intra-volume corrections and consistent tSNR improvements of 8% on average throughout the brain. The two methods prove to be highly synergetic. Conclusion: Servo navigation achieves high-precision motion correction for 3D-EPI fMRI in run-time and, in synergy with PEERS, provides stable frequency corrections with short navigators even for long echo times. With its automatic self-calibration and no hardware requirements, servo navigation and PEERS enable effective plug-and-play motion correction for 3D fMRI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-06</td>
<td style='padding: 8px;'>The classification of Alzheimer's disease and mild cognitive impairment improved by dynamic functional network analysis</td>
<td style='padding: 6px;'>Nicolas Rubido, Venia Batziou, Marwan Fuad, Vesna Vuksanovic</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.03458v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain network analysis using functional MRI has advanced our understanding of cortical activity and its changes in neurodegenerative disorders that cause dementia. Recently, research in brain connectivity has focused on dynamic (time-varying) brain networks that capture both spatial and temporal information on cortical, regional co-activity patterns. However, this approach has been largely unexplored within the Alzheimer's spectrum. We analysed age- and sex-matched static and dynamic fMRI brain networks from 315 individuals with Alzheimer's Disease (AD), Mild Cognitive Impairment (MCI), and cognitively-normal Healthy Elderly (HE), using data from the ADNI-3 protocol. We examined both similarities and differences between these groups, employing the Juelich brain atlas for network nodes, sliding-window correlations for time-varying network links, and non-parametric statistics to assess between-group differences at the link or the node centrality level. While the HE and MCI groups show similar static and dynamic networks at the link level, significant differences emerge compared to AD participants. We found stable (stationary) differences in patterns of functional connections between the white matter regions and the parietal lobe's, and somatosensory cortices, while metastable (temporal) networks' differences were consistently found between the amygdala and hippocampal formation. In addition, our node centrality analysis showed that the white matter connectivity patterns are local in nature. Our results highlight shared and unique functional connectivity patterns in both stationary and dynamic functional networks, emphasising the need to include dynamic information in brain network analysis in studies of Alzheimer's spectrum.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-03</td>
<td style='padding: 8px;'>Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations</td>
<td style='padding: 6px;'>Christos Zangos, Danish Ebadulla, Thomas Christopher Sprague, Ambuj Singh</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.01670v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This work introduces a novel approach to fMRI-based visual image reconstruction using a subject-agnostic common representation space. We show that the brain signals of the subjects can be aligned in this common space during training to form a semantically aligned common brain. This is leveraged to demonstrate that aligning subject-specific lightweight modules to a reference subject is significantly more efficient than traditional end-to-end training methods. Our approach excels in low-data scenarios. We evaluate our methods on different datasets, demonstrating that the common space is subject and dataset-agnostic.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-30</td>
<td style='padding: 8px;'>Mapping minds not averages: a scalable subject-specific manifold learning framework for neuroimaging data</td>
<td style='padding: 6px;'>Eloy Geenjaar, Vince Calhoun</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.00196v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Mental and cognitive representations are believed to reside on low-dimensional, non-linear manifolds embedded within high-dimensional brain activity. Uncovering these manifolds is key to understanding individual differences in brain function, yet most existing machine learning methods either rely on population-level spatial alignment or assume data that is temporally structured, either because data is aligned among subjects or because event timings are known. We introduce a manifold learning framework that can capture subject-specific spatial variations across both structured and temporally unstructured neuroimaging data. On simulated data and two naturalistic fMRI datasets (Sherlock and Forrest Gump), our framework outperforms group-based baselines by recovering more accurate and individualized representations. We further show that the framework scales efficiently to large datasets and generalizes well to new subjects. To test this, we apply the framework to temporally unstructured resting-state fMRI data from individuals with schizophrenia and healthy controls. We further apply our method to a large resting-state fMRI dataset comprising individuals with schizophrenia and controls. In this setting, we demonstrate that the framework scales efficiently to large populations and generalizes robustly to unseen subjects. The learned subject-specific spatial maps our model finds reveal clinically relevant patterns, including increased activation in the basal ganglia, visual, auditory, and somatosensory regions, and decreased activation in the insula, inferior frontal gyrus, and angular gyrus. These findings suggest that our framework can uncover clinically relevant subject-specific brain activity patterns. Our approach thus provides a scalable and individualized framework for modeling brain activity, with applications in computational neuroscience and clinical research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-30</td>
<td style='padding: 8px;'>Rootlets-based registration to the spinal cord PAM50 template</td>
<td style='padding: 6px;'>Sandrine Bédard, Jan Valošek, Valeria Oliva, Kenneth A. Weber II, Julien Cohen-Adad</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.00115v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Spinal cord functional MRI studies require precise localization of spinal levels for reliable voxelwise group analyses. Traditional template-based registration of the spinal cord uses intervertebral discs for alignment. However, substantial anatomical variability across individuals exists between vertebral and spinal levels. This study proposes a novel registration approach that leverages spinal nerve rootlets to improve alignment accuracy and reproducibility across individuals. We developed a registration method leveraging dorsal cervical rootlets segmentation and aligning them non-linearly with the PAM50 spinal cord template. Validation was performed on a multi-subject, multi-site dataset (n=267, 44 sites) and a multi-subject dataset with various neck positions (n=10, 3 sessions). We further validated the method on task-based functional MRI (n=23) to compare group-level activation maps using rootlet-based registration to traditional disc-based methods. Rootlet-based registration showed superior alignment across individuals compared to the traditional disc-based method. Notably, rootlet positions were more stable across neck positions. Group-level analysis of task-based functional MRI using rootlet-based increased Z scores and activation cluster size compared to disc-based registration (number of active voxels from 3292 to 7978). Rootlet-based registration enhances both inter- and intra-subject anatomical alignment and yields better spatial normalization for group-level fMRI analyses. Our findings highlight the potential of rootlet-based registration to improve the precision and reliability of spinal cord neuroimaging group analysis.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-25</td>
<td style='padding: 8px;'>A Neuro-Dynamic Mathematical Model of Dream Formation and Spontaneous Cognitive Activity</td>
<td style='padding: 6px;'>Shirmohammad Tavangari, Sajjad Janfaza, Zahra Shakarami, Aref Yelghi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.05483v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper introduces a biomathematical model designed to describe the internal dynamics of dream formation and spontaneous cognitive processes. The model incorporates neurocognitive factors such as dissatisfaction, acceptance, forgetting, and mental activity, each of which is linked to established neural systems. We formulate a system of differential equations to simulate interactions among these variables and validate the model using simulated neural data. Our results demonstrate biologically plausible cognitive patterns consistent with findings from EEG and fMRI studies, particularly related to the default mode network (DMN), anterior cingulate cortex (ACC), and hippocampal memory mechanisms.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-23</td>
<td style='padding: 8px;'>4D Multimodal Co-attention Fusion Network with Latent Contrastive Alignment for Alzheimer's Diagnosis</td>
<td style='padding: 6px;'>Yuxiang Wei, Yanteng Zhang, Xi Xiao, Tianyang Wang, Xiao Wang, Vince D. Calhoun</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2504.16798v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Multimodal neuroimaging provides complementary structural and functional insights into both human brain organization and disease-related dynamics. Recent studies demonstrate enhanced diagnostic sensitivity for Alzheimer's disease (AD) through synergistic integration of neuroimaging data (e.g., sMRI, fMRI) with behavioral cognitive scores tabular data biomarkers. However, the intrinsic heterogeneity across modalities (e.g., 4D spatiotemporal fMRI dynamics vs. 3D anatomical sMRI structure) presents critical challenges for discriminative feature fusion. To bridge this gap, we propose M2M-AlignNet: a geometry-aware multimodal co-attention network with latent alignment for early AD diagnosis using sMRI and fMRI. At the core of our approach is a multi-patch-to-multi-patch (M2M) contrastive loss function that quantifies and reduces representational discrepancies via geometry-weighted patch correspondence, explicitly aligning fMRI components across brain regions with their sMRI structural substrates without one-to-one constraints. Additionally, we propose a latent-as-query co-attention module to autonomously discover fusion patterns, circumventing modality prioritization biases while minimizing feature redundancy. We conduct extensive experiments to confirm the effectiveness of our method and highlight the correspondance between fMRI and sMRI as AD biomarkers.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-07</td>
<td style='padding: 8px;'>Charged Lepton Flavor Violating Experiments with Muons</td>
<td style='padding: 6px;'>Dylan Palo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.04764v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We report on the status of charged lepton flavor violating (CLFV) experiments with muons. We focus on the three "golden channels": $\mu^{+} \rightarrow e^{+} \gamma$, $\mu^{+} \rightarrow e^{+} e^{-} e^{+}$ and $\mu^{-} N \rightarrow e^{-} N$. The collection of upcoming experiments aim for sensitivity improvements up to $10^{4}$ with respect to previous searches. The MEG II experiment, searching for $\mu^{+} \rightarrow e^{+} \gamma$, is currently in its 4th year of physics data-taking with a published result from its first year of data. The Mu3e experiment is an upcoming experiment searching for $\mu^{+} \rightarrow e^{+} e^{-} e^{+}$ with plans of physics data-taking as soon as 2025. The Mu2e and COMET experiments are upcoming searches for $\mu^{-} N \rightarrow e^{-} N$ with the goal of physics data-taking starting in 2027 and 2026 respectively. This proceeding summarizes the signal signature, expected background, resolutions, and timelines for the mentioned searches.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-22</td>
<td style='padding: 8px;'>New limit on the μ+->e+γdecay with the MEG II experiment</td>
<td style='padding: 6px;'>K. Afanaciev, A. M. Baldini, S. Ban, H. Benmansour, G. Boca, P. W. Cattaneo, G. Cavoto, F. Cei, M. Chiappini, A. Corvaglia, G. Dal Maso, A. De Bari, M. De Gerone, L. Ferrari Barusso, M. Francesconi, L. Galli, G. Gallucci, F. Gatti, L. Gerritzen, F. Grancagnolo, E. G. Grandoni, M. Grassi, D. N. Grigoriev, M. Hildebrandt, F. Ignatov, F. Ikeda, T. Iwamoto, S. Karpov, P. -R. Kettle, N. Khomutov, A. Kolesnikov, N. Kravchuk, V. Krylov, N. Kuchinskiy, F. Leonetti, W. Li, V. Malyshev, A. Matsushita, S. Mihara, W. Moltzon, Toshinori Mori, D. Nicolo', H. Nishiguchi, A. Ochi, H. Ogawa, W. Ootani, A. Oya, D. Palo, M. Panareo, A. Papa, D. Pasciuto, A. Popov, F. Renga, S. Ritt, M. Rossella, A. Rozhdestvensky, S. Scarpellini, G. Signorelli, H. Suzuki, M. Takahashi, Y. Uchiyama, R. Umakoshi, A. Venturini, B. Vitali, C. Voena, K. Yamamoto, R. Yokota, T. Yonemoto</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2504.15711v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This letter reports the result of the search for the decay \mu+->e+\gamma undertaken at the Paul Scherrer Institut in Switzerland with the MEG II experiment using the data collected in the 2021- 2022 physics runs. The sensitivity of this search is 2.2x10-13, a factor of 2.4 better than that of the full MEG dataset and obtained in a data taking period of about one fourth that of MEG, thanks to the superior performances of the new detector. The result is consistent with the expected background, yielding an upper limit on the branching ratio of B(\mu+->e+\gamma)<1.5 x 10-13 (90 % C.L.). Additional improvements are expected with the data collected during the years 2023-2024. The data-taking will continue in the coming years.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-15</td>
<td style='padding: 8px;'>Constraints from muon $g-2$ on a gauged non-universal $U(1)_{X}$ model with inverse see-saw neutrinos</td>
<td style='padding: 6px;'>J. S. Alvarado, R. Martinez, Cristian Sierra</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2504.11332v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We study the effects on a non-universal $U(1)_{X}$ extension of the Standard Model given the alternative value obtained by the Budapest-Marseille-Wuppertal (BMW) group for the anomalous magnetic moment of the muon $g-2$. The model explains the fermion mass hierarchy through the non-universality of the extra gauge symmetry and by an additional $\mathbb{Z}_{2}$ discrete symmetry, where the heaviest fermions acquire their masses from two different scales determined by two Higgs doublets and one singlet, whereas the lightest fermions obtain their masses from radiative corrections. From cancellation of chiral anomalies, the model also includes heavy extra fermions, both charged and neutral. The latter are right-handed neutrinos that acquire masses via an inverse see-saw mechanism, reproducing the observed squared mass differences for the active neutrinos. Using the latest lattice calculation of the leading hadronic vacuum polarization (HVP) contribution to the muon $g-2$, we compute the dominant one-loop diagrams mediated by the $W$ and charged Higgs bosons, both with a heavy Majorana neutrino in the loop, setting bounds for masses of the new particles. We also provide predictions for observables that can probe our model in the future such as charged lepton flavor violating searches at Belle II like $\tau\to \mu\gamma$, $\tau\to e\gamma$ and at MEG II for $\mu\to e\gamma$.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-03-21</td>
<td style='padding: 8px;'>Multifractal analysis based on the weak scaling exponent and applications to MEG recordings in neuroscience</td>
<td style='padding: 6px;'>Patrice Abry, Phipippe Ciuciu, Merlin Dumeur, Stéphane Jaffard, Guillaume Saës</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2503.16892v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We develop the mathematical properties of a multifractal analysis of data based on the weak scaling exponent. The advantage of this analysis is that it does not require any a priori global regularity assumption on the analyzed signal, in contrast with the previously used H{\"o}lder or p-exponents. As an illustration, we show that this technique allows one to perform a multifractal analysis of MEG signals, which records electromagnetic brain activity, that was not theoretically valid using the formerly introduced methods based on H{\"o}lder or p-exponents.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-03-28</td>
<td style='padding: 8px;'>Auditing language models for hidden objectives</td>
<td style='padding: 6px;'>Samuel Marks, Johannes Treutlein, Trenton Bricken, Jack Lindsey, Jonathan Marcus, Siddharth Mishra-Sharma, Daniel Ziegler, Emmanuel Ameisen, Joshua Batson, Tim Belonax, Samuel R. Bowman, Shan Carter, Brian Chen, Hoagy Cunningham, Carson Denison, Florian Dietz, Satvik Golechha, Akbir Khan, Jan Kirchner, Jan Leike, Austin Meek, Kei Nishimura-Gasparian, Euan Ong, Christopher Olah, Adam Pearce, Fabien Roger, Jeanne Salle, Andy Shih, Meg Tong, Drake Thomas, Kelley Rivoire, Adam Jermyn, Monte MacDiarmid, Tom Henighan, Evan Hubinger</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2503.10965v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We study the feasibility of conducting alignment audits: investigations into whether models have undesired objectives. As a testbed, we train a language model with a hidden objective. Our training pipeline first teaches the model about exploitable errors in RLHF reward models (RMs), then trains the model to exploit some of these errors. We verify via out-of-distribution evaluations that the model generalizes to exhibit whatever behaviors it believes RMs rate highly, including ones not reinforced during training. We leverage this model to study alignment audits in two ways. First, we conduct a blind auditing game where four teams, unaware of the model's hidden objective or training, investigate it for concerning behaviors and their causes. Three teams successfully uncovered the model's hidden objective using techniques including interpretability with sparse autoencoders (SAEs), behavioral attacks, and training data analysis. Second, we conduct an unblinded follow-up study of eight techniques for auditing the model, analyzing their strengths and limitations. Overall, our work provides a concrete example of using alignment audits to discover a model's hidden objective and proposes a methodology for practicing and validating progress in alignment auditing.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-03-08</td>
<td style='padding: 8px;'>Characterizing optimal monitoring edge-geodetic sets for some structured graph classes</td>
<td style='padding: 6px;'>Florent Foucaud, Arti Pandey, Kaustav Paul</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2503.06086v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Given a graph $G=(V,E)$, a set $S\subseteq V$ is said to be a monitoring edge-geodetic set if the deletion of any edge in the graph results in a change in the distance between at least one pair of vertices in $S$. The minimum size of such a set in $G$ is called the monitoring edge-geodetic number of $G$ and is denoted by $meg(G)$.   In this work, we compute the monitoring edge-geodetic number efficiently for the following graph classes: distance-hereditary graphs, $P_4$-sparse graphs, bipartite permutation graphs, and strongly chordal graphs. The algorithms follow from structural characterizations of the optimal monitoring edge-geodetic sets for these graph classes in terms of \emph{mandatory vertices} (those that need to be in every solution). This extends previous results from the literature for cographs, interval graphs and block graphs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-03-07</td>
<td style='padding: 8px;'>Language-specific Tonal Features Drive Speaker-Listener Neural Synchronization</td>
<td style='padding: 6px;'>Chen Hong, Xiangbin Teng, Yu Li, Shen-Mou Hsu, Feng-Ming Tsao, Patrick C. M. Wong, Gangyi Feng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2503.05211v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Verbal communication transmits information across diverse linguistic levels, with neural synchronization (NS) between speakers and listeners emerging as a putative mechanism underlying successful exchange. However, the specific speech features driving this synchronization and how language-specific versus universal characteristics facilitate information transfer remain poorly understood. We developed a novel content-based interbrain encoding model to disentangle the contributions of acoustic and linguistic features to speaker-listener NS during Mandarin storytelling and listening, as measured via magnetoencephalography (MEG). Results revealed robust NS throughout frontotemporal-parietal networks with systematic time lags between speech production and perception. Crucially, suprasegmental lexical tone features (tone categories, pitch height, and pitch contour), essential for lexical meaning in Mandarin, contributed more significantly to NS than either acoustic elements or universal segmental units (consonants and vowels). These tonal features generated distinctive spatiotemporal NS patterns, creating language-specific neural "communication channels" that facilitated efficient representation sharing between interlocutors. Furthermore, the strength and patterns of NS driven by these language-specific features predicted communication success. These findings demonstrate the neural mechanisms underlying shared representations during verbal exchange and highlight how language-specific features can shape neural coupling to optimize information transfer during human communication.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-02-24</td>
<td style='padding: 8px;'>Forecasting Rare Language Model Behaviors</td>
<td style='padding: 6px;'>Erik Jones, Meg Tong, Jesse Mu, Mohammed Mahfoud, Jan Leike, Roger Grosse, Jared Kaplan, William Fithian, Ethan Perez, Mrinank Sharma</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2502.16797v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Standard language model evaluations can fail to capture risks that emerge only at deployment scale. For example, a model may produce safe responses during a small-scale beta test, yet reveal dangerous information when processing billions of requests at deployment. To remedy this, we introduce a method to forecast potential risks across orders of magnitude more queries than we test during evaluation. We make forecasts by studying each query's elicitation probability -- the probability the query produces a target behavior -- and demonstrate that the largest observed elicitation probabilities predictably scale with the number of queries. We find that our forecasts can predict the emergence of diverse undesirable behaviors -- such as assisting users with dangerous chemical synthesis or taking power-seeking actions -- across up to three orders of magnitude of query volume. Our work enables model developers to proactively anticipate and patch rare failures before they manifest during large-scale deployments.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-02-18</td>
<td style='padding: 8px;'>Brain-to-Text Decoding: A Non-invasive Approach via Typing</td>
<td style='padding: 6px;'>Jarod Lévy, Mingfang Zhang, Svetlana Pinet, Jérémy Rapin, Hubert Banville, Stéphane d'Ascoli, Jean-Rémi King</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2502.17480v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Modern neuroprostheses can now restore communication in patients who have lost the ability to speak or move. However, these invasive devices entail risks inherent to neurosurgery. Here, we introduce a non-invasive method to decode the production of sentences from brain activity and demonstrate its efficacy in a cohort of 35 healthy volunteers. For this, we present Brain2Qwerty, a new deep learning architecture trained to decode sentences from either electro- (EEG) or magneto-encephalography (MEG), while participants typed briefly memorized sentences on a QWERTY keyboard. With MEG, Brain2Qwerty reaches, on average, a character-error-rate (CER) of 32% and substantially outperforms EEG (CER: 67%). For the best participants, the model achieves a CER of 19%, and can perfectly decode a variety of sentences outside of the training set. While error analyses suggest that decoding depends on motor processes, the analysis of typographical errors suggests that it also involves higher-level cognitive factors. Overall, these results narrow the gap between invasive and non-invasive methods and thus open the path for developing safe brain-computer interfaces for non-communicating patients.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-02-18</td>
<td style='padding: 8px;'>From Thought to Action: How a Hierarchy of Neural Dynamics Supports Language Production</td>
<td style='padding: 6px;'>Mingfang Zhang, Jarod Lévy, Stéphane d'Ascoli, Jérémy Rapin, F. -Xavier Alario, Pierre Bourdillon, Svetlana Pinet, Jean-Rémi King</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2502.07429v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Humans effortlessly communicate their thoughts through intricate sequences of motor actions. Yet, the neural processes that coordinate language production remain largely unknown, in part because speech artifacts limit the use of neuroimaging. To elucidate the unfolding of language production in the brain, we investigate with magnetoencephalography (MEG) and electroencephalography (EEG) the neurophysiological activity of 35 skilled typists, while they typed sentences on a keyboard. This approach confirms the hierarchical predictions of linguistic theories: the neural activity preceding the production of each word is marked by the sequential rise and fall of context-, word-, syllable-, and letter-level representations. Remarkably, each of these neural representations is maintained over long time periods within each level of the language hierarchy. This phenomenon results in a superposition of successive representations that is supported by a hierarchy of dynamic neural codes. Overall, these findings provide a precise computational breakdown of the neural dynamics that coordinate the production of language in the human brain.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-03-08</td>
<td style='padding: 8px;'>A 7T fMRI dataset of synthetic images for out-of-distribution modeling of vision</td>
<td style='padding: 6px;'>Alessandro T. Gifford, Radoslaw M. Cichy, Thomas Naselaris, Kendrick Kay</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2503.06286v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD) are boosting NeuroAI research by enabling computational models of the brain with performances beyond what was possible just a decade ago. However, these datasets lack out-of-distribution (OOD) components, which are crucial for the development of more robust models. Here, we address this limitation by releasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the eight NSD subjects for 284 carefully controlled synthetic images. We show that NSD-synthetic's fMRI responses reliably encode stimulus-related information and are OOD with respect to NSD. Furthermore, OOD generalization tests on NSD-synthetic reveal differences between models of the brain that are not detected with NSD - specifically, self-supervised deep neural networks better explain neural responses than their task-supervised counterparts. These results showcase how NSD-synthetic enables OOD generalization tests that facilitate the development of more robust models of visual processing, and the formulation of more accurate theories of human vision.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-02-22</td>
<td style='padding: 8px;'>Brain-Model Evaluations Need the NeuroAI Turing Test</td>
<td style='padding: 6px;'>Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2502.16238v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-04</td>
<td style='padding: 8px;'>Asynchronous Hebbian/anti-Hebbian networks</td>
<td style='padding: 6px;'>Henrique Reis Aguiar, Matthias H. Hennig</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02402v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-03</td>
<td style='padding: 8px;'>NeuroAI for AI Safety</td>
<td style='padding: 6px;'>Patrick Mineault, Niccolò Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.18526v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-21</td>
<td style='padding: 8px;'>Evaluating Representational Similarity Measures from the Lens of Functional Correspondence</td>
<td style='padding: 6px;'>Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.14633v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-09-09</td>
<td style='padding: 8px;'>Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models</td>
<td style='padding: 6px;'>Emily Cheng, Richard J. Antonello</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2409.05771v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first "composition" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-07-22</td>
<td style='padding: 8px;'>Predictive Coding Networks and Inference Learning: Tutorial and Survey</td>
<td style='padding: 6px;'>Björn van Zwol, Ro Jefferson, Egon L. van den Broek</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2407.04117v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent years have witnessed a growing call for renewed emphasis on neuroscience-inspired approaches in artificial intelligence research, under the banner of NeuroAI. A prime example of this is predictive coding networks (PCNs), based on the neuroscientific framework of predictive coding. This framework views the brain as a hierarchical Bayesian inference model that minimizes prediction errors through feedback connections. Unlike traditional neural networks trained with backpropagation (BP), PCNs utilize inference learning (IL), a more biologically plausible algorithm that explains patterns of neural activity that BP cannot. Historically, IL has been more computationally intensive, but recent advancements have demonstrated that it can achieve higher efficiency than BP with sufficient parallelization. Furthermore, PCNs can be mathematically considered a superset of traditional feedforward neural networks (FNNs), significantly extending the range of trainable architectures. As inherently probabilistic (graphical) latent variable models, PCNs provide a versatile framework for both supervised learning and unsupervised (generative) modeling that goes beyond traditional artificial neural networks. This work provides a comprehensive review and detailed formal specification of PCNs, particularly situating them within the context of modern ML methods. Additionally, we introduce a Python library (PRECO) for practical implementation. This positions PC as a promising framework for future ML innovations.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-10-29</td>
<td style='padding: 8px;'>Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis</td>
<td style='padding: 6px;'>Mitchell Ostrow, Adam Eisen, Leo Kozachkov, Ila Fiete</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2306.10168v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-05-25</td>
<td style='padding: 8px;'>Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture</td>
<td style='padding: 6px;'>Galen Pogoncheff, Jacob Granley, Michael Beyeler</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2305.11275v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-11-09</td>
<td style='padding: 8px;'>A Deep Probabilistic Spatiotemporal Framework for Dynamic Graph Representation Learning with Application to Brain Disorder Identification</td>
<td style='padding: 6px;'>Sin-Yee Yap, Junn Yong Loo, Chee-Ming Ting, Fuad Noman, Raphael C. -W. Phan, Adeel Razi, David L. Dowe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2302.07243v4' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent applications of pattern recognition techniques on brain connectome classification using functional connectivity (FC) are shifting towards acknowledging the non-Euclidean topology and dynamic aspects of brain connectivity across time. In this paper, a deep spatiotemporal variational Bayes (DSVB) framework is proposed to learn time-varying topological structures in dynamic FC networks for identifying autism spectrum disorder (ASD) in human participants. The framework incorporates a spatial-aware recurrent neural network with an attention-based message passing scheme to capture rich spatiotemporal patterns across dynamic FC networks. To overcome model overfitting on limited training datasets, an adversarial training strategy is introduced to learn graph embedding models that generalize well to unseen brain networks. Evaluation on the ABIDE resting-state functional magnetic resonance imaging dataset shows that our proposed framework substantially outperforms state-of-the-art methods in identifying patients with ASD. Dynamic FC analyses with DSVB-learned embeddings reveal apparent group differences between ASD and healthy controls in brain network connectivity patterns and switching dynamics of brain states. The code is available at https://github.com/Monash-NeuroAI/Deep-Spatiotemporal-Variational-Bayes.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>Disentangling Reasoning and Knowledge in Medical Large Language Models</td>
<td style='padding: 6px;'>Rahul Thapa, Qingyang Wu, Kevin Wu, Harrison Zhang, Angela Zhang, Eric Wu, Haotian Ye, Suhana Bedi, Nevin Aresh, Joseph Boen, Shriya Reddy, Ben Athiwaratkun, Shuaiwen Leon Song, James Zou</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11462v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Medical reasoning in large language models (LLMs) aims to emulate clinicians' diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and PubMedQA often mix reasoning with factual recall. We address this by separating 11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human performance. Our analysis shows that only 32.8 percent of questions require complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1) and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent gaps between knowledge and reasoning performance. For example, m1 scores 60.5 on knowledge but only 47.1 on reasoning. In adversarial tests where models are misled with incorrect initial reasoning, biomedical models degrade sharply, while larger or RL-trained general models show more robustness. To address this, we train BioMed-R1 using fine-tuning and reinforcement learning on reasoning-heavy examples. It achieves the strongest performance among similarly sized models. Further gains may come from incorporating clinical case reports and training with adversarial and backtracking scenarios.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>Extracting Explainable Dates From Medical Images By Reverse-Engineering UNIX Timestamps</td>
<td style='padding: 6px;'>Lee Harris, James Bentham, Philippe De Wilde</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11451v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Dates often contribute towards highly impactful medical decisions, but it is rarely clear how to extract this data. AI has only just begun to be used transcribe such documents, and common methods are either to trust that the output produced by a complex AI model, or to parse the text using regular expressions. Recent work has established that regular expressions are an explainable form of logic, but it is difficult to decompose these into the component parts that are required to construct precise UNIX timestamps. First, we test publicly-available regular expressions, and we found that these were unable to capture a significant number of our dates. Next, we manually created easily-decomposable regular expressions, and we found that these were able to detect the majority of real dates, but also a lot of sequences of text that look like dates. Finally, we used regular expression synthesis to automatically identify regular expressions from the reverse-engineered UNIX timestamps that we created. We find that regular expressions created by regular expression synthesis detect far fewer sequences of text that look like dates than those that were manually created, at the cost of a slight increase to the number of missed dates. Overall, our results show that regular expressions can be created through regular expression synthesis to identify complex dates and date ranges in text transcriptions. To our knowledge, our proposed way of learning deterministic logic by reverse-engineering several many-one mappings and feeding these into a regular expression synthesiser is a new approach.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs</td>
<td style='padding: 6px;'>Sijia Chen, Xiaomin Li, Mengxue Zhang, Eric Hanchen Jiang, Qingcheng Zeng, Chen-Hsiang Yu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11413v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large language models (LLMs) are increasingly deployed in medical contexts, raising critical concerns about safety, alignment, and susceptibility to adversarial manipulation. While prior benchmarks assess model refusal capabilities for harmful prompts, they often lack clinical specificity, graded harmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES (Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for evaluating LLM safety in healthcare. CARES includes over 18,000 prompts spanning eight medical safety principles, four harm levels, and four prompting styles: direct, indirect, obfuscated, and role-play, to simulate both malicious and benign use cases. We propose a three-way response evaluation protocol (Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess model behavior. Our analysis reveals that many state-of-the-art LLMs remain vulnerable to jailbreaks that subtly rephrase harmful prompts, while also over-refusing safe but atypically phrased queries. Finally, we propose a mitigation strategy using a lightweight classifier to detect jailbreak attempts and steer models toward safer behavior via reminder-based conditioning. CARES provides a rigorous framework for testing and improving medical LLM safety under adversarial and ambiguous conditions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner</td>
<td style='padding: 6px;'>Wenchuan Zhang, Penghao Zhang, Jingru Guo, Tao Cheng, Jie Chen, Shuwan Zhang, Zhang Zhang, Yuhao Yi, Hong Bu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11404v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent advances in vision language models (VLMs) have enabled broad progress in the general medical field. However, pathology still remains a more challenging subdomain, with current pathology specific VLMs exhibiting limitations in both diagnostic accuracy and reasoning plausibility. Such shortcomings are largely attributable to the nature of current pathology datasets, which are primarily composed of image description pairs that lack the depth and structured diagnostic paradigms employed by real world pathologists. In this study, we leverage pathology textbooks and real world pathology experts to construct high-quality, reasoning-oriented datasets. Building on this, we introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs for knowledge infusion; (2) supervised fine-tuning on 500k high-quality Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement learning using Group Relative Policy Optimization and Decoupled Clip and Dynamic sAmpling Policy Optimization strategies for multimodal reasoning quality refinement. To further assess the alignment quality of our dataset, we propose PathoCLIP, trained on the same figure-caption corpus used for continued pretraining. Comprehensive experimental results demonstrate that both PathoCLIP and Patho-R1 achieve robust performance across a wide range of pathology-related tasks, including zero-shot classification, cross-modal retrieval, Visual Question Answering, and Multiple Choice Question. Our project is available at the Patho-R1 repository: https://github.com/Wenchuan-Zhang/Patho-R1.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>Quantum compressed sensing tomographic reconstruction algorithm</td>
<td style='padding: 6px;'>Arim Ryou, Kiwoong Kim, Kyungtaek Jun</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11286v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Computed tomography (CT) is a non-destructive technique for observing internal images and has proven highly valuable in medical diagnostics. Recent advances in quantum computing have begun to influence tomographic reconstruction techniques. The quantum tomographic reconstruction algorithm is less affected by artifacts or noise than classical algorithms by using the square function of the difference between pixels obtained by projecting CT images in quantum superposition states and pixels obtained from experimental data. In particular, by using quantum linear systems, a fast quadratic unconstrained binary optimization (QUBO) model formulation for quantum tomographic reconstruction is possible. In this paper, we formulate the QUBO model for quantum compressed sensing tomographic reconstruction, which is a linear combination of the QUBO model for quantum tomographic reconstruction and the QUBO model for total variation in quantum superposition-state CT images. In our experiments, we used sinograms obtained by using the Radon transform of Shepp-Logan images and body CT images. We evaluate the performance of the new algorithm by reconstructing CT images using a hybrid solver with the QUBO model computed from each sinogram. The new algorithm was able to obtain a solution within 5 projection images for 30 by 30 image samples and within 6 projection images for 60 by 60 image samples, reconstructing error-free CT images. We anticipate that quantum compressed sensing tomographic reconstruction algorithms could significantly reduce the total radiation dose when quantum computing performance advances.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>CheX-DS: Improving Chest X-ray Image Classification with Ensemble Learning Based on DenseNet and Swin Transformer</td>
<td style='padding: 6px;'>Xinran Li, Yu Liu, Xiujuan Xu, Xiaowei Zhao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11168v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The automatic diagnosis of chest diseases is a popular and challenging task. Most current methods are based on convolutional neural networks (CNNs), which focus on local features while neglecting global features. Recently, self-attention mechanisms have been introduced into the field of computer vision, demonstrating superior performance. Therefore, this paper proposes an effective model, CheX-DS, for classifying long-tail multi-label data in the medical field of chest X-rays. The model is based on the excellent CNN model DenseNet for medical imaging and the newly popular Swin Transformer model, utilizing ensemble deep learning techniques to combine the two models and leverage the advantages of both CNNs and Transformers. The loss function of CheX-DS combines weighted binary cross-entropy loss with asymmetric loss, effectively addressing the issue of data imbalance. The NIH ChestX-ray14 dataset is selected to evaluate the model's effectiveness. The model outperforms previous studies with an excellent average AUC score of 83.76\%, demonstrating its superior performance.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>Controlling spatial correlation in k-space interpolation networks for MRI reconstruction: denoising versus apparent blurring</td>
<td style='padding: 6px;'>Istvan Homolya, Peter Dawood, Jannik Stebani, Felix Breuer, Grit Hein, Matthias Gamer, Florian Knoll, Martin Blaimer</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11155v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Purpose: To improve the interpretability of noise amplification and apparent blurring of k-space interpolation networks, and to optimize for them in the loss function as a model-based regularizer in k-space interpolation networks.   Methods: Network is subjected to noise amplification analysis through automatic differentiation of the input with respect to the input. Noise variance maps are decomposed into terms accounting for the linear and nonlinear characteristics of the network. Variance maps are derived in each iteration, allowing for runtime quality monitoring. Maximum variance (eigenpixel) and residual variance maps (pixel contamination) are introduced, which describe the network noise amplification and apparent blurring, respectively. By including the variance maps in the training, the loss function is enriched with a model-based regularizer beyond the k-space data consistency term. Accordingly, the proposed g-factor-informed RAKI (GIF-RAKI) establishes a recurrent flow of noise and apparent blurring information into the training, that drives the denoising via the trainable nonlinear activation function.   Results: GIF-RAKI outperforms other RAKI implementations, supported by difference maps, and image quality metrics. Eigenpixel and pixel contamination maps provide quantitative metrics for noise amplification and apparent blurring, respectively, without the need for a gold standard reference. RAKI with tuneable Leaky ReLU is capable of adjusting its own nonlinearity automatically.   Conclusion: The additional model-based loss terms allow to optimize for the trade-off between denoising and apparent blurring during RAKI training. This has the potential to eliminate the need for heuristic hyperparameter tweaking.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>Determining the utility of ultrafast nonlinear contrast enhanced and super resolution ultrasound for imaging microcirculation in the human small intestine</td>
<td style='padding: 6px;'>Clotilde Vié, Martina Tashkova, James Burn, Matthieu Toulemonde, Jipeng Yan, Jingwen Zhu, Cameron A. B. Smith, Biao Huang, Su Yan, Kevin G. Murphy, Gary Frost, Meng-Xing Tang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11096v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The regulation of intestinal blood flow is critical to gastrointestinal function. Imaging the intestinal mucosal micro-circulation in vivo has the potential to provide new insight into the gut physiology and pathophysiology. We aimed to determine whether ultrafast contrast enhanced ultrasound (CEUS) and super-resolution ultrasound localisation microscopy (SRUS/ULM) could be a useful tool for imaging the small intestine microcirculation in vivo non-invasively and for detecting changes in blood flow in the duodenum. Ultrafast CEUS and SRUS/ULM were used to image the small intestinal microcirculation in a cohort of 20 healthy volunteers (BMI<25). Participants were imaged while conscious and either having been fasted, or following ingestion of a liquid meal or water control, or under acute stress. For the first time we have performed ultrafast CEUS and ULM on the human small intestine, providing unprecedented resolution images of the intestinal microcirculation. We evaluated flow speed inside small vessels in healthy volunteers (2.78 +/- 0.05 mm/s, mean +/- SEM) and quantified changes in the perfusion of this microcirculation in response to nutrient ingestion. Perfusion of the microvasculature of the intestinal mucosa significantly increased post-prandially (36.2% +/- 12.2%, mean +/- SEM, p<0.05). The feasibility of 3D SRUS/ULM was also demonstrated. This study demonstrates the potential utility of ultrafast CEUS for assessing perfusion and detecting changes in blood flow in the duodenum. SRUS/ULM also proved a useful tool to image the microvascular blood flow in vivo non-invasively and to evaluate blood speed inside the microvasculature of the human small intestine.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>CleanPatrick: A Benchmark for Image Data Cleaning</td>
<td style='padding: 6px;'>Fabian Gröger, Simone Lionetti, Philippe Gottfrois, Alvaro Gonzalez-Jimenez, Ludovic Amruthalingam, Elisabeth Victoria Goessinger, Hanna Lindemann, Marie Bargiela, Marie Hofbauer, Omar Badri, Philipp Tschandl, Arash Koochek, Matthew Groh, Alexander A. Navarini, Marc Pouly</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11034v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Robust machine learning depends on clean data, yet current image data cleaning benchmarks rely on synthetic noise or narrow human studies, limiting comparison and real-world relevance. We introduce CleanPatrick, the first large-scale benchmark for data cleaning in the image domain, built upon the publicly available Fitzpatrick17k dermatology dataset. We collect 496,377 binary annotations from 933 medical crowd workers, identify off-topic samples (4%), near-duplicates (21%), and label errors (22%), and employ an aggregation model inspired by item-response theory followed by expert review to derive high-quality ground truth. CleanPatrick formalizes issue detection as a ranking task and adopts typical ranking metrics mirroring real audit workflows. Benchmarking classical anomaly detectors, perceptual hashing, SSIM, Confident Learning, NoiseRank, and SelfClean, we find that, on CleanPatrick, self-supervised representations excel at near-duplicate detection, classical methods achieve competitive off-topic detection under constrained review budgets, and label-error detection remains an open challenge for fine-grained medical classification. By releasing both the dataset and the evaluation framework, CleanPatrick enables a systematic comparison of image-cleaning strategies and paves the way for more reliable data-centric artificial intelligence.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-16</td>
<td style='padding: 8px;'>Rethinking the Mean Teacher Strategy from the Perspective of Self-paced Learning</td>
<td style='padding: 6px;'>Pengchen Zhang, Alan J. X. Guo, Sipin Luo, Zhe Han, Lin Guo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.11018v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Semi-supervised medical image segmentation has attracted significant attention due to its potential to reduce manual annotation costs. The mean teacher (MT) strategy, commonly understood as introducing smoothed, temporally lagged consistency regularization, has demonstrated strong performance across various tasks in this field. In this work, we reinterpret the MT strategy on supervised data as a form of self-paced learning, regulated by the output agreement between the temporally lagged teacher model and the ground truth labels. This idea is further extended to incorporate agreement between a temporally lagged model and a cross-architectural model, which offers greater flexibility in regulating the learning pace and enables application to unlabeled data. Specifically, we propose dual teacher-student learning (DTSL), a framework that introduces two groups of teacher-student models with different architectures. The output agreement between the cross-group teacher and student models is used as pseudo-labels, generated via a Jensen-Shannon divergence-based consensus label generator (CLG). Extensive experiments on popular datasets demonstrate that the proposed method consistently outperforms existing state-of-the-art approaches. Ablation studies further validate the effectiveness of the proposed modules.</td>
</tr>
</tbody>
</table>

