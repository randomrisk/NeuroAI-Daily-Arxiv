<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2024-09-03</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-30</td>
<td style='padding: 8px;'>Role of Data-driven Regional Growth Model in Shaping Brain Folding Patterns</td>
<td style='padding: 6px;'>Jixin Hou, Zhengwang Wu, Xianyan Chen, Dajiang Zhu, Tianming Liu, Gang Li, Xianqiao Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.17334v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The surface morphology of the developing mammalian brain is crucial for understanding brain function and dysfunction. Computational modeling offers valuable insights into the underlying mechanisms for early brain folding. While previous studies generally assume uniform growth, recent findings indicate significant regional variations in brain tissue growth. However, the role of these variations in cortical development remains unclear. In this study, we explored how regional cortical growth affects brain folding patterns. We first developed growth models for typical cortical regions using ML-assisted symbolic regression, based on longitudinal data from over 1,000 infant MRI scans that captured cortical surface area and thickness during perinatal and postnatal brains development. These models were subsequently integrated into computational software to simulate cortical development with anatomically realistic geometric models. We quantified the resulting folding patterns using metrics such as mean curvature, sulcal depth, and gyrification index. Our results demonstrate that regional growth models generate complex brain folding patterns that more closely match actual brains structures, both quantitatively and qualitatively, compared to uniform growth models. Growth magnitude plays a dominant role in shaping folding patterns, while growth trajectory has a minor influence. Moreover, multi-region models better capture the intricacies of brain folding than single-region models. Our results underscore the necessity and importance of incorporating regional growth heterogeneity into brain folding simulations, which could enhance early diagnosis and treatment of cortical malformations and neurodevelopmental disorders such as epilepsy and autism.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-30</td>
<td style='padding: 8px;'>The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by Leveraging Second-Order Information</td>
<td style='padding: 6px;'>Diyuan Wu, Ionut-Vlad Modoranu, Mher Safaryan, Denis Kuznedelev, Dan Alistarh</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.17163v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The rising footprint of machine learning has led to a focus on imposing \emph{model sparsity} as a means of reducing computational and memory costs. For deep neural networks (DNNs), the state-of-the-art accuracy-vs-sparsity is achieved by heuristics inspired by the classical Optimal Brain Surgeon (OBS) framework~\citep{lecun90brain, hassibi1992second, hassibi1993optimal}, which leverages loss curvature information to make better pruning decisions. Yet, these results still lack a solid theoretical understanding, and it is unclear whether they can be improved by leveraging connections to the wealth of work on sparse recovery algorithms. In this paper, we draw new connections between these two areas and present new sparse recovery algorithms inspired by the OBS framework that comes with theoretical guarantees under reasonable assumptions and have strong practical performance. Specifically, our work starts from the observation that we can leverage curvature information in OBS-like fashion upon the projection step of classic iterative sparse recovery algorithms such as IHT. We show for the first time that this leads both to improved convergence bounds under standard assumptions. Furthermore, we present extensions of this approach to the practical task of obtaining accurate sparse DNNs, and validate it experimentally at scale for Transformer-based models on vision and language tasks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-30</td>
<td style='padding: 8px;'>Model-based clustering for covariance matrices via penalized Wishart mixture models</td>
<td style='padding: 6px;'>Andrea Cappozzo, Alessandro Casa</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.17040v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Covariance matrices provide a valuable source of information about complex interactions and dependencies within the data. However, from a clustering perspective, this information has often been underutilized and overlooked. Indeed, commonly adopted distance-based approaches tend to rely primarily on mean levels to characterize and differentiate between groups. Recently, there have been promising efforts to cluster covariance matrices directly, thereby distinguishing groups solely based on the relationships between variables. From a model-based perspective, a probabilistic formalization has been provided by considering a mixture model with component densities following a Wishart distribution. Notwithstanding, this approach faces challenges when dealing with a large number of variables, as the number of parameters to be estimated increases quadratically. To address this issue, we propose a sparse Wishart mixture model, which assumes that the component scale matrices possess a cluster-dependent degree of sparsity. Model estimation is performed by maximizing a penalized log-likelihood, enforcing a covariance graphical lasso penalty on the component scale matrices. This penalty not only reduces the number of non-zero parameters, mitigating the challenges of high-dimensional settings, but also enhances the interpretability of results by emphasizing the most relevant relationships among variables. The proposed methodology is tested on both simulated and real data, demonstrating its ability to unravel the complexities of neuroimaging data and effectively cluster subjects based on the relational patterns among distinct brain regions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-29</td>
<td style='padding: 8px;'>A Computational Framework for Modeling Emergence of Color Vision in the Human Brain</td>
<td style='padding: 6px;'>Atsunobu Kotani, Ren Ng</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.16916v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>It is a mystery how the brain decodes color vision purely from the optic nerve signals it receives, with a core inferential challenge being how it disentangles internal perception with the correct color dimensionality from the unknown encoding properties of the eye. In this paper, we introduce a computational framework for modeling this emergence of human color vision by simulating both the eye and the cortex. Existing research often overlooks how the cortex develops color vision or represents color space internally, assuming that the color dimensionality is known a priori; however, we argue that the visual cortex has the capability and the challenge of inferring the color dimensionality purely from fluctuations in the optic nerve signals. To validate our theory, we introduce a simulation engine for biological eyes based on established vision science and generate optic nerve signals resulting from looking at natural images. Further, we propose a model of cortical learning based on self-supervised principle and show that this model naturally learns to generate color vision by disentangling retinal invariants from the sensory signals. When the retina contains N types of color photoreceptors, our simulation shows that N-dimensional color vision naturally emerges, verified through formal colorimetry. Using this framework, we also present the first simulation work that successfully boosts the color dimensionality, as observed in gene therapy on squirrel monkeys, and demonstrates the possibility of enhancing human color vision from 3D to 4D.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-29</td>
<td style='padding: 8px;'>Probabilistic Decomposed Linear Dynamical Systems for Robust Discovery of Latent Neural Dynamics</td>
<td style='padding: 6px;'>Yenho Chen, Noga Mudrik, Kyle A. Johnsen, Sankaraleengam Alagapan, Adam S. Charles, Christopher J. Rozell</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.16862v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Time-varying linear state-space models are powerful tools for obtaining mathematically interpretable representations of neural signals. For example, switching and decomposed models describe complex systems using latent variables that evolve according to simple locally linear dynamics. However, existing methods for latent variable estimation are not robust to dynamical noise and system nonlinearity due to noise-sensitive inference procedures and limited model formulations. This can lead to inconsistent results on signals with similar dynamics, limiting the model's ability to provide scientific insight. In this work, we address these limitations and propose a probabilistic approach to latent variable estimation in decomposed models that improves robustness against dynamical noise. Additionally, we introduce an extended latent dynamics model to improve robustness against system nonlinearities. We evaluate our approach on several synthetic dynamical systems, including an empirically-derived brain-computer interface experiment, and demonstrate more accurate latent variable inference in nonlinear systems with diverse noise conditions. Furthermore, we apply our method to a real-world clinical neurophysiology dataset, illustrating the ability to identify interpretable and coherent structure where previous models cannot.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-29</td>
<td style='padding: 8px;'>H-SGANet: Hybrid Sparse Graph Attention Network for Deformable Medical Image Registration</td>
<td style='padding: 6px;'>Yufeng Zhou, Wenming Cao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.16719v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The integration of Convolutional Neural Network (ConvNet) and Transformer has emerged as a strong candidate for image registration, leveraging the strengths of both models and a large parameter space. However, this hybrid model, treating brain MRI volumes as grid or sequence structures, faces challenges in accurately representing anatomical connectivity, diverse brain regions, and vital connections contributing to the brain's internal architecture. Concerns also arise regarding the computational expense and GPU memory usage associated with this model. To tackle these issues, a lightweight hybrid sparse graph attention network (H-SGANet) has been developed. This network incorporates a central mechanism, Sparse Graph Attention (SGA), based on a Vision Graph Neural Network (ViG) with predetermined anatomical connections. The SGA module expands the model's receptive field and seamlessly integrates into the network. To further amplify the advantages of the hybrid network, the Separable Self-Attention (SSA) is employed as an enhanced token mixer, integrated with depth-wise convolution to constitute SSAFormer. This strategic integration is designed to more effectively extract long-range dependencies. As a hybrid ConvNet-ViG-Transformer model, H-SGANet offers threefold benefits for volumetric medical image registration. It optimizes fixed and moving images concurrently through a hybrid feature fusion layer and an end-to-end learning framework. Compared to VoxelMorph, a model with a similar parameter count, H-SGANet demonstrates significant performance enhancements of 3.5% and 1.5% in Dice score on the OASIS dataset and LPBA40 dataset, respectively.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-29</td>
<td style='padding: 8px;'>Human-Inspired Audio-Visual Speech Recognition: Spike Activity, Cueing Interaction and Causal Processing</td>
<td style='padding: 6px;'>Qianhui Liu, Jiadong Wang, Yang Wang, Xin Yang, Gang Pan, Haizhou Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.16564v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Humans naturally perform audiovisual speech recognition (AVSR), enhancing the accuracy and robustness by integrating auditory and visual information. Spiking neural networks (SNNs), which mimic the brain's information-processing mechanisms, are well-suited for emulating the human capability of AVSR. Despite their potential, research on SNNs for AVSR is scarce, with most existing audio-visual multimodal methods focused on object or digit recognition. These models simply integrate features from both modalities, neglecting their unique characteristics and interactions. Additionally, they often rely on future information for current processing, which increases recognition latency and limits real-time applicability. Inspired by human speech perception, this paper proposes a novel human-inspired SNN named HI-AVSNN for AVSR, incorporating three key characteristics: cueing interaction, causal processing and spike activity. For cueing interaction, we propose a visual-cued auditory attention module (VCA2M) that leverages visual cues to guide attention to auditory features. We achieve causal processing by aligning the SNN's temporal dimension with that of visual and auditory features and applying temporal masking to utilize only past and current information. To implement spike activity, in addition to using SNNs, we leverage the event camera to capture lip movement as spikes, mimicking the human retina and providing efficient visual data. We evaluate HI-AVSNN on an audiovisual speech recognition dataset combining the DVS-Lip dataset with its corresponding audio samples. Experimental results demonstrate the superiority of our proposed fusion method, outperforming existing audio-visual SNN fusion methods and achieving a 2.27% improvement in accuracy over the only existing SNN-based AVSR method.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-29</td>
<td style='padding: 8px;'>Desynchronization Index: a New Approach for Exploring Complex Epileptogenic Networks in Stereoelectroencephalography</td>
<td style='padding: 6px;'>Federico Mason, Lorenzo Ferri, Lidia Di Vito, Lara Alvisi, Luca Zanuttini, Matteo Martinoni, Roberto Mai, Francesco Cardinale, Paolo Tinuper, Roberto Michelucci, Elena Pasini, Francesca Bisulli</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.16347v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Stereoelectroencephalography (SEEG) is an invasive surgical procedure to record the electrical activities in cortical brain regions, aiming at identifying the Epileptogenic Zone (EZ) in patients with drug-resistant epilepsy. To improve the accuracy of the EZ definition, SEEG analysis can be supported by computational tools, among which the Epileptogenic Index (EI) represents the most common solution. However, the scientific community has still not found an agreement on which quantitative biomarkers can characterize the cortical sites within the EZ. In this work, we design a new algorithm, named Desynchronization Index (DI), to assist neurophysiologists in SEEG interpretation. Our algorithm estimates the effective connectivity between cortical sites and hypothesizes that the EZ is identified by those sites getting abnormally desynchronized from the network during the seizure generation. We test the proposed method over a SEEG dataset of 10 seizures, comparing its accuracy in terms of EZ definition against the EI algorithm and clinical ground truth. Our results indicate that the DI algorithm underscores specific connectivity dynamics that can hardly be identified with a pure visual analysis, increasing sensitivity in detecting epileptogenic cortical sites.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-29</td>
<td style='padding: 8px;'>Action potential dynamics on heterogenous neural networks: from kinetic to macroscopic equations</td>
<td style='padding: 6px;'>Marzia Bisi, Martina Conte, Maria Groppi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.16214v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In the context of multi-agent systems of binary interacting particles, a kinetic model for action potential dynamics on a neural network is proposed, accounting for heterogeneity in the neuron-to-neuron connections, as well as in the brain structure. Two levels of description are coupled: in a single area, pairwise neuron interactions for the exchange of membrane potential are statistically described; among different areas, a graph description of the brain network topology is included. Equilibria of the kinetic and macroscopic settings are determined and numerical simulations of the system dynamics are performed with the aim of studying the influence of the network heterogeneities on the membrane potential propagation and synchronization.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-28</td>
<td style='padding: 8px;'>DrowzEE-G-Mamba: Leveraging EEG and State Space Models for Driver Drowsiness Detection</td>
<td style='padding: 6px;'>Gourav Siddhad, Sayantan Dey, Partha Pratim Roy</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.16145v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Driver drowsiness is identified as a critical factor in road accidents, necessitating robust detection systems to enhance road safety. This study proposes a driver drowsiness detection system, DrowzEE-G-Mamba, that combines Electroencephalography (EEG) with State Space Models (SSMs). EEG data, known for its sensitivity to alertness, is used to model driver state transitions between alert and drowsy. Compared to traditional methods, DrowzEE-G-Mamba achieves significantly improved detection rates and reduced false positives. Notably, it achieves a peak accuracy of 83.24% on the SEED-VIG dataset, surpassing existing techniques. The system maintains high accuracy across varying complexities, making it suitable for real-time applications with limited resources. This robustness is attributed to the combination of channel-split, channel-concatenation, and channel-shuffle operations within the architecture, optimizing information flow from EEG data. Additionally, the integration of convolutional layers and SSMs facilitates comprehensive analysis, capturing both local features and long-range dependencies in the EEG signals. These findings suggest the potential of DrowzEE-G-Mamba for enhancing road safety through accurate drowsiness detection. It also paves the way for developing powerful SSM-based AI algorithms in Brain-Computer Interface applications.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-30</td>
<td style='padding: 8px;'>Efficient Estimation of Unique Components in Independent Component Analysis by Matrix Representation</td>
<td style='padding: 6px;'>Yoshitatsu Matsuda, Kazunori Yamaguch</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.17118v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Independent component analysis (ICA) is a widely used method in various applications of signal processing and feature extraction. It extends principal component analysis (PCA) and can extract important and complicated components with small variances. One of the major problems of ICA is that the uniqueness of the solution is not guaranteed, unlike PCA. That is because there are many local optima in optimizing the objective function of ICA. It has been shown previously that the unique global optimum of ICA can be estimated from many random initializations by handcrafted thread computation. In this paper, the unique estimation of ICA is highly accelerated by reformulating the algorithm in matrix representation and reducing redundant calculations. Experimental results on artificial datasets and EEG data verified the efficiency of the proposed method.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-29</td>
<td style='padding: 8px;'>Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach</td>
<td style='padding: 6px;'>Yifei Chen, Shenghao Zhu, Zhaojie Fang, Chang Liu, Binfeng Zou, Yuhe Wang, Shuo Chang, Fan Jia, Feiwei Qin, Jin Fan, Yong Peng, Changmiao Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.16343v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Alzheimer's Disease (AD) is a complex neurodegenerative disorder marked by memory loss, executive dysfunction, and personality changes. Early diagnosis is challenging due to subtle symptoms and varied presentations, often leading to misdiagnosis with traditional unimodal diagnostic methods due to their limited scope. This study introduces an advanced multimodal classification model that integrates clinical, cognitive, neuroimaging, and EEG data to enhance diagnostic accuracy. The model incorporates a feature tagger with a tabular data coding architecture and utilizes the TimesBlock module to capture intricate temporal patterns in Electroencephalograms (EEG) data. By employing Cross-modal Attention Aggregation module, the model effectively fuses Magnetic Resonance Imaging (MRI) spatial information with EEG temporal data, significantly improving the distinction between AD, Mild Cognitive Impairment, and Normal Cognition. Simultaneously, we have constructed the first AD classification dataset that includes three modalities: EEG, MRI, and tabular data. Our innovative approach aims to facilitate early diagnosis and intervention, potentially slowing the progression of AD. The source code and our private ADMC dataset are available at https://github.com/JustlfC03/MSTNet.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-29</td>
<td style='padding: 8px;'>Passenger hazard perception based on EEG signals for highly automated driving vehicles</td>
<td style='padding: 6px;'>Ashton Yu Xuan Tan, Yingkai Yang, Xiaofei Zhang, Bowen Li, Xiaorong Gao, Sifa Zheng, Jianqiang Wang, Xinyu Gu, Jun Li, Yang Zhao, Yuxin Zhang, Tania Stathaki</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.16315v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Enhancing the safety of autonomous vehicles is crucial, especially given recent accidents involving automated systems. As passengers in these vehicles, humans' sensory perception and decision-making can be integrated with autonomous systems to improve safety. This study explores neural mechanisms in passenger-vehicle interactions, leading to the development of a Passenger Cognitive Model (PCM) and the Passenger EEG Decoding Strategy (PEDS). Central to PEDS is a novel Convolutional Recurrent Neural Network (CRNN) that captures spatial and temporal EEG data patterns. The CRNN, combined with stacking algorithms, achieves an accuracy of $85.0\% \pm 3.18\%$. Our findings highlight the predictive power of pre-event EEG data, enhancing the detection of hazardous scenarios and offering a network-driven framework for safer autonomous vehicles.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-28</td>
<td style='padding: 8px;'>DrowzEE-G-Mamba: Leveraging EEG and State Space Models for Driver Drowsiness Detection</td>
<td style='padding: 6px;'>Gourav Siddhad, Sayantan Dey, Partha Pratim Roy</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.16145v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Driver drowsiness is identified as a critical factor in road accidents, necessitating robust detection systems to enhance road safety. This study proposes a driver drowsiness detection system, DrowzEE-G-Mamba, that combines Electroencephalography (EEG) with State Space Models (SSMs). EEG data, known for its sensitivity to alertness, is used to model driver state transitions between alert and drowsy. Compared to traditional methods, DrowzEE-G-Mamba achieves significantly improved detection rates and reduced false positives. Notably, it achieves a peak accuracy of 83.24% on the SEED-VIG dataset, surpassing existing techniques. The system maintains high accuracy across varying complexities, making it suitable for real-time applications with limited resources. This robustness is attributed to the combination of channel-split, channel-concatenation, and channel-shuffle operations within the architecture, optimizing information flow from EEG data. Additionally, the integration of convolutional layers and SSMs facilitates comprehensive analysis, capturing both local features and long-range dependencies in the EEG signals. These findings suggest the potential of DrowzEE-G-Mamba for enhancing road safety through accurate drowsiness detection. It also paves the way for developing powerful SSM-based AI algorithms in Brain-Computer Interface applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-28</td>
<td style='padding: 8px;'>Exploring the potential of AI in nurturing learner empathy, prosocial values and environmental stewardship</td>
<td style='padding: 6px;'>Kenneth Y T Lim, Minh Anh Nguyen Duc, Minh Tuan Nguyen Thien</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.15906v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>With Artificial Intelligence (AI) becoming a powerful tool for education (Zawacki-Richter et al., 2019), this chapter describes the concept of combining generative and traditional AI, citizen-science physiological, neuroergonomic wearables and environmental sensors into activities for learners to understand their own well-being and emotional states better with a view to developing empathy and environmental stewardship. Alongside bespoke and affordable wearables (DIY EEG headsets and biometric wristbands), interpretable AI and data science are used for learners to explore how the environment affects them physiologically and mentally in authentic environments. For example, relationships between environmental changes (e.g. poorer air quality) and their well-being (e.g. cognitive functioning) can be discovered. This is particularly crucial, as relevant knowledge can influence the way people treat the environment, as suggested by the disciplines of environmental neuroscience and environmental psychology (Doell et al., 2023). Yet, according to Palme and Salvati, there have been relatively few studies on the relationships between microclimates and human health and emotions (Palme and Salvati, 2021). As anthropogenic environmental pollution is becoming a prevalent problem, our research also aims to leverage on generative AI to introduce hypothetical scenarios of the environment as emotionally strong stimuli of relevance to the learners. This would provoke an emotional response for them to learn about their own physiological and neurological responses (using neuro-physiological data). Ultimately, we hope to establish a bidirectional understanding of how the environment affects humans physiologically and mentally; after which, to gain insights as to how AI can be used to effectively foster empathy, pro-environmental attitudes and stewardship.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-25</td>
<td style='padding: 8px;'>Neural Networks Meet Neural Activity: Utilizing EEG for Mental Workload Estimation</td>
<td style='padding: 6px;'>Gourav Siddhad, Partha Pratim Roy, Byung-Gyu Kim</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.13930v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) offers non-invasive, real-time mental workload assessment, which is crucial in high-stakes domains like aviation and medicine and for advancing brain-computer interface (BCI) technologies. This study introduces a customized ConvNeXt architecture, a powerful convolutional neural network, specifically adapted for EEG analysis. ConvNeXt addresses traditional EEG challenges like high dimensionality, noise, and variability, enhancing the precision of mental workload classification. Using the STEW dataset, the proposed ConvNeXt model is evaluated alongside SVM, EEGNet, and TSception on binary (No vs SIMKAP task) and ternary (SIMKAP multitask) class mental workload tasks. Results demonstrated that ConvNeXt significantly outperformed the other models, achieving accuracies of 95.76% for binary and 95.11% for multi-class classification. This demonstrates ConvNeXt's resilience and efficiency for EEG data analysis, establishing new standards for mental workload evaluation. These findings represent a considerable advancement in EEG-based mental workload estimation, laying the foundation for future improvements in cognitive state measurements. This has broad implications for safety, efficiency, and user experience across various scenarios. Integrating powerful neural networks such as ConvNeXt is a critical step forward in non-invasive cognitive monitoring.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-25</td>
<td style='padding: 8px;'>Awake at the Wheel: Enhancing Automotive Safety through EEG-Based Fatigue Detection</td>
<td style='padding: 6px;'>Gourav Siddhad, Sayantan Dey, Partha Pratim Roy, Masakazu Iwamura</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.13929v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Driver fatigue detection is increasingly recognized as critical for enhancing road safety. This study introduces a method for detecting driver fatigue using the SEED-VIG dataset, a well-established benchmark in EEG-based vigilance analysis. By employing advanced pattern recognition technologies, including machine learning and deep neural networks, EEG signals are meticulously analyzed to discern patterns indicative of fatigue. This methodology combines feature extraction with a classification framework to improve the accuracy of fatigue detection. The proposed NLMDA-Net reached an impressive accuracy of 83.71% in detecting fatigue from EEG signals by incorporating two novel attention modules designed specifically for EEG signals, the channel and depth attention modules. NLMDA-Net effectively integrate features from multiple dimensions, resulting in improved classification performance. This success stems from integrating temporal convolutions and attention mechanisms, which effectively interpret EEG data. Designed to capture both temporal and spatial characteristics of EEG signals, deep learning classifiers have proven superior to traditional methods. The results of this study reveal a substantial enhancement in detection rates over existing models, highlighting the efficacy of the proposed approach for practical applications. The implications of this research are profound, extending beyond academic realms to inform the development of more sophisticated driver assistance systems. Incorporating this fatigue detection algorithm into these systems could significantly reduce fatigue-related incidents on the road, thus fostering safer driving conditions. This paper provides an exhaustive analysis of the dataset, methods employed, results obtained, and the potential real-world applications of the findings, aiming to contribute significantly to advancements in automotive safety.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-27</td>
<td style='padding: 8px;'>Quantum Multimodal Contrastive Learning Framework</td>
<td style='padding: 6px;'>Chi-Sheng Chen, Aidan Hung-Wen Tsai, Sheng-Chieh Huang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.13919v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In this paper, we propose a novel framework for multimodal contrastive learning utilizing a quantum encoder to integrate EEG (electroencephalogram) and image data. This groundbreaking attempt explores the integration of quantum encoders within the traditional multimodal learning framework. By leveraging the unique properties of quantum computing, our method enhances the representation learning capabilities, providing a robust framework for analyzing time series and visual information concurrently. We demonstrate that the quantum encoder effectively captures intricate patterns within EEG signals and image features, facilitating improved contrastive learning across modalities. This work opens new avenues for integrating quantum computing with multimodal data analysis, particularly in applications requiring simultaneous interpretation of temporal and visual data.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-23</td>
<td style='padding: 8px;'>Symbolic dynamics of joint brain states during dyadic coordination</td>
<td style='padding: 6px;'>Italo Ivo Lima Dias Pinto, Zhibin Zhou, Javier O. Garcia, Ramesh Srinivasan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.13360v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We propose a novel approach to investigate the brain mechanisms that support coordination of behavior between individuals. Brain states in single individuals defined by the patterns of functional connectivity between brain regions are used to create joint symbolic representations of the evolution of brain states in two or more individuals performing a task together. These symbolic dynamics can be analyzed to reveal aspects of the dynamics of joint brain states that are related to coordination or other interactive behaviors. We apply this approach to simultaneous electroencephalographic (EEG) data from pairs of subjects engaged in two different modes of finger-tapping coordination tasks (synchronization and syncopation) under different interaction conditions (Uncoupled, Leader-Follower, and Mutual) to explore the neural mechanisms of multi-person motor coordination. Our results reveal that the dyads exhibit mostly the same joint symbols in different interaction conditions - the most important differences are reflected in the symbolic dynamics. Recurrence analysis shows that interaction influences the dwell time in specific joint symbols and the structure of joint symbol sequences (motif length). In synchronization, increasing feedback promotes stability with longer dwell times and motif length. In syncopation, Leader-Follower interactions enhance stability (increase dwell time and motif length), but Mutual feedback dramatically reduces stability. Network analysis reveals distinct topological changes with task and feedback. In synchronization, stronger coupling stabilizes a few states restricting the pattern of flow between states, preserving a core-periphery structure of the joint brain states. In syncopation, a more distributed flow amongst a larger set of joint brain states reduces the dominance of core joint brain states.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-22</td>
<td style='padding: 8px;'>Recording Brain Activity While Listening to Music Using Wearable EEG Devices Combined with Bidirectional Long Short-Term Memory Networks</td>
<td style='padding: 6px;'>Jingyi Wang, Zhiqun Wang, Guiran Liu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.12124v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) signals are crucial for investigating brain function and cognitive processes. This study aims to address the challenges of efficiently recording and analyzing high-dimensional EEG signals while listening to music to recognize emotional states. We propose a method combining Bidirectional Long Short-Term Memory (Bi-LSTM) networks with attention mechanisms for EEG signal processing. Using wearable EEG devices, we collected brain activity data from participants listening to music. The data was preprocessed, segmented, and Differential Entropy (DE) features were extracted. We then constructed and trained a Bi-LSTM model to enhance key feature extraction and improve emotion recognition accuracy. Experiments were conducted on the SEED and DEAP datasets. The Bi-LSTM-AttGW model achieved 98.28% accuracy on the SEED dataset and 92.46% on the DEAP dataset in multi-class emotion recognition tasks, significantly outperforming traditional models such as SVM and EEG-Net. This study demonstrates the effectiveness of combining Bi-LSTM with attention mechanisms, providing robust technical support for applications in brain-computer interfaces (BCI) and affective computing. Future work will focus on improving device design, incorporating multimodal data, and further enhancing emotion recognition accuracy, aiming to achieve practical applications in real-world scenarios.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-28</td>
<td style='padding: 8px;'>Research Advances and New Paradigms for Biology-inspired Spiking Neural Networks</td>
<td style='padding: 6px;'>Tianyu Zheng, Liyuan Han, Tielin Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.13996v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Spiking neural networks (SNNs) are gaining popularity in the computational simulation and artificial intelligence fields owing to their biological plausibility and computational efficiency. This paper explores the historical development of SNN and concludes that these two fields are intersecting and merging rapidly. Following the successful application of Dynamic Vision Sensors (DVS) and Dynamic Audio Sensors (DAS), SNNs have found some proper paradigms, such as continuous visual signal tracking, automatic speech recognition, and reinforcement learning for continuous control, that have extensively supported their key features, including spike encoding, neuronal heterogeneity, specific functional circuits, and multiscale plasticity. Compared to these real-world paradigms, the brain contains a spiking version of the biology-world paradigm, which exhibits a similar level of complexity and is usually considered a mirror of the real world. Considering the projected rapid development of invasive and parallel Brain-Computer Interface (BCI), as well as the new BCI-based paradigms that include online pattern recognition and stimulus control of biological spike trains, SNNs naturally leverage their advantages in energy efficiency, robustness, and flexibility. The biological brain has inspired the present study of SNNs and effective SNN machine-learning algorithms, which can help enhance neuroscience discoveries in the brain by applying them to the new BCI paradigm. Such two-way interactions with positive feedback can accelerate brain science research and brain-inspired intelligence technology.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-25</td>
<td style='padding: 8px;'>Neural Networks Meet Neural Activity: Utilizing EEG for Mental Workload Estimation</td>
<td style='padding: 6px;'>Gourav Siddhad, Partha Pratim Roy, Byung-Gyu Kim</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.13930v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) offers non-invasive, real-time mental workload assessment, which is crucial in high-stakes domains like aviation and medicine and for advancing brain-computer interface (BCI) technologies. This study introduces a customized ConvNeXt architecture, a powerful convolutional neural network, specifically adapted for EEG analysis. ConvNeXt addresses traditional EEG challenges like high dimensionality, noise, and variability, enhancing the precision of mental workload classification. Using the STEW dataset, the proposed ConvNeXt model is evaluated alongside SVM, EEGNet, and TSception on binary (No vs SIMKAP task) and ternary (SIMKAP multitask) class mental workload tasks. Results demonstrated that ConvNeXt significantly outperformed the other models, achieving accuracies of 95.76% for binary and 95.11% for multi-class classification. This demonstrates ConvNeXt's resilience and efficiency for EEG data analysis, establishing new standards for mental workload evaluation. These findings represent a considerable advancement in EEG-based mental workload estimation, laying the foundation for future improvements in cognitive state measurements. This has broad implications for safety, efficiency, and user experience across various scenarios. Integrating powerful neural networks such as ConvNeXt is a critical step forward in non-invasive cognitive monitoring.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-22</td>
<td style='padding: 8px;'>Recording Brain Activity While Listening to Music Using Wearable EEG Devices Combined with Bidirectional Long Short-Term Memory Networks</td>
<td style='padding: 6px;'>Jingyi Wang, Zhiqun Wang, Guiran Liu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.12124v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) signals are crucial for investigating brain function and cognitive processes. This study aims to address the challenges of efficiently recording and analyzing high-dimensional EEG signals while listening to music to recognize emotional states. We propose a method combining Bidirectional Long Short-Term Memory (Bi-LSTM) networks with attention mechanisms for EEG signal processing. Using wearable EEG devices, we collected brain activity data from participants listening to music. The data was preprocessed, segmented, and Differential Entropy (DE) features were extracted. We then constructed and trained a Bi-LSTM model to enhance key feature extraction and improve emotion recognition accuracy. Experiments were conducted on the SEED and DEAP datasets. The Bi-LSTM-AttGW model achieved 98.28% accuracy on the SEED dataset and 92.46% on the DEAP dataset in multi-class emotion recognition tasks, significantly outperforming traditional models such as SVM and EEG-Net. This study demonstrates the effectiveness of combining Bi-LSTM with attention mechanisms, providing robust technical support for applications in brain-computer interfaces (BCI) and affective computing. Future work will focus on improving device design, incorporating multimodal data, and further enhancing emotion recognition accuracy, aiming to achieve practical applications in real-world scenarios.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-22</td>
<td style='padding: 8px;'>Emotion-Agent: Unsupervised Deep Reinforcement Learning with Distribution-Prototype Reward for Continuous Emotional EEG Analysis</td>
<td style='padding: 6px;'>Zhihao Zhou, Qile Liu, Jiyuan Wang, Zhen Liang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.12121v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Continuous electroencephalography (EEG) signals are widely used in affective brain-computer interface (aBCI) applications. However, not all continuously collected EEG signals are relevant or meaningful to the task at hand (e.g., wondering thoughts). On the other hand, manually labeling the relevant parts is nearly impossible due to varying engagement patterns across different tasks and individuals. Therefore, effectively and efficiently identifying the important parts from continuous EEG recordings is crucial for downstream BCI tasks, as it directly impacts the accuracy and reliability of the results. In this paper, we propose a novel unsupervised deep reinforcement learning framework, called Emotion-Agent, to automatically identify relevant and informative emotional moments from continuous EEG signals. Specifically, Emotion-Agent involves unsupervised deep reinforcement learning combined with a heuristic algorithm. We first use the heuristic algorithm to perform an initial global search and form prototype representations of the EEG signals, which facilitates the efficient exploration of the signal space and identify potential regions of interest. Then, we design distribution-prototype reward functions to estimate the interactions between samples and prototypes, ensuring that the identified parts are both relevant and representative of the underlying emotional states. Emotion-Agent is trained using Proximal Policy Optimization (PPO) to achieve stable and efficient convergence. Our experiments compare the performance with and without Emotion-Agent. The results demonstrate that selecting relevant and informative emotional parts before inputting them into downstream tasks enhances the accuracy and reliability of aBCI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-08</td>
<td style='padding: 8px;'>Masked EEG Modeling for Driving Intention Prediction</td>
<td style='padding: 6px;'>Jinzhao Zhou, Justin Sia, Yiqun Duan, Yu-Cheng Chang, Yu-Kai Wang, Chin-Teng Lin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.07083v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Driving under drowsy conditions significantly escalates the risk of vehicular accidents. Although recent efforts have focused on using electroencephalography to detect drowsiness, helping prevent accidents caused by driving in such states, seamless human-machine interaction in driving scenarios requires a more versatile EEG-based system. This system should be capable of understanding a driver's intention while demonstrating resilience to artifacts induced by sudden movements. This paper pioneers a novel research direction in BCI-assisted driving, studying the neural patterns related to driving intentions and presenting a novel method for driving intention prediction. In particular, our preliminary analysis of the EEG signal using independent component analysis suggests a close relation between the intention of driving maneuvers and the neural activities in central-frontal and parietal areas. Power spectral density analysis at a group level also reveals a notable distinction among various driving intentions in the frequency domain. To exploit these brain dynamics, we propose a novel Masked EEG Modeling framework for predicting human driving intentions, including the intention for left turning, right turning, and straight proceeding. Extensive experiments, encompassing comprehensive quantitative and qualitative assessments on public dataset, demonstrate the proposed method is proficient in predicting driving intentions across various vigilance states. Specifically, our model attains an accuracy of 85.19% when predicting driving intentions for drowsy subjects, which shows its promising potential for mitigating traffic accidents related to drowsy driving. Notably, our method maintains over 75% accuracy when more than half of the channels are missing or corrupted, underscoring its adaptability in real-life driving.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-08</td>
<td style='padding: 8px;'>Towards Linguistic Neural Representation Learning and Sentence Retrieval from Electroencephalogram Recordings</td>
<td style='padding: 6px;'>Jinzhao Zhou, Yiqun Duan, Ziyi Zhao, Yu-Cheng Chang, Yu-Kai Wang, Thomas Do, Chin-Teng Lin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.04679v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Decoding linguistic information from non-invasive brain signals using EEG has gained increasing research attention due to its vast applicational potential. Recently, a number of works have adopted a generative-based framework to decode electroencephalogram (EEG) signals into sentences by utilizing the power generative capacity of pretrained large language models (LLMs). However, this approach has several drawbacks that hinder the further development of linguistic applications for brain-computer interfaces (BCIs). Specifically, the ability of the EEG encoder to learn semantic information from EEG data remains questionable, and the LLM decoder's tendency to generate sentences based on its training memory can be hard to avoid. These issues necessitate a novel approach for converting EEG signals into sentences. In this paper, we propose a novel two-step pipeline that addresses these limitations and enhances the validity of linguistic EEG decoding research. We first confirm that word-level semantic information can be learned from EEG data recorded during natural reading by training a Conformer encoder via a masked contrastive objective for word-level classification. To achieve sentence decoding results, we employ a training-free retrieval method to retrieve sentences based on the predictions from the EEG encoder. Extensive experiments and ablation studies were conducted in this paper for a comprehensive evaluation of the proposed approach. Visualization of the top prediction candidates reveals that our model effectively groups EEG segments into semantic categories with similar meanings, thereby validating its ability to learn patterns from unspoken EEG recordings. Despite the exploratory nature of this work, these results suggest that our method holds promise for providing more reliable solutions for converting EEG signals into text.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-07</td>
<td style='padding: 8px;'>Knowledge-based Emotion Recognition using Large Language Models</td>
<td style='padding: 6px;'>Bin Han, Cleo Yau, Su Lei, Jonathan Gratch</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.04123v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Emotion recognition in social situations is a complex task that requires integrating information from both facial expressions and the situational context. While traditional approaches to automatic emotion recognition have focused on decontextualized signals, recent research emphasizes the importance of context in shaping emotion perceptions. This paper contributes to the emerging field of context-based emotion recognition by leveraging psychological theories of human emotion perception to inform the design of automated methods. We propose an approach that combines emotion recognition methods with Bayesian Cue Integration (BCI) to integrate emotion inferences from decontextualized facial expressions and contextual knowledge inferred via Large-language Models. We test this approach in the context of interpreting facial expressions during a social task, the prisoner's dilemma. Our results provide clear support for BCI across a range of automatic emotion recognition methods. The best automated method achieved results comparable to human observers, suggesting the potential for this approach to advance the field of affective computing.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-06</td>
<td style='padding: 8px;'>EEGMobile: Enhancing Speed and Accuracy in EEG-Based Gaze Prediction with Advanced Mobile Architectures</td>
<td style='padding: 6px;'>Teng Liang, Andrews Damoah</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.03449v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) analysis is an important domain in the realm of Brain-Computer Interface (BCI) research. To ensure BCI devices are capable of providing practical applications in the real world, brain signal processing techniques must be fast, accurate, and resource-conscious to deliver low-latency neural analytics. This study presents a model that leverages a pre-trained MobileViT alongside Knowledge Distillation (KD) for EEG regression tasks. Our results showcase that this model is capable of performing at a level comparable (only 3% lower) to the previous State-Of-The-Art (SOTA) on the EEGEyeNet Absolute Position Task while being 33% faster and 60% smaller. Our research presents a cost-effective model applicable to resource-constrained devices and contributes to expanding future research on lightweight, mobile-friendly models for EEG regression.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-02</td>
<td style='padding: 8px;'>A modular open-source software platform for BCI research with application in closed-loop deep brain stimulation</td>
<td style='padding: 6px;'>Matthias Dold, Joana Pereira, Bastian Sajonz, Volker A. Coenen, Marcus L. F. Janssen, Michael Tangermann</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.01242v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This work introduces Dareplane, a modular and broad technology agnostic open source software platform for brain-computer interface research with an application focus on adaptive deep brain stimulation (aDBS). While the search for suitable biomarkers to inform aDBS has provided rich results over the last two decades, development of control strategies is not progressing at the same pace. One difficulty for investigating control approaches resides with the complex setups required for aDBS experiments. The Dareplane platform supports aDBS setups, and more generally brain computer interfaces, by providing a modular, technology-agnostic, and easy-to-implement software platform to make experimental setups more resilient and replicable. The key features of the platform are presented and the composition of modules into a full experimental setup is discussed in the context of a Python-based orchestration module. The performance of a typical experimental setup on Dareplane for aDBS is evaluated in three benchtop experiments, covering (a) an easy-to-replicate setup using an Arduino microcontroller, (b) a setup with hardware of an implantable pulse generator, and (c) a setup using an established and CE certified external neurostimulator. Benchmark results are presented for individual processing steps and full closed-loop processing. The results show that the microcontroller setup in (a) provides timing comparable to the realistic setups in (b) and (c). The Dareplane platform was successfully used in a total of 19 open-loop DBS sessions with externalized DBS and electrocorticography (ECoG) leads. In addition, the full technical feasibility of the platform in the aDBS context is demonstrated in a first closed-loop session with externalized leads on a patient with Parkinson's disease receiving DBS treatment.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-07-30</td>
<td style='padding: 8px;'>Decoding Linguistic Representations of Human Brain</td>
<td style='padding: 6px;'>Yu Wang, Heyang Liu, Yuhao Wang, Chuan Xuan, Yixuan Hou, Sheng Feng, Hongcheng Liu, Yusheng Liao, Yanfeng Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2407.20622v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Language, as an information medium created by advanced organisms, has always been a concern of neuroscience regarding how it is represented in the brain. Decoding linguistic representations in the evoked brain has shown groundbreaking achievements, thanks to the rapid improvement of neuroimaging, medical technology, life sciences and artificial intelligence. In this work, we present a taxonomy of brain-to-language decoding of both textual and speech formats. This work integrates two types of research: neuroscience focusing on language understanding and deep learning-based brain decoding. Generating discernible language information from brain activity could not only help those with limited articulation, especially amyotrophic lateral sclerosis (ALS) patients but also open up a new way for the next generation's brain-computer interface (BCI). This article will help brain scientists and deep-learning researchers to gain a bird's eye view of fine-grained language perception, and thus facilitate their further investigation and research of neural process and language decoding.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-28</td>
<td style='padding: 8px;'>Leveraging Persistent Homology for Differential Diagnosis of Mild Cognitive Impairment</td>
<td style='padding: 6px;'>Ninad Aithal, Debanjali Bhattacharya, Neelam Sinha, Thomas Gregor Issac</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.15647v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Mild cognitive impairment (MCI) is characterized by subtle changes in cognitive functions, often associated with disruptions in brain connectivity. The present study introduces a novel fine-grained analysis to examine topological alterations in neurodegeneration pertaining to six different brain networks of MCI subjects (Early/Late MCI). To achieve this, fMRI time series from two distinct populations are investigated: (i) the publicly accessible ADNI dataset and (ii) our in-house dataset. The study utilizes sliding window embedding to convert each fMRI time series into a sequence of 3-dimensional vectors, facilitating the assessment of changes in regional brain topology. Distinct persistence diagrams are computed for Betti descriptors of dimension-0, 1, and 2. Wasserstein distance metric is used to quantify differences in topological characteristics. We have examined both (i) ROI-specific inter-subject interactions and (ii) subject-specific inter-ROI interactions. Further, a new deep learning model is proposed for classification, achieving a maximum classification accuracy of 95% for the ADNI dataset and 85% for the in-house dataset. This methodology is further adapted for the differential diagnosis of MCI sub-types, resulting in a peak accuracy of 76.5%, 91.1% and 80% in classifying HC Vs. EMCI, HC Vs. LMCI and EMCI Vs. LMCI, respectively. We showed that the proposed approach surpasses current state-of-the-art techniques designed for classifying MCI and its sub-types using fMRI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-27</td>
<td style='padding: 8px;'>NeuralOOD: Improving Out-of-Distribution Generalization Performance with Brain-machine Fusion Learning Framework</td>
<td style='padding: 6px;'>Shuangchen Zhao, Changde Du, Hui Li, Huiguang He</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.14950v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Deep Neural Networks (DNNs) have demonstrated exceptional recognition capabilities in traditional computer vision (CV) tasks. However, existing CV models often suffer a significant decrease in accuracy when confronted with out-of-distribution (OOD) data. In contrast to these DNN models, human can maintain a consistently low error rate when facing OOD scenes, partly attributed to the rich prior cognitive knowledge stored in the human brain. Previous OOD generalization researches only focus on the single modal, overlooking the advantages of multimodal learning method. In this paper, we utilize the multimodal learning method to improve the OOD generalization and propose a novel Brain-machine Fusion Learning (BMFL) framework. We adopt the cross-attention mechanism to fuse the visual knowledge from CV model and prior cognitive knowledge from the human brain. Specially, we employ a pre-trained visual neural encoding model to predict the functional Magnetic Resonance Imaging (fMRI) from visual features which eliminates the need for the fMRI data collection and pre-processing, effectively reduces the workload associated with conventional BMFL methods. Furthermore, we construct a brain transformer to facilitate the extraction of knowledge inside the fMRI data. Moreover, we introduce the Pearson correlation coefficient maximization regularization method into the training process, which improves the fusion capability with better constrains. Our model outperforms the DINOv2 and baseline models on the ImageNet-1k validation dataset as well as six curated OOD datasets, showcasing its superior performance in diverse scenarios.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-26</td>
<td style='padding: 8px;'>Reconstructing physiological signals from fMRI across the adult lifespan</td>
<td style='padding: 6px;'>Shiyu Wang, Ziyuan Xu, Yamin Li, Mara Mather, Roza G. Bayrak, Catie Chang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.14453v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Interactions between the brain and body are of fundamental importance for human behavior and health. Functional magnetic resonance imaging (fMRI) captures whole-brain activity noninvasively, and modeling how fMRI signals interact with physiological dynamics of the body can provide new insight into brain function and offer potential biomarkers of disease. However, physiological recordings are not always possible to acquire since they require extra equipment and setup, and even when they are, the recorded physiological signals may contain substantial artifacts. To overcome this limitation, machine learning models have been proposed to directly extract features of respiratory and cardiac activity from resting-state fMRI signals. To date, such work has been carried out only in healthy young adults and in a pediatric population, leaving open questions about the efficacy of these approaches on older adults. Here, we propose a novel framework that leverages Transformer-based architectures for reconstructing two key physiological signals - low-frequency respiratory volume (RV) and heart rate (HR) fluctuations - from fMRI data, and test these models on a dataset of individuals aged 36-89 years old. Our framework outperforms previously proposed approaches (attaining median correlations between predicted and measured signals of r ~ .698 for RV and r ~ .618 for HR), indicating the potential of leveraging attention mechanisms to model fMRI-physiological signal relationships. We also evaluate several model training and fine-tuning strategies, and find that incorporating young-adult data during training improves the performance when predicting physiological signals in the aging cohort. Overall, our approach successfully infers key physiological variables directly from fMRI data from individuals across a wide range of the adult lifespan.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-26</td>
<td style='padding: 8px;'>Integrated Brain Connectivity Analysis with fMRI, DTI, and sMRI Powered by Interpretable Graph Neural Networks</td>
<td style='padding: 6px;'>Gang Qu, Ziyu Zhou, Vince D. Calhoun, Aiying Zhang, Yu-Ping Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.14254v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Multimodal neuroimaging modeling has becomes a widely used approach but confronts considerable challenges due to heterogeneity, which encompasses variability in data types, scales, and formats across modalities. This variability necessitates the deployment of advanced computational methods to integrate and interpret these diverse datasets within a cohesive analytical framework. In our research, we amalgamate functional magnetic resonance imaging, diffusion tensor imaging, and structural MRI into a cohesive framework. This integration capitalizes on the unique strengths of each modality and their inherent interconnections, aiming for a comprehensive understanding of the brain's connectivity and anatomical characteristics. Utilizing the Glasser atlas for parcellation, we integrate imaging derived features from various modalities: functional connectivity from fMRI, structural connectivity from DTI, and anatomical features from sMRI within consistent regions. Our approach incorporates a masking strategy to differentially weight neural connections, thereby facilitating a holistic amalgamation of multimodal imaging data. This technique enhances interpretability at connectivity level, transcending traditional analyses centered on singular regional attributes. The model is applied to the Human Connectome Project's Development study to elucidate the associations between multimodal imaging and cognitive functions throughout youth. The analysis demonstrates improved predictive accuracy and uncovers crucial anatomical features and essential neural connections, deepening our understanding of brain structure and function.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-23</td>
<td style='padding: 8px;'>Hierarchical Spatio-Temporal State-Space Modeling for fMRI Analysis</td>
<td style='padding: 6px;'>Yuxiang Wei, Anees Abrol, Reihaneh Hassanzadeh, Vince Calhoun</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.13074v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent advances in deep learning structured state space models, especially the Mamba architecture, have demonstrated remarkable performance improvements while maintaining linear complexity. In this study, we introduce functional spatiotemporal Mamba (FST-Mamba), a Mamba-based model designed for discovering neurological biomarkers using functional magnetic resonance imaging (fMRI). We focus on dynamic functional network connectivity (dFNC) derived from fMRI and propose a hierarchical spatiotemporal Mamba-based network that processes spatial and temporal information separately using Mamba-based encoders. Leveraging the topological uniqueness of the FNC matrix, we introduce a component-wise varied-scale aggregation (CVA) mechanism to aggregate connectivity across individual components within brain networks, enabling the model to capture both inter-component and inter-network information. To better handle the FNC data, we develop a new component-specific scanning order. Additionally, we propose symmetric rotary position encoding (SymRope) to encode the relative positions of each functional connection while considering the symmetric nature of the FNC matrix. Experimental results demonstrate significant improvements in the proposed FST-Mamba model on various brain-based classification and regression tasks. Our work reveals the substantial potential of attention-free sequence modeling in brain discovery.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-23</td>
<td style='padding: 8px;'>Universal dimensions of visual representation</td>
<td style='padding: 6px;'>Zirui Chen, Michael F. Bonner</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.12804v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Do neural network models of vision learn brain-aligned representations because they share architectural constraints and task objectives with biological vision or because they learn universal features of natural image processing? We characterized the universality of hundreds of thousands of representational dimensions from visual neural networks with varied construction. We found that networks with varied architectures and task objectives learn to represent natural images using a shared set of latent dimensions, despite appearing highly distinct at a surface level. Next, by comparing these networks with human brain representations measured with fMRI, we found that the most brain-aligned representations in neural networks are those that are universal and independent of a network's specific characteristics. Remarkably, each network can be reduced to fewer than ten of its most universal dimensions with little impact on its representational similarity to the human brain. These results suggest that the underlying similarities between artificial and biological vision are primarily governed by a core set of universal image representations that are convergently learned by diverse systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-20</td>
<td style='padding: 8px;'>Prompt Your Brain: Scaffold Prompt Tuning for Efficient Adaptation of fMRI Pre-trained Model</td>
<td style='padding: 6px;'>Zijian Dong, Yilei Wu, Zijiao Chen, Yichi Zhang, Yueming Jin, Juan Helen Zhou</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.10567v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We introduce Scaffold Prompt Tuning (ScaPT), a novel prompt-based framework for adapting large-scale functional magnetic resonance imaging (fMRI) pre-trained models to downstream tasks, with high parameter efficiency and improved performance compared to fine-tuning and baselines for prompt tuning. The full fine-tuning updates all pre-trained parameters, which may distort the learned feature space and lead to overfitting with limited training data which is common in fMRI fields. In contrast, we design a hierarchical prompt structure that transfers the knowledge learned from high-resource tasks to low-resource ones. This structure, equipped with a Deeply-conditioned Input-Prompt (DIP) mapping module, allows for efficient adaptation by updating only 2% of the trainable parameters. The framework enhances semantic interpretability through attention mechanisms between inputs and prompts, and it clusters prompts in the latent space in alignment with prior knowledge. Experiments on public resting state fMRI datasets reveal ScaPT outperforms fine-tuning and multitask-based prompt tuning in neurodegenerative diseases diagnosis/prognosis and personality trait prediction, even with fewer than 20 participants. It highlights ScaPT's efficiency in adapting pre-trained fMRI models to low-resource tasks.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-15</td>
<td style='padding: 8px;'>Causal Discovery from Time-Series Data with Short-Term Invariance-Based Convolutional Neural Networks</td>
<td style='padding: 6px;'>Rujia Shen, Boran Wang, Chao Zhao, Yi Guan, Jingchi Jiang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.08023v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Causal discovery from time-series data aims to capture both intra-slice (contemporaneous) and inter-slice (time-lagged) causality between variables within the temporal chain, which is crucial for various scientific disciplines. Compared to causal discovery from non-time-series data, causal discovery from time-series data necessitates more serialized samples with a larger amount of observed time steps. To address the challenges, we propose a novel gradient-based causal discovery approach STIC, which focuses on \textbf{S}hort-\textbf{T}erm \textbf{I}nvariance using \textbf{C}onvolutional neural networks to uncover the causal relationships from time-series data. Specifically, STIC leverages both the short-term time and mechanism invariance of causality within each window observation, which possesses the property of independence, to enhance sample efficiency. Furthermore, we construct two causal convolution kernels, which correspond to the short-term time and mechanism invariance respectively, to estimate the window causal graph. To demonstrate the necessity of convolutional neural networks for causal discovery from time-series data, we theoretically derive the equivalence between convolution and the underlying generative principle of time-series data under the assumption that the additive noise model is identifiable. Experimental evaluations conducted on both synthetic and FMRI benchmark datasets demonstrate that our STIC outperforms baselines significantly and achieves the state-of-the-art performance, particularly when the datasets contain a limited number of observed time steps. Code is available at \url{https://github.com/HITshenrj/STIC}.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-15</td>
<td style='padding: 8px;'>Cortical network reconfiguration aligns with shifts of basal ganglia and cerebellar influence</td>
<td style='padding: 6px;'>Kimberly Nestor, Javier Rasero, Richard Betzel, Peter J. Gianaros, Timothy Verstynen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.07977v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Mammalian functional architecture flexibly adapts, transitioning from integration where information is distributed across the cortex, to segregation where information is focal in densely connected communities of brain regions. This flexibility in cortical brain networks is hypothesized to be driven by control signals originating from subcortical pathways, with the basal ganglia shifting the cortex towards integrated processing states and the cerebellum towards segregated states. In a sample of healthy human participants (N=242), we used fMRI to measure temporal variation in global brain networks while participants performed two tasks with similar cognitive demands (Stroop and Multi-Source Inference Task (MSIT)). Using the modularity index, we determined cortical networks shifted from integration (low modularity) at rest to high modularity during easier i.e. congruent (segregation). Increased task difficulty (incongruent) resulted in lower modularity in comparison to the easier counterpart indicating more integration of the cortical network. Influence of basal ganglia and cerebellum was measured using eigenvector centrality. Results correlated with decreases and increases in cortical modularity respectively, with only the basal ganglia influence preceding cortical integration. Our results support the theory the basal ganglia shifts cortical networks to integrated states due to environmental demand. Cerebellar influence correlates with shifts to segregated cortical states, though may not play a causal role.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-13</td>
<td style='padding: 8px;'>Hierarchical Quantum Control Gates for Functional MRI Understanding</td>
<td style='padding: 6px;'>Xuan-Bac Nguyen, Hoang-Quan Nguyen, Hugh Churchill, Samee U. Khan, Khoa Luu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.03596v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Quantum computing has emerged as a powerful tool for solving complex problems intractable for classical computers, particularly in popular fields such as cryptography, optimization, and neurocomputing. In this paper, we present a new quantum-based approach named the Hierarchical Quantum Control Gates (HQCG) method for efficient understanding of Functional Magnetic Resonance Imaging (fMRI) data. This approach includes two novel modules: the Local Quantum Control Gate (LQCG) and the Global Quantum Control Gate (GQCG), which are designed to extract local and global features of fMRI signals, respectively. Our method operates end-to-end on a quantum machine, leveraging quantum mechanics to learn patterns within extremely high-dimensional fMRI signals, such as 30,000 samples which is a challenge for classical computers. Empirical results demonstrate that our approach significantly outperforms classical methods. Additionally, we found that the proposed quantum model is more stable and less prone to overfitting than the classical methods.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-27</td>
<td style='padding: 8px;'>Bayesian Inference General Procedures for A Single-subject Test Study</td>
<td style='padding: 6px;'>Jie Li, Gary Green, Sarah J. A. Carr, Peng Liu, Jian Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.15419v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper presents a Bayesian Inference General Procedures for A Single-Subject Test (BIGPAST), designed to mitigate the effects of skewness. BIGPAST operates under the null hypothesis that the single-subject follows the same distribution as the control group.   We assess BIGPAST's performance against other methods through a series of simulation studies. The results demonstrate that BIGPAST is robust against deviations from normality and outperforms the existing approaches in terms of accuracy. This is because BIGPAST can effectively reduce model misspecification errors under the skewed Student's \( t \) assumption. We apply BIGPAST to a MEG dataset consisting of an individual with mild traumatic brain injury and an age and gender-matched control group, demonstrating its effectiveness in detecting abnormalities in the single-subject.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-09</td>
<td style='padding: 8px;'>Towards improving Alzheimer's intervention: a machine learning approach for biomarker detection through combining MEG and MRI pipelines</td>
<td style='padding: 6px;'>Alwani Liyana Ahmad, Jose Sanchez-Bornot, Roberto C. Sotero, Damien Coyle, Zamzuri Idris, Ibrahima Faye</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.04815v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>MEG are non invasive neuroimaging techniques with excellent temporal and spatial resolution, crucial for studying brain function in dementia and Alzheimer Disease. They identify changes in brain activity at various Alzheimer stages, including preclinical and prodromal phases. MEG may detect pathological changes before clinical symptoms, offering potential biomarkers for intervention. This study evaluates classification techniques using MEG features to distinguish between healthy controls and mild cognitive impairment participants from the BioFIND study. We compare MEG based biomarkers with MRI based anatomical features, both independently and combined. We used 3 Tesla MRI and MEG data from 324 BioFIND participants;158 MCI and 166 HC. Analyses were performed using MATLAB with SPM12 and OSL toolboxes. Machine learning analyses, including 100 Monte Carlo replications of 10 fold cross validation, were conducted on sensor and source spaces. Combining MRI with MEG features achieved the best performance; 0.76 accuracy and AUC of 0.82 for GLMNET using LCMV source based MEG. MEG only analyses using LCMV and eLORETA also performed well, suggesting that combining uncorrected MEG with z-score-corrected MRI features is optimal.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-05</td>
<td style='padding: 8px;'>Classification of Raw MEG/EEG Data with Detach-Rocket Ensemble: An Improved ROCKET Algorithm for Multivariate Time Series Analysis</td>
<td style='padding: 6px;'>Adri Solana, Erik Fransn, Gonzalo Uribarri</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.02760v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Multivariate Time Series Classification (MTSC) is a ubiquitous problem in science and engineering, particularly in neuroscience, where most data acquisition modalities involve the simultaneous time-dependent recording of brain activity in multiple brain regions. In recent years, Random Convolutional Kernel models such as ROCKET and MiniRocket have emerged as highly effective time series classification algorithms, capable of achieving state-of-the-art accuracy results with low computational load. Despite their success, these types of models face two major challenges when employed in neuroscience: 1) they struggle to deal with high-dimensional data such as EEG and MEG, and 2) they are difficult to interpret. In this work, we present a novel ROCKET-based algorithm, named Detach-Rocket Ensemble, that is specifically designed to address these two problems in MTSC. Our algorithm leverages pruning to provide an integrated estimation of channel importance, and ensembles to achieve better accuracy and provide a label probability. Using a synthetic multivariate time series classification dataset in which we control the amount of information carried by each of the channels, we first show that our algorithm is able to correctly recover the channel importance for classification. Then, using two real-world datasets, a MEG dataset and an EEG dataset, we show that Detach-Rocket Ensemble is able to provide both interpretable channel relevance and competitive classification accuracy, even when applied directly to the raw brain data, without the need for feature engineering.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-02</td>
<td style='padding: 8px;'>Hotspots and Trends in Magnetoencephalography Research (2013-2022): A Bibliometric Analysis</td>
<td style='padding: 6px;'>Shen Liu, Jingwen Zhao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.08877v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study aimed to utilize bibliometric methods to analyze trends in international Magnetoencephalography (MEG) research from 2013 to 2022. Due to the limited volume of domestic literature on MEG, this analysis focuses solely on the global research landscape, providing insights from the past decade as a representative sample. This study utilized bibliometric methods to explore and analyze the progress, hotspots and developmental trends in international MEG research spanning from 1995 to 2022. The results indicated a dynamic and steady growth trend in the overall number of publications in MEG. Ryusuke Kakigi emerged as the most prolific author, while Neuroimage led as the most prolific journal. Current hotspots in MEG research encompass resting state, networks, functional connectivity, phase dynamics, oscillation, and more. Future trends in MEG research are poised to advance across three key aspects: disease treatment and practical applications, experimental foundations and technical advancements, and fundamental and advanced human cognition. In the future, there should be a focus on enhancing cross-integration and utilization of MEG with other instruments to diversify research methodologies in this field</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-02</td>
<td style='padding: 8px;'>Gemma 2: Improving Open Language Models at a Practical Size</td>
<td style='padding: 6px;'>Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Lonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogoziska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Pluciska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju-yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Grner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Perrin, Sbastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, Alek Andreev</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.00118v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3 times bigger. We release all our models to the community.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-07-28</td>
<td style='padding: 8px;'>Photon energy reconstruction with the MEG II liquid xenon calorimeter</td>
<td style='padding: 6px;'>Kensuke Yamamoto, Sei Ban, Lukas Gerritzen, Toshiyuki Iwamoto, Satoru Kobayashi, Ayaka Matsushita, Toshinori Mori, Rina Onda, Wataru Ootani, Atsushi Oya</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2407.19417v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The MEG II experiment searches for a charged-lepton-flavour-violating $\mu \to e \gamma$ with the target sensitivity of $6 \times 10^{-14}$. A liquid xenon calorimeter with VUV-sensitive photosensors measures photon position, timing, and energy. This paper concentrates on the precise photon energy reconstruction with the MEG II liquid xenon calorimeter. Since a muon beam rate is $3\text{-}5 \times 10^{7}~\text{s}^{-1}$, multi-photon elimination analysis is performed using waveform analysis techniques such as a template waveform fit. As a result, background events in the energy range of 48-58 MeV were reduced by 34 %. The calibration of an energy scale of the calorimeter with several calibration sources is also discussed to achieve a high resolution of 1.8 %.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-07-23</td>
<td style='padding: 8px;'>A Sub-solar Fe/O, logT~7.5 Gas Component Permeating the Milky Way's CGM</td>
<td style='padding: 6px;'>Armando Lara-DI, Yair Krongold, Smita Mathur, Sanskriti Das, Anjali Gupta, O. Segura Montero</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2407.16784v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Our study focuses on characterizing the highly ionized gas within the Milky Way's (MW) Circumgalactic Medium (CGM) that gives rise to ionic transitions in the X-ray band 2 - 25 \AA. Utilizing stacked \Chandra/\ACISS\ \MEG\ and \LETG\ spectra toward QSO sightlines, we employ the self-consistent hybrid ionization code PHASE to model our data. The stacked spectra are optimally described by three distinct gas phase components: a \warm\ (\logT\ $\sim$ 5.5), \warmhot\ (\logT\ $\sim 6$), and \hot\ (\logT\ $\sim$ 7.5) components. These findings confirm the presence of the \hot\ component in the MW's CGM indicating its coexistence with a \warm\ and a \warmhot\ gas phases. We find this \hot\ component to be homogeneous in temperature but inhomogeneous in column density. The gas in the \hot\ component requires over-abundances relative to solar to be consistent with the Dispersion Measure (DM) from the Galactic halo reported in the literature. {For the hot phase we estimated a DM = $55.1^{+29.9}_{-23.7}$ pc cm$^{-3}$}. We conclude that this phase is either enriched in Oxygen, Silicon, and Sulfur, or has metallicity {over 6} times solar value, or a combination of both. We do not detect Fe L-shell absorption lines, implying O/Fe $\geq$ 4. The non-solar abundance ratios found in the super-virial gas component in the Galactic halo suggest that this phase arises from Galactic feedback.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-07-18</td>
<td style='padding: 8px;'>Revisiting Neutrino Masses In Clockwork Models</td>
<td style='padding: 6px;'>Aadarsh Singh</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2407.13733v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In this paper, we have looked at various variants of the clockwork model and studied their impact on the neutrino masses. Some of the generalizations such as generalized CW and next-to-nearest neighbour interaction CW have already been explored by a few authors. In this study, we studied non-local CW for the fermionic case and found that non-local models relax the $\left| q \right| > 1$ constraint to produce localization of the zero mode. We also made a comparison among them and have shown that for some parameter ranges, non-local variants of CW are more efficient than ordinary CW in generating the hierarchy required for the $\nu$ mass scale. Finally, phenomenological constraints from $BR(\mu \rightarrow e \gamma )$ FCNC process and Higgs decay width have been imposed on the parameter space in non-local and both-sided clockwork models. We have listed benchmark points which are surviving current experimental bounds from MEG and are within the reach of the upcoming MEG-II experiment.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-06</td>
<td style='padding: 8px;'>Accelerating Mobile Edge Generation (MEG) by Constrained Learning</td>
<td style='padding: 6px;'>Xiaoxia Xu, Yuanwei Liu, Xidong Mu, Hong Xing, Arumugam Nallanathan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2407.07245v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A novel accelerated mobile edge generation (MEG) framework is proposed for generating high-resolution images on mobile devices. Exploiting a large-scale latent diffusion model (LDM) distributed across edge server (ES) and user equipment (UE), cost-efficient artificial intelligence generated content (AIGC) is achieved by transmitting low-dimensional features between ES and UE. To reduce overheads of both distributed computations and transmissions, a dynamic diffusion and feature merging scheme is conceived. By jointly optimizing the denoising steps and feature merging ratio, the image generation quality is maximized subject to latency and energy consumption constraints. To address this problem and tailor LDM sub-models, a low-complexity MEG acceleration protocol is developed. Particularly, a backbone meta-architecture is trained via offline distillation. Then, dynamic diffusion and feature merging are determined in online channel environment, which can be viewed as a constrained Markov Decision Process (MDP). A constrained variational policy optimization (CVPO) based MEG algorithm is further proposed for constraint-guaranteed learning, namely MEG-CVPO. Numerical results verify that: 1) The proposed framework can generate 1024$\times$1024 high-quality images over noisy channels while reducing over $40\%$ latency compared to conventional generation schemes. 2) The developed MEG-CVPO effectively mitigates constraint violations, thus flexibly controlling the trade-off between image distortion and generation costs.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-07-23</td>
<td style='padding: 8px;'>Volume-optimal persistence homological scaffolds of hemodynamic networks covary with MEG theta-alpha aperiodic dynamics</td>
<td style='padding: 6px;'>Nghi Nguyen, Tao Hou, Enrico Amico, Jingyi Zheng, Huajun Huang, Alan D. Kaplan, Giovanni Petri, Joaqun Goi, Ralph Kaufmann, Yize Zhao, Duy Duong-Tran, Li Shen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2407.05060v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Higher-order properties of functional magnetic resonance imaging (fMRI) induced connectivity have been shown to unravel many exclusive topological and dynamical insights beyond pairwise interactions. Nonetheless, whether these fMRI-induced higher-order properties play a role in disentangling other neuroimaging modalities' insights remains largely unexplored and poorly understood. In this work, by analyzing fMRI data from the Human Connectome Project Young Adult dataset using persistent homology, we discovered that the volume-optimal persistence homological scaffolds of fMRI-based functional connectomes exhibited conservative topological reconfigurations from the resting state to attentional task-positive state. Specifically, while reflecting the extent to which each cortical region contributed to functional cycles following different cognitive demands, these reconfigurations were constrained such that the spatial distribution of cavities in the connectome is relatively conserved. Most importantly, such level of contributions covaried with powers of aperiodic activities mostly within the theta-alpha (4-12 Hz) band measured by magnetoencephalography (MEG). This comprehensive result suggests that fMRI-induced hemodynamics and MEG theta-alpha aperiodic activities are governed by the same functional constraints specific to each cortical morpho-structure. Methodologically, our work paves the way toward an innovative computing paradigm in multimodal neuroimaging topological learning.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-07-22</td>
<td style='padding: 8px;'>Predictive Coding Networks and Inference Learning: Tutorial and Survey</td>
<td style='padding: 6px;'>Bjrn van Zwol, Ro Jefferson, Egon L. van den Broek</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2407.04117v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent years have witnessed a growing call for renewed emphasis on neuroscience-inspired approaches in artificial intelligence research, under the banner of NeuroAI. A prime example of this is predictive coding networks (PCNs), based on the neuroscientific framework of predictive coding. This framework views the brain as a hierarchical Bayesian inference model that minimizes prediction errors through feedback connections. Unlike traditional neural networks trained with backpropagation (BP), PCNs utilize inference learning (IL), a more biologically plausible algorithm that explains patterns of neural activity that BP cannot. Historically, IL has been more computationally intensive, but recent advancements have demonstrated that it can achieve higher efficiency than BP with sufficient parallelization. Furthermore, PCNs can be mathematically considered a superset of traditional feedforward neural networks (FNNs), significantly extending the range of trainable architectures. As inherently probabilistic (graphical) latent variable models, PCNs provide a versatile framework for both supervised learning and unsupervised (generative) modeling that goes beyond traditional artificial neural networks. This work provides a comprehensive review and detailed formal specification of PCNs, particularly situating them within the context of modern ML methods. Additionally, we introduce a Python library (PRECO) for practical implementation. This positions PC as a promising framework for future ML innovations.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-10-29</td>
<td style='padding: 8px;'>Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis</td>
<td style='padding: 6px;'>Mitchell Ostrow, Adam Eisen, Leo Kozachkov, Ila Fiete</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2306.10168v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-05-25</td>
<td style='padding: 8px;'>Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture</td>
<td style='padding: 6px;'>Galen Pogoncheff, Jacob Granley, Michael Beyeler</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2305.11275v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-03-11</td>
<td style='padding: 8px;'>Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks</td>
<td style='padding: 6px;'>Feng-Lei Fan, Yingxin Li, Hanchuan Peng, Tieyong Zeng, Fei Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2301.09245v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Throughout history, the development of artificial intelligence, particularly artificial neural networks, has been open to and constantly inspired by the increasingly deepened understanding of the brain, such as the inspiration of neocognitron, which is the pioneering work of convolutional neural networks. Per the motives of the emerging field: NeuroAI, a great amount of neuroscience knowledge can help catalyze the next generation of AI by endowing a network with more powerful capabilities. As we know, the human brain has numerous morphologically and functionally different neurons, while artificial neural networks are almost exclusively built on a single neuron type. In the human brain, neuronal diversity is an enabling factor for all kinds of biological intelligent behaviors. Since an artificial network is a miniature of the human brain, introducing neuronal diversity should be valuable in terms of addressing those essential problems of artificial networks such as efficiency, interpretability, and memory. In this Primer, we first discuss the preliminaries of biological neuronal diversity and the characteristics of information transmission and processing in a biological neuron. Then, we review studies of designing new neurons for artificial networks. Next, we discuss what gains can neuronal diversity bring into artificial networks and exemplary applications in several important fields. Lastly, we discuss the challenges and future directions of neuronal diversity to explore the potential of NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2022-12-08</td>
<td style='padding: 8px;'>A Rubric for Human-like Agents and NeuroAI</td>
<td style='padding: 6px;'>Ida Momennejad</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2212.04401v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Researchers across cognitive, neuro-, and computer sciences increasingly reference human-like artificial intelligence and neuroAI. However, the scope and use of the terms are often inconsistent. Contributed research ranges widely from mimicking behaviour, to testing machine learning methods as neurally plausible hypotheses at the cellular or functional levels, or solving engineering problems. However, it cannot be assumed nor expected that progress on one of these three goals will automatically translate to progress in others. Here a simple rubric is proposed to clarify the scope of individual contributions, grounded in their commitments to human-like behaviour, neural plausibility, or benchmark/engineering goals. This is clarified using examples of weak and strong neuroAI and human-like agents, and discussing the generative, corroborate, and corrective ways in which the three dimensions interact with one another. The author maintains that future progress in artificial intelligence will need strong interactions across the disciplines, with iterative feedback loops and meticulous validity tests, leading to both known and yet-unknown advances that may span decades to come.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2023-02-22</td>
<td style='padding: 8px;'>Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolution</td>
<td style='padding: 6px;'>Anthony Zador, Sean Escola, Blake Richards, Bence lveczky, Yoshua Bengio, Kwabena Boahen, Matthew Botvinick, Dmitri Chklovskii, Anne Churchland, Claudia Clopath, James DiCarlo, Surya Ganguli, Jeff Hawkins, Konrad Koerding, Alexei Koulakov, Yann LeCun, Timothy Lillicrap, Adam Marblestone, Bruno Olshausen, Alexandre Pouget, Cristina Savin, Terrence Sejnowski, Eero Simoncelli, Sara Solla, David Sussillo, Andreas S. Tolias, Doris Tsao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2210.08340v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience has long been an essential driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI. A core component of this is the embodied Turing test, which challenges AI animal models to interact with the sensorimotor world at skill levels akin to their living counterparts. The embodied Turing test shifts the focus from those capabilities like game playing and language that are especially well-developed or uniquely human to those capabilities, inherited from over 500 million years of evolution, that are shared with all animals. Building models that can pass the embodied Turing test will provide a roadmap for the next generation of AI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2022-04-11</td>
<td style='padding: 8px;'>Social Neuro AI: Social Interaction as the "dark matter" of AI</td>
<td style='padding: 6px;'>Samuele Bolotta, Guillaume Dumas</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2112.15459v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This article introduces a three-axis framework indicating how AI can be informed by biological examples of social learning mechanisms. We argue that the complex human cognitive architecture owes a large portion of its expressive power to its ability to engage in social and cultural learning. However, the field of AI has mostly embraced a solipsistic perspective on intelligence. We thus argue that social interactions not only are largely unexplored in this field but also are an essential element of advanced cognitive ability, and therefore constitute metaphorically the dark matter of AI. In the first section, we discuss how social learning plays a key role in the development of intelligence. We do so by discussing social and cultural learning theories and empirical findings from social neuroscience. Then, we discuss three lines of research that fall under the umbrella of Social NeuroAI and can contribute to developing socially intelligent embodied agents in complex environments. First, neuroscientific theories of cognitive architecture, such as the global workspace theory and the attention schema theory, can enhance biological plausibility and help us understand how we could bridge individual and social theories of intelligence. Second, intelligence occurs in time as opposed to over time, and this is naturally incorporated by dynamical systems. Third, embodiment has been demonstrated to provide more sophisticated array of communicative signals. To conclude, we discuss the example of active inference, which offers powerful insights for developing agents that possess biological realism, can self-organize in time, and are socially embodied.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2021-10-23</td>
<td style='padding: 8px;'>Predictive Coding, Variational Autoencoders, and Biological Connections</td>
<td style='padding: 6px;'>Joseph Marino</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2011.07464v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper reviews predictive coding, from theoretical neuroscience, and variational autoencoders, from machine learning, identifying the common origin and mathematical framework underlying both areas. As each area is prominent within its respective field, more firmly connecting these areas could prove useful in the dialogue between neuroscience and machine learning. After reviewing each area, we discuss two possible correspondences implied by this perspective: cortical pyramidal dendrites as analogous to (non-linear) deep networks and lateral inhibition as analogous to normalizing flows. These connections may provide new directions for further investigations in each field.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2019-09-13</td>
<td style='padding: 8px;'>Additive function approximation in the brain</td>
<td style='padding: 6px;'>Kameron Decker Harris</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/1909.02603v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Many biological learning systems such as the mushroom body, hippocampus, and cerebellum are built from sparsely connected networks of neurons. For a new understanding of such networks, we study the function spaces induced by sparse random features and characterize what functions may and may not be learned. A network with $d$ inputs per neuron is found to be equivalent to an additive model of order $d$, whereas with a degree distribution the network combines additive terms of different orders. We identify three specific advantages of sparsity: additive function approximation is a powerful inductive bias that limits the curse of dimensionality, sparse networks are stable to outlier noise in the inputs, and sparse random features are scalable. Thus, even simple brain architectures can be powerful function approximators. Finally, we hope that this work helps popularize kernel theories of networks among computational neuroscientists.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-30</td>
<td style='padding: 8px;'>Generative AI Enables Medical Image Segmentation in Ultra Low-Data Regimes</td>
<td style='padding: 6px;'>Li Zhang, Basu Jindal, Ahmed Alaa, Robert Weinreb, David Wilson, Eran Segal, James Zou, Pengtao Xie</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.17421v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Semantic segmentation of medical images is pivotal in applications like disease diagnosis and treatment planning. While deep learning has excelled in automating this task, a major hurdle is the need for numerous annotated segmentation masks, which are resource-intensive to produce due to the required expertise and time. This scenario often leads to ultra low-data regimes, where annotated images are extremely limited, posing significant challenges for the generalization of conventional deep learning methods on test images. To address this, we introduce a generative deep learning framework, which uniquely generates high-quality paired segmentation masks and medical images, serving as auxiliary data for training robust models in data-scarce environments. Unlike traditional generative models that treat data generation and segmentation model training as separate processes, our method employs multi-level optimization for end-to-end data generation. This approach allows segmentation performance to directly influence the data generation process, ensuring that the generated data is specifically tailored to enhance the performance of the segmentation model. Our method demonstrated strong generalization performance across 9 diverse medical image segmentation tasks and on 16 datasets, in ultra-low data regimes, spanning various diseases, organs, and imaging modalities. When applied to various segmentation models, it achieved performance improvements of 10-20\% (absolute), in both same-domain and out-of-domain scenarios. Notably, it requires 8 to 20 times less training data than existing methods to achieve comparable results. This advancement significantly improves the feasibility and cost-effectiveness of applying deep learning in medical imaging, particularly in scenarios with limited data availability.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-30</td>
<td style='padding: 8px;'>Exploring the Effect of Explanation Content and Format on User Comprehension and Trust</td>
<td style='padding: 6px;'>Antonio Rago, Bence Palfi, Purin Sukpanichnant, Hannibal Nabli, Kavyesh Vivek, Olga Kostopoulou, James Kinross, Francesca Toni</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.17401v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In recent years, various methods have been introduced for explaining the outputs of "black-box" AI models. However, it is not well understood whether users actually comprehend and trust these explanations. In this paper, we focus on explanations for a regression tool for assessing cancer risk and examine the effect of the explanations' content and format on the user-centric metrics of comprehension and trust. Regarding content, we experiment with two explanation methods: the popular SHAP, based on game-theoretic notions and thus potentially complex for everyday users to comprehend, and occlusion-1, based on feature occlusion which may be more comprehensible. Regarding format, we present SHAP explanations as charts (SC), as is conventional, and occlusion-1 explanations as charts (OC) as well as text (OT), to which their simpler nature also lends itself. The experiments amount to user studies questioning participants, with two different levels of expertise (the general population and those with some medical training), on their subjective and objective comprehension of and trust in explanations for the outputs of the regression tool. In both studies we found a clear preference in terms of subjective comprehension and trust for occlusion-1 over SHAP explanations in general, when comparing based on content. However, direct comparisons of explanations when controlling for format only revealed evidence for OT over SC explanations in most cases, suggesting that the dominance of occlusion-1 over SHAP explanations may be driven by a preference for text over charts as explanations. Finally, we found no evidence of a difference between the explanation types in terms of objective comprehension. Thus overall, the choice of the content and format of explanations needs careful attention, since in some contexts format, rather than content, may play the critical role in improving user experience.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-30</td>
<td style='padding: 8px;'>NDP: Next Distribution Prediction as a More Broad Target</td>
<td style='padding: 6px;'>Junhao Ruan, Abudukeyumu Abudula, Xinyu Liu, Bei Li, Yinqiao Li, Chenglong Wang, Yuchun Fan, Yuan Ge, Tong Xiao, Jingbo Zhu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.17377v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Large language models (LLMs) trained on next-token prediction (NTP) paradigm have demonstrated powerful capabilities. However, the existing NTP paradigm contains several limitations, particularly related to planned task complications and error propagation during inference. In our work, we extend the critique of NTP, highlighting its limitation also due to training with a narrow objective: the prediction of a sub-optimal one-hot distribution. To support this critique, we conducted a pre-experiment treating the output distribution from powerful LLMs as efficient world data compression. By evaluating the similarity between the $n$-gram distribution and the one-hot distribution with LLMs, we observed that the $n$-gram distributions align more closely with the output distribution of LLMs. Based on this insight, we introduce Next Distribution Prediction (NDP), which uses $n$-gram distributions to replace the one-hot targets, enhancing learning without extra online training time. We conducted experiments across translation, general task, language transfer, and medical domain adaptation. Compared to NTP, NDP can achieve up to +2.97 COMET improvement in translation tasks, +0.61 average improvement in general tasks, and incredible +10.75 average improvement in the medical domain. This demonstrates the concrete benefits of addressing the target narrowing problem, pointing to a new direction for future work on improving NTP.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-30</td>
<td style='padding: 8px;'>LSMS: Language-guided Scale-aware MedSegmentor for Medical Image Referring Segmentation</td>
<td style='padding: 6px;'>Shuyi Ouyang, Jinyang Zhang, Xiangye Lin, Xilai Wang, Qingqing Chen, Yen-Wei Chen, Lanfen Lin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.17347v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Conventional medical image segmentation methods have been found inadequate in facilitating physicians with the identification of specific lesions for diagnosis and treatment. Given the utility of text as an instructional format, we introduce a novel task termed Medical Image Referring Segmentation (MIRS), which requires segmenting specified lesions in images based on the given language expressions. Due to the varying object scales in medical images, MIRS demands robust vision-language modeling and comprehensive multi-scale interaction for precise localization and segmentation under linguistic guidance. However, existing medical image segmentation methods fall short in meeting these demands, resulting in insufficient segmentation accuracy. In response, we propose an approach named Language-guided Scale-aware MedSegmentor (LSMS), incorporating two appealing designs: (1)~a Scale-aware Vision-Language Attention module that leverages diverse convolutional kernels to acquire rich visual knowledge and interact closely with linguistic features, thereby enhancing lesion localization capability; (2)~a Full-Scale Decoder that globally models multi-modal features across various scales, capturing complementary information between scales to accurately outline lesion boundaries. Addressing the lack of suitable datasets for MIRS, we constructed a vision-language medical dataset called Reference Hepatic Lesion Segmentation (RefHL-Seg). This dataset comprises 2,283 abdominal CT slices from 231 cases, with corresponding textual annotations and segmentation masks for various liver lesions in images. We validated the performance of LSMS for MIRS and conventional medical image segmentation tasks across various datasets. Our LSMS consistently outperforms on all datasets with lower computational costs. The code and datasets will be released.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-30</td>
<td style='padding: 8px;'>Evaluating Reliability in Medical DNNs: A Critical Analysis of Feature and Confidence-Based OOD Detection</td>
<td style='padding: 6px;'>Harry Anthony, Konstantinos Kamnitsas</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.17337v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Reliable use of deep neural networks (DNNs) for medical image analysis requires methods to identify inputs that differ significantly from the training data, called out-of-distribution (OOD), to prevent erroneous predictions. OOD detection methods can be categorised as either confidence-based (using the model's output layer for OOD detection) or feature-based (not using the output layer). We created two new OOD benchmarks by dividing the D7P (dermatology) and BreastMNIST (ultrasound) datasets into subsets which either contain or don't contain an artefact (rulers or annotations respectively). Models were trained with artefact-free images, and images with the artefacts were used as OOD test sets. For each OOD image, we created a counterfactual by manually removing the artefact via image processing, to assess the artefact's impact on the model's predictions. We show that OOD artefacts can boost a model's softmax confidence in its predictions, due to correlations in training data among other factors. This contradicts the common assumption that OOD artefacts should lead to more uncertain outputs, an assumption on which most confidence-based methods rely. We use this to explain why feature-based methods (e.g. Mahalanobis score) typically have greater OOD detection performance than confidence-based methods (e.g. MCP). However, we also show that feature-based methods typically perform worse at distinguishing between inputs that lead to correct and incorrect predictions (for both OOD and ID data). Following from these insights, we argue that a combination of feature-based and confidence-based methods should be used within DNN pipelines to mitigate their respective weaknesses. These project's code and OOD benchmarks are available at: https://github.com/HarryAnthony/Evaluating_OOD_detection.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-30</td>
<td style='padding: 8px;'>Estimation of Cardiac and Non-cardiac Diagnosis from Electrocardiogram Features</td>
<td style='padding: 6px;'>Juan Miguel Lopez Alcaraz, Nils Strodthoff</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.17329v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Introduction: Ensuring timely and accurate diagnosis of medical conditions is paramount for effective patient care. Electrocardiogram (ECG) signals are fundamental for evaluating a patient's cardiac health and are readily available. Despite this, little attention has been given to the remarkable potential of ECG data in detecting non-cardiac conditions.   Methods: In our study, we used publicly available datasets (MIMIC-IV-ECG-ICD and ECG-VIEW II) to investigate the feasibility of inferring general diagnostic conditions from ECG features. To this end, we trained a tree-based model (XGBoost) based on ECG features and basic demographic features to estimate a wide range of diagnoses, encompassing both cardiac and non-cardiac conditions.   Results: Our results demonstrate the reliability of estimating 23 cardiac as well as 21 non-cardiac conditions above 0.7 AUROC in a statistically significant manner across a wide range of physiological categories. Our findings underscore the predictive potential of ECG data in identifying well-known cardiac conditions. However, even more striking, this research represents a pioneering effort in systematically expanding the scope of ECG-based diagnosis to conditions not traditionally associated with the cardiac system.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-30</td>
<td style='padding: 8px;'>Democratizing AI in Africa: FL for Low-Resource Edge Devices</td>
<td style='padding: 6px;'>Jorge Fabila, Vctor M. Campello, Carlos Martn-Isla, Johnes Obungoloch, Kinyera Leo, Amodoi Ronald, Karim Lekadir</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.17216v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Africa faces significant challenges in healthcare delivery due to limited infrastructure and access to advanced medical technologies. This study explores the use of federated learning to overcome these barriers, focusing on perinatal health. We trained a fetal plane classifier using perinatal data from five African countries: Algeria, Ghana, Egypt, Malawi, and Uganda, along with data from Spanish hospitals. To incorporate the lack of computational resources in the analysis, we considered a heterogeneous set of devices, including a Raspberry Pi and several laptops, for model training. We demonstrate comparative performance between a centralized and a federated model, despite the compute limitations, and a significant improvement in model generalizability when compared to models trained only locally. These results show the potential for a future implementation at a large scale of a federated learning platform to bridge the accessibility gap and improve model generalizability with very little requirements.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-30</td>
<td style='padding: 8px;'>Improving Extraction of Clinical Event Contextual Properties from Electronic Health Records: A Comparative Study</td>
<td style='padding: 6px;'>Shubham Agarwal, Thomas Searle, Mart Ratas, Anthony Shek, James Teo, Richard Dobson</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.17181v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electronic Health Records are large repositories of valuable clinical data, with a significant portion stored in unstructured text format. This textual data includes clinical events (e.g., disorders, symptoms, findings, medications and procedures) in context that if extracted accurately at scale can unlock valuable downstream applications such as disease prediction. Using an existing Named Entity Recognition and Linking methodology, MedCAT, these identified concepts need to be further classified (contextualised) for their relevance to the patient, and their temporal and negated status for example, to be useful downstream. This study performs a comparative analysis of various natural language models for medical text classification. Extensive experimentation reveals the effectiveness of transformer-based language models, particularly BERT. When combined with class imbalance mitigation techniques, BERT outperforms Bi-LSTM models by up to 28% and the baseline BERT model by up to 16% for recall of the minority classes. The method has been implemented as part of CogStack/MedCAT framework and made available to the community for further research.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-30</td>
<td style='padding: 8px;'>A theoretical framework for the assessment of water fraction-dependent longitudinal decay rates and magnetisation transfer in membrane lipid phantoms</td>
<td style='padding: 6px;'>Heiko Neeb, Felix Schyboll, Rona Shaharabani, Aviv A. Mezer, Oshrat Shtangel</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.17085v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Phantom systems consisting of liposome suspensions are widely employed to investigate quantitative MRI parameters mimicking cellular membranes. The proper physical understanding of the measurement results, however, requires proper models for liposomes and their interaction with the surrounding water molecules. Here, we present an MD-based approach for the theoretical prediction of R1=1/T1, the dependence of R1 on water concentration and the magnetization exchange between lipids and interacting water layer in lipids and lipid mixtures. Moreover, a new parameter is introduced which quantitatively measures the amount of hydration water (hydration water fraction, f_HW) based on conventional spoiled gradient echo MR acquisitions. Both f_HW and the magnetisation exchange rate between lipids and hydration water were determined quantitatively from spoiled gradient echo data. We observed that liposome systems behaved similarly, apart from PLPC which showed both lower hydration water fraction and lower exchange rate. The extracted parameters accurately predicted the measured water fraction-dependent R1 rates and allowed for a theoretical understanding of MR parameters in liposomes of different composition.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2024-08-30</td>
<td style='padding: 8px;'>Disease Classification and Impact of Pretrained Deep Convolution Neural Networks on Diverse Medical Imaging Datasets across Imaging Modalities</td>
<td style='padding: 6px;'>Jutika Borah, Kumaresh Sarmah, Hidam Kumarjit Singh</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2408.17011v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Imaging techniques such as Chest X-rays, whole slide images, and optical coherence tomography serve as the initial screening and detection for a wide variety of medical pulmonary and ophthalmic conditions respectively. This paper investigates the intricacies of using pretrained deep convolutional neural networks with transfer learning across diverse medical imaging datasets with varying modalities for binary and multiclass classification. We conducted a comprehensive performance analysis with ten network architectures and model families each with pretraining and random initialization. Our finding showed that the use of pretrained models as fixed feature extractors yields poor performance irrespective of the datasets. Contrary, histopathology microscopy whole slide images have better performance. It is also found that deeper and more complex architectures did not necessarily result in the best performance. This observation implies that the improvements in ImageNet are not parallel to the medical imaging tasks. Within a medical domain, the performance of the network architectures varies within model families with shifts in datasets. This indicates that the performance of models within a specific modality may not be conclusive for another modality within the same domain. This study provides a deeper understanding of the applications of deep learning techniques in medical imaging and highlights the impact of pretrained networks across different medical imaging datasets under five different experimental settings.</td>
</tr>
</tbody>
</table>

