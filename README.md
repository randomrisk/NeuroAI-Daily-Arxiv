<h2 style='font-family: Helvetica, Arial, sans-serif; font-size: 16px; color: #4a4a4a;'>Updated on 2025-11-06</h2>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>Brain</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Microbes in the Moonlight: How the Gut Microbiota Influences Sleep</td>
<td style='padding: 6px;'>Enso Onill Torres Alegre</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02766v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The gut microbiota has emerged as a fundamental regulator of sleep physiology, influencing neural, endocrine, and immune pathways through the gut-microbiota-brain axis (GMBA). This bidirectional communication system modulates neurotransmitter production, circadian rhythms, and metabolic homeostasis, while disruptions in microbial composition have been linked to sleep disorders, neuroinflammation, and systemic immune dysfunction. Recent findings suggest that gut dysbiosis contributes to sleep disturbances by altering serotonin, GABA, and short-chain fatty acid (SCFA) metabolism, with implications for neurodegenerative diseases, metabolic syndromes, and mood disorders. Additionally, the gut microbiota interacts with the endocrine and immune systems, shaping inflammatory responses and stress adaptation mechanisms. This review explores the intricate connections between sleep and the gut microbiota, integrating emerging research on microbiota-targeted therapies, such as probiotics, fecal microbiota transplantation (FMT), and chrononutrition, as potential interventions to restore sleep homeostasis and improve health outcomes</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Spatial Insight: How Data-Driven Regions of Interest Selection Enhances Single-Trial P300 Classification in EEG-Based BCIs</td>
<td style='padding: 6px;'>Eva Guttmann-Flury, Jian Zhao, Mohamad Sawan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02735v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>EEG-based Brain-Computer Interfaces (BCIs) frequently face spatial specificity limitations in detecting single-trial P300 potentials, a neurophysiological hallmark leveraged for both BCI control and neurodegenerative disease diagnostics. We present a novel framework combining eLORETA source localization with cross-subject functional connectivity to identify stable regions of interest (ROIs) across sessions. Analyzing 62-channel EEG data from 31 subjects (63 sessions, 2,520 trials), we demonstrate that phase-lagged connectivity metrics can reliably isolate task-relevant hubs in deeper cortical-subcortical structures like the insula and parietal regions - critical for Alzheimer's disease biomarkers. By integrating spatially stable ROIs with dynamic temporal agreement, our hybrid classification systematically outperforms whole-brain approaches in different frequency bands (up to 5.4% depending on the connectivity method and the spectral range) while maintaining millisecond-level temporal precision.   To the best of our knowledge, this is the first study to establish cross-subject ROI consensus through source-space connectivity, bypassing scalp EEG's depth constraints to probe Alzheimer's-relevant networks. The framework's robustness to noise and compatibility with portable systems offer significant potential for global deployment in early neurodegenerative disease detection. Future integration of individualized anatomical data or adaptive parameter optimization could refine this tool for clinical deployment, enhancing the current max accuracy of 81.57% in the 1-15 Hz range.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Association-sensory spatiotemporal hierarchy and functional gradient-regularised recurrent neural network with implications for schizophrenia</td>
<td style='padding: 6px;'>Subati Abulikemu, Puria Radmard, Michail Mamalakis, John Suckling</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02722v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The human neocortex is functionally organised at its highest level along a continuous sensory-to-association (AS) hierarchy. This study characterises the AS hierarchy of patients with schizophrenia in a comparison with controls. Using a large fMRI dataset (N=355), we extracted individual AS gradients via spectral analysis of brain connectivity, quantified hierarchical specialisation by gradient spread, and related this spread with connectivity geometry. We found that schizophrenia compresses the AS hierarchy indicating reduced functional differentiation. By modelling neural timescale with the Ornstein-Uhlenbeck process, we observed that the most specialised, locally cohesive regions at the gradient extremes exhibit dynamics with a longer time constant, an effect that is attenuated in schizophrenia. To study computation, we used the gradients to regularise subject-specific recurrent neural networks (RNNs) trained on working memory tasks. Networks endowed with greater gradient spread learned more efficiently, plateaued at lower task loss, and maintained stronger alignment to the prescribed AS hierarchical geometry. Fixed point linearisation showed that high-range networks settled into more stable neural states during memory delay, evidenced by lower energy and smaller maximal Jacobian eigenvalues. This gradient-regularised RNN framework therefore links large-scale cortical architecture with fixed point stability, providing a mechanistic account of how gradient de-differentiation could destabilise neural computations in schizophrenia, convergently supported by empirical timescale flattening and model-based evidence of less stable fixed points.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding</td>
<td style='padding: 6px;'>Jingyu Lu, Haonan Wang, Qixiang Zhang, Xiaomeng Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02565v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Subject-agnostic brain decoding, which aims to reconstruct continuous visual experiences from fMRI without subject-specific training, holds great potential for clinical applications. However, this direction remains underexplored due to challenges in cross-subject generalization and the complex nature of brain signals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a novel hierarchical decoding framework that explicitly models the ventral-dorsal architecture of the human visual system to learn multi-dimensional representations. By disentangling and leveraging features from early visual cortex, ventral, and dorsal streams, VCFlow captures diverse and complementary cognitive information essential for visual reconstruction. Furthermore, we introduce a feature-level contrastive learning strategy to enhance the extraction of subject-invariant semantic representations, thereby enhancing subject-agnostic applicability to previously unseen subjects. Unlike conventional pipelines that need more than 12 hours of per-subject data and heavy computation, VCFlow sacrifices only 7\% accuracy on average yet generates each reconstructed video in 10 seconds without any retraining, offering a fast and clinically scalable solution. The source code will be released upon acceptance of the paper.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction</td>
<td style='padding: 6px;'>Ali Farki, Elaheh Moradi, Deepika Koundal, Jussi Tohka</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02558v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Predicting future brain state from a baseline magnetic resonance image (MRI) is a central challenge in neuroimaging and has important implications for studying neurodegenerative diseases such as Alzheimer's disease (AD). Most existing approaches predict future cognitive scores or clinical outcomes, such as conversion from mild cognitive impairment to dementia. Instead, here we investigate longitudinal MRI image-to-image prediction that forecasts a participant's entire brain MRI several years into the future, intrinsically modeling complex, spatially distributed neurodegenerative patterns. We implement and evaluate five deep learning architectures (UNet, U2-Net, UNETR, Time-Embedding UNet, and ODE-UNet) on two longitudinal cohorts (ADNI and AIBL). Predicted follow-up MRIs are directly compared with the actual follow-up scans using metrics that capture global similarity and local differences. The best performing models achieve high-fidelity predictions, and all models generalize well to an independent external dataset, demonstrating robust cross-cohort performance. Our results indicate that deep learning can reliably predict participant-specific brain MRI at the voxel level, offering new opportunities for individualized prognosis.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Causal Graph Neural Networks for Healthcare</td>
<td style='padding: 6px;'>Munib Mesinovic, Max Buhlan, Tingting Zhu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02531v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Healthcare artificial intelligence systems routinely fail when deployed across institutions, with documented performance drops and perpetuation of discriminatory patterns embedded in historical data. This brittleness stems, in part, from learning statistical associations rather than causal mechanisms. Causal graph neural networks address this triple crisis of distribution shift, discrimination, and inscrutability by combining graph-based representations of biomedical data with causal inference principles to learn invariant mechanisms rather than spurious correlations. This Review examines methodological foundations spanning structural causal models, disentangled causal representation learning, and techniques for interventional prediction and counterfactual reasoning on graphs. We analyse applications demonstrating clinical value across psychiatric diagnosis through brain network analysis, cancer subtyping via multi-omics causal integration, continuous physiological monitoring with mechanistic interpretation, and drug recommendation correcting prescription bias. These advances establish foundations for patient-specific Causal Digital Twins, enabling in silico clinical experimentation, with integration of large language models for hypothesis generation and causal graph neural networks for mechanistic validation. Substantial barriers remain, including computational requirements precluding real-time deployment, validation challenges demanding multi-modal evidence triangulation beyond cross-validation, and risks of causal-washing where methods employ causal terminology without rigorous evidentiary support. We propose tiered frameworks distinguishing causally-inspired architectures from causally-validated discoveries and identify critical research priorities making causal rather than purely associational claims.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring</td>
<td style='padding: 6px;'>Rajan Das Gupta, Md Kishor Morol, Nafiz Fahad, Md Tanzib Hosain, Sumaya Binte Zilani Choya, Md Jakir Hossen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02490v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As the global burden of Alzheimer's disease (AD) continues to grow, early and accurate detection has become increasingly critical, especially in regions with limited access to advanced diagnostic tools. We propose BRAINS (Biomedical Retrieval-Augmented Intelligence for Neurodegeneration Screening) to address this challenge. This novel system harnesses the powerful reasoning capabilities of Large Language Models (LLMs) for Alzheimer's detection and monitoring. BRAINS features a dual-module architecture: a cognitive diagnostic module and a case-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on cognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain volume metrics -- to perform structured assessments of Alzheimer's risk. Meanwhile, the Case Retrieval Module encodes patient profiles into latent representations and retrieves similar cases from a curated knowledge base. These auxiliary cases are fused with the input profile via a Case Fusion Layer to enhance contextual understanding. The combined representation is then processed with clinical prompts for inference. Evaluations on real-world datasets demonstrate BRAINS effectiveness in classifying disease severity and identifying early signs of cognitive decline. This system not only shows strong potential as an assistive tool for scalable, explainable, and early-stage Alzheimer's disease detection, but also offers hope for future applications in the field.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Investigating Brain Connectivity and Information Flow in Mental Workload Using EEG and fNIRS Integration</td>
<td style='padding: 6px;'>Mohaddese Qaremohammadlou, Mohammad Bagher Shamsollahi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02457v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study investigates brain connectivity and information flow during mental workload (MWL) by integrating electroencephalogram (EEG) and functional near-infrared spectroscopy (fNIRS) signals. Utilizing the N-back task to induce varying levels of MWL in 26 participants, we analyzed both functional and effective connectivity across 25 cortical regions derived from combined EEG and fNIRS signals. Functional connectivity was assessed using Pearson Correlation Coefficient (PCC), Phase Locking Value (PLV), and Magnitude Squared Coherence (MSC), while effective connectivity was evaluated using directed Directed Transfer Function (dDTF) and generalized Partial Directed Coherence (gPDC). Our findings reveal increased functional connectivity in frontal regions during higher MWL conditions (3-back compared to 0-back). Furthermore, effective connectivity analysis demonstrates a significant directional information flow from EEG to fNIRS, indicating a dominant influence of neural activity on hemodynamic responses. Statistical tests confirm significant differences in connectivity patterns between low and high MWL states. These results underscore the utility of EEG-fNIRS integration for characterizing brain network dynamics under varying cognitive demands and provide insights into neurovascular coupling mechanisms during mental workload.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-03</td>
<td style='padding: 8px;'>NeuResonance: Exploring Feedback Experiences for Fostering the Inter-brain Synchronization</td>
<td style='padding: 6px;'>Jamie Ngoc Dinh, Snehesh Shrestha, You-Jin Kim, Jun Nishida, Myungin Lee</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02079v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>When several individuals collaborate on a shared task, their brain activities often synchronize. This phenomenon, known as Inter-brain Synchronization (IBS), is notable for inducing prosocial outcomes such as enhanced interpersonal feelings, including closeness, trust, empathy, and more. Further strengthening the IBS with the aid of external feedback would be beneficial for scenarios where those prosocial feelings play a vital role in interpersonal communication, such as rehabilitation between a therapist and a patient, motor skill learning between a teacher and a student, and group performance art. This paper investigates whether visual, auditory, and haptic feedback of the IBS level can further enhance its intensity, offering design recommendations for feedback systems in IBS. We report findings when three different types of feedback were provided: IBS level feedback by means of on-body projection mapping, sonification using chords, and vibration bands attached to the wrist.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-03</td>
<td style='padding: 8px;'>Neural dynamics of cognitive control: Current tensions and future promise</td>
<td style='padding: 6px;'>Dale Zhou, Danielle Cosme, Yoona Kang, Ovidia Stanoi, David M. Lydon-Staley, Peter J. Mucha, Emily B. Falk, Kevin N. Ochsner, Dani S. Bassett</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02063v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Cognitive control is a suite of processes that helps individuals pursue goals despite resistance or uncertainty about what to do. Although cognitive control has been extensively studied as a dynamic feedback loop of perception, valuation, and action, it remains incompletely understood as a cohesive dynamic and distributed neural process. Here, we critically examine the history of and advances in the study of cognitive control, including how metaphors and cultural norms of power, morality, and rationality are intertwined with definitions of control, to consider holistically how different models explain which brain regions act as controllers. Controllers, the source of top-down signals, are typically localized in regions whose neural activations implement elementary component processes of control, including conflict monitoring and behavioral inhibition. Top-down signals from these regions guide the activation of other task-specific regions, biasing them towards task-specific activity patterns. A relatively new approach, network control theory, has roots in dynamical systems theory and systems engineering. This approach can mathematically show that controllers are regions with strongly nested and recurrent anatomical connectivity that efficiently propagate top-down signals, and precisely estimate the amount, location, and timing of signaling required to bias global activity to task-specific patterns. The theory converges with prior evidence, offers new mathematical tools and intuitions for understanding control loops across levels of analysis, and naturally produces graded predictions of control across brain regions and modules of psychological function that have been unconsidered or marginalized. We describe how prior approaches converge and diverge, noting directions for future integration to improve understanding of how the brain instantiates cognitive control.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>EEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Spatial Insight: How Data-Driven Regions of Interest Selection Enhances Single-Trial P300 Classification in EEG-Based BCIs</td>
<td style='padding: 6px;'>Eva Guttmann-Flury, Jian Zhao, Mohamad Sawan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02735v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>EEG-based Brain-Computer Interfaces (BCIs) frequently face spatial specificity limitations in detecting single-trial P300 potentials, a neurophysiological hallmark leveraged for both BCI control and neurodegenerative disease diagnostics. We present a novel framework combining eLORETA source localization with cross-subject functional connectivity to identify stable regions of interest (ROIs) across sessions. Analyzing 62-channel EEG data from 31 subjects (63 sessions, 2,520 trials), we demonstrate that phase-lagged connectivity metrics can reliably isolate task-relevant hubs in deeper cortical-subcortical structures like the insula and parietal regions - critical for Alzheimer's disease biomarkers. By integrating spatially stable ROIs with dynamic temporal agreement, our hybrid classification systematically outperforms whole-brain approaches in different frequency bands (up to 5.4% depending on the connectivity method and the spectral range) while maintaining millisecond-level temporal precision.   To the best of our knowledge, this is the first study to establish cross-subject ROI consensus through source-space connectivity, bypassing scalp EEG's depth constraints to probe Alzheimer's-relevant networks. The framework's robustness to noise and compatibility with portable systems offer significant potential for global deployment in early neurodegenerative disease detection. Future integration of individualized anatomical data or adaptive parameter optimization could refine this tool for clinical deployment, enhancing the current max accuracy of 81.57% in the 1-15 Hz range.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Investigating Brain Connectivity and Information Flow in Mental Workload Using EEG and fNIRS Integration</td>
<td style='padding: 6px;'>Mohaddese Qaremohammadlou, Mohammad Bagher Shamsollahi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02457v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study investigates brain connectivity and information flow during mental workload (MWL) by integrating electroencephalogram (EEG) and functional near-infrared spectroscopy (fNIRS) signals. Utilizing the N-back task to induce varying levels of MWL in 26 participants, we analyzed both functional and effective connectivity across 25 cortical regions derived from combined EEG and fNIRS signals. Functional connectivity was assessed using Pearson Correlation Coefficient (PCC), Phase Locking Value (PLV), and Magnitude Squared Coherence (MSC), while effective connectivity was evaluated using directed Directed Transfer Function (dDTF) and generalized Partial Directed Coherence (gPDC). Our findings reveal increased functional connectivity in frontal regions during higher MWL conditions (3-back compared to 0-back). Furthermore, effective connectivity analysis demonstrates a significant directional information flow from EEG to fNIRS, indicating a dominant influence of neural activity on hemodynamic responses. Statistical tests confirm significant differences in connectivity patterns between low and high MWL states. These results underscore the utility of EEG-fNIRS integration for characterizing brain network dynamics under varying cognitive demands and provide insights into neurovascular coupling mechanisms during mental workload.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-03</td>
<td style='padding: 8px;'>Variational Representational Similarity Analysis (vRSA) for M/EEG</td>
<td style='padding: 6px;'>Alex Lepauvre, Lucia Melloni, Karl Friston, Peter Zeidman</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.01784v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper introduces variational representational similarity analysis RSA (vRSA) for electromagnetic recordings of neural responses (e.g., EEG, MEG, ECoG or LFP). Variational RSA is a Bayesian approach for testing whether the similarity of stimuli or experimental conditions is expressed in univariate or multivariate neural recordings. Extending an approach previously introduced in the context of functional MRI, vRSA decomposes the condition-by-condition data covariance matrix into hypothesised effects and observation noise, thereby casting RSA as a covariance component estimation problem. In this context, peristimulus time may be treated as an experimental factor, enabling one to test for the probability that different experimental effects are expressed in data at different times. Variational Bayesian methods are used for model estimation and model comparison, which confer a number of advantages over classical approaches, including statistically efficient hypothesis testing, quantification of uncertainty using Bayesian credible intervals and computational efficiency. After introducing the theory, we provide a worked example using openly available EEG data. Software functions implementing vRSA for the SPM software package accompany this paper, together with exemplar analysis scripts.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-03</td>
<td style='padding: 8px;'>NeuroClean: A Generalized Machine-Learning Approach to Neural Time-Series Conditioning</td>
<td style='padding: 6px;'>Manuel A. Hernandez Alonso, Michael Depass, Stephan Quessy, Numa Dancause, Ignasi Cos</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.01951v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Electroencephalography (EEG) and local field potentials (LFP) are two widely used techniques to record electrical activity from the brain. These signals are used in both the clinical and research domains for multiple applications. However, most brain data recordings suffer from a myriad of artifacts and noise sources other than the brain itself. Thus, a major requirement for their use is proper and, given current volumes of data, a fully automatized conditioning. As a means to this end, here we introduce an unsupervised, multipurpose EEG/LFP preprocessing method, the NeuroClean pipeline. In addition to its completeness and reliability, NeuroClean is an unsupervised series of algorithms intended to mitigate reproducibility issues and biases caused by human intervention. The pipeline is designed as a five-step process, including the common bandpass and line noise filtering, and bad channel rejection. However, it incorporates an efficient independent component analysis with an automatic component rejection based on a clustering algorithm. This machine learning classifier is used to ensure that task-relevant information is preserved after each step of the cleaning process. We used several data sets to validate the pipeline. NeuroClean removed several common types of artifacts from the signal. Moreover, in the context of motor tasks of varying complexity, it yielded more than 97% accuracy (vs. a chance-level of 33.3%) in an optimized Multinomial Logistic Regression model after cleaning the data, compared to the raw data, which performed at 74% accuracy. These results show that NeuroClean is a promising pipeline and workflow that can be applied to future work and studies to achieve better generalization and performance on machine learning pipelines.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-03</td>
<td style='padding: 8px;'>Embodied Cognition Augmented End2End Autonomous Driving</td>
<td style='padding: 6px;'>Ling Niu, Xiaoji Zheng, Han Wang, Chen Zheng, Ziyuan Yang, Bokui Chen, Jiangtao Gong</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.01334v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In recent years, vision-based end-to-end autonomous driving has emerged as a new paradigm. However, popular end-to-end approaches typically rely on visual feature extraction networks trained under label supervision. This limited supervision framework restricts the generality and applicability of driving models. In this paper, we propose a novel paradigm termed $E^{3}AD$, which advocates for comparative learning between visual feature extraction networks and the general EEG large model, in order to learn latent human driving cognition for enhancing end-to-end planning. In this work, we collected a cognitive dataset for the mentioned contrastive learning process. Subsequently, we investigated the methods and potential mechanisms for enhancing end-to-end planning with human driving cognition, using popular driving models as baselines on publicly available autonomous driving datasets. Both open-loop and closed-loop tests are conducted for a comprehensive evaluation of planning performance. Experimental results demonstrate that the $E^{3}AD$ paradigm significantly enhances the end-to-end planning performance of baseline models. Ablation studies further validate the contribution of driving cognition and the effectiveness of comparative learning process. To the best of our knowledge, this is the first work to integrate human driving cognition for improving end-to-end autonomous driving planning. It represents an initial attempt to incorporate embodied cognitive data into end-to-end autonomous driving, providing valuable insights for future brain-inspired autonomous driving systems. Our code will be made available at Github</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-03</td>
<td style='padding: 8px;'>Adversarial Spatio-Temporal Attention Networks for Epileptic Seizure Forecasting</td>
<td style='padding: 6px;'>Zan Li, Kyongmin Yeo, Wesley Gifford, Lara Marcuse, Madeline Fields, Bülent Yener</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.01275v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Forecasting epileptic seizures from multivariate EEG signals represents a critical challenge in healthcare time series prediction, requiring high sensitivity, low false alarm rates, and subject-specific adaptability. We present STAN, an Adversarial Spatio-Temporal Attention Network that jointly models spatial brain connectivity and temporal neural dynamics through cascaded attention blocks with alternating spatial and temporal modules. Unlike existing approaches that assume fixed preictal durations or separately process spatial and temporal features, STAN captures bidirectional dependencies between spatial and temporal patterns through a unified cascaded architecture. Adversarial training with gradient penalty enables robust discrimination between interictal and preictal states learned from clearly defined 15-minute preictal windows. Continuous 90-minute pre-seizure monitoring reveals that the learned spatio-temporal attention patterns enable early detection: reliable alarms trigger at subject-specific times (typically 15-45 minutes before onset), reflecting the model's capacity to capture subtle preictal dynamics without requiring individualized training. Experiments on two benchmark EEG datasets (CHB-MIT scalp: 8 subjects, 46 events; MSSM intracranial: 4 subjects, 14 events) demonstrate state-of-the-art performance: 96.6% sensitivity with 0.011 false detections per hour and 94.2% sensitivity with 0.063 false detections per hour, respectively, while maintaining computational efficiency (2.3M parameters, 45 ms latency, 180 MB memory) for real-time edge deployment. Beyond epilepsy, the proposed framework provides a general paradigm for spatio-temporal forecasting in healthcare and other time series domains where individual heterogeneity and interpretability are crucial.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-01</td>
<td style='padding: 8px;'>Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet</td>
<td style='padding: 6px;'>Farjana Aktar, Mohd Ruhul Ameen, Akif Islam, Md Ekramul Hamid</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.00369v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Achieving both accurate and interpretable classification of motor imagery EEG remains a key challenge in brain computer interface (BCI) research. This paper compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS pipeline combines filter bank common spatial pattern feature extraction with fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet learns hierarchical spatial temporal representations directly from raw EEG data. In within-subject experiments, the fuzzy neural model performed better (68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43), while in cross-subject (LOSO) tests, the deep model exhibited stronger generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent +/- 16.22). The study provides practical guidance for selecting MI-BCI systems according to design goals: interpretability or robustness across users. Future investigations into transformer based and hybrid neuro symbolic frameworks are expected to advance transparent EEG decoding.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-31</td>
<td style='padding: 8px;'>Tailored robotic training improves hand function and proprioceptive processing in stroke survivors with proprioceptive deficits: A randomized controlled trial</td>
<td style='padding: 6px;'>Andria J. Farrens, Luis Garcia-Fernandez, Raymond Diaz Rojas, Jillian Obeso Estrada, Dylan Reinsdorf, Vicky Chan, Disha Gupta, Joel Perry, Eric Wolbrecht, An Do, Steven C. Cramer, David J. Reinkensmeyer</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.00259v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Precision rehabilitation aims to tailor movement training to improve outcomes. We tested whether proprioceptively-tailored robotic training improves hand function and neural processing in stroke survivors. Using a robotic finger exoskeleton, we tested two proprioceptively-tailored approaches: Propriopixel Training, which uses robot-facilitated, gamified movements to enhance proprioceptive processing, and Virtual Assistance Training, which reduces robotic aid to increase reliance on self-generated feedback. In a randomized controlled trial, forty-six chronic stroke survivors completed nine 2-hour sessions of Standard, Propriopixel or Virtual training. Among participants with proprioceptive deficits, Propriopixel ((Box and Block Test: 7 +/- 4.2, p=0.002) and Virtual Assistance (4.5 +/- 4.4 , p=0.068) yielded greater gains in hand function (Standard: 0.8 +/- 2.3 blocks). Proprioceptive gains correlated with improvements in hand function. Tailored training enhanced neural sensitivity to proprioceptive cues, evidenced by a novel EEG biomarker, the proprioceptive Contingent Negative Variation. These findings support proprioceptively-tailored training as a pathway to precision neurorehabilitation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-31</td>
<td style='padding: 8px;'>Leveraging Generic Time Series Foundation Models for EEG Classification</td>
<td style='padding: 6px;'>Théo Gnassounou, Yessin Moakher, Shifeng Xie, Vasilii Feofanov, Ievgen Redko</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.27522v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Foundation models for time series are emerging as powerful general-purpose backbones, yet their potential for domain-specific biomedical signals such as electroencephalography (EEG) remains rather unexplored. In this work, we investigate the applicability a recently proposed time series classification foundation model, to a different EEG tasks such as motor imagery classification and sleep stage prediction. We test two pretraining regimes: (a) pretraining on heterogeneous real-world time series from multiple domains, and (b) pretraining on purely synthetic data. We find that both variants yield strong performance, consistently outperforming EEGNet, a widely used convolutional baseline, and CBraMod, the most recent EEG-specific foundation model. These results suggest that generalist time series foundation models, even when pretrained on data of non-neural origin or on synthetic signals, can transfer effectively to EEG. Our findings highlight the promise of leveraging cross-domain pretrained models for brain signal analysis, suggesting that EEG may benefit from advances in the broader time series literature.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-31</td>
<td style='padding: 8px;'>Inferring trust in recommendation systems from brain, behavioural, and physiological data</td>
<td style='padding: 6px;'>Vincent K. M. Cheung, Pei-Cheng Shih, Masato Hirano, Masataka Goto, Shinichi Furuya</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.27272v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As people nowadays increasingly rely on artificial intelligence (AI) to curate information and make decisions, assigning the appropriate amount of trust in automated intelligent systems has become ever more important. However, current measurements of trust in automation still largely rely on self-reports that are subjective and disruptive to the user. Here, we take music recommendation as a model to investigate the neural and cognitive processes underlying trust in automation. We observed that system accuracy was directly related to users' trust and modulated the influence of recommendation cues on music preference. Modelling users' reward encoding process with a reinforcement learning model further revealed that system accuracy, expected reward, and prediction error were related to oscillatory neural activity recorded via EEG and changes in pupil diameter. Our results provide a neurally grounded account of calibrating trust in automation and highlight the promises of a multimodal approach towards developing trustable AI systems.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>BCI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Spatial Insight: How Data-Driven Regions of Interest Selection Enhances Single-Trial P300 Classification in EEG-Based BCIs</td>
<td style='padding: 6px;'>Eva Guttmann-Flury, Jian Zhao, Mohamad Sawan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02735v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>EEG-based Brain-Computer Interfaces (BCIs) frequently face spatial specificity limitations in detecting single-trial P300 potentials, a neurophysiological hallmark leveraged for both BCI control and neurodegenerative disease diagnostics. We present a novel framework combining eLORETA source localization with cross-subject functional connectivity to identify stable regions of interest (ROIs) across sessions. Analyzing 62-channel EEG data from 31 subjects (63 sessions, 2,520 trials), we demonstrate that phase-lagged connectivity metrics can reliably isolate task-relevant hubs in deeper cortical-subcortical structures like the insula and parietal regions - critical for Alzheimer's disease biomarkers. By integrating spatially stable ROIs with dynamic temporal agreement, our hybrid classification systematically outperforms whole-brain approaches in different frequency bands (up to 5.4% depending on the connectivity method and the spectral range) while maintaining millisecond-level temporal precision.   To the best of our knowledge, this is the first study to establish cross-subject ROI consensus through source-space connectivity, bypassing scalp EEG's depth constraints to probe Alzheimer's-relevant networks. The framework's robustness to noise and compatibility with portable systems offer significant potential for global deployment in early neurodegenerative disease detection. Future integration of individualized anatomical data or adaptive parameter optimization could refine this tool for clinical deployment, enhancing the current max accuracy of 81.57% in the 1-15 Hz range.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-01</td>
<td style='padding: 8px;'>Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet</td>
<td style='padding: 6px;'>Farjana Aktar, Mohd Ruhul Ameen, Akif Islam, Md Ekramul Hamid</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.00369v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Achieving both accurate and interpretable classification of motor imagery EEG remains a key challenge in brain computer interface (BCI) research. This paper compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS pipeline combines filter bank common spatial pattern feature extraction with fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet learns hierarchical spatial temporal representations directly from raw EEG data. In within-subject experiments, the fuzzy neural model performed better (68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43), while in cross-subject (LOSO) tests, the deep model exhibited stronger generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent +/- 16.22). The study provides practical guidance for selecting MI-BCI systems according to design goals: interpretability or robustness across users. Future investigations into transformer based and hybrid neuro symbolic frameworks are expected to advance transparent EEG decoding.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-31</td>
<td style='padding: 8px;'>Functional connectivity guided deep neural network for decoding high-level visual imagery</td>
<td style='padding: 6px;'>Byoung-Hee Kwon, Minji Lee, Seong-Whan Lee</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.27075v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This study introduces a pioneering approach in brain-computer interface (BCI) technology, featuring our novel concept of high-level visual imagery for non-invasive electroencephalography (EEG)-based communication. High-level visual imagery, as proposed in our work, involves the user engaging in the mental visualization of complex upper limb movements. This innovative approach significantly enhances the BCI system, facilitating the extension of its applications to more sophisticated tasks such as EEG-based robotic arm control. By leveraging this advanced form of visual imagery, our study opens new horizons for intricate and intuitive mind-controlled interfaces. We developed an advanced deep learning architecture that integrates functional connectivity metrics with a convolutional neural network-image transformer. This framework is adept at decoding subtle user intentions, addressing the spatial variability in high-level visual tasks, and effectively translating these into precise commands for robotic arm control. Our comprehensive offline and pseudo-online evaluations demonstrate the framework's efficacy in real-time applications, including the nuanced control of robotic arms. The robustness of our approach is further validated through leave-one-subject-out cross-validation, marking a significant step towards versatile, subject-independent BCI applications. This research highlights the transformative impact of advanced visual imagery and deep learning in enhancing the usability and adaptability of BCI systems, particularly in robotic arm manipulation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-25</td>
<td style='padding: 8px;'>Lateral Ventricular Brain-Computer Interface System with Lantern-Inspired Electrode for Stable Performance and Memory Decoding</td>
<td style='padding: 6px;'>Yike Sun, Yaxuan Gao, Kewei Wang, Jingnan Sun, Yuzhen Chen, Yanan Yang, Tianhua Zhao, Haochen Zhu, Ran Liu, Xiaogang Chen, Bai Lu, Xiaorong Gao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.22262v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We present a lateral ventricular brain-computer interface (LV-BCI) that deploys an expandable, flexible electrode into the lateral ventricle through a minimally invasive external ventricular drainage pathway. Inspired by the framework of traditional Chinese lanterns, the electrode expands uniformly within the ventricle and conforms to the ependymal wall. Compared with conventional subdural ECoG electrodes, the LV-BCI shows superior signal stability and immunocompatibility. Resting-state spectral analyses revealed a maximum effective bandwidth comparable to subdural ECoG. In evoked potential tests, the LV-BCI maintained a consistently higher signal-to-noise ratio over 112 days without the decline typically associated with scarring or other immune responses. Immunohistochemistry showed only a transient, early microglial activation after implantation, returning to control levels and remaining stable through 168 days. We further designed an "action-memory T-maze" task and developed a microstate sequence classifier (MSSC) to predict rats' turn decisions. The LV-BCI achieved prediction accuracy up to 98%, significantly outperforming subdural ECoG, indicating enhanced access to decision-related information from deep structures such as the hippocampus. These results establish the lateral ventricle as a viable route for neural signal acquisition. Using a lantern-inspired flexible electrode, we achieve long-term stable recordings and robust memory decision decoding from within the ventricular system, opening new directions for BCI technology and systems neuroscience.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-25</td>
<td style='padding: 8px;'>Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies</td>
<td style='padding: 6px;'>Yankai Chen, Xinni Zhang, Yifei Zhang, Yangning Li, Henry Peng Zou, Chunyu Miao, Weizhi Zhang, Xue Liu, Philip S. Yu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.22095v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-Computer Interfaces (BCIs) offer a direct communication pathway between the human brain and external devices, holding significant promise for individuals with severe neurological impairments. However, their widespread adoption is hindered by critical limitations, such as low information transfer rates and extensive user-specific calibration. To overcome these challenges, recent research has explored the integration of Large Language Models (LLMs), extending the focus from simple command decoding to understanding complex cognitive states. Despite these advancements, deploying agentic AI faces technical hurdles and ethical concerns. Due to the lack of comprehensive discussion on this emerging direction, this position paper argues that the field is poised for a paradigm extension from BCI to Brain-Agent Collaboration (BAC). We emphasize reframing agents as active and collaborative partners for intelligent assistance rather than passive brain signal data processors, demanding a focus on ethical data handling, model reliability, and a robust human-agent collaboration framework to ensure these systems are safe, trustworthy, and effective.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-30</td>
<td style='padding: 8px;'>Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset</td>
<td style='padding: 6px;'>Gereon Elvers, Gilad Landau, Oiwi Parker Jones</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.21038v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from large, public benchmarks. However, current benchmarks target relatively simple, foundational tasks like Speech Detection and Phoneme Classification, while application-ready results on tasks like Brain-to-Text remain elusive. We propose Keyword Spotting (KWS) as a practically applicable, privacy-aware intermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we provide standardized train/validation/test splits for reproducible benchmarking, and adopt an evaluation protocol tailored to extreme class imbalance. Concretely, we use area under the precision-recall curve (AUPRC) as a robust evaluation metric, complemented by false alarms per hour (FA/h) at fixed recall to capture user-facing trade-offs. To simplify deployment and further experimentation within the research community, we are releasing an updated version of the pnpl library with word-level dataloaders and Colab-ready tutorials. As an initial reference model, we present a compact 1-D Conv/ResNet baseline with focal loss and top-k pooling that is trainable on a single consumer-class GPU. The reference model achieves approximately 13x the permutation baseline AUPRC on held-out sessions, demonstrating the viability of the task. Exploratory analyses reveal: (i) predictable within-subject scaling - performance improves log-linearly with more training hours - and (ii) the existence of word-level factors (frequency and duration) that systematically modulate detectability.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-28</td>
<td style='padding: 8px;'>NeuroPilot: A Realtime Brain-Computer Interface system to enhance concentration of students in online learning</td>
<td style='padding: 6px;'>Asif Islam, Farhan Ishtiaque, Md. Muhyminul Haque, Farhana Sarker, Ravi Vaidyanathan, Khondaker A. Mamun</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.20958v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The prevalence of online learning poses a vital challenge in real-time monitoring of students' concentration. Traditional methods such as questionnaire assessments require manual intervention, and webcam-based monitoring fails to provide accurate insights about learners' mental focus as it is deceived by mere screen fixation without cognitive engagement. Existing BCI-based approaches lack real-time validation and evaluation procedures. To address these limitations, a Brain-Computer Interface (BCI) system is developed using a non-invasive Electroencephalogram (EEG) headband, FocusCalm, to record brainwave activity under attentive and non-attentive states. 20 minutes of data were collected from each of 20 participants watching a pre-recorded educational video. The data validation employed a novel intra-video questionnaire assessment. Subsequently, collected signals were segmented (sliding window), filtered (Butterworth bandpass), and cleaned (removal of high-amplitude and EOG artifacts such as eye blinks). Time, frequency, wavelet, and statistical features were extracted, followed by recursive feature elimination (RFE) with support vector machines (SVMs) to classify attention and non-attention states. The leave-one-subject-out (LOSO) cross-validation accuracy was found to be 88.77%. The system provides feedback alerts upon detection of a non-attention state and maintains focus profile logs. A pilot study was conducted to evaluate the effectiveness of real-time feedback. Five participants underwent a 10-minute session comprising a 5-minute baseline phase devoid of feedback, succeeded by a 5-minute feedback phase, during which alerts were activated if participants exhibited inattention for approximately 8 consecutive seconds. A paired t-test (t = 5.73, p = 0.007) indicated a statistically significant improvement in concentration during the feedback phase.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-23</td>
<td style='padding: 8px;'>A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks</td>
<td style='padding: 6px;'>Georgios Mentzelopoulos, Ioannis Asmanis, Konrad P. Kording, Eva L. Dyer, Kostas Daniilidis, Flavia Vitale</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.20683v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-computer interfaces (BCIs) promise to enable vital functions, such as speech and prosthetic control, for individuals with neuromotor impairments. Central to their success are neural decoders, models that map neural activity to intended behavior. Current learning-based decoding approaches fall into two classes: simple, causal models that lack generalization, or complex, non-causal models that generalize and scale offline but struggle in real-time settings. Both face a common challenge, their reliance on power-hungry artificial neural network backbones, which makes integration into real-world, resource-limited systems difficult. Spiking neural networks (SNNs) offer a promising alternative. Because they operate causally these models are suitable for real-time use, and their low energy demands make them ideal for battery-constrained environments. To this end, we introduce Spikachu: a scalable, causal, and energy-efficient neural decoding framework based on SNNs. Our approach processes binned spikes directly by projecting them into a shared latent space, where spiking modules, adapted to the timing of the input, extract relevant features; these latent representations are then integrated and decoded to generate behavioral predictions. We evaluate our approach on 113 recording sessions from 6 non-human primates, totaling 43 hours of recordings. Our method outperforms causal baselines when trained on single sessions using between 2.26 and 418.81 times less energy. Furthermore, we demonstrate that scaling up training to multiple sessions and subjects improves performance and enables few-shot transfer to unseen sessions, subjects, and tasks. Overall, Spikachu introduces a scalable, online-compatible neural decoding framework based on SNNs, whose performance is competitive relative to state-of-the-art models while consuming orders of magnitude less energy.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-22</td>
<td style='padding: 8px;'>RatioWaveNet: A Learnable RDWT Front-End for Robust and Interpretable EEG Motor-Imagery Classification</td>
<td style='padding: 6px;'>Marco Siino, Giuseppe Bonomo, Rosario Sorbello, Ilenia Tinnirello</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.21841v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Brain-computer interfaces (BCIs) based on motor imagery (MI) translate covert movement intentions into actionable commands, yet reliable decoding from non-invasive EEG remains challenging due to nonstationarity, low SNR, and subject variability. We present RatioWaveNet, which augments a strong temporal CNN-Transformer backbone (TCFormer) with a trainable, Rationally-Dilated Wavelet Transform (RDWT) front end. The RDWT performs an undecimated, multi-resolution subband decomposition that preserves temporal length and shift-invariance, enhancing sensorimotor rhythms while mitigating jitter and mild artifacts; subbands are fused via lightweight grouped 1-D convolutions and passed to a multi-kernel CNN for local temporal-spatial feature extraction, a grouped-query attention encoder for long-range context, and a compact TCN head for causal temporal integration.   Our goal is to test whether this principled wavelet front end improves robustness precisely where BCIs typically fail - on the hardest subjects - and whether such gains persist on average across seeds under both intra- and inter-subject protocols. On BCI-IV-2a and BCI-IV-2b, across five seeds, RatioWaveNet improves worst-subject accuracy over the Transformer backbone by +0.17 / +0.42 percentage points (Sub-Dependent / LOSO) on 2a and by +1.07 / +2.54 percentage points on 2b, with consistent average-case gains and modest computational overhead. These results indicate that a simple, trainable wavelet front end is an effective plug-in to strengthen Transformer-based BCIs, improving worst-case reliability without sacrificing efficiency.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-22</td>
<td style='padding: 8px;'>Step-Aware Residual-Guided Diffusion for EEG Spatial Super-Resolution</td>
<td style='padding: 6px;'>Hongjun Liu, Leyu Zhou, Zijianghao Yang, Chao Yao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.19166v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>For real-world BCI applications, lightweight Electroencephalography (EEG) systems offer the best cost-deployment balance. However, such spatial sparsity of EEG limits spatial fidelity, hurting learning and introducing bias. EEG spatial super-resolution methods aim to recover high-density EEG signals from sparse measurements, yet is often hindered by distribution shift and signal distortion and thus reducing fidelity and usability for EEG analysis and visualization. To overcome these challenges, we introduce SRGDiff, a step-aware residual-guided diffusion model that formulates EEG spatial super-resolution as dynamic conditional generation. Our key idea is to learn a dynamic residual condition from the low-density input that predicts the step-wise temporal and spatial details to add and uses the evolving cue to steer the denoising process toward high-density reconstructions. At each denoising step, the proposed residual condition is additively fused with the previous denoiser feature maps, then a step-dependent affine modulation scales and shifts the activation to produce the current features. This iterative procedure dynamically extracts step-wise temporal rhythms and spatial-topographic cues to steer high-density recovery and maintain a fidelity-consistency balance. We adopt a comprehensive evaluation protocol spanning signal-, feature-, and downstream-level metrics across SEED, SEED-IV, and Localize-MI and multiple upsampling scales. SRGDiff achieves consistent gains of up to 40% over strong baselines, proving its superiority in the task of EEG spatial super-resolution. Moreover, topographic visualizations comparison and substantial EEG-FID gains jointly indicate that our SR EEG mitigates the spatial-spectral shift between low- and high-density recordings.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>fMRI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Association-sensory spatiotemporal hierarchy and functional gradient-regularised recurrent neural network with implications for schizophrenia</td>
<td style='padding: 6px;'>Subati Abulikemu, Puria Radmard, Michail Mamalakis, John Suckling</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02722v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The human neocortex is functionally organised at its highest level along a continuous sensory-to-association (AS) hierarchy. This study characterises the AS hierarchy of patients with schizophrenia in a comparison with controls. Using a large fMRI dataset (N=355), we extracted individual AS gradients via spectral analysis of brain connectivity, quantified hierarchical specialisation by gradient spread, and related this spread with connectivity geometry. We found that schizophrenia compresses the AS hierarchy indicating reduced functional differentiation. By modelling neural timescale with the Ornstein-Uhlenbeck process, we observed that the most specialised, locally cohesive regions at the gradient extremes exhibit dynamics with a longer time constant, an effect that is attenuated in schizophrenia. To study computation, we used the gradients to regularise subject-specific recurrent neural networks (RNNs) trained on working memory tasks. Networks endowed with greater gradient spread learned more efficiently, plateaued at lower task loss, and maintained stronger alignment to the prescribed AS hierarchical geometry. Fixed point linearisation showed that high-range networks settled into more stable neural states during memory delay, evidenced by lower energy and smaller maximal Jacobian eigenvalues. This gradient-regularised RNN framework therefore links large-scale cortical architecture with fixed point stability, providing a mechanistic account of how gradient de-differentiation could destabilise neural computations in schizophrenia, convergently supported by empirical timescale flattening and model-based evidence of less stable fixed points.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding</td>
<td style='padding: 6px;'>Jingyu Lu, Haonan Wang, Qixiang Zhang, Xiaomeng Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02565v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Subject-agnostic brain decoding, which aims to reconstruct continuous visual experiences from fMRI without subject-specific training, holds great potential for clinical applications. However, this direction remains underexplored due to challenges in cross-subject generalization and the complex nature of brain signals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a novel hierarchical decoding framework that explicitly models the ventral-dorsal architecture of the human visual system to learn multi-dimensional representations. By disentangling and leveraging features from early visual cortex, ventral, and dorsal streams, VCFlow captures diverse and complementary cognitive information essential for visual reconstruction. Furthermore, we introduce a feature-level contrastive learning strategy to enhance the extraction of subject-invariant semantic representations, thereby enhancing subject-agnostic applicability to previously unseen subjects. Unlike conventional pipelines that need more than 12 hours of per-subject data and heavy computation, VCFlow sacrifices only 7\% accuracy on average yet generates each reconstructed video in 10 seconds without any retraining, offering a fast and clinically scalable solution. The source code will be released upon acceptance of the paper.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Cluster Size Matters: A Comparative Study of Notip and pARI for Post Hoc Inference in fMRI</td>
<td style='padding: 6px;'>Nils Peyrouset, Pierre Neuvial, Bertrand Thirion</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02422v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>All Resolutions Inference (ARI) is a post hoc inference method for functional Magnetic Resonance Imaging (fMRI) data analysis that provides valid lower bounds on the proportion of truly active voxels within any, possibly data-driven, cluster. As such, it addresses the paradox of spatial specificity encountered with more classical cluster-extent thresholding methods. It allows the cluster-forming threshold to be increased in order to locate the signal with greater spatial precision without overfitting, also known as the drill-down approach. Notip and pARI are two recent permutation-based extensions of ARI designed to increase statistical power by accounting for the strong dependence structure typical of fMRI data. A recent comparison between these papers based on large voxel clusters concluded that pARI outperforms Notip. We revisit this conclusion by conducting a systematic comparison of the two. Our reanalysis of the same fMRI data sets from the Neurovault database demonstrates the existence of complementary performance regimes: while pARI indeed achieves higher sensitivity for large clusters, Notip provides more informative and robust results for smaller clusters. In particular, while Notip supports informative ``drill-down'' exploration into subregions of activation, pARI often yields non-informative bounds in such cases, and can even underperform the baseline ARI method.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-01</td>
<td style='padding: 8px;'>Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model</td>
<td style='padding: 6px;'>Ruthwik Reddy Doodipala, Pankaj Pandey, Carolina Torres Rojas, Manob Jyoti Saikia, Ranganatha Sitaram</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.00443v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The emergence of foundation models in neuroimaging is driven by the increasing availability of large-scale and heterogeneous brain imaging datasets. Recent advances in self-supervised learning, particularly reconstruction-based objectives, have demonstrated strong potential for pretraining models that generalize effectively across diverse downstream functional MRI (fMRI) tasks. In this study, we explore region-aware reconstruction strategies for a foundation model in resting-state fMRI, moving beyond approaches that rely on random region masking. Specifically, we introduce an ROI-guided masking strategy using the Automated Anatomical Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively mask semantically coherent brain regions during self-supervised pretraining. Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI scans, we show that our method achieves a 4.23% improvement in classification accuracy for distinguishing healthy controls from individuals diagnosed with ADHD, compared to conventional random masking. Region-level attribution analysis reveals that brain volumes within the limbic region and cerebellum contribute most significantly to reconstruction fidelity and model representation. Our results demonstrate that masking anatomical regions during model pretraining not only enhances interpretability but also yields more robust and discriminative representations. In future work, we plan to extend this approach by evaluating it on additional neuroimaging datasets, and developing new loss functions explicitly derived from region-aware reconstruction objectives. These directions aim to further improve the robustness and interpretability of foundation models for functional neuroimaging.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-01</td>
<td style='padding: 8px;'>Is Representational Similarity Analysis Reliable? A Comparison with Regression</td>
<td style='padding: 6px;'>Chuanji Gao, Gang Chen, Svetlana V. Shinkareva, Rutvik H. Desai</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.00395v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Representational Similarity Analysis (RSA) is a popular method for analyzing neuroimaging and behavioral data. Here we evaluate the accuracy and reliability of RSA in the context of model selection, and compare it to that of regression. Although RSA offers flexibility in handling high-dimensional, cross-modal, and cross-species data, its reliance on a transformation of raw data into similarity structures may result in the loss of critical stimulus-response information. Across extensive simulation studies and empirical analyses, we show that RSA leads to lower model selection accuracy, regardless of sample size, noise level, feature dimensionality, or multicollinearity, relative to regression. While principal component analysis and feature reweighting mitigate RSA's deficits driven by multicollinearity, regression remains superior in accurately distinguishing between models. Empirical data and a follow-up fMRI simulation further support these conclusions. Our findings suggest that researchers should carefully consider which approach to use: RSA is less effective than linear regression for model selection and fitting when direct stimulus-response mappings are available.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-31</td>
<td style='padding: 8px;'>ZEBRA: Towards Zero-Shot Cross-Subject Generalization for Universal Brain Visual Decoding</td>
<td style='padding: 6px;'>Haonan Wang, Jingyu Lu, Hongrui Li, Xiaomeng Li</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.27128v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Recent advances in neural decoding have enabled the reconstruction of visual experiences from brain activity, positioning fMRI-to-image reconstruction as a promising bridge between neuroscience and computer vision. However, current methods predominantly rely on subject-specific models or require subject-specific fine-tuning, limiting their scalability and real-world applicability. In this work, we introduce ZEBRA, the first zero-shot brain visual decoding framework that eliminates the need for subject-specific adaptation. ZEBRA is built on the key insight that fMRI representations can be decomposed into subject-related and semantic-related components. By leveraging adversarial training, our method explicitly disentangles these components to isolate subject-invariant, semantic-specific representations. This disentanglement allows ZEBRA to generalize to unseen subjects without any additional fMRI data or retraining. Extensive experiments show that ZEBRA significantly outperforms zero-shot baselines and achieves performance comparable to fully finetuned models on several metrics. Our work represents a scalable and practical step toward universal neural decoding. Code and model weights are available at: https://github.com/xmed-lab/ZEBRA.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-30</td>
<td style='padding: 8px;'>Functional Connectome Fingerprinting Using Convolutional and Dictionary Learning</td>
<td style='padding: 6px;'>Yashaswini, Sanjay Ghosh</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.26120v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Advances in data analysis and machine learning have revolutionized the study of brain signatures using fMRI, enabling non-invasive exploration of cognition and behavior through individual neural patterns. Functional connectivity (FC), which quantifies statistical relationships between brain regions, has emerged as a key metric for studying individual variability and developing biomarkers for personalized medicine in neurological and psychiatric disorders. The concept of subject fingerprinting, introduced by Finn et al. (2015), leverages neural connectivity variability to identify individuals based on their unique patterns. While traditional FC methods perform well on small datasets, machine learning techniques are more effective with larger datasets, isolating individual-specific features and maximizing inter-subject differences. In this study, we propose a framework combining convolutional autoencoders and sparse dictionary learning to enhance fingerprint accuracy. Autoencoders capture shared connectivity patterns while isolating subject-specific features in residual FC matrices, which are analyzed using sparse coding to identify distinctive features. Tested on the Human Connectome Project dataset, this approach achieved a 10% improvement over baseline group-averaged FC models. Our results highlight the potential of integrating deep learning and sparse coding techniques for scalable and robust functional connectome fingerprinting, advancing personalized neuroscience applications and biomarker development.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-29</td>
<td style='padding: 8px;'>Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer</td>
<td style='padding: 6px;'>Roman Beliy, Amit Zalcher, Jonathan Kogman, Navve Wasserman, Michal Irani</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.25976v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Reconstructing images seen by people from their fMRI brain recordings provides a non-invasive window into the human brain. Despite recent progress enabled by diffusion models, current methods often lack faithfulness to the actual seen images. We present "Brain-IT", a brain-inspired approach that addresses this challenge through a Brain Interaction Transformer (BIT), allowing effective interactions between clusters of functionally-similar brain-voxels. These functional-clusters are shared by all subjects, serving as building blocks for integrating information both within and across brains. All model components are shared by all clusters & subjects, allowing efficient training with a limited amount of data. To guide the image reconstruction, BIT predicts two complementary localized patch-level image features: (i)high-level semantic features which steer the diffusion model toward the correct semantic content of the image; and (ii)low-level structural features which help to initialize the diffusion process with the correct coarse layout of the image. BIT's design enables direct flow of information from brain-voxel clusters to localized image features. Through these principles, our method achieves image reconstructions from fMRI that faithfully reconstruct the seen images, and surpass current SotA approaches both visually and by standard objective metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve results comparable to current methods trained on full 40-hour recordings.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-29</td>
<td style='padding: 8px;'>NeuroPathNet: Dynamic Path Trajectory Learning for Brain Functional Connectivity Analysis</td>
<td style='padding: 6px;'>Tianqi Guo, Liping Chen, Ciyuan Peng, Jingjing Zhou, Jing Ren</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.24025v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Understanding the evolution of brain functional networks over time is of great significance for the analysis of cognitive mechanisms and the diagnosis of neurological diseases. Existing methods often have difficulty in capturing the temporal evolution characteristics of connections between specific functional communities. To this end, this paper proposes a new path-level trajectory modeling framework (NeuroPathNet) to characterize the dynamic behavior of connection pathways between brain functional partitions. Based on medically supported static partitioning schemes (such as Yeo and Smith ICA), we extract the time series of connection strengths between each pair of functional partitions and model them using a temporal neural network. We validate the model performance on three public functional Magnetic Resonance Imaging (fMRI) datasets, and the results show that it outperforms existing mainstream methods in multiple indicators. This study can promote the development of dynamic graph learning methods for brain network analysis, and provide possible clinical applications for the diagnosis of neurological diseases.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-27</td>
<td style='padding: 8px;'>Leveraging LLMs for Early Alzheimer's Prediction</td>
<td style='padding: 6px;'>Tananun Songdechakraiwut</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.23946v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We present a connectome-informed LLM framework that encodes dynamic fMRI connectivity as temporal sequences, applies robust normalization, and maps these data into a representation suitable for a frozen pre-trained LLM for clinical prediction. Applied to early Alzheimer's detection, our method achieves sensitive prediction with error rates well below clinically recognized margins, with implications for timely Alzheimer's intervention.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>MEG</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-03</td>
<td style='padding: 8px;'>Variational Representational Similarity Analysis (vRSA) for M/EEG</td>
<td style='padding: 6px;'>Alex Lepauvre, Lucia Melloni, Karl Friston, Peter Zeidman</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.01784v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This paper introduces variational representational similarity analysis RSA (vRSA) for electromagnetic recordings of neural responses (e.g., EEG, MEG, ECoG or LFP). Variational RSA is a Bayesian approach for testing whether the similarity of stimuli or experimental conditions is expressed in univariate or multivariate neural recordings. Extending an approach previously introduced in the context of functional MRI, vRSA decomposes the condition-by-condition data covariance matrix into hypothesised effects and observation noise, thereby casting RSA as a covariance component estimation problem. In this context, peristimulus time may be treated as an experimental factor, enabling one to test for the probability that different experimental effects are expressed in data at different times. Variational Bayesian methods are used for model estimation and model comparison, which confer a number of advantages over classical approaches, including statistically efficient hypothesis testing, quantification of uncertainty using Bayesian credible intervals and computational efficiency. After introducing the theory, we provide a worked example using openly available EEG data. Software functions implementing vRSA for the SPM software package accompany this paper, together with exemplar analysis scripts.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-01</td>
<td style='padding: 8px;'>Smooth Models of Fibered Partially Hyperbolic Systems</td>
<td style='padding: 6px;'>Jonathan DeWitt, Meg Doucette, Oliver Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.00697v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We study fibered partially hyperbolic diffeomorphisms. We show that as long as certain topological obstructions vanish and as long as homological minimum expansion dominates the distortion on the fibers that a fibered partially hyperbolic system can be homotoped to a fibered partially hyperbolic system with a $C^{\infty}$-center fibering. In addition, we study obstructions to the existence of smooth lifts of Anosov diffeomorphisms to bundles. In particular, we give an example of smooth topologically trivial bundle over a torus, where an Anosov diffeomorphism can lift continuously but not smoothly to the bundle.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-29</td>
<td style='padding: 8px;'>Risk-Aware Safety Filters with Poisson Safety Functions and Laplace Guidance Fields</td>
<td style='padding: 6px;'>Gilbert Bahati, Ryan M. Bena, Meg Wilkinson, Pol Mestres, Ryan K. Cosner, Aaron D. Ames</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.25913v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Robotic systems navigating in real-world settings require a semantic understanding of their environment to properly determine safe actions. This work aims to develop the mathematical underpinnings of such a representation -- specifically, the goal is to develop safety filters that are risk-aware. To this end, we take a two step approach: encoding an understanding of the environment via Poisson's equation, and associated risk via Laplace guidance fields. That is, we first solve a Dirichlet problem for Poisson's equation to generate a safety function that encodes system safety as its 0-superlevel set. We then separately solve a Dirichlet problem for Laplace's equation to synthesize a safe \textit{guidance field} that encodes variable levels of caution around obstacles -- by enforcing a tunable flux boundary condition. The safety function and guidance fields are then combined to define a safety constraint and used to synthesize a risk-aware safety filter which, given a semantic understanding of an environment with associated risk levels of environmental features, guarantees safety while prioritizing avoidance of higher risk obstacles. We demonstrate this method in simulation and discuss how \textit{a priori} understandings of obstacle risk can be directly incorporated into the safety filter to generate safe behaviors that are risk-aware.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-28</td>
<td style='padding: 8px;'>Breaking the Timescale Barrier: Generative Discovery of Conformational Free-Energy Landscapes and Transition Pathways</td>
<td style='padding: 6px;'>Chenyu Tang, Mayank Prakash Pandey, Cheng Giuseppe Chen, Alberto Megías, François Dehez, Christophe Chipot</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.24979v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Molecular transitions -- such as protein folding, allostery, and membrane transport -- are central to biology yet remain notoriously difficult to simulate. Their intrinsic rarity pushes them beyond reach of standard molecular dynamics, while enhanced-sampling methods are costly and often depend on arbitrary variables that bias outcomes. We introduce Gen-COMPAS, a generative committor-guided path sampling framework that reconstructs transition pathways without predefined variables and at a fraction of the cost. Gen-COMPAS couples a generative diffusion model, which produces physically realistic intermediates, with committor-based filtering to pinpoint transition states. Short unbiased simulations from these intermediates rapidly yield full transition-path ensembles that converge within nanoseconds, where conventional methods require orders of magnitude more sampling. Applied to systems from a miniprotein to a ribose-binding protein to a mitochondrial carrier, Gen-COMPAS retrieves committors, transition states, and free-energy landscapes efficiently, uniting machine learning and molecular dynamics for broad mechanistic and practical insight.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-27</td>
<td style='padding: 8px;'>Molecular Gas in Major Mergers Hosting Dual and Single AGN at <10 kpc Nuclear Separations</td>
<td style='padding: 6px;'>Makoto A. Johnstone, Ezequiel Treister, Franz E. Bauer, Chin-Shin Chang, Claudia Cicone, Michael J. Koss, Ignacio del Moral-Castro, Francisco Muller-Sanchez, George C. Privon, Claudio Ricci, Nick Scoville, Giacomo Venturi, Loreto Barcos-Muñoz, Lee Armus, Laura Blecha, Caitlin Casey, Julia Comerford, Aaron Evans, Taiki Kawamuro, Anne M. Medling, Hugo Messias, Neil Nagar, Alejandra Rojas, David Sanders, Benny Trakhtenbrot, Vivian U, Meg Urry</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.23742v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We present high-resolution ($\sim$50$-$100 pc) Atacama Large Millimeter Array (ALMA) observations of $^{12}$CO(2-1) or $^{12}$CO(1-0) emission in seven local ($z$ $\lesssim$ 0.05) major mergers -- five of which are dual active galactic nuclei (AGN) systems, and two of which are single AGN systems. We model the molecular gas kinematics through rotating disk profiles using a Bayesian Markov chain Monte Carlo approach. The residuals were then used to isolate non-rotating components of the molecular gas -- the most likely contributor to future SMBH growth. We find that more massive SMBHs have higher surface densities of non-rotating molecular gas within their sphere of influence. This potential molecular gas supply, however, does not correlate with the current accretion efficiency of the SMBHs, suggesting that only a fraction of the observed non-rotating gas is currently reaching the SMBH. Finally, we tentatively find no significant differences in the nuclear molecular gas masses of single AGN and dual AGN hosts, both within the SMBH sphere of influence and within the central kiloparsec. Our results indicate that the probability of occurrence of the dual AGN phenomenon is likely dependent on AGN variability and/or obscuration rather than the availability of molecular gas in the nuclear regions.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-24</td>
<td style='padding: 8px;'>Automated interictal epileptic spike detection from simple and noisy annotations in MEG data</td>
<td style='padding: 6px;'>Pauline Mouches, Julien Jung, Armand Demasson, Agnès Guinard, Romain Bouet, Rosalie Marchal, Romain Quentin</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.21596v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>In drug-resistant epilepsy, presurgical evaluation of epilepsy can be considered. Magnetoencephalography (MEG) has been shown to be an effective exam to inform the localization of the epileptogenic zone through the localization of interictal epileptic spikes. Manual detection of these pathological biomarkers remains a fastidious and error-prone task due to the high dimensionality of MEG recordings, and interrater agreement has been reported to be only moderate. Current automated methods are unsuitable for clinical practice, either requiring extensively annotated data or lacking robustness on non-typical data. In this work, we demonstrate that deep learning models can be used for detecting interictal spikes in MEG recordings, even when only temporal and single-expert annotations are available, which represents real-world clinical practice. We propose two model architectures: a feature-based artificial neural network (ANN) and a convolutional neural network (CNN), trained on a database of 59 patients, and evaluated against a state-of-the-art model to classify short time windows of signal. In addition, we employ an interactive machine learning strategy to iteratively improve our data annotation quality using intermediary model outputs. Both proposed models outperform the state-of-the-art model (F1-scores: CNN=0.46, ANN=0.44) when tested on 10 holdout test patients. The interactive machine learning strategy demonstrates that our models are robust to noisy annotations. Overall, results highlight the robustness of models with simple architectures when analyzing complex and imperfectly annotated data. Our method of interactive machine learning offers great potential for faster data annotation, while our models represent useful and efficient tools for automated interictal spikes detection.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-22</td>
<td style='padding: 8px;'>Dictionary learning methods for brain activity mapping with MEG data</td>
<td style='padding: 6px;'>Daniela Calvetti, Erkki Somersalo</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.19702v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>A central goal in many brain studies is the identification of those brain regions that are activated during an observation window that may correspond to a motor task, a stimulus, or simply a resting state. While functional MRI is currently the most commonly employed modality for such task, methods based on the electromagnetic activity of the brain are valuable alternatives because of their excellent time resolution and of the fact that the measured signals are directly related to brain activation and not to a secondary effect such as the hemodynamic response. In this work we focus on the MEG modality, investigating the performance of a recently proposed Bayesian dictionary learning (BDL) algorithm for brain region identification. The partitioning of the source space into the 148 regions of interest (ROI) corresponding to parcellation of the Destrieux atlas provides a natural determination of the subdictionaries necessary for the BDL algorithm. We design a simulation protocol where a small randomly selected patch in each ROI is activated, the MEG signal is computed and the inverse problem of active brain region identification is solved using the BDL algorithm. The BDL algorithm consists of two phases, the first one comprising dictionary compression and Bayesian compression error analysis, and the second one performing dictionary coding with a deflated dictionary built on the output of the first phase, both steps relying on Bayesian sparsity promoting computations. For assessing the performance, we give a probabilistic interpretation of the confusion matrix, and consider different impurity measures for a multi-class classifier.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-20</td>
<td style='padding: 8px;'>MEG-GPT: A transformer-based foundation model for magnetoencephalography data</td>
<td style='padding: 6px;'>Rukuang Huang, Sungjun Cho, Chetan Gohil, Oiwi Parker Jones, Mark Woolrich</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.18080v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Modelling the complex spatiotemporal patterns of large-scale brain dynamics is crucial for neuroscience, but traditional methods fail to capture the rich structure in modalities such as magnetoencephalography (MEG). Recent advances in deep learning have enabled significant progress in other domains, such as language and vision, by using foundation models at scale. Here, we introduce MEG-GPT, a transformer based foundation model that uses time-attention and next time-point prediction. To facilitate this, we also introduce a novel data-driven tokeniser for continuous MEG data, which preserves the high temporal resolution of continuous MEG signals without lossy transformations. We trained MEG-GPT on tokenised brain region time-courses extracted from a large-scale MEG dataset (N=612, eyes-closed rest, Cam-CAN data), and show that the learnt model can generate data with realistic spatio-spectral properties, including transient events and population variability. Critically, it performs well in downstream decoding tasks, improving downstream supervised prediction task, showing improved zero-shot generalisation across sessions (improving accuracy from 0.54 to 0.59) and subjects (improving accuracy from 0.41 to 0.49) compared to a baseline methods. Furthermore, we show the model can be efficiently fine-tuned on a smaller labelled dataset to boost performance in cross-subject decoding scenarios. This work establishes a powerful foundation model for electrophysiological data, paving the way for applications in computational neuroscience and neural decoding.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-16</td>
<td style='padding: 8px;'>EARS-UDE: Evaluating Auditory Response in Sensory Overload with Universal Differential Equations</td>
<td style='padding: 6px;'>Miheer Salunke, Prathamesh Dinesh Joshi, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.26804v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Auditory sensory overload affects 50-70% of individuals with Autism Spectrum Disorder (ASD), yet existing approaches, such as mechanistic models (Hodgkin Huxley type, Wilson Cowan, excitation inhibition balance), clinical tools (EEG/MEG, Sensory Profile scales), and ML methods (Neural ODEs, predictive coding), either assume fixed parameters or lack interpretability, missing autism heterogeneity. We present a Scientific Machine Learning approach using Universal Differential Equations (UDEs) to model sensory adaptation dynamics in autism. Our framework combines ordinary differential equations grounded in biophysics with neural networks to capture both mechanistic understanding and individual variability. We demonstrate that UDEs achieve a 90.8% improvement over pure Neural ODEs while using 73.5% fewer parameters. The model successfully recovers physiological parameters within the 2% error and provides a quantitative risk assessment for sensory overload, predicting 17.2% risk for pulse stimuli with specific temporal patterns. This framework establishes foundations for personalized, evidence-based interventions in autism, with direct applications to wearable technology and clinical practice.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-13</td>
<td style='padding: 8px;'>Decoding non-invasive brain activity with novel deep-learning approaches</td>
<td style='padding: 6px;'>Richard Csaky</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.24733v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>This thesis delves into the world of non-invasive electrophysiological brain signals like electroencephalography (EEG) and magnetoencephalography (MEG), focusing on modelling and decoding such data. The research aims to investigate what happens in the brain when we perceive visual stimuli or engage in covert speech (inner speech) and enhance the decoding performance of such stimuli. The thesis is divided into two main sections, methodological and experimental work. A central concern in both sections is the large variability present in electrophysiological recordings, whether it be within-subject or between-subject variability, and to a certain extent between-dataset variability. In the methodological sections, we explore the potential of deep learning for brain decoding. We present advancements in decoding visual stimuli using linear models at the individual subject level. We then explore how deep learning techniques can be employed for group decoding, introducing new methods to deal with between-subject variability. Finally, we also explores novel forecasting models of MEG data based on convolutional and Transformer-based architectures. In particular, Transformer-based models demonstrate superior capabilities in generating signals that closely match real brain data, thereby enhancing the accuracy and reliability of modelling the brain's electrophysiology. In the experimental section, we present a unique dataset containing high-trial inner speech EEG, MEG, and preliminary optically pumped magnetometer (OPM) data. Our aim is to investigate different types of inner speech and push decoding performance by collecting a high number of trials and sessions from a few participants. However, the decoding results are found to be mostly negative, underscoring the difficulty of decoding inner speech.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>neuroAI</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-25</td>
<td style='padding: 8px;'>Dopamine-driven synaptic credit assignment in neural networks</td>
<td style='padding: 6px;'>Saranraj Nambusubramaniyan, Shervin Safavi, Raja Guru, Andreas Knoblauch</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2510.22178v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Solving the synaptic Credit Assignment Problem(CAP) is central to learning in both biological and artificial neural systems. Finding an optimal solution for synaptic CAP means setting the synaptic weights that assign credit to each neuron for influencing the final output and behavior of neural networks or animals. Gradient-based methods solve this problem in artificial neural networks using back-propagation, however, not in the most efficient way. For instance, back-propagation requires a chain of top-down gradient computations. This leads to an expensive optimization process in terms of computing power and memory linked with well-known weight transport and update locking problems. To address these shortcomings, we take a NeuroAI approach and draw inspiration from neural Reinforcement Learning to develop a derivative-free optimizer for training neural networks, Dopamine. Dopamine is developed for Weight Perturbation (WP) learning that exploits stochastic updating of weights towards optima. It achieves this by minimizing the regret, a form of Reward Prediction Error (RPE) between the expected outcome from the perturbed model and the actual outcome from the unperturbed model. We use this RPE to adjust the learning rate in the network (i.e., creating an adaptive learning rate strategy, similar to the role of dopamine in the brain). We tested the Dopamine optimizer for training multi-layered perceptrons for XOR tasks, and recurrent neural networks for chaotic time series forecasting. Dopamine-trained models demonstrate accelerated convergence and outperform standard WP, and give comparable performance to gradient-based algorithms, while consuming significantly less computation and memory. Overall, the Dopamine optimizer not only finds robust solutions and comparable performance to the state-of-the-art Machine Learning optimizers but is also neurobiologically more plausible.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-09</td>
<td style='padding: 8px;'>A Computational Perspective on NeuroAI and Synthetic Biological Intelligence</td>
<td style='padding: 6px;'>Dhruvik Patel, Md Sayed Tanveer, Jesus Gonzalez-Ferrer, Alon Loeffler, Brett J. Kagan, Mohammed A. Mostajo-Radji, Ge Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2509.23896v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>NeuroAI is an emerging field at the intersection of neuroscience and artificial intelligence, where insights from brain function guide the design of intelligent systems. A central area within this field is synthetic biological intelligence (SBI), which combines the adaptive learning properties of biological neural networks with engineered hardware and software. SBI systems provide a platform for modeling neural computation, developing biohybrid architectures, and enabling new forms of embodied intelligence. In this review, we organize the NeuroAI landscape into three interacting domains: hardware, software, and wetware. We outline computational frameworks that integrate biological and non-biological systems and highlight recent advances in organoid intelligence, neuromorphic computing, and neuro-symbolic learning. These developments collectively point toward a new class of systems that compute through interactions between living neural tissue and digital algorithms.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-09</td>
<td style='padding: 8px;'>Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers</td>
<td style='padding: 6px;'>Thomas Klein, Sascha Meyen, Wieland Brendel, Felix A. Wichmann, Kristof Meding</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.06645v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Benchmarking models is a key factor for the rapid progress in machine learning (ML) research. Thus, further progress depends on improving benchmarking metrics. A standard metric to measure the behavioral alignment between ML models and human observers is error consistency (EC). EC allows for more fine-grained comparisons of behavior than other metrics such as e.g. accuracy, and has been used in the influential Brain-Score benchmark to rank different DNNs by their behavioral consistency with humans. Previously, EC values have been reported without confidence intervals. However, empirically measured EC values are typically noisy -- thus, without confidence intervals, valid benchmarking conclusions are problematic. Here we improve on standard EC in two ways: First, we show how to obtain confidence intervals for EC using a bootstrapping technique, allowing us to derive significance tests for EC. Second, we propose a new computational model relating the EC between two classifiers to the implicit probability that one of them copies responses from the other. This view of EC allows us to give practical guidance to scientists regarding the number of trials required for sufficiently powerful, conclusive experiments. Finally, we use our methodology to revisit popular NeuroAI-results. We find that while the general trend of behavioral differences between humans and machines holds up to scrutiny, many reported differences between deep vision models are statistically insignificant. Our methodology enables researchers to design adequately powered experiments that can reliably detect behavioral differences between models, providing a foundation for more rigorous benchmarking of behavioral alignment.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-07-02</td>
<td style='padding: 8px;'>What Neuroscience Can Teach AI About Learning in Continuously Changing Environments</td>
<td style='padding: 6px;'>Daniel Durstewitz, Bruno Averbeck, Georgia Koppe</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2507.02103v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-10-27</td>
<td style='padding: 8px;'>NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models</td>
<td style='padding: 6px;'>Luca Ghafourpour, Valentin Duruisseaux, Bahareh Tolooshams, Philip H. Wong, Costas A. Anastassiou, Anima Anandkumar</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2506.04536v3' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Characterizing the cellular properties of neurons is fundamental to understanding their function in the brain. In this quest, the generation of bio-realistic models is central towards integrating multimodal cellular data sets and establishing causal relationships. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. The deterministic formalism of bio-realistic models currently precludes accounting for the natural variability observed experimentally. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on synthetic data generated from bio-realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE enables the efficient generation of synthetic neurons that closely resemble experimental data and exhibit trial-to-trial variability, offering a $4200\times$ speedup over the numerical solver. NOBLE is the first scaled-up deep learning framework that validates its generalization with real experimental data. To this end, NOBLE captures fundamental neural properties in a unique and emergent manner that opens the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-05-21</td>
<td style='padding: 8px;'>SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation</td>
<td style='padding: 6px;'>Jiayue Liu, Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Kuo Yang, Xu Wang, Yang Wang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2505.16080v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-02-22</td>
<td style='padding: 8px;'>Brain-Model Evaluations Need the NeuroAI Turing Test</td>
<td style='padding: 6px;'>Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2502.16238v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-01-04</td>
<td style='padding: 8px;'>Asynchronous Hebbian/anti-Hebbian networks</td>
<td style='padding: 6px;'>Henrique Reis Aguiar, Matthias H. Hennig</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2501.02402v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Lateral inhibition models coupled with Hebbian plasticity have been shown to learn factorised causal representations of input stimuli, for instance, oriented edges are learned from natural images. Currently, these models require the recurrent dynamics to settle into a stable state before weight changes can be applied, which is not only biologically implausible, but also impractical for real-time learning systems. Here, we propose a new Hebbian learning rule which is implemented using plausible biological mechanisms that have been observed experimentally. We find that this rule allows for efficient, time-continuous learning of factorised representations, very similar to the classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that this rule naturally prevents catastrophic forgetting when stimuli from different distributions are shown sequentially.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-04-03</td>
<td style='padding: 8px;'>NeuroAI for AI Safety</td>
<td style='padding: 6px;'>Patrick Mineault, Niccolò Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.18526v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-09-14</td>
<td style='padding: 8px;'>Evaluating Representational Similarity Measures from the Lens of Functional Correspondence</td>
<td style='padding: 6px;'>Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2411.14633v2' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research.</td>
</tr>
</tbody>
</table>

<h3 style='font-family: Helvetica, Arial, sans-serif; font-size: 14px; color: #2a7ae2;'>medical</h3>

<table style='width: 100%; border-collapse: collapse; font-family: Helvetica, Arial, sans-serif; font-size: 12px;'>
<thead style='background-color: #f4f4f4;'>
<tr style='border-bottom: 2px solid #d4d4d4;'>
<th style='padding: 8px; text-align: left;'>Publish Date</th>
<th style='padding: 8px; text-align: left;'>Title</th>
<th style='padding: 8px; text-align: left;'>Authors</th>
<th style='padding: 8px; text-align: left;'>URL</th>
<th style='padding: 8px; text-align: left;'>Abstract</th>
</tr>
</thead>
<tbody>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Production of High-Specific-Activity Radioisotopes Using High-Energy Fusion Neutrons</td>
<td style='padding: 6px;'>J. F. Parisi, A. Rutkowski, J. Harter, J. A. Schwartz, S. Chen</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02814v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We show that transmutation driven by high-energy neutrons from deuterium-tritium (D-T) fusion can produce many medically important radioisotopes-including $^{99}$Mo/${}^{99\mathrm{m}}\mathrm{Tc}$, $^{131}$I, $^{177}$Lu, $^{153}$Sm, $^{111}$In, $^{133}$Xe, $^{32}$P, $^{64}$Cu, $^{60}$Co, $^{103}$Pd, $^{89}$Sr, $^{188}$Re, $^{117}$In/$^{117\mathrm{m}1}$Sn, $^{90}$Y, and $^{166}$Ho - and emerging isotopes such as $^{161}$Tb, $^{195\mathrm{m}1}$Ir/$^{195\mathrm{m}}$Pt, $^{47}$Sc, ${}^{103}\mathrm{Ru}/{}^{103\mathrm{m}}\mathrm{Rh}$, ${}^{103}\mathrm{Pd}/{}^{103\mathrm{m}}\mathrm{Rh}$, and $^{119}$Sb with high specific activity and in large quantities. These reactions involve stable, abundant feedstocks and non-fission transmutation channels that change the proton number, enabling chemical rather than isotopic separation. Fusion-based transmutation could provide a flexible and proliferation-resistant platform for supply of high-purity isotopes. A D-T neutron source operating at a few megawatts of fusion power could meet or exceed global demand for most major radioisotopes. Further research is required to develop tailored approaches for feedstock processing and product extraction.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>DANIEL: A Distributed and Scalable Approach for Global Representation Learning with EHR Applications</td>
<td style='padding: 6px;'>Zebin Wang, Ziming Gan, Weijing Tang, Zongqi Xia, Tianrun Cai, Tianxi Cai, Junwei Lu</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02754v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Classical probabilistic graphical models face fundamental challenges in modern data environments, which are characterized by high dimensionality, source heterogeneity, and stringent data-sharing constraints. In this work, we revisit the Ising model, a well-established member of the Markov Random Field (MRF) family, and develop a distributed framework that enables scalable and privacy-preserving representation learning from large-scale binary data with inherent low-rank structure. Our approach optimizes a non-convex surrogate loss function via bi-factored gradient descent, offering substantial computational and communication advantages over conventional convex approaches. We evaluate our algorithm on multi-institutional electronic health record (EHR) datasets from 58,248 patients across the University of Pittsburgh Medical Center (UPMC) and Mass General Brigham (MGB), demonstrating superior performance in global representation learning and downstream clinical tasks, including relationship detection, patient phenotyping, and patient clustering. These results highlight a broader potential for statistical inference in federated, high-dimensional settings while addressing the practical challenges of data complexity and multi-institutional integration.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>The Analysis of Lexical Errors in Machine Translation from English into Romanian</td>
<td style='padding: 6px;'>Angela Stamatie</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02587v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>The research explores error analysis in the performance of translating by Machine Translation from English into Romanian, and it focuses on lexical errors found in texts which include official information, provided by the World Health Organization (WHO), the Gavi Organization, by the patient information leaflet (the information about the active ingredients of the vaccines or the medication, the indications, the dosage instructions, the storage instructions, the side effects and warning, etc.). All of these texts are related to Covid-19 and have been translated by Google Translate, a multilingual Machine Translation that was created by Google. In the last decades, Google has actively worked to develop a more accurate and fluent automatic translation system. This research, specifically focused on improving Google Translate, aims to enhance the overall quality of Machine Translation by achieving better lexical selection and by reducing errors. The investigation involves a comprehensive analysis of 230 texts that have been translated from English into Romanian.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Resource-efficient Automatic Refinement of Segmentations via Weak Supervision from Light Feedback</td>
<td style='padding: 6px;'>Alix de Langlais, Benjamin Billot, Théo Aguilar Vidal, Marc-Olivier Gauci, Hervé Delingette</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02576v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Delineating anatomical regions is a key task in medical image analysis. Manual segmentation achieves high accuracy but is labor-intensive and prone to variability, thus prompting the development of automated approaches. Recently, a breadth of foundation models has enabled automated segmentations across diverse anatomies and imaging modalities, but these may not always meet the clinical accuracy standards. While segmentation refinement strategies can improve performance, current methods depend on heavy user interactions or require fully supervised segmentations for training. Here, we present SCORE (Segmentation COrrection from Regional Evaluations), a weakly supervised framework that learns to refine mask predictions only using light feedback during training. Specifically, instead of relying on dense training image annotations, SCORE introduces a novel loss that leverages region-wise quality scores and over/under-segmentation error labels. We demonstrate SCORE on humerus CT scans, where it considerably improves initial predictions from TotalSegmentator, and achieves performance on par with existing refinement methods, while greatly reducing their supervision requirements and annotation time. Our code is available at: https://gitlab.inria.fr/adelangl/SCORE.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Identification of Separable OTUs for Multinomial Classification in Compositional Data Analysis</td>
<td style='padding: 6px;'>R. Alberich, N. A. Cruz, R. Fernández, I. García Mosquera, A. Mir, F. Roselló</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02509v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>High-throughput sequencing has transformed microbiome research, but it also produces inherently compositional data that challenge standard statistical and machine learning methods. In this work, we propose a multinomial classification framework for compositional microbiome data based on penalized log-ratio regression and pairwise separability screening. The method quantifies the discriminative ability of each OTU through the area under the receiver operating characteristic curve ($AUC$) for all pairwise log-ratios and aggregates these values into a global separability index $S_k$, yielding interpretable rankings of taxa together with confidence intervals. We illustrate the approach by reanalyzing the Baxter colorectal adenoma dataset and comparing our results with Greenacre's ordination-based analysis using Correspondence Analysis and Canonical Correspondence Analysis. Our models consistently recover a core subset of taxa previously identified as discriminant, thereby corroborating Greenacre's main findings, while also revealing additional OTUs that become important once demographic covariates are taken into account. In particular, adjustment for age, gender, and diabetes medication improves the precision of the separation index and highlights new, potentially relevant taxa, suggesting that part of the original signal may have been influenced by confounding. Overall, the integration of log-ratio modeling, covariate adjustment, and uncertainty estimation provides a robust and interpretable framework for OTU selection in compositional microbiome data. The proposed method complements existing ordination-based approaches by adding a probabilistic and inferential perspective, strengthening the identification of biologically meaningful microbial signatures.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Accounting for Underspecification in Statistical Claims of Model Superiority</td>
<td style='padding: 6px;'>Thomas Sanchez, Pedro M. Gordaliza, Meritxell Bach Cuadra</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02453v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Machine learning methods are increasingly applied in medical imaging, yet many reported improvements lack statistical robustness: recent works have highlighted that small but significant performance gains are highly likely to be false positives. However, these analyses do not take \emph{underspecification} into account -- the fact that models achieving similar validation scores may behave differently on unseen data due to random initialization or training dynamics. Here, we extend a recent statistical framework modeling false outperformance claims to include underspecification as an additional variance component. Our simulations demonstrate that even modest seed variability ($\sim1\%$) substantially increases the evidence required to support superiority claims. Our findings underscore the need for explicit modeling of training variance when validating medical imaging systems.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda</td>
<td style='padding: 6px;'>Mohd Nauman, Sravan Gvm, Vijay Devane, Shyam Pawar, Viraj Thakur, Kundeshwar Pundalik, Piyush Sawarkar, Rohit Saluja, Maunendra Desarkar, Ganesh Ramakrishnan</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02374v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Current large language models excel at broad, general-purpose tasks, but consistently underperform when exposed to highly specialized domains that require deep cultural, linguistic, and subject-matter expertise. In particular, traditional medical systems such as Ayurveda embody centuries of nuanced textual and clinical knowledge that mainstream LLMs fail to accurately interpret or apply. We introduce AyurParam-2.9B, a domain-specialized, bilingual language model fine-tuned from Param-1-2.9B using an extensive, expertly curated Ayurveda dataset spanning classical texts and clinical guidance. AyurParam's dataset incorporates context-aware, reasoning, and objective-style Q&A in both English and Hindi, with rigorous annotation protocols for factual precision and instructional clarity. Benchmarked on BhashaBench-Ayur, AyurParam not only surpasses all open-source instruction-tuned models in its size class (1.5--3B parameters), but also demonstrates competitive or superior performance compared to much larger models. The results from AyurParam highlight the necessity for authentic domain adaptation and high-quality supervision in delivering reliable, culturally congruent AI for specialized medical knowledge.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Large-scale automatic carbon ion treatment planning for head and neck cancers via parallel multi-agent reinforcement learning</td>
<td style='padding: 6px;'>Jueye Zhang, Chao Yang, Youfang Lai, Kai-Wen Li, Wenting Yan, Yunzhou Xia, Haimei Zhang, Jingjing Zhou, Gen Yang, Chen Lin, Tian Li, Yibao Zhang</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02314v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Head-and-neck cancer (HNC) planning is difficult because multiple critical organs-at-risk (OARs) are close to complex targets. Intensity-modulated carbon-ion therapy (IMCT) offers superior dose conformity and OAR sparing but remains slow due to relative biological effectiveness (RBE) modeling, leading to laborious, experience-based, and often suboptimal tuning of many treatment-planning parameters (TPPs). Recent deep learning (DL) methods are limited by data bias and plan feasibility, while reinforcement learning (RL) struggles to efficiently explore the exponentially large TPP search space. We propose a scalable multi-agent RL (MARL) framework for parallel tuning of 45 TPPs in IMCT. It uses a centralized-training decentralized-execution (CTDE) QMIX backbone with Double DQN, Dueling DQN, and recurrent encoding (DRQN) for stable learning in a high-dimensional, non-stationary environment. To enhance efficiency, we (1) use compact historical DVH vectors as state inputs, (2) apply a linear action-to-value transform mapping small discrete actions to uniform parameter adjustments, and (3) design an absolute, clinically informed piecewise reward aligned with plan scores. A synchronous multi-process worker system interfaces with the PHOENIX TPS for parallel optimization and accelerated data collection. On a head-and-neck dataset (10 training, 10 testing), the method tuned 45 parameters simultaneously and produced plans comparable to or better than expert manual ones (relative plan score: RL $85.93\pm7.85%$ vs Manual $85.02\pm6.92%$), with significant (p-value $<$ 0.05) improvements for five OARs. The framework efficiently explores high-dimensional TPP spaces and generates clinically competitive IMCT plans through direct TPS interaction, notably improving OAR sparing.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>Medical Report Generation: A Hierarchical Task Structure-Based Cross-Modal Causal Intervention Framework</td>
<td style='padding: 6px;'>Yucheng Song, Yifan Ge, Junhao Li, Zhining Liao, Zhifang Liao</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02271v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>Medical Report Generation (MRG) is a key part of modern medical diagnostics, as it automatically generates reports from radiological images to reduce radiologists' burden. However, reliable MRG models for lesion description face three main challenges: insufficient domain knowledge understanding, poor text-visual entity embedding alignment, and spurious correlations from cross-modal biases. Previous work only addresses single challenges, while this paper tackles all three via a novel hierarchical task decomposition approach, proposing the HTSC-CIF framework. HTSC-CIF classifies the three challenges into low-, mid-, and high-level tasks: 1) Low-level: align medical entity features with spatial locations to enhance domain knowledge for visual encoders; 2) Mid-level: use Prefix Language Modeling (text) and Masked Image Modeling (images) to boost cross-modal alignment via mutual guidance; 3) High-level: a cross-modal causal intervention module (via front-door intervention) to reduce confounders and improve interpretability. Extensive experiments confirm HTSC-CIF's effectiveness, significantly outperforming state-of-the-art (SOTA) MRG methods. Code will be made public upon paper acceptance.</td>
</tr>
<tr style='border-bottom: 1px solid #d4d4d4;'>
<td style='padding: 8px;'>2025-11-04</td>
<td style='padding: 8px;'>CT density estimation and beam hardening correction for single material objects using reference scans - a simulation and experimental study</td>
<td style='padding: 6px;'>Nikhil Deshmukh</td>
<td style='padding: 8px;'><a href='http://arxiv.org/abs/2511.02259v1' style='color: #1a73e8;'>Link</a></td>
<td style='padding: 4px;'>We describe the results from a simulation and experimental study of density estimation and beam hardening correction for Bremsstrahlung source computed tomography scan of objects made of a single material. The correction is done using transmission scans of a number of sheets of known thickness and density and made of the same material as the object under test.</td>
</tr>
</tbody>
</table>

